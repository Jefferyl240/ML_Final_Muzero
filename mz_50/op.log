A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2025-05-07 09:21:04] [command] train weight_iter_0.pkl 1 1
[2025-05-07 09:21:18] nn step 50, lr: 0.1.
	loss_policy_0: 0.2077
	accuracy_policy_0: 0.40328
	loss_value_0: 0.25861
	loss_policy_1: 0.04289
	accuracy_policy_1: 0.21238
	loss_value_1: 0.05224
	loss_reward_1: 0.03458
	loss_policy_2: 0.04269
	accuracy_policy_2: 0.29078
	loss_value_2: 0.05233
	loss_reward_2: 0.03389
	loss_policy_3: 0.04277
	accuracy_policy_3: 0.35113
	loss_value_3: 0.05258
	loss_reward_3: 0.03239
	loss_policy_4: 0.04286
	accuracy_policy_4: 0.33512
	loss_value_4: 0.05176
	loss_reward_4: 0.03362
	loss_policy_5: 0.04291
	accuracy_policy_5: 0.34355
	loss_value_5: 0.05071
	loss_reward_5: 0.03442
	loss_policy: 0.42182
	loss_value: 0.51823
	loss_reward: 0.1689
[2025-05-07 09:21:25] nn step 100, lr: 0.1.
	loss_policy_0: 0.19375
	accuracy_policy_0: 0.4957
	loss_value_0: 0.17303
	loss_policy_1: 0.03897
	accuracy_policy_1: 0.41129
	loss_value_1: 0.03394
	loss_reward_1: 0.01446
	loss_policy_2: 0.03915
	accuracy_policy_2: 0.44801
	loss_value_2: 0.03343
	loss_reward_2: 0.01305
	loss_policy_3: 0.03944
	accuracy_policy_3: 0.47707
	loss_value_3: 0.03345
	loss_reward_3: 0.01107
	loss_policy_4: 0.03961
	accuracy_policy_4: 0.42719
	loss_value_4: 0.0326
	loss_reward_4: 0.01292
	loss_policy_5: 0.0398
	accuracy_policy_5: 0.42699
	loss_value_5: 0.03078
	loss_reward_5: 0.01436
	loss_policy: 0.39073
	loss_value: 0.33723
	loss_reward: 0.06586
[2025-05-07 09:21:33] nn step 150, lr: 0.1.
	loss_policy_0: 0.1908
	accuracy_policy_0: 0.47277
	loss_value_0: 0.16724
	loss_policy_1: 0.03845
	accuracy_policy_1: 0.40965
	loss_value_1: 0.03217
	loss_reward_1: 0.01319
	loss_policy_2: 0.03868
	accuracy_policy_2: 0.41516
	loss_value_2: 0.03152
	loss_reward_2: 0.01154
	loss_policy_3: 0.03887
	accuracy_policy_3: 0.45957
	loss_value_3: 0.03183
	loss_reward_3: 0.00909
	loss_policy_4: 0.03907
	accuracy_policy_4: 0.43848
	loss_value_4: 0.03131
	loss_reward_4: 0.01125
	loss_policy_5: 0.03928
	accuracy_policy_5: 0.43672
	loss_value_5: 0.02994
	loss_reward_5: 0.01347
	loss_policy: 0.38515
	loss_value: 0.32402
	loss_reward: 0.05854
[2025-05-07 09:21:38] nn step 200, lr: 0.1.
	loss_policy_0: 0.18801
	accuracy_policy_0: 0.48691
	loss_value_0: 0.15701
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.46125
	loss_value_1: 0.03145
	loss_reward_1: 0.01149
	loss_policy_2: 0.038
	accuracy_policy_2: 0.43543
	loss_value_2: 0.03133
	loss_reward_2: 0.01018
	loss_policy_3: 0.03812
	accuracy_policy_3: 0.47055
	loss_value_3: 0.03165
	loss_reward_3: 0.00823
	loss_policy_4: 0.03823
	accuracy_policy_4: 0.47449
	loss_value_4: 0.03149
	loss_reward_4: 0.01013
	loss_policy_5: 0.03837
	accuracy_policy_5: 0.4709
	loss_value_5: 0.03074
	loss_reward_5: 0.01188
	loss_policy: 0.37856
	loss_value: 0.31367
	loss_reward: 0.05191
Optimization_Done 200
[2025-05-07 09:24:30] [command] train weight_iter_200.pkl 1 2
[2025-05-07 09:24:39] nn step 250, lr: 0.1.
	loss_policy_0: 0.21286
	accuracy_policy_0: 0.46562
	loss_value_0: 0.19486
	loss_policy_1: 0.0424
	accuracy_policy_1: 0.45738
	loss_value_1: 0.03958
	loss_reward_1: 0.01298
	loss_policy_2: 0.04246
	accuracy_policy_2: 0.46383
	loss_value_2: 0.03935
	loss_reward_2: 0.01233
	loss_policy_3: 0.04242
	accuracy_policy_3: 0.46289
	loss_value_3: 0.0394
	loss_reward_3: 0.01095
	loss_policy_4: 0.04255
	accuracy_policy_4: 0.45883
	loss_value_4: 0.03907
	loss_reward_4: 0.01218
	loss_policy_5: 0.04266
	accuracy_policy_5: 0.45496
	loss_value_5: 0.03809
	loss_reward_5: 0.01356
	loss_policy: 0.42536
	loss_value: 0.39036
	loss_reward: 0.062
[2025-05-07 09:24:46] nn step 300, lr: 0.1.
	loss_policy_0: 0.21376
	accuracy_policy_0: 0.46609
	loss_value_0: 0.1963
	loss_policy_1: 0.04273
	accuracy_policy_1: 0.47281
	loss_value_1: 0.04046
	loss_reward_1: 0.01252
	loss_policy_2: 0.04265
	accuracy_policy_2: 0.47273
	loss_value_2: 0.04034
	loss_reward_2: 0.01199
	loss_policy_3: 0.04256
	accuracy_policy_3: 0.47879
	loss_value_3: 0.04069
	loss_reward_3: 0.01097
	loss_policy_4: 0.04268
	accuracy_policy_4: 0.48074
	loss_value_4: 0.0404
	loss_reward_4: 0.01196
	loss_policy_5: 0.04281
	accuracy_policy_5: 0.48078
	loss_value_5: 0.03975
	loss_reward_5: 0.01302
	loss_policy: 0.42718
	loss_value: 0.39794
	loss_reward: 0.06045
[2025-05-07 09:24:53] nn step 350, lr: 0.1.
	loss_policy_0: 0.20849
	accuracy_policy_0: 0.45895
	loss_value_0: 0.19149
	loss_policy_1: 0.04164
	accuracy_policy_1: 0.48035
	loss_value_1: 0.0394
	loss_reward_1: 0.01194
	loss_policy_2: 0.04156
	accuracy_policy_2: 0.485
	loss_value_2: 0.03917
	loss_reward_2: 0.01133
	loss_policy_3: 0.04153
	accuracy_policy_3: 0.50453
	loss_value_3: 0.03971
	loss_reward_3: 0.01013
	loss_policy_4: 0.04157
	accuracy_policy_4: 0.51258
	loss_value_4: 0.03957
	loss_reward_4: 0.01121
	loss_policy_5: 0.04177
	accuracy_policy_5: 0.51863
	loss_value_5: 0.03928
	loss_reward_5: 0.0121
	loss_policy: 0.41654
	loss_value: 0.38861
	loss_reward: 0.0567
[2025-05-07 09:25:01] nn step 400, lr: 0.1.
	loss_policy_0: 0.22798
	accuracy_policy_0: 0.45613
	loss_value_0: 0.21239
	loss_policy_1: 0.04538
	accuracy_policy_1: 0.47547
	loss_value_1: 0.04338
	loss_reward_1: 0.01293
	loss_policy_2: 0.04528
	accuracy_policy_2: 0.49121
	loss_value_2: 0.04326
	loss_reward_2: 0.01207
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.50285
	loss_value_3: 0.0434
	loss_reward_3: 0.01132
	loss_policy_4: 0.04535
	accuracy_policy_4: 0.51723
	loss_value_4: 0.04365
	loss_reward_4: 0.01219
	loss_policy_5: 0.04556
	accuracy_policy_5: 0.52316
	loss_value_5: 0.04391
	loss_reward_5: 0.01309
	loss_policy: 0.45475
	loss_value: 0.42997
	loss_reward: 0.06159
Optimization_Done 400
[2025-05-07 09:27:48] [command] train weight_iter_400.pkl 1 3
[2025-05-07 09:27:55] nn step 450, lr: 0.1.
	loss_policy_0: 0.28749
	accuracy_policy_0: 0.33203
	loss_value_0: 0.27322
	loss_policy_1: 0.05649
	accuracy_policy_1: 0.42637
	loss_value_1: 0.05632
	loss_reward_1: 0.01487
	loss_policy_2: 0.05633
	accuracy_policy_2: 0.43004
	loss_value_2: 0.05717
	loss_reward_2: 0.01399
	loss_policy_3: 0.05622
	accuracy_policy_3: 0.43211
	loss_value_3: 0.05783
	loss_reward_3: 0.01322
	loss_policy_4: 0.05636
	accuracy_policy_4: 0.4393
	loss_value_4: 0.05827
	loss_reward_4: 0.01406
	loss_policy_5: 0.05665
	accuracy_policy_5: 0.4318
	loss_value_5: 0.05836
	loss_reward_5: 0.01501
	loss_policy: 0.56954
	loss_value: 0.56117
	loss_reward: 0.07115
[2025-05-07 09:28:03] nn step 500, lr: 0.1.
	loss_policy_0: 0.27048
	accuracy_policy_0: 0.4052
	loss_value_0: 0.24181
	loss_policy_1: 0.05332
	accuracy_policy_1: 0.46312
	loss_value_1: 0.04988
	loss_reward_1: 0.01364
	loss_policy_2: 0.05322
	accuracy_policy_2: 0.47797
	loss_value_2: 0.05027
	loss_reward_2: 0.01325
	loss_policy_3: 0.05312
	accuracy_policy_3: 0.48379
	loss_value_3: 0.05111
	loss_reward_3: 0.01214
	loss_policy_4: 0.05312
	accuracy_policy_4: 0.48945
	loss_value_4: 0.05144
	loss_reward_4: 0.01356
	loss_policy_5: 0.05332
	accuracy_policy_5: 0.47875
	loss_value_5: 0.05183
	loss_reward_5: 0.01448
	loss_policy: 0.53658
	loss_value: 0.49634
	loss_reward: 0.06707
[2025-05-07 09:28:10] nn step 550, lr: 0.1.
	loss_policy_0: 0.23983
	accuracy_policy_0: 0.43246
	loss_value_0: 0.21222
	loss_policy_1: 0.04747
	accuracy_policy_1: 0.47664
	loss_value_1: 0.04351
	loss_reward_1: 0.01209
	loss_policy_2: 0.04734
	accuracy_policy_2: 0.48984
	loss_value_2: 0.04403
	loss_reward_2: 0.01174
	loss_policy_3: 0.04733
	accuracy_policy_3: 0.49695
	loss_value_3: 0.0449
	loss_reward_3: 0.01071
	loss_policy_4: 0.04738
	accuracy_policy_4: 0.49812
	loss_value_4: 0.0454
	loss_reward_4: 0.01159
	loss_policy_5: 0.0475
	accuracy_policy_5: 0.49289
	loss_value_5: 0.04571
	loss_reward_5: 0.01272
	loss_policy: 0.47686
	loss_value: 0.43576
	loss_reward: 0.05883
[2025-05-07 09:28:18] nn step 600, lr: 0.1.
	loss_policy_0: 0.26982
	accuracy_policy_0: 0.44875
	loss_value_0: 0.24165
	loss_policy_1: 0.05347
	accuracy_policy_1: 0.48348
	loss_value_1: 0.04962
	loss_reward_1: 0.01325
	loss_policy_2: 0.05337
	accuracy_policy_2: 0.5016
	loss_value_2: 0.05013
	loss_reward_2: 0.01305
	loss_policy_3: 0.05332
	accuracy_policy_3: 0.50734
	loss_value_3: 0.051
	loss_reward_3: 0.01186
	loss_policy_4: 0.05323
	accuracy_policy_4: 0.51238
	loss_value_4: 0.05139
	loss_reward_4: 0.01308
	loss_policy_5: 0.05353
	accuracy_policy_5: 0.50609
	loss_value_5: 0.05189
	loss_reward_5: 0.01391
	loss_policy: 0.53674
	loss_value: 0.49568
	loss_reward: 0.06515
Optimization_Done 600
[2025-05-07 09:31:21] [command] train weight_iter_600.pkl 1 4
[2025-05-07 09:31:30] nn step 650, lr: 0.1.
	loss_policy_0: 0.23507
	accuracy_policy_0: 0.53539
	loss_value_0: 0.26009
	loss_policy_1: 0.04622
	accuracy_policy_1: 0.58441
	loss_value_1: 0.05248
	loss_reward_1: 0.01286
	loss_policy_2: 0.04609
	accuracy_policy_2: 0.58883
	loss_value_2: 0.05262
	loss_reward_2: 0.01258
	loss_policy_3: 0.04587
	accuracy_policy_3: 0.59324
	loss_value_3: 0.05297
	loss_reward_3: 0.01146
	loss_policy_4: 0.04594
	accuracy_policy_4: 0.59242
	loss_value_4: 0.05323
	loss_reward_4: 0.01255
	loss_policy_5: 0.04631
	accuracy_policy_5: 0.58426
	loss_value_5: 0.05349
	loss_reward_5: 0.01383
	loss_policy: 0.4655
	loss_value: 0.52488
	loss_reward: 0.06328
[2025-05-07 09:31:37] nn step 700, lr: 0.1.
	loss_policy_0: 0.24732
	accuracy_policy_0: 0.55937
	loss_value_0: 0.26565
	loss_policy_1: 0.0489
	accuracy_policy_1: 0.58859
	loss_value_1: 0.054
	loss_reward_1: 0.01354
	loss_policy_2: 0.04849
	accuracy_policy_2: 0.59773
	loss_value_2: 0.05454
	loss_reward_2: 0.01309
	loss_policy_3: 0.04844
	accuracy_policy_3: 0.60516
	loss_value_3: 0.0551
	loss_reward_3: 0.01232
	loss_policy_4: 0.04853
	accuracy_policy_4: 0.60828
	loss_value_4: 0.05561
	loss_reward_4: 0.01347
	loss_policy_5: 0.04892
	accuracy_policy_5: 0.59609
	loss_value_5: 0.05612
	loss_reward_5: 0.01464
	loss_policy: 0.49059
	loss_value: 0.54103
	loss_reward: 0.06706
[2025-05-07 09:31:45] nn step 750, lr: 0.1.
	loss_policy_0: 0.25389
	accuracy_policy_0: 0.56586
	loss_value_0: 0.27394
	loss_policy_1: 0.04996
	accuracy_policy_1: 0.60238
	loss_value_1: 0.05576
	loss_reward_1: 0.01362
	loss_policy_2: 0.04992
	accuracy_policy_2: 0.60695
	loss_value_2: 0.05608
	loss_reward_2: 0.01345
	loss_policy_3: 0.04992
	accuracy_policy_3: 0.61469
	loss_value_3: 0.057
	loss_reward_3: 0.01225
	loss_policy_4: 0.04999
	accuracy_policy_4: 0.61129
	loss_value_4: 0.05767
	loss_reward_4: 0.01308
	loss_policy_5: 0.05005
	accuracy_policy_5: 0.60828
	loss_value_5: 0.05824
	loss_reward_5: 0.01426
	loss_policy: 0.50372
	loss_value: 0.55869
	loss_reward: 0.06667
[2025-05-07 09:31:52] nn step 800, lr: 0.1.
	loss_policy_0: 0.24205
	accuracy_policy_0: 0.56242
	loss_value_0: 0.25782
	loss_policy_1: 0.0478
	accuracy_policy_1: 0.59852
	loss_value_1: 0.05267
	loss_reward_1: 0.01262
	loss_policy_2: 0.04761
	accuracy_policy_2: 0.6034
	loss_value_2: 0.05349
	loss_reward_2: 0.0122
	loss_policy_3: 0.04755
	accuracy_policy_3: 0.60852
	loss_value_3: 0.05431
	loss_reward_3: 0.01121
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.6109
	loss_value_4: 0.05497
	loss_reward_4: 0.01224
	loss_policy_5: 0.04785
	accuracy_policy_5: 0.60879
	loss_value_5: 0.05562
	loss_reward_5: 0.0135
	loss_policy: 0.48044
	loss_value: 0.52888
	loss_reward: 0.06177
Optimization_Done 800
[2025-05-07 09:34:52] [command] train weight_iter_800.pkl 1 5
[2025-05-07 09:35:00] nn step 850, lr: 0.1.
	loss_policy_0: 0.22315
	accuracy_policy_0: 0.60738
	loss_value_0: 0.27388
	loss_policy_1: 0.04427
	accuracy_policy_1: 0.63434
	loss_value_1: 0.0557
	loss_reward_1: 0.01245
	loss_policy_2: 0.04443
	accuracy_policy_2: 0.63609
	loss_value_2: 0.05656
	loss_reward_2: 0.01231
	loss_policy_3: 0.04443
	accuracy_policy_3: 0.64016
	loss_value_3: 0.05735
	loss_reward_3: 0.01134
	loss_policy_4: 0.04451
	accuracy_policy_4: 0.63895
	loss_value_4: 0.05804
	loss_reward_4: 0.0123
	loss_policy_5: 0.04494
	accuracy_policy_5: 0.63621
	loss_value_5: 0.05885
	loss_reward_5: 0.01347
	loss_policy: 0.44572
	loss_value: 0.56038
	loss_reward: 0.06186
[2025-05-07 09:35:07] nn step 900, lr: 0.1.
	loss_policy_0: 0.24078
	accuracy_policy_0: 0.60781
	loss_value_0: 0.27835
	loss_policy_1: 0.0476
	accuracy_policy_1: 0.63684
	loss_value_1: 0.05745
	loss_reward_1: 0.01293
	loss_policy_2: 0.0476
	accuracy_policy_2: 0.63887
	loss_value_2: 0.05857
	loss_reward_2: 0.01279
	loss_policy_3: 0.04782
	accuracy_policy_3: 0.64027
	loss_value_3: 0.05957
	loss_reward_3: 0.01165
	loss_policy_4: 0.04759
	accuracy_policy_4: 0.6484
	loss_value_4: 0.06058
	loss_reward_4: 0.01298
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.63883
	loss_value_5: 0.06127
	loss_reward_5: 0.01401
	loss_policy: 0.47912
	loss_value: 0.57579
	loss_reward: 0.06436
[2025-05-07 09:35:14] nn step 950, lr: 0.1.
	loss_policy_0: 0.20842
	accuracy_policy_0: 0.60402
	loss_value_0: 0.23578
	loss_policy_1: 0.04132
	accuracy_policy_1: 0.63125
	loss_value_1: 0.04858
	loss_reward_1: 0.01115
	loss_policy_2: 0.04122
	accuracy_policy_2: 0.64078
	loss_value_2: 0.04951
	loss_reward_2: 0.01095
	loss_policy_3: 0.04123
	accuracy_policy_3: 0.64746
	loss_value_3: 0.05033
	loss_reward_3: 0.01039
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.64898
	loss_value_4: 0.05143
	loss_reward_4: 0.01124
	loss_policy_5: 0.04147
	accuracy_policy_5: 0.64516
	loss_value_5: 0.05214
	loss_reward_5: 0.01193
	loss_policy: 0.41501
	loss_value: 0.48778
	loss_reward: 0.05566
[2025-05-07 09:35:22] nn step 1000, lr: 0.1.
	loss_policy_0: 0.2444
	accuracy_policy_0: 0.6043
	loss_value_0: 0.27144
	loss_policy_1: 0.04844
	accuracy_policy_1: 0.63492
	loss_value_1: 0.05594
	loss_reward_1: 0.01256
	loss_policy_2: 0.04862
	accuracy_policy_2: 0.63551
	loss_value_2: 0.05706
	loss_reward_2: 0.01248
	loss_policy_3: 0.04851
	accuracy_policy_3: 0.64457
	loss_value_3: 0.05797
	loss_reward_3: 0.01155
	loss_policy_4: 0.04878
	accuracy_policy_4: 0.64555
	loss_value_4: 0.05894
	loss_reward_4: 0.01274
	loss_policy_5: 0.04884
	accuracy_policy_5: 0.64574
	loss_value_5: 0.06004
	loss_reward_5: 0.01366
	loss_policy: 0.48758
	loss_value: 0.56138
	loss_reward: 0.06298
Optimization_Done 1000
[2025-05-07 09:38:16] [command] train weight_iter_1000.pkl 1 6
[2025-05-07 09:38:24] nn step 1050, lr: 0.1.
	loss_policy_0: 0.22927
	accuracy_policy_0: 0.64969
	loss_value_0: 0.2941
	loss_policy_1: 0.04538
	accuracy_policy_1: 0.67859
	loss_value_1: 0.05996
	loss_reward_1: 0.01257
	loss_policy_2: 0.04518
	accuracy_policy_2: 0.68543
	loss_value_2: 0.06085
	loss_reward_2: 0.01263
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.68734
	loss_value_3: 0.06186
	loss_reward_3: 0.01189
	loss_policy_4: 0.04561
	accuracy_policy_4: 0.68375
	loss_value_4: 0.06281
	loss_reward_4: 0.01263
	loss_policy_5: 0.04559
	accuracy_policy_5: 0.68395
	loss_value_5: 0.06361
	loss_reward_5: 0.01366
	loss_policy: 0.45624
	loss_value: 0.6032
	loss_reward: 0.06338
[2025-05-07 09:38:31] nn step 1100, lr: 0.1.
	loss_policy_0: 0.23289
	accuracy_policy_0: 0.64492
	loss_value_0: 0.28569
	loss_policy_1: 0.04602
	accuracy_policy_1: 0.6766
	loss_value_1: 0.0588
	loss_reward_1: 0.01261
	loss_policy_2: 0.04599
	accuracy_policy_2: 0.68
	loss_value_2: 0.05953
	loss_reward_2: 0.01222
	loss_policy_3: 0.04593
	accuracy_policy_3: 0.68508
	loss_value_3: 0.06055
	loss_reward_3: 0.01205
	loss_policy_4: 0.04633
	accuracy_policy_4: 0.68344
	loss_value_4: 0.06193
	loss_reward_4: 0.01278
	loss_policy_5: 0.04627
	accuracy_policy_5: 0.68527
	loss_value_5: 0.06281
	loss_reward_5: 0.01375
	loss_policy: 0.46343
	loss_value: 0.58931
	loss_reward: 0.06341
[2025-05-07 09:38:38] nn step 1150, lr: 0.1.
	loss_policy_0: 0.21151
	accuracy_policy_0: 0.63957
	loss_value_0: 0.25651
	loss_policy_1: 0.04166
	accuracy_policy_1: 0.67012
	loss_value_1: 0.05224
	loss_reward_1: 0.01111
	loss_policy_2: 0.04151
	accuracy_policy_2: 0.68062
	loss_value_2: 0.05307
	loss_reward_2: 0.01072
	loss_policy_3: 0.04177
	accuracy_policy_3: 0.68082
	loss_value_3: 0.05383
	loss_reward_3: 0.01066
	loss_policy_4: 0.04169
	accuracy_policy_4: 0.68281
	loss_value_4: 0.05502
	loss_reward_4: 0.01125
	loss_policy_5: 0.04182
	accuracy_policy_5: 0.6775
	loss_value_5: 0.05592
	loss_reward_5: 0.01221
	loss_policy: 0.41998
	loss_value: 0.52659
	loss_reward: 0.05596
[2025-05-07 09:38:45] nn step 1200, lr: 0.1.
	loss_policy_0: 0.22362
	accuracy_policy_0: 0.64062
	loss_value_0: 0.26775
	loss_policy_1: 0.04441
	accuracy_policy_1: 0.66301
	loss_value_1: 0.05504
	loss_reward_1: 0.01146
	loss_policy_2: 0.04425
	accuracy_policy_2: 0.67301
	loss_value_2: 0.05624
	loss_reward_2: 0.01166
	loss_policy_3: 0.04434
	accuracy_policy_3: 0.67328
	loss_value_3: 0.05716
	loss_reward_3: 0.01087
	loss_policy_4: 0.04426
	accuracy_policy_4: 0.67711
	loss_value_4: 0.05824
	loss_reward_4: 0.01195
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.67766
	loss_value_5: 0.05935
	loss_reward_5: 0.01298
	loss_policy: 0.44518
	loss_value: 0.55378
	loss_reward: 0.05893
Optimization_Done 1200
[2025-05-07 09:41:35] [command] train weight_iter_1200.pkl 1 7
[2025-05-07 09:41:43] nn step 1250, lr: 0.1.
	loss_policy_0: 0.22826
	accuracy_policy_0: 0.63777
	loss_value_0: 0.27495
	loss_policy_1: 0.04496
	accuracy_policy_1: 0.66469
	loss_value_1: 0.0563
	loss_reward_1: 0.01216
	loss_policy_2: 0.04494
	accuracy_policy_2: 0.67297
	loss_value_2: 0.05731
	loss_reward_2: 0.01239
	loss_policy_3: 0.04512
	accuracy_policy_3: 0.67281
	loss_value_3: 0.0582
	loss_reward_3: 0.01211
	loss_policy_4: 0.04527
	accuracy_policy_4: 0.67035
	loss_value_4: 0.05945
	loss_reward_4: 0.01291
	loss_policy_5: 0.04559
	accuracy_policy_5: 0.67348
	loss_value_5: 0.06014
	loss_reward_5: 0.01358
	loss_policy: 0.45413
	loss_value: 0.56634
	loss_reward: 0.06316
[2025-05-07 09:41:50] nn step 1300, lr: 0.1.
	loss_policy_0: 0.23654
	accuracy_policy_0: 0.63785
	loss_value_0: 0.28368
	loss_policy_1: 0.04664
	accuracy_policy_1: 0.66805
	loss_value_1: 0.05822
	loss_reward_1: 0.01301
	loss_policy_2: 0.04641
	accuracy_policy_2: 0.66945
	loss_value_2: 0.05947
	loss_reward_2: 0.01299
	loss_policy_3: 0.04654
	accuracy_policy_3: 0.67434
	loss_value_3: 0.06079
	loss_reward_3: 0.01221
	loss_policy_4: 0.04664
	accuracy_policy_4: 0.6716
	loss_value_4: 0.06197
	loss_reward_4: 0.01331
	loss_policy_5: 0.04675
	accuracy_policy_5: 0.66863
	loss_value_5: 0.06283
	loss_reward_5: 0.01436
	loss_policy: 0.46952
	loss_value: 0.58697
	loss_reward: 0.06589
[2025-05-07 09:41:57] nn step 1350, lr: 0.1.
	loss_policy_0: 0.24107
	accuracy_policy_0: 0.63469
	loss_value_0: 0.29087
	loss_policy_1: 0.04767
	accuracy_policy_1: 0.66273
	loss_value_1: 0.05985
	loss_reward_1: 0.01311
	loss_policy_2: 0.04756
	accuracy_policy_2: 0.66902
	loss_value_2: 0.06109
	loss_reward_2: 0.01319
	loss_policy_3: 0.04798
	accuracy_policy_3: 0.66855
	loss_value_3: 0.06235
	loss_reward_3: 0.01293
	loss_policy_4: 0.04807
	accuracy_policy_4: 0.66938
	loss_value_4: 0.06383
	loss_reward_4: 0.01364
	loss_policy_5: 0.04817
	accuracy_policy_5: 0.67004
	loss_value_5: 0.0649
	loss_reward_5: 0.01455
	loss_policy: 0.48052
	loss_value: 0.60291
	loss_reward: 0.06742
[2025-05-07 09:42:04] nn step 1400, lr: 0.1.
	loss_policy_0: 0.23138
	accuracy_policy_0: 0.63152
	loss_value_0: 0.27422
	loss_policy_1: 0.04559
	accuracy_policy_1: 0.65879
	loss_value_1: 0.05625
	loss_reward_1: 0.01225
	loss_policy_2: 0.04553
	accuracy_policy_2: 0.66238
	loss_value_2: 0.05738
	loss_reward_2: 0.01237
	loss_policy_3: 0.04552
	accuracy_policy_3: 0.66445
	loss_value_3: 0.05853
	loss_reward_3: 0.01192
	loss_policy_4: 0.04574
	accuracy_policy_4: 0.6648
	loss_value_4: 0.0597
	loss_reward_4: 0.0126
	loss_policy_5: 0.04579
	accuracy_policy_5: 0.66496
	loss_value_5: 0.06094
	loss_reward_5: 0.01396
	loss_policy: 0.45956
	loss_value: 0.56701
	loss_reward: 0.0631
Optimization_Done 1400
[2025-05-07 09:45:02] [command] train weight_iter_1400.pkl 1 8
[2025-05-07 09:45:10] nn step 1450, lr: 0.1.
	loss_policy_0: 0.26054
	accuracy_policy_0: 0.61129
	loss_value_0: 0.31776
	loss_policy_1: 0.05109
	accuracy_policy_1: 0.63746
	loss_value_1: 0.06549
	loss_reward_1: 0.01455
	loss_policy_2: 0.0512
	accuracy_policy_2: 0.64207
	loss_value_2: 0.06671
	loss_reward_2: 0.01446
	loss_policy_3: 0.05118
	accuracy_policy_3: 0.63906
	loss_value_3: 0.06776
	loss_reward_3: 0.01378
	loss_policy_4: 0.05147
	accuracy_policy_4: 0.63637
	loss_value_4: 0.06884
	loss_reward_4: 0.01501
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.64152
	loss_value_5: 0.0697
	loss_reward_5: 0.01643
	loss_policy: 0.5173
	loss_value: 0.65626
	loss_reward: 0.07422
[2025-05-07 09:45:16] nn step 1500, lr: 0.1.
	loss_policy_0: 0.25413
	accuracy_policy_0: 0.61934
	loss_value_0: 0.30306
	loss_policy_1: 0.05026
	accuracy_policy_1: 0.6432
	loss_value_1: 0.06259
	loss_reward_1: 0.01379
	loss_policy_2: 0.0502
	accuracy_policy_2: 0.6452
	loss_value_2: 0.06404
	loss_reward_2: 0.01418
	loss_policy_3: 0.05033
	accuracy_policy_3: 0.64324
	loss_value_3: 0.06547
	loss_reward_3: 0.01331
	loss_policy_4: 0.0505
	accuracy_policy_4: 0.63891
	loss_value_4: 0.06689
	loss_reward_4: 0.01463
	loss_policy_5: 0.05071
	accuracy_policy_5: 0.64406
	loss_value_5: 0.06756
	loss_reward_5: 0.01552
	loss_policy: 0.50614
	loss_value: 0.62962
	loss_reward: 0.07143
[2025-05-07 09:45:24] nn step 1550, lr: 0.1.
	loss_policy_0: 0.23047
	accuracy_policy_0: 0.61746
	loss_value_0: 0.27448
	loss_policy_1: 0.04565
	accuracy_policy_1: 0.6473
	loss_value_1: 0.05655
	loss_reward_1: 0.01263
	loss_policy_2: 0.04545
	accuracy_policy_2: 0.64887
	loss_value_2: 0.05753
	loss_reward_2: 0.01298
	loss_policy_3: 0.04572
	accuracy_policy_3: 0.64926
	loss_value_3: 0.05869
	loss_reward_3: 0.01258
	loss_policy_4: 0.04588
	accuracy_policy_4: 0.65043
	loss_value_4: 0.05984
	loss_reward_4: 0.01321
	loss_policy_5: 0.04594
	accuracy_policy_5: 0.65156
	loss_value_5: 0.06085
	loss_reward_5: 0.01434
	loss_policy: 0.45912
	loss_value: 0.56794
	loss_reward: 0.06573
[2025-05-07 09:45:31] nn step 1600, lr: 0.1.
	loss_policy_0: 0.23999
	accuracy_policy_0: 0.62773
	loss_value_0: 0.28767
	loss_policy_1: 0.04757
	accuracy_policy_1: 0.64391
	loss_value_1: 0.05879
	loss_reward_1: 0.01247
	loss_policy_2: 0.04733
	accuracy_policy_2: 0.64801
	loss_value_2: 0.0601
	loss_reward_2: 0.01353
	loss_policy_3: 0.04775
	accuracy_policy_3: 0.64504
	loss_value_3: 0.06115
	loss_reward_3: 0.01284
	loss_policy_4: 0.04769
	accuracy_policy_4: 0.6473
	loss_value_4: 0.06238
	loss_reward_4: 0.0134
	loss_policy_5: 0.04781
	accuracy_policy_5: 0.65332
	loss_value_5: 0.06381
	loss_reward_5: 0.01496
	loss_policy: 0.47815
	loss_value: 0.5939
	loss_reward: 0.0672
Optimization_Done 1600
[2025-05-07 09:48:19] [command] train weight_iter_1600.pkl 1 9
[2025-05-07 09:48:28] nn step 1650, lr: 0.1.
	loss_policy_0: 0.24622
	accuracy_policy_0: 0.60004
	loss_value_0: 0.30818
	loss_policy_1: 0.04899
	accuracy_policy_1: 0.62613
	loss_value_1: 0.06289
	loss_reward_1: 0.01281
	loss_policy_2: 0.04898
	accuracy_policy_2: 0.63266
	loss_value_2: 0.06413
	loss_reward_2: 0.014
	loss_policy_3: 0.04892
	accuracy_policy_3: 0.62945
	loss_value_3: 0.0653
	loss_reward_3: 0.01364
	loss_policy_4: 0.04937
	accuracy_policy_4: 0.62324
	loss_value_4: 0.06635
	loss_reward_4: 0.01383
	loss_policy_5: 0.04923
	accuracy_policy_5: 0.6282
	loss_value_5: 0.06747
	loss_reward_5: 0.01514
	loss_policy: 0.49172
	loss_value: 0.63432
	loss_reward: 0.06941
[2025-05-07 09:48:35] nn step 1700, lr: 0.1.
	loss_policy_0: 0.25221
	accuracy_policy_0: 0.61242
	loss_value_0: 0.30212
	loss_policy_1: 0.05016
	accuracy_policy_1: 0.63234
	loss_value_1: 0.06201
	loss_reward_1: 0.01292
	loss_policy_2: 0.04996
	accuracy_policy_2: 0.63582
	loss_value_2: 0.06375
	loss_reward_2: 0.01431
	loss_policy_3: 0.05021
	accuracy_policy_3: 0.63715
	loss_value_3: 0.06488
	loss_reward_3: 0.01364
	loss_policy_4: 0.05055
	accuracy_policy_4: 0.63355
	loss_value_4: 0.06658
	loss_reward_4: 0.01416
	loss_policy_5: 0.0508
	accuracy_policy_5: 0.63254
	loss_value_5: 0.06783
	loss_reward_5: 0.0156
	loss_policy: 0.50389
	loss_value: 0.62717
	loss_reward: 0.07063
[2025-05-07 09:48:42] nn step 1750, lr: 0.1.
	loss_policy_0: 0.27454
	accuracy_policy_0: 0.60742
	loss_value_0: 0.32309
	loss_policy_1: 0.05423
	accuracy_policy_1: 0.62605
	loss_value_1: 0.06702
	loss_reward_1: 0.01406
	loss_policy_2: 0.05424
	accuracy_policy_2: 0.63375
	loss_value_2: 0.06849
	loss_reward_2: 0.01587
	loss_policy_3: 0.05429
	accuracy_policy_3: 0.63758
	loss_value_3: 0.06972
	loss_reward_3: 0.01502
	loss_policy_4: 0.05451
	accuracy_policy_4: 0.63152
	loss_value_4: 0.0714
	loss_reward_4: 0.01561
	loss_policy_5: 0.0544
	accuracy_policy_5: 0.64
	loss_value_5: 0.07263
	loss_reward_5: 0.01682
	loss_policy: 0.5462
	loss_value: 0.67235
	loss_reward: 0.07737
[2025-05-07 09:48:50] nn step 1800, lr: 0.1.
	loss_policy_0: 0.24919
	accuracy_policy_0: 0.62391
	loss_value_0: 0.29358
	loss_policy_1: 0.0494
	accuracy_policy_1: 0.64012
	loss_value_1: 0.06055
	loss_reward_1: 0.01249
	loss_policy_2: 0.04918
	accuracy_policy_2: 0.6516
	loss_value_2: 0.06204
	loss_reward_2: 0.01386
	loss_policy_3: 0.04948
	accuracy_policy_3: 0.64348
	loss_value_3: 0.06337
	loss_reward_3: 0.01333
	loss_policy_4: 0.04965
	accuracy_policy_4: 0.64152
	loss_value_4: 0.06473
	loss_reward_4: 0.01434
	loss_policy_5: 0.04993
	accuracy_policy_5: 0.63902
	loss_value_5: 0.06601
	loss_reward_5: 0.01492
	loss_policy: 0.49682
	loss_value: 0.61027
	loss_reward: 0.06893
Optimization_Done 1800
[2025-05-07 09:51:55] [command] train weight_iter_1800.pkl 1 10
[2025-05-07 09:52:03] nn step 1850, lr: 0.1.
	loss_policy_0: 0.25516
	accuracy_policy_0: 0.62891
	loss_value_0: 0.31904
	loss_policy_1: 0.05065
	accuracy_policy_1: 0.65227
	loss_value_1: 0.06586
	loss_reward_1: 0.01266
	loss_policy_2: 0.05074
	accuracy_policy_2: 0.65012
	loss_value_2: 0.06707
	loss_reward_2: 0.01522
	loss_policy_3: 0.05088
	accuracy_policy_3: 0.65094
	loss_value_3: 0.06824
	loss_reward_3: 0.01423
	loss_policy_4: 0.05107
	accuracy_policy_4: 0.65039
	loss_value_4: 0.06967
	loss_reward_4: 0.01489
	loss_policy_5: 0.0509
	accuracy_policy_5: 0.65324
	loss_value_5: 0.07085
	loss_reward_5: 0.01672
	loss_policy: 0.5094
	loss_value: 0.66074
	loss_reward: 0.07371
[2025-05-07 09:52:11] nn step 1900, lr: 0.1.
	loss_policy_0: 0.26291
	accuracy_policy_0: 0.64035
	loss_value_0: 0.31845
	loss_policy_1: 0.05232
	accuracy_policy_1: 0.65418
	loss_value_1: 0.06616
	loss_reward_1: 0.01315
	loss_policy_2: 0.05251
	accuracy_policy_2: 0.65527
	loss_value_2: 0.0678
	loss_reward_2: 0.01555
	loss_policy_3: 0.0525
	accuracy_policy_3: 0.65941
	loss_value_3: 0.06934
	loss_reward_3: 0.01472
	loss_policy_4: 0.05278
	accuracy_policy_4: 0.65098
	loss_value_4: 0.07096
	loss_reward_4: 0.01518
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.65562
	loss_value_5: 0.07217
	loss_reward_5: 0.01692
	loss_policy: 0.52582
	loss_value: 0.66488
	loss_reward: 0.07553
[2025-05-07 09:52:18] nn step 1950, lr: 0.1.
	loss_policy_0: 0.27251
	accuracy_policy_0: 0.63723
	loss_value_0: 0.32729
	loss_policy_1: 0.05392
	accuracy_policy_1: 0.64836
	loss_value_1: 0.06803
	loss_reward_1: 0.01321
	loss_policy_2: 0.05382
	accuracy_policy_2: 0.65617
	loss_value_2: 0.06948
	loss_reward_2: 0.01639
	loss_policy_3: 0.05424
	accuracy_policy_3: 0.66129
	loss_value_3: 0.07102
	loss_reward_3: 0.01473
	loss_policy_4: 0.05443
	accuracy_policy_4: 0.65781
	loss_value_4: 0.07294
	loss_reward_4: 0.0159
	loss_policy_5: 0.0547
	accuracy_policy_5: 0.6573
	loss_value_5: 0.07418
	loss_reward_5: 0.01727
	loss_policy: 0.54362
	loss_value: 0.68294
	loss_reward: 0.0775
[2025-05-07 09:52:25] nn step 2000, lr: 0.1.
	loss_policy_0: 0.25765
	accuracy_policy_0: 0.63473
	loss_value_0: 0.31598
	loss_policy_1: 0.05105
	accuracy_policy_1: 0.65105
	loss_value_1: 0.06512
	loss_reward_1: 0.01234
	loss_policy_2: 0.05102
	accuracy_policy_2: 0.65543
	loss_value_2: 0.06647
	loss_reward_2: 0.01463
	loss_policy_3: 0.05112
	accuracy_policy_3: 0.65805
	loss_value_3: 0.06798
	loss_reward_3: 0.01399
	loss_policy_4: 0.05172
	accuracy_policy_4: 0.6523
	loss_value_4: 0.06978
	loss_reward_4: 0.01493
	loss_policy_5: 0.05172
	accuracy_policy_5: 0.65523
	loss_value_5: 0.07107
	loss_reward_5: 0.01646
	loss_policy: 0.51428
	loss_value: 0.6564
	loss_reward: 0.07234
Optimization_Done 2000
[2025-05-07 09:55:22] [command] train weight_iter_2000.pkl 1 11
[2025-05-07 09:55:30] nn step 2050, lr: 0.1.
	loss_policy_0: 0.24356
	accuracy_policy_0: 0.66758
	loss_value_0: 0.31472
	loss_policy_1: 0.04815
	accuracy_policy_1: 0.67863
	loss_value_1: 0.06467
	loss_reward_1: 0.01223
	loss_policy_2: 0.04835
	accuracy_policy_2: 0.68258
	loss_value_2: 0.06602
	loss_reward_2: 0.015
	loss_policy_3: 0.04841
	accuracy_policy_3: 0.68164
	loss_value_3: 0.06787
	loss_reward_3: 0.01433
	loss_policy_4: 0.04865
	accuracy_policy_4: 0.67543
	loss_value_4: 0.0695
	loss_reward_4: 0.01507
	loss_policy_5: 0.04887
	accuracy_policy_5: 0.67332
	loss_value_5: 0.07057
	loss_reward_5: 0.01652
	loss_policy: 0.486
	loss_value: 0.65335
	loss_reward: 0.07314
[2025-05-07 09:55:38] nn step 2100, lr: 0.1.
	loss_policy_0: 0.24352
	accuracy_policy_0: 0.65926
	loss_value_0: 0.30744
	loss_policy_1: 0.04812
	accuracy_policy_1: 0.66883
	loss_value_1: 0.06329
	loss_reward_1: 0.01212
	loss_policy_2: 0.04839
	accuracy_policy_2: 0.67242
	loss_value_2: 0.06491
	loss_reward_2: 0.01477
	loss_policy_3: 0.04852
	accuracy_policy_3: 0.67359
	loss_value_3: 0.06629
	loss_reward_3: 0.01419
	loss_policy_4: 0.04887
	accuracy_policy_4: 0.67371
	loss_value_4: 0.06776
	loss_reward_4: 0.01499
	loss_policy_5: 0.04874
	accuracy_policy_5: 0.67551
	loss_value_5: 0.06886
	loss_reward_5: 0.01613
	loss_policy: 0.48617
	loss_value: 0.63857
	loss_reward: 0.0722
[2025-05-07 09:55:45] nn step 2150, lr: 0.1.
	loss_policy_0: 0.2541
	accuracy_policy_0: 0.66797
	loss_value_0: 0.31802
	loss_policy_1: 0.05059
	accuracy_policy_1: 0.66945
	loss_value_1: 0.06577
	loss_reward_1: 0.01204
	loss_policy_2: 0.05069
	accuracy_policy_2: 0.6723
	loss_value_2: 0.06721
	loss_reward_2: 0.01514
	loss_policy_3: 0.05064
	accuracy_policy_3: 0.66867
	loss_value_3: 0.06888
	loss_reward_3: 0.01428
	loss_policy_4: 0.05103
	accuracy_policy_4: 0.66969
	loss_value_4: 0.07067
	loss_reward_4: 0.01591
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.66867
	loss_value_5: 0.07236
	loss_reward_5: 0.01672
	loss_policy: 0.50845
	loss_value: 0.66292
	loss_reward: 0.07408
[2025-05-07 09:55:52] nn step 2200, lr: 0.1.
	loss_policy_0: 0.25576
	accuracy_policy_0: 0.66195
	loss_value_0: 0.3213
	loss_policy_1: 0.05073
	accuracy_policy_1: 0.66652
	loss_value_1: 0.06627
	loss_reward_1: 0.01194
	loss_policy_2: 0.05046
	accuracy_policy_2: 0.67312
	loss_value_2: 0.06811
	loss_reward_2: 0.01524
	loss_policy_3: 0.05053
	accuracy_policy_3: 0.67875
	loss_value_3: 0.0696
	loss_reward_3: 0.01434
	loss_policy_4: 0.051
	accuracy_policy_4: 0.67234
	loss_value_4: 0.07143
	loss_reward_4: 0.01537
	loss_policy_5: 0.05109
	accuracy_policy_5: 0.67684
	loss_value_5: 0.07273
	loss_reward_5: 0.01691
	loss_policy: 0.50958
	loss_value: 0.66945
	loss_reward: 0.0738
Optimization_Done 2200
[2025-05-07 09:59:08] [command] train weight_iter_2200.pkl 1 12
[2025-05-07 09:59:15] nn step 2250, lr: 0.1.
	loss_policy_0: 0.2345
	accuracy_policy_0: 0.69117
	loss_value_0: 0.32463
	loss_policy_1: 0.04631
	accuracy_policy_1: 0.70621
	loss_value_1: 0.06603
	loss_reward_1: 0.0116
	loss_policy_2: 0.04649
	accuracy_policy_2: 0.70582
	loss_value_2: 0.06738
	loss_reward_2: 0.01462
	loss_policy_3: 0.04667
	accuracy_policy_3: 0.70422
	loss_value_3: 0.06872
	loss_reward_3: 0.01395
	loss_policy_4: 0.04647
	accuracy_policy_4: 0.70117
	loss_value_4: 0.07018
	loss_reward_4: 0.01518
	loss_policy_5: 0.04679
	accuracy_policy_5: 0.70168
	loss_value_5: 0.07164
	loss_reward_5: 0.01633
	loss_policy: 0.46723
	loss_value: 0.66858
	loss_reward: 0.07168
[2025-05-07 09:59:23] nn step 2300, lr: 0.1.
	loss_policy_0: 0.21881
	accuracy_policy_0: 0.69789
	loss_value_0: 0.2979
	loss_policy_1: 0.04356
	accuracy_policy_1: 0.69652
	loss_value_1: 0.06117
	loss_reward_1: 0.01029
	loss_policy_2: 0.04369
	accuracy_policy_2: 0.705
	loss_value_2: 0.06249
	loss_reward_2: 0.01353
	loss_policy_3: 0.04346
	accuracy_policy_3: 0.70555
	loss_value_3: 0.064
	loss_reward_3: 0.01288
	loss_policy_4: 0.04404
	accuracy_policy_4: 0.69961
	loss_value_4: 0.06538
	loss_reward_4: 0.01385
	loss_policy_5: 0.044
	accuracy_policy_5: 0.7009
	loss_value_5: 0.06615
	loss_reward_5: 0.01494
	loss_policy: 0.43756
	loss_value: 0.61708
	loss_reward: 0.06549
[2025-05-07 09:59:30] nn step 2350, lr: 0.1.
	loss_policy_0: 0.22547
	accuracy_policy_0: 0.69289
	loss_value_0: 0.30271
	loss_policy_1: 0.04504
	accuracy_policy_1: 0.69305
	loss_value_1: 0.06271
	loss_reward_1: 0.01057
	loss_policy_2: 0.04514
	accuracy_policy_2: 0.70301
	loss_value_2: 0.06437
	loss_reward_2: 0.01363
	loss_policy_3: 0.04504
	accuracy_policy_3: 0.70414
	loss_value_3: 0.06567
	loss_reward_3: 0.0134
	loss_policy_4: 0.0453
	accuracy_policy_4: 0.7016
	loss_value_4: 0.06722
	loss_reward_4: 0.01456
	loss_policy_5: 0.04541
	accuracy_policy_5: 0.69844
	loss_value_5: 0.06841
	loss_reward_5: 0.0156
	loss_policy: 0.4514
	loss_value: 0.6311
	loss_reward: 0.06776
[2025-05-07 09:59:37] nn step 2400, lr: 0.1.
	loss_policy_0: 0.23647
	accuracy_policy_0: 0.69312
	loss_value_0: 0.31389
	loss_policy_1: 0.04729
	accuracy_policy_1: 0.69664
	loss_value_1: 0.06457
	loss_reward_1: 0.01079
	loss_policy_2: 0.04727
	accuracy_policy_2: 0.69184
	loss_value_2: 0.06626
	loss_reward_2: 0.01453
	loss_policy_3: 0.04722
	accuracy_policy_3: 0.69383
	loss_value_3: 0.06808
	loss_reward_3: 0.01394
	loss_policy_4: 0.04747
	accuracy_policy_4: 0.69918
	loss_value_4: 0.06968
	loss_reward_4: 0.01504
	loss_policy_5: 0.04716
	accuracy_policy_5: 0.70512
	loss_value_5: 0.07112
	loss_reward_5: 0.01643
	loss_policy: 0.47288
	loss_value: 0.65359
	loss_reward: 0.07074
Optimization_Done 2400
[2025-05-07 10:02:45] [command] train weight_iter_2400.pkl 1 13
[2025-05-07 10:02:54] nn step 2450, lr: 0.1.
	loss_policy_0: 0.23167
	accuracy_policy_0: 0.69512
	loss_value_0: 0.32651
	loss_policy_1: 0.04617
	accuracy_policy_1: 0.69773
	loss_value_1: 0.06701
	loss_reward_1: 0.01046
	loss_policy_2: 0.04628
	accuracy_policy_2: 0.69934
	loss_value_2: 0.06872
	loss_reward_2: 0.01406
	loss_policy_3: 0.04612
	accuracy_policy_3: 0.70387
	loss_value_3: 0.0702
	loss_reward_3: 0.01385
	loss_policy_4: 0.04646
	accuracy_policy_4: 0.70242
	loss_value_4: 0.07159
	loss_reward_4: 0.01519
	loss_policy_5: 0.04674
	accuracy_policy_5: 0.70418
	loss_value_5: 0.07331
	loss_reward_5: 0.01569
	loss_policy: 0.46344
	loss_value: 0.67734
	loss_reward: 0.06925
[2025-05-07 10:03:00] nn step 2500, lr: 0.1.
	loss_policy_0: 0.2458
	accuracy_policy_0: 0.69891
	loss_value_0: 0.3396
	loss_policy_1: 0.04893
	accuracy_policy_1: 0.70336
	loss_value_1: 0.06965
	loss_reward_1: 0.0108
	loss_policy_2: 0.04895
	accuracy_policy_2: 0.7007
	loss_value_2: 0.0717
	loss_reward_2: 0.01462
	loss_policy_3: 0.04895
	accuracy_policy_3: 0.70395
	loss_value_3: 0.07308
	loss_reward_3: 0.01471
	loss_policy_4: 0.04907
	accuracy_policy_4: 0.70473
	loss_value_4: 0.0748
	loss_reward_4: 0.01539
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.69895
	loss_value_5: 0.0764
	loss_reward_5: 0.01684
	loss_policy: 0.49113
	loss_value: 0.70523
	loss_reward: 0.07237
[2025-05-07 10:03:08] nn step 2550, lr: 0.1.
	loss_policy_0: 0.23848
	accuracy_policy_0: 0.69816
	loss_value_0: 0.32487
	loss_policy_1: 0.04785
	accuracy_policy_1: 0.69836
	loss_value_1: 0.0667
	loss_reward_1: 0.01024
	loss_policy_2: 0.0476
	accuracy_policy_2: 0.705
	loss_value_2: 0.06851
	loss_reward_2: 0.01419
	loss_policy_3: 0.04769
	accuracy_policy_3: 0.70605
	loss_value_3: 0.07007
	loss_reward_3: 0.01391
	loss_policy_4: 0.04802
	accuracy_policy_4: 0.70207
	loss_value_4: 0.07198
	loss_reward_4: 0.0149
	loss_policy_5: 0.04824
	accuracy_policy_5: 0.70195
	loss_value_5: 0.07358
	loss_reward_5: 0.0162
	loss_policy: 0.47788
	loss_value: 0.67571
	loss_reward: 0.06944
[2025-05-07 10:03:15] nn step 2600, lr: 0.1.
	loss_policy_0: 0.2544
	accuracy_policy_0: 0.69535
	loss_value_0: 0.34905
	loss_policy_1: 0.05064
	accuracy_policy_1: 0.69789
	loss_value_1: 0.07158
	loss_reward_1: 0.01061
	loss_policy_2: 0.0508
	accuracy_policy_2: 0.69871
	loss_value_2: 0.0733
	loss_reward_2: 0.01531
	loss_policy_3: 0.05077
	accuracy_policy_3: 0.70074
	loss_value_3: 0.07498
	loss_reward_3: 0.0149
	loss_policy_4: 0.05123
	accuracy_policy_4: 0.69734
	loss_value_4: 0.07678
	loss_reward_4: 0.0158
	loss_policy_5: 0.05161
	accuracy_policy_5: 0.69629
	loss_value_5: 0.07851
	loss_reward_5: 0.01735
	loss_policy: 0.50945
	loss_value: 0.72419
	loss_reward: 0.07396
Optimization_Done 2600
[2025-05-07 10:06:30] [command] train weight_iter_2600.pkl 1 14
[2025-05-07 10:06:39] nn step 2650, lr: 0.1.
	loss_policy_0: 0.21644
	accuracy_policy_0: 0.72906
	loss_value_0: 0.32767
	loss_policy_1: 0.04322
	accuracy_policy_1: 0.72723
	loss_value_1: 0.06712
	loss_reward_1: 0.00973
	loss_policy_2: 0.04363
	accuracy_policy_2: 0.72363
	loss_value_2: 0.06894
	loss_reward_2: 0.01385
	loss_policy_3: 0.04346
	accuracy_policy_3: 0.72098
	loss_value_3: 0.07033
	loss_reward_3: 0.01379
	loss_policy_4: 0.04379
	accuracy_policy_4: 0.72055
	loss_value_4: 0.07198
	loss_reward_4: 0.01499
	loss_policy_5: 0.04391
	accuracy_policy_5: 0.71816
	loss_value_5: 0.07342
	loss_reward_5: 0.01567
	loss_policy: 0.43446
	loss_value: 0.67946
	loss_reward: 0.06803
[2025-05-07 10:06:47] nn step 2700, lr: 0.1.
	loss_policy_0: 0.23342
	accuracy_policy_0: 0.71477
	loss_value_0: 0.33971
	loss_policy_1: 0.04681
	accuracy_policy_1: 0.71395
	loss_value_1: 0.06977
	loss_reward_1: 0.01043
	loss_policy_2: 0.04656
	accuracy_policy_2: 0.71703
	loss_value_2: 0.07137
	loss_reward_2: 0.01456
	loss_policy_3: 0.04672
	accuracy_policy_3: 0.71898
	loss_value_3: 0.07311
	loss_reward_3: 0.01421
	loss_policy_4: 0.04703
	accuracy_policy_4: 0.71559
	loss_value_4: 0.07477
	loss_reward_4: 0.01549
	loss_policy_5: 0.04703
	accuracy_policy_5: 0.71656
	loss_value_5: 0.07622
	loss_reward_5: 0.01672
	loss_policy: 0.46758
	loss_value: 0.70496
	loss_reward: 0.0714
[2025-05-07 10:06:53] nn step 2750, lr: 0.1.
	loss_policy_0: 0.23076
	accuracy_policy_0: 0.71844
	loss_value_0: 0.33043
	loss_policy_1: 0.04577
	accuracy_policy_1: 0.71797
	loss_value_1: 0.06804
	loss_reward_1: 0.01021
	loss_policy_2: 0.04569
	accuracy_policy_2: 0.72512
	loss_value_2: 0.06988
	loss_reward_2: 0.0143
	loss_policy_3: 0.04586
	accuracy_policy_3: 0.72789
	loss_value_3: 0.07155
	loss_reward_3: 0.01399
	loss_policy_4: 0.04608
	accuracy_policy_4: 0.72668
	loss_value_4: 0.07302
	loss_reward_4: 0.01554
	loss_policy_5: 0.0462
	accuracy_policy_5: 0.72125
	loss_value_5: 0.07478
	loss_reward_5: 0.01692
	loss_policy: 0.46036
	loss_value: 0.68771
	loss_reward: 0.07097
[2025-05-07 10:07:01] nn step 2800, lr: 0.1.
	loss_policy_0: 0.22417
	accuracy_policy_0: 0.71961
	loss_value_0: 0.32182
	loss_policy_1: 0.04464
	accuracy_policy_1: 0.72117
	loss_value_1: 0.06638
	loss_reward_1: 0.00979
	loss_policy_2: 0.04482
	accuracy_policy_2: 0.72246
	loss_value_2: 0.06801
	loss_reward_2: 0.01417
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.72762
	loss_value_3: 0.06965
	loss_reward_3: 0.01345
	loss_policy_4: 0.04505
	accuracy_policy_4: 0.71969
	loss_value_4: 0.07104
	loss_reward_4: 0.01486
	loss_policy_5: 0.0452
	accuracy_policy_5: 0.72039
	loss_value_5: 0.07216
	loss_reward_5: 0.01619
	loss_policy: 0.44869
	loss_value: 0.66907
	loss_reward: 0.06847
Optimization_Done 2800
[2025-05-07 10:10:11] [command] train weight_iter_2800.pkl 1 15
[2025-05-07 10:10:20] nn step 2850, lr: 0.1.
	loss_policy_0: 0.21023
	accuracy_policy_0: 0.74199
	loss_value_0: 0.32175
	loss_policy_1: 0.04173
	accuracy_policy_1: 0.74484
	loss_value_1: 0.066
	loss_reward_1: 0.00929
	loss_policy_2: 0.04222
	accuracy_policy_2: 0.74395
	loss_value_2: 0.06741
	loss_reward_2: 0.01352
	loss_policy_3: 0.04204
	accuracy_policy_3: 0.7466
	loss_value_3: 0.06881
	loss_reward_3: 0.01323
	loss_policy_4: 0.04221
	accuracy_policy_4: 0.74703
	loss_value_4: 0.07038
	loss_reward_4: 0.01431
	loss_policy_5: 0.04244
	accuracy_policy_5: 0.74668
	loss_value_5: 0.07181
	loss_reward_5: 0.0155
	loss_policy: 0.42087
	loss_value: 0.66616
	loss_reward: 0.06585
[2025-05-07 10:10:27] nn step 2900, lr: 0.1.
	loss_policy_0: 0.21894
	accuracy_policy_0: 0.73598
	loss_value_0: 0.33115
	loss_policy_1: 0.04334
	accuracy_policy_1: 0.73926
	loss_value_1: 0.06807
	loss_reward_1: 0.00971
	loss_policy_2: 0.04337
	accuracy_policy_2: 0.73781
	loss_value_2: 0.06994
	loss_reward_2: 0.01392
	loss_policy_3: 0.04361
	accuracy_policy_3: 0.73738
	loss_value_3: 0.07159
	loss_reward_3: 0.0137
	loss_policy_4: 0.04334
	accuracy_policy_4: 0.74398
	loss_value_4: 0.07301
	loss_reward_4: 0.0149
	loss_policy_5: 0.04364
	accuracy_policy_5: 0.74512
	loss_value_5: 0.07475
	loss_reward_5: 0.01609
	loss_policy: 0.43624
	loss_value: 0.6885
	loss_reward: 0.06832
[2025-05-07 10:10:35] nn step 2950, lr: 0.1.
	loss_policy_0: 0.21248
	accuracy_policy_0: 0.73371
	loss_value_0: 0.31489
	loss_policy_1: 0.04238
	accuracy_policy_1: 0.73332
	loss_value_1: 0.06451
	loss_reward_1: 0.00915
	loss_policy_2: 0.04215
	accuracy_policy_2: 0.73918
	loss_value_2: 0.06661
	loss_reward_2: 0.01295
	loss_policy_3: 0.04201
	accuracy_policy_3: 0.73949
	loss_value_3: 0.06792
	loss_reward_3: 0.01366
	loss_policy_4: 0.04251
	accuracy_policy_4: 0.74047
	loss_value_4: 0.06912
	loss_reward_4: 0.01442
	loss_policy_5: 0.04253
	accuracy_policy_5: 0.73855
	loss_value_5: 0.0706
	loss_reward_5: 0.01547
	loss_policy: 0.42407
	loss_value: 0.65364
	loss_reward: 0.06565
[2025-05-07 10:10:42] nn step 3000, lr: 0.1.
	loss_policy_0: 0.21849
	accuracy_policy_0: 0.73281
	loss_value_0: 0.3292
	loss_policy_1: 0.04382
	accuracy_policy_1: 0.73699
	loss_value_1: 0.06754
	loss_reward_1: 0.0092
	loss_policy_2: 0.04361
	accuracy_policy_2: 0.73797
	loss_value_2: 0.06898
	loss_reward_2: 0.01392
	loss_policy_3: 0.04402
	accuracy_policy_3: 0.73668
	loss_value_3: 0.07083
	loss_reward_3: 0.01387
	loss_policy_4: 0.04424
	accuracy_policy_4: 0.73922
	loss_value_4: 0.07258
	loss_reward_4: 0.01506
	loss_policy_5: 0.04422
	accuracy_policy_5: 0.73816
	loss_value_5: 0.07412
	loss_reward_5: 0.01634
	loss_policy: 0.4384
	loss_value: 0.68325
	loss_reward: 0.06839
Optimization_Done 3000
[2025-05-07 10:13:58] [command] train weight_iter_3000.pkl 1 16
[2025-05-07 10:14:05] nn step 3050, lr: 0.1.
	loss_policy_0: 0.18985
	accuracy_policy_0: 0.7527
	loss_value_0: 0.29783
	loss_policy_1: 0.03814
	accuracy_policy_1: 0.75082
	loss_value_1: 0.06093
	loss_reward_1: 0.0082
	loss_policy_2: 0.03799
	accuracy_policy_2: 0.755
	loss_value_2: 0.06247
	loss_reward_2: 0.01212
	loss_policy_3: 0.03785
	accuracy_policy_3: 0.75453
	loss_value_3: 0.06394
	loss_reward_3: 0.01232
	loss_policy_4: 0.03806
	accuracy_policy_4: 0.75324
	loss_value_4: 0.06541
	loss_reward_4: 0.01346
	loss_policy_5: 0.03812
	accuracy_policy_5: 0.7559
	loss_value_5: 0.06669
	loss_reward_5: 0.01457
	loss_policy: 0.38003
	loss_value: 0.61727
	loss_reward: 0.06066
[2025-05-07 10:14:13] nn step 3100, lr: 0.1.
	loss_policy_0: 0.21002
	accuracy_policy_0: 0.74812
	loss_value_0: 0.32438
	loss_policy_1: 0.04211
	accuracy_policy_1: 0.74547
	loss_value_1: 0.06681
	loss_reward_1: 0.00916
	loss_policy_2: 0.04188
	accuracy_policy_2: 0.74785
	loss_value_2: 0.06852
	loss_reward_2: 0.01351
	loss_policy_3: 0.0419
	accuracy_policy_3: 0.74805
	loss_value_3: 0.07008
	loss_reward_3: 0.01321
	loss_policy_4: 0.0421
	accuracy_policy_4: 0.74723
	loss_value_4: 0.07166
	loss_reward_4: 0.01486
	loss_policy_5: 0.04211
	accuracy_policy_5: 0.74809
	loss_value_5: 0.07302
	loss_reward_5: 0.01623
	loss_policy: 0.42013
	loss_value: 0.67447
	loss_reward: 0.06697
[2025-05-07 10:14:21] nn step 3150, lr: 0.1.
	loss_policy_0: 0.22469
	accuracy_policy_0: 0.74418
	loss_value_0: 0.34642
	loss_policy_1: 0.04455
	accuracy_policy_1: 0.75207
	loss_value_1: 0.0713
	loss_reward_1: 0.00953
	loss_policy_2: 0.04451
	accuracy_policy_2: 0.75156
	loss_value_2: 0.07344
	loss_reward_2: 0.01416
	loss_policy_3: 0.04485
	accuracy_policy_3: 0.7493
	loss_value_3: 0.07481
	loss_reward_3: 0.01435
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.75328
	loss_value_4: 0.07675
	loss_reward_4: 0.01609
	loss_policy_5: 0.04511
	accuracy_policy_5: 0.74895
	loss_value_5: 0.07845
	loss_reward_5: 0.01726
	loss_policy: 0.44864
	loss_value: 0.72118
	loss_reward: 0.07139
[2025-05-07 10:14:28] nn step 3200, lr: 0.1.
	loss_policy_0: 0.20613
	accuracy_policy_0: 0.74453
	loss_value_0: 0.31631
	loss_policy_1: 0.04144
	accuracy_policy_1: 0.74164
	loss_value_1: 0.06522
	loss_reward_1: 0.00907
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.74688
	loss_value_2: 0.0668
	loss_reward_2: 0.01317
	loss_policy_3: 0.04121
	accuracy_policy_3: 0.74844
	loss_value_3: 0.06807
	loss_reward_3: 0.01346
	loss_policy_4: 0.04173
	accuracy_policy_4: 0.74723
	loss_value_4: 0.06972
	loss_reward_4: 0.01458
	loss_policy_5: 0.04191
	accuracy_policy_5: 0.7432
	loss_value_5: 0.07151
	loss_reward_5: 0.01538
	loss_policy: 0.41382
	loss_value: 0.65761
	loss_reward: 0.06566
Optimization_Done 3200
[2025-05-07 10:17:42] [command] train weight_iter_3200.pkl 1 17
[2025-05-07 10:17:49] nn step 3250, lr: 0.1.
	loss_policy_0: 0.20618
	accuracy_policy_0: 0.75836
	loss_value_0: 0.32985
	loss_policy_1: 0.04173
	accuracy_policy_1: 0.75488
	loss_value_1: 0.0681
	loss_reward_1: 0.00909
	loss_policy_2: 0.04177
	accuracy_policy_2: 0.75125
	loss_value_2: 0.06948
	loss_reward_2: 0.01356
	loss_policy_3: 0.04186
	accuracy_policy_3: 0.75508
	loss_value_3: 0.07128
	loss_reward_3: 0.01367
	loss_policy_4: 0.04184
	accuracy_policy_4: 0.75234
	loss_value_4: 0.07253
	loss_reward_4: 0.01562
	loss_policy_5: 0.04161
	accuracy_policy_5: 0.75605
	loss_value_5: 0.07359
	loss_reward_5: 0.01633
	loss_policy: 0.415
	loss_value: 0.68483
	loss_reward: 0.06827
[2025-05-07 10:17:57] nn step 3300, lr: 0.1.
	loss_policy_0: 0.20184
	accuracy_policy_0: 0.75387
	loss_value_0: 0.32136
	loss_policy_1: 0.04048
	accuracy_policy_1: 0.75527
	loss_value_1: 0.0659
	loss_reward_1: 0.00896
	loss_policy_2: 0.04039
	accuracy_policy_2: 0.76059
	loss_value_2: 0.0677
	loss_reward_2: 0.01354
	loss_policy_3: 0.04074
	accuracy_policy_3: 0.75547
	loss_value_3: 0.06897
	loss_reward_3: 0.01349
	loss_policy_4: 0.04109
	accuracy_policy_4: 0.75594
	loss_value_4: 0.07011
	loss_reward_4: 0.01485
	loss_policy_5: 0.04117
	accuracy_policy_5: 0.75129
	loss_value_5: 0.07172
	loss_reward_5: 0.01595
	loss_policy: 0.40571
	loss_value: 0.66576
	loss_reward: 0.06678
[2025-05-07 10:18:04] nn step 3350, lr: 0.1.
	loss_policy_0: 0.20467
	accuracy_policy_0: 0.75086
	loss_value_0: 0.32082
	loss_policy_1: 0.04115
	accuracy_policy_1: 0.74734
	loss_value_1: 0.06596
	loss_reward_1: 0.0089
	loss_policy_2: 0.04127
	accuracy_policy_2: 0.74809
	loss_value_2: 0.06775
	loss_reward_2: 0.01341
	loss_policy_3: 0.04121
	accuracy_policy_3: 0.74785
	loss_value_3: 0.06911
	loss_reward_3: 0.01325
	loss_policy_4: 0.04117
	accuracy_policy_4: 0.75098
	loss_value_4: 0.07037
	loss_reward_4: 0.01498
	loss_policy_5: 0.04137
	accuracy_policy_5: 0.75102
	loss_value_5: 0.07184
	loss_reward_5: 0.01605
	loss_policy: 0.41084
	loss_value: 0.66584
	loss_reward: 0.06658
[2025-05-07 10:18:12] nn step 3400, lr: 0.1.
	loss_policy_0: 0.2012
	accuracy_policy_0: 0.75598
	loss_value_0: 0.31964
	loss_policy_1: 0.04069
	accuracy_policy_1: 0.7518
	loss_value_1: 0.06563
	loss_reward_1: 0.00864
	loss_policy_2: 0.04075
	accuracy_policy_2: 0.75004
	loss_value_2: 0.06743
	loss_reward_2: 0.01287
	loss_policy_3: 0.04063
	accuracy_policy_3: 0.75211
	loss_value_3: 0.06883
	loss_reward_3: 0.01305
	loss_policy_4: 0.04061
	accuracy_policy_4: 0.75418
	loss_value_4: 0.07051
	loss_reward_4: 0.01462
	loss_policy_5: 0.04096
	accuracy_policy_5: 0.75402
	loss_value_5: 0.07214
	loss_reward_5: 0.0158
	loss_policy: 0.40483
	loss_value: 0.66417
	loss_reward: 0.06498
Optimization_Done 3400
[2025-05-07 10:21:16] [command] train weight_iter_3400.pkl 1 18
[2025-05-07 10:21:25] nn step 3450, lr: 0.1.
	loss_policy_0: 0.20081
	accuracy_policy_0: 0.76023
	loss_value_0: 0.32629
	loss_policy_1: 0.04014
	accuracy_policy_1: 0.76332
	loss_value_1: 0.06742
	loss_reward_1: 0.00879
	loss_policy_2: 0.04016
	accuracy_policy_2: 0.76184
	loss_value_2: 0.06909
	loss_reward_2: 0.01311
	loss_policy_3: 0.04017
	accuracy_policy_3: 0.76348
	loss_value_3: 0.07066
	loss_reward_3: 0.01344
	loss_policy_4: 0.04068
	accuracy_policy_4: 0.76449
	loss_value_4: 0.07227
	loss_reward_4: 0.01515
	loss_policy_5: 0.04037
	accuracy_policy_5: 0.76656
	loss_value_5: 0.07348
	loss_reward_5: 0.01565
	loss_policy: 0.40233
	loss_value: 0.67921
	loss_reward: 0.06615
[2025-05-07 10:21:33] nn step 3500, lr: 0.1.
	loss_policy_0: 0.20868
	accuracy_policy_0: 0.75508
	loss_value_0: 0.33332
	loss_policy_1: 0.0416
	accuracy_policy_1: 0.75926
	loss_value_1: 0.06856
	loss_reward_1: 0.0091
	loss_policy_2: 0.04191
	accuracy_policy_2: 0.75797
	loss_value_2: 0.07049
	loss_reward_2: 0.01376
	loss_policy_3: 0.04206
	accuracy_policy_3: 0.75848
	loss_value_3: 0.07206
	loss_reward_3: 0.01356
	loss_policy_4: 0.04255
	accuracy_policy_4: 0.75719
	loss_value_4: 0.07366
	loss_reward_4: 0.0157
	loss_policy_5: 0.04218
	accuracy_policy_5: 0.75988
	loss_value_5: 0.07527
	loss_reward_5: 0.01618
	loss_policy: 0.41898
	loss_value: 0.69338
	loss_reward: 0.06829
[2025-05-07 10:21:39] nn step 3550, lr: 0.1.
	loss_policy_0: 0.19797
	accuracy_policy_0: 0.75742
	loss_value_0: 0.31338
	loss_policy_1: 0.03938
	accuracy_policy_1: 0.75855
	loss_value_1: 0.06427
	loss_reward_1: 0.00833
	loss_policy_2: 0.03928
	accuracy_policy_2: 0.76094
	loss_value_2: 0.06625
	loss_reward_2: 0.01285
	loss_policy_3: 0.03966
	accuracy_policy_3: 0.7607
	loss_value_3: 0.06763
	loss_reward_3: 0.01248
	loss_policy_4: 0.0395
	accuracy_policy_4: 0.76223
	loss_value_4: 0.06934
	loss_reward_4: 0.0141
	loss_policy_5: 0.03984
	accuracy_policy_5: 0.76402
	loss_value_5: 0.07077
	loss_reward_5: 0.01556
	loss_policy: 0.39563
	loss_value: 0.65164
	loss_reward: 0.06332
[2025-05-07 10:21:47] nn step 3600, lr: 0.1.
	loss_policy_0: 0.19271
	accuracy_policy_0: 0.75438
	loss_value_0: 0.30352
	loss_policy_1: 0.03827
	accuracy_policy_1: 0.75727
	loss_value_1: 0.06234
	loss_reward_1: 0.00796
	loss_policy_2: 0.03826
	accuracy_policy_2: 0.75738
	loss_value_2: 0.06413
	loss_reward_2: 0.01221
	loss_policy_3: 0.03831
	accuracy_policy_3: 0.76066
	loss_value_3: 0.06598
	loss_reward_3: 0.01251
	loss_policy_4: 0.03851
	accuracy_policy_4: 0.75828
	loss_value_4: 0.06742
	loss_reward_4: 0.01375
	loss_policy_5: 0.03834
	accuracy_policy_5: 0.76555
	loss_value_5: 0.06888
	loss_reward_5: 0.01495
	loss_policy: 0.3844
	loss_value: 0.63227
	loss_reward: 0.06138
Optimization_Done 3600
[2025-05-07 10:25:07] [command] train weight_iter_3600.pkl 1 19
[2025-05-07 10:25:16] nn step 3650, lr: 0.1.
	loss_policy_0: 0.195
	accuracy_policy_0: 0.74289
	loss_value_0: 0.31136
	loss_policy_1: 0.03872
	accuracy_policy_1: 0.75141
	loss_value_1: 0.06372
	loss_reward_1: 0.00826
	loss_policy_2: 0.03854
	accuracy_policy_2: 0.75207
	loss_value_2: 0.06522
	loss_reward_2: 0.01254
	loss_policy_3: 0.03883
	accuracy_policy_3: 0.75102
	loss_value_3: 0.06673
	loss_reward_3: 0.01255
	loss_policy_4: 0.03905
	accuracy_policy_4: 0.75078
	loss_value_4: 0.06829
	loss_reward_4: 0.01384
	loss_policy_5: 0.03905
	accuracy_policy_5: 0.75184
	loss_value_5: 0.06971
	loss_reward_5: 0.01511
	loss_policy: 0.38917
	loss_value: 0.64504
	loss_reward: 0.0623
[2025-05-07 10:25:23] nn step 3700, lr: 0.1.
	loss_policy_0: 0.21804
	accuracy_policy_0: 0.74789
	loss_value_0: 0.34612
	loss_policy_1: 0.04362
	accuracy_policy_1: 0.75035
	loss_value_1: 0.071
	loss_reward_1: 0.00919
	loss_policy_2: 0.04377
	accuracy_policy_2: 0.75102
	loss_value_2: 0.0731
	loss_reward_2: 0.014
	loss_policy_3: 0.04389
	accuracy_policy_3: 0.75383
	loss_value_3: 0.07481
	loss_reward_3: 0.01425
	loss_policy_4: 0.04389
	accuracy_policy_4: 0.75352
	loss_value_4: 0.07675
	loss_reward_4: 0.0153
	loss_policy_5: 0.04393
	accuracy_policy_5: 0.75621
	loss_value_5: 0.07832
	loss_reward_5: 0.01708
	loss_policy: 0.43712
	loss_value: 0.72009
	loss_reward: 0.06983
[2025-05-07 10:25:30] nn step 3750, lr: 0.1.
	loss_policy_0: 0.21014
	accuracy_policy_0: 0.75078
	loss_value_0: 0.33449
	loss_policy_1: 0.04224
	accuracy_policy_1: 0.75191
	loss_value_1: 0.06869
	loss_reward_1: 0.00848
	loss_policy_2: 0.04208
	accuracy_policy_2: 0.75371
	loss_value_2: 0.07057
	loss_reward_2: 0.01336
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.74922
	loss_value_3: 0.07204
	loss_reward_3: 0.01391
	loss_policy_4: 0.04229
	accuracy_policy_4: 0.7493
	loss_value_4: 0.07376
	loss_reward_4: 0.01552
	loss_policy_5: 0.04234
	accuracy_policy_5: 0.75348
	loss_value_5: 0.07517
	loss_reward_5: 0.0162
	loss_policy: 0.42127
	loss_value: 0.69473
	loss_reward: 0.06746
[2025-05-07 10:25:38] nn step 3800, lr: 0.1.
	loss_policy_0: 0.20093
	accuracy_policy_0: 0.75008
	loss_value_0: 0.31753
	loss_policy_1: 0.04042
	accuracy_policy_1: 0.74941
	loss_value_1: 0.06566
	loss_reward_1: 0.00851
	loss_policy_2: 0.04018
	accuracy_policy_2: 0.74969
	loss_value_2: 0.06718
	loss_reward_2: 0.01288
	loss_policy_3: 0.04022
	accuracy_policy_3: 0.75848
	loss_value_3: 0.06889
	loss_reward_3: 0.01308
	loss_policy_4: 0.04042
	accuracy_policy_4: 0.75477
	loss_value_4: 0.07034
	loss_reward_4: 0.01457
	loss_policy_5: 0.04044
	accuracy_policy_5: 0.7534
	loss_value_5: 0.07189
	loss_reward_5: 0.01573
	loss_policy: 0.40261
	loss_value: 0.66149
	loss_reward: 0.06476
Optimization_Done 3800
[2025-05-07 10:28:49] [command] train weight_iter_3800.pkl 1 20
[2025-05-07 10:28:57] nn step 3850, lr: 0.1.
	loss_policy_0: 0.20742
	accuracy_policy_0: 0.75277
	loss_value_0: 0.33934
	loss_policy_1: 0.04162
	accuracy_policy_1: 0.75473
	loss_value_1: 0.06952
	loss_reward_1: 0.0083
	loss_policy_2: 0.04151
	accuracy_policy_2: 0.75641
	loss_value_2: 0.07118
	loss_reward_2: 0.01343
	loss_policy_3: 0.04138
	accuracy_policy_3: 0.76094
	loss_value_3: 0.07295
	loss_reward_3: 0.01382
	loss_policy_4: 0.04176
	accuracy_policy_4: 0.75434
	loss_value_4: 0.07456
	loss_reward_4: 0.0152
	loss_policy_5: 0.04167
	accuracy_policy_5: 0.75434
	loss_value_5: 0.07591
	loss_reward_5: 0.01625
	loss_policy: 0.41537
	loss_value: 0.70347
	loss_reward: 0.067
[2025-05-07 10:29:05] nn step 3900, lr: 0.1.
	loss_policy_0: 0.20785
	accuracy_policy_0: 0.74566
	loss_value_0: 0.33095
	loss_policy_1: 0.04144
	accuracy_policy_1: 0.75254
	loss_value_1: 0.06775
	loss_reward_1: 0.00833
	loss_policy_2: 0.04106
	accuracy_policy_2: 0.7577
	loss_value_2: 0.06957
	loss_reward_2: 0.01339
	loss_policy_3: 0.04143
	accuracy_policy_3: 0.7577
	loss_value_3: 0.07154
	loss_reward_3: 0.01349
	loss_policy_4: 0.04174
	accuracy_policy_4: 0.74902
	loss_value_4: 0.07296
	loss_reward_4: 0.01517
	loss_policy_5: 0.04158
	accuracy_policy_5: 0.75203
	loss_value_5: 0.07467
	loss_reward_5: 0.01568
	loss_policy: 0.41509
	loss_value: 0.68744
	loss_reward: 0.06605
[2025-05-07 10:29:13] nn step 3950, lr: 0.1.
	loss_policy_0: 0.21475
	accuracy_policy_0: 0.74988
	loss_value_0: 0.34214
	loss_policy_1: 0.0429
	accuracy_policy_1: 0.74805
	loss_value_1: 0.07006
	loss_reward_1: 0.00873
	loss_policy_2: 0.04286
	accuracy_policy_2: 0.74984
	loss_value_2: 0.07223
	loss_reward_2: 0.01339
	loss_policy_3: 0.04301
	accuracy_policy_3: 0.75266
	loss_value_3: 0.07401
	loss_reward_3: 0.01413
	loss_policy_4: 0.04304
	accuracy_policy_4: 0.75363
	loss_value_4: 0.07573
	loss_reward_4: 0.01535
	loss_policy_5: 0.0429
	accuracy_policy_5: 0.75668
	loss_value_5: 0.07714
	loss_reward_5: 0.01662
	loss_policy: 0.42945
	loss_value: 0.71131
	loss_reward: 0.06821
[2025-05-07 10:29:19] nn step 4000, lr: 0.1.
	loss_policy_0: 0.21519
	accuracy_policy_0: 0.75301
	loss_value_0: 0.34009
	loss_policy_1: 0.04289
	accuracy_policy_1: 0.75285
	loss_value_1: 0.06967
	loss_reward_1: 0.0085
	loss_policy_2: 0.04287
	accuracy_policy_2: 0.75414
	loss_value_2: 0.07142
	loss_reward_2: 0.01354
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.75598
	loss_value_3: 0.07322
	loss_reward_3: 0.01409
	loss_policy_4: 0.04289
	accuracy_policy_4: 0.75539
	loss_value_4: 0.07491
	loss_reward_4: 0.01564
	loss_policy_5: 0.04262
	accuracy_policy_5: 0.75641
	loss_value_5: 0.07653
	loss_reward_5: 0.01682
	loss_policy: 0.42899
	loss_value: 0.70584
	loss_reward: 0.06859
Optimization_Done 4000
[2025-05-07 10:32:38] [command] train weight_iter_4000.pkl 2 21
[2025-05-07 10:32:50] nn step 4050, lr: 0.1.
	loss_policy_0: 0.20028
	accuracy_policy_0: 0.76531
	loss_value_0: 0.32848
	loss_policy_1: 0.04011
	accuracy_policy_1: 0.76746
	loss_value_1: 0.06714
	loss_reward_1: 0.00816
	loss_policy_2: 0.03998
	accuracy_policy_2: 0.76613
	loss_value_2: 0.06869
	loss_reward_2: 0.01277
	loss_policy_3: 0.0401
	accuracy_policy_3: 0.76582
	loss_value_3: 0.06986
	loss_reward_3: 0.0132
	loss_policy_4: 0.04009
	accuracy_policy_4: 0.76359
	loss_value_4: 0.07134
	loss_reward_4: 0.01491
	loss_policy_5: 0.04021
	accuracy_policy_5: 0.76551
	loss_value_5: 0.07267
	loss_reward_5: 0.01562
	loss_policy: 0.40077
	loss_value: 0.67817
	loss_reward: 0.06466
[2025-05-07 10:32:58] nn step 4100, lr: 0.1.
	loss_policy_0: 0.21787
	accuracy_policy_0: 0.76973
	loss_value_0: 0.35411
	loss_policy_1: 0.04355
	accuracy_policy_1: 0.76879
	loss_value_1: 0.07275
	loss_reward_1: 0.00887
	loss_policy_2: 0.04343
	accuracy_policy_2: 0.77148
	loss_value_2: 0.07488
	loss_reward_2: 0.01406
	loss_policy_3: 0.04358
	accuracy_policy_3: 0.7727
	loss_value_3: 0.07657
	loss_reward_3: 0.01432
	loss_policy_4: 0.04369
	accuracy_policy_4: 0.7732
	loss_value_4: 0.07801
	loss_reward_4: 0.01637
	loss_policy_5: 0.04382
	accuracy_policy_5: 0.77387
	loss_value_5: 0.07958
	loss_reward_5: 0.0175
	loss_policy: 0.43593
	loss_value: 0.7359
	loss_reward: 0.07112
[2025-05-07 10:33:06] nn step 4150, lr: 0.1.
	loss_policy_0: 0.20983
	accuracy_policy_0: 0.76383
	loss_value_0: 0.3383
	loss_policy_1: 0.04199
	accuracy_policy_1: 0.7652
	loss_value_1: 0.06919
	loss_reward_1: 0.00886
	loss_policy_2: 0.04192
	accuracy_policy_2: 0.76723
	loss_value_2: 0.07128
	loss_reward_2: 0.0138
	loss_policy_3: 0.04208
	accuracy_policy_3: 0.76773
	loss_value_3: 0.07318
	loss_reward_3: 0.01433
	loss_policy_4: 0.04189
	accuracy_policy_4: 0.76906
	loss_value_4: 0.07487
	loss_reward_4: 0.01572
	loss_policy_5: 0.04211
	accuracy_policy_5: 0.76488
	loss_value_5: 0.07655
	loss_reward_5: 0.01695
	loss_policy: 0.41982
	loss_value: 0.70337
	loss_reward: 0.06967
[2025-05-07 10:33:13] nn step 4200, lr: 0.1.
	loss_policy_0: 0.20726
	accuracy_policy_0: 0.77059
	loss_value_0: 0.3365
	loss_policy_1: 0.04177
	accuracy_policy_1: 0.77184
	loss_value_1: 0.06941
	loss_reward_1: 0.00861
	loss_policy_2: 0.04159
	accuracy_policy_2: 0.76953
	loss_value_2: 0.07172
	loss_reward_2: 0.01338
	loss_policy_3: 0.04176
	accuracy_policy_3: 0.77148
	loss_value_3: 0.0736
	loss_reward_3: 0.01383
	loss_policy_4: 0.04157
	accuracy_policy_4: 0.76996
	loss_value_4: 0.07561
	loss_reward_4: 0.01546
	loss_policy_5: 0.042
	accuracy_policy_5: 0.77223
	loss_value_5: 0.07732
	loss_reward_5: 0.01665
	loss_policy: 0.41596
	loss_value: 0.70417
	loss_reward: 0.06793
Optimization_Done 4200
[2025-05-07 10:36:11] [command] train weight_iter_4200.pkl 3 22
[2025-05-07 10:36:19] nn step 4250, lr: 0.1.
	loss_policy_0: 0.19956
	accuracy_policy_0: 0.78234
	loss_value_0: 0.34963
	loss_policy_1: 0.03983
	accuracy_policy_1: 0.78738
	loss_value_1: 0.07143
	loss_reward_1: 0.00841
	loss_policy_2: 0.03997
	accuracy_policy_2: 0.78453
	loss_value_2: 0.07354
	loss_reward_2: 0.01327
	loss_policy_3: 0.04014
	accuracy_policy_3: 0.78879
	loss_value_3: 0.07516
	loss_reward_3: 0.01389
	loss_policy_4: 0.04046
	accuracy_policy_4: 0.78414
	loss_value_4: 0.07633
	loss_reward_4: 0.01606
	loss_policy_5: 0.04062
	accuracy_policy_5: 0.78426
	loss_value_5: 0.07783
	loss_reward_5: 0.01704
	loss_policy: 0.40058
	loss_value: 0.72393
	loss_reward: 0.06867
[2025-05-07 10:36:27] nn step 4300, lr: 0.1.
	loss_policy_0: 0.20993
	accuracy_policy_0: 0.7857
	loss_value_0: 0.36302
	loss_policy_1: 0.04187
	accuracy_policy_1: 0.78715
	loss_value_1: 0.07456
	loss_reward_1: 0.00912
	loss_policy_2: 0.04215
	accuracy_policy_2: 0.78348
	loss_value_2: 0.0767
	loss_reward_2: 0.01426
	loss_policy_3: 0.04221
	accuracy_policy_3: 0.78566
	loss_value_3: 0.07827
	loss_reward_3: 0.015
	loss_policy_4: 0.04229
	accuracy_policy_4: 0.78773
	loss_value_4: 0.08021
	loss_reward_4: 0.01663
	loss_policy_5: 0.04239
	accuracy_policy_5: 0.78309
	loss_value_5: 0.08181
	loss_reward_5: 0.01813
	loss_policy: 0.42084
	loss_value: 0.75457
	loss_reward: 0.07313
[2025-05-07 10:36:34] nn step 4350, lr: 0.1.
	loss_policy_0: 0.19866
	accuracy_policy_0: 0.78172
	loss_value_0: 0.33634
	loss_policy_1: 0.03966
	accuracy_policy_1: 0.78355
	loss_value_1: 0.0693
	loss_reward_1: 0.00866
	loss_policy_2: 0.03986
	accuracy_policy_2: 0.78145
	loss_value_2: 0.07138
	loss_reward_2: 0.01383
	loss_policy_3: 0.03975
	accuracy_policy_3: 0.78637
	loss_value_3: 0.07289
	loss_reward_3: 0.01398
	loss_policy_4: 0.04008
	accuracy_policy_4: 0.78191
	loss_value_4: 0.07468
	loss_reward_4: 0.01551
	loss_policy_5: 0.04003
	accuracy_policy_5: 0.78723
	loss_value_5: 0.07633
	loss_reward_5: 0.01693
	loss_policy: 0.39804
	loss_value: 0.70091
	loss_reward: 0.06892
[2025-05-07 10:36:42] nn step 4400, lr: 0.1.
	loss_policy_0: 0.21784
	accuracy_policy_0: 0.77805
	loss_value_0: 0.36966
	loss_policy_1: 0.04329
	accuracy_policy_1: 0.78207
	loss_value_1: 0.07582
	loss_reward_1: 0.00939
	loss_policy_2: 0.04339
	accuracy_policy_2: 0.78488
	loss_value_2: 0.07766
	loss_reward_2: 0.01452
	loss_policy_3: 0.04355
	accuracy_policy_3: 0.78789
	loss_value_3: 0.07998
	loss_reward_3: 0.01515
	loss_policy_4: 0.04372
	accuracy_policy_4: 0.78613
	loss_value_4: 0.08167
	loss_reward_4: 0.01703
	loss_policy_5: 0.04369
	accuracy_policy_5: 0.78508
	loss_value_5: 0.08333
	loss_reward_5: 0.01823
	loss_policy: 0.43546
	loss_value: 0.76812
	loss_reward: 0.07432
Optimization_Done 4400
[2025-05-07 10:39:46] [command] train weight_iter_4400.pkl 4 23
[2025-05-07 10:39:53] nn step 4450, lr: 0.1.
	loss_policy_0: 0.18863
	accuracy_policy_0: 0.78438
	loss_value_0: 0.34448
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.78781
	loss_value_1: 0.07074
	loss_reward_1: 0.0084
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.78746
	loss_value_2: 0.07266
	loss_reward_2: 0.01337
	loss_policy_3: 0.03824
	accuracy_policy_3: 0.78879
	loss_value_3: 0.07434
	loss_reward_3: 0.01382
	loss_policy_4: 0.03812
	accuracy_policy_4: 0.78879
	loss_value_4: 0.07612
	loss_reward_4: 0.01537
	loss_policy_5: 0.03826
	accuracy_policy_5: 0.78855
	loss_value_5: 0.07738
	loss_reward_5: 0.01669
	loss_policy: 0.37901
	loss_value: 0.71573
	loss_reward: 0.06765
[2025-05-07 10:40:01] nn step 4500, lr: 0.1.
	loss_policy_0: 0.19825
	accuracy_policy_0: 0.78426
	loss_value_0: 0.35439
	loss_policy_1: 0.04033
	accuracy_policy_1: 0.78121
	loss_value_1: 0.0727
	loss_reward_1: 0.00852
	loss_policy_2: 0.04013
	accuracy_policy_2: 0.78488
	loss_value_2: 0.07445
	loss_reward_2: 0.01354
	loss_policy_3: 0.03977
	accuracy_policy_3: 0.79191
	loss_value_3: 0.07623
	loss_reward_3: 0.01438
	loss_policy_4: 0.04005
	accuracy_policy_4: 0.79168
	loss_value_4: 0.07797
	loss_reward_4: 0.01616
	loss_policy_5: 0.04001
	accuracy_policy_5: 0.79102
	loss_value_5: 0.07979
	loss_reward_5: 0.01709
	loss_policy: 0.39854
	loss_value: 0.73553
	loss_reward: 0.06969
[2025-05-07 10:40:09] nn step 4550, lr: 0.1.
	loss_policy_0: 0.21075
	accuracy_policy_0: 0.78648
	loss_value_0: 0.37376
	loss_policy_1: 0.04216
	accuracy_policy_1: 0.7875
	loss_value_1: 0.07704
	loss_reward_1: 0.0094
	loss_policy_2: 0.04228
	accuracy_policy_2: 0.78816
	loss_value_2: 0.07867
	loss_reward_2: 0.0143
	loss_policy_3: 0.042
	accuracy_policy_3: 0.79129
	loss_value_3: 0.08057
	loss_reward_3: 0.01497
	loss_policy_4: 0.04249
	accuracy_policy_4: 0.79348
	loss_value_4: 0.0822
	loss_reward_4: 0.01776
	loss_policy_5: 0.0423
	accuracy_policy_5: 0.79172
	loss_value_5: 0.08354
	loss_reward_5: 0.01918
	loss_policy: 0.42197
	loss_value: 0.77578
	loss_reward: 0.07562
[2025-05-07 10:40:16] nn step 4600, lr: 0.1.
	loss_policy_0: 0.18761
	accuracy_policy_0: 0.78594
	loss_value_0: 0.32937
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.79043
	loss_value_1: 0.06765
	loss_reward_1: 0.0082
	loss_policy_2: 0.03777
	accuracy_policy_2: 0.78836
	loss_value_2: 0.06963
	loss_reward_2: 0.01269
	loss_policy_3: 0.03765
	accuracy_policy_3: 0.78984
	loss_value_3: 0.07111
	loss_reward_3: 0.01354
	loss_policy_4: 0.03757
	accuracy_policy_4: 0.79066
	loss_value_4: 0.07287
	loss_reward_4: 0.01547
	loss_policy_5: 0.03783
	accuracy_policy_5: 0.79234
	loss_value_5: 0.07459
	loss_reward_5: 0.01573
	loss_policy: 0.3757
	loss_value: 0.68522
	loss_reward: 0.06562
Optimization_Done 4600
[2025-05-07 10:43:23] [command] train weight_iter_4600.pkl 5 24
[2025-05-07 10:43:32] nn step 4650, lr: 0.1.
	loss_policy_0: 0.20742
	accuracy_policy_0: 0.78031
	loss_value_0: 0.37897
	loss_policy_1: 0.04175
	accuracy_policy_1: 0.77941
	loss_value_1: 0.07772
	loss_reward_1: 0.00935
	loss_policy_2: 0.04219
	accuracy_policy_2: 0.78148
	loss_value_2: 0.07933
	loss_reward_2: 0.01474
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.78367
	loss_value_3: 0.08107
	loss_reward_3: 0.01497
	loss_policy_4: 0.04296
	accuracy_policy_4: 0.77914
	loss_value_4: 0.08303
	loss_reward_4: 0.01729
	loss_policy_5: 0.04279
	accuracy_policy_5: 0.77637
	loss_value_5: 0.08469
	loss_reward_5: 0.01861
	loss_policy: 0.41939
	loss_value: 0.7848
	loss_reward: 0.07496
[2025-05-07 10:43:39] nn step 4700, lr: 0.1.
	loss_policy_0: 0.19387
	accuracy_policy_0: 0.78055
	loss_value_0: 0.34356
	loss_policy_1: 0.0391
	accuracy_policy_1: 0.78316
	loss_value_1: 0.07051
	loss_reward_1: 0.0088
	loss_policy_2: 0.03892
	accuracy_policy_2: 0.78012
	loss_value_2: 0.07231
	loss_reward_2: 0.01313
	loss_policy_3: 0.03898
	accuracy_policy_3: 0.78137
	loss_value_3: 0.07421
	loss_reward_3: 0.014
	loss_policy_4: 0.03892
	accuracy_policy_4: 0.78648
	loss_value_4: 0.07598
	loss_reward_4: 0.01618
	loss_policy_5: 0.03893
	accuracy_policy_5: 0.78613
	loss_value_5: 0.07769
	loss_reward_5: 0.01698
	loss_policy: 0.38872
	loss_value: 0.71428
	loss_reward: 0.0691
[2025-05-07 10:43:47] nn step 4750, lr: 0.1.
	loss_policy_0: 0.18575
	accuracy_policy_0: 0.78488
	loss_value_0: 0.32992
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.77844
	loss_value_1: 0.06787
	loss_reward_1: 0.0084
	loss_policy_2: 0.03713
	accuracy_policy_2: 0.78621
	loss_value_2: 0.06989
	loss_reward_2: 0.013
	loss_policy_3: 0.03721
	accuracy_policy_3: 0.78766
	loss_value_3: 0.07159
	loss_reward_3: 0.01371
	loss_policy_4: 0.03711
	accuracy_policy_4: 0.7891
	loss_value_4: 0.07335
	loss_reward_4: 0.01563
	loss_policy_5: 0.03732
	accuracy_policy_5: 0.78902
	loss_value_5: 0.07504
	loss_reward_5: 0.01629
	loss_policy: 0.37193
	loss_value: 0.68765
	loss_reward: 0.06702
[2025-05-07 10:43:55] nn step 4800, lr: 0.1.
	loss_policy_0: 0.20019
	accuracy_policy_0: 0.78207
	loss_value_0: 0.35324
	loss_policy_1: 0.03986
	accuracy_policy_1: 0.77711
	loss_value_1: 0.07284
	loss_reward_1: 0.00873
	loss_policy_2: 0.04025
	accuracy_policy_2: 0.78242
	loss_value_2: 0.07503
	loss_reward_2: 0.01423
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.78676
	loss_value_3: 0.07677
	loss_reward_3: 0.01485
	loss_policy_4: 0.04034
	accuracy_policy_4: 0.78664
	loss_value_4: 0.07885
	loss_reward_4: 0.01622
	loss_policy_5: 0.04016
	accuracy_policy_5: 0.79035
	loss_value_5: 0.08028
	loss_reward_5: 0.01742
	loss_policy: 0.40076
	loss_value: 0.73702
	loss_reward: 0.07146
Optimization_Done 4800
[2025-05-07 10:47:02] [command] train weight_iter_4800.pkl 6 25
[2025-05-07 10:47:11] nn step 4850, lr: 0.1.
	loss_policy_0: 0.2146
	accuracy_policy_0: 0.77164
	loss_value_0: 0.38184
	loss_policy_1: 0.04298
	accuracy_policy_1: 0.77484
	loss_value_1: 0.07839
	loss_reward_1: 0.00945
	loss_policy_2: 0.04308
	accuracy_policy_2: 0.77723
	loss_value_2: 0.08029
	loss_reward_2: 0.01469
	loss_policy_3: 0.04316
	accuracy_policy_3: 0.77938
	loss_value_3: 0.08214
	loss_reward_3: 0.01541
	loss_policy_4: 0.04311
	accuracy_policy_4: 0.78098
	loss_value_4: 0.08423
	loss_reward_4: 0.01726
	loss_policy_5: 0.0432
	accuracy_policy_5: 0.77625
	loss_value_5: 0.08592
	loss_reward_5: 0.01887
	loss_policy: 0.43012
	loss_value: 0.79282
	loss_reward: 0.07568
[2025-05-07 10:47:19] nn step 4900, lr: 0.1.
	loss_policy_0: 0.20118
	accuracy_policy_0: 0.7675
	loss_value_0: 0.34894
	loss_policy_1: 0.03997
	accuracy_policy_1: 0.77574
	loss_value_1: 0.07128
	loss_reward_1: 0.00851
	loss_policy_2: 0.04027
	accuracy_policy_2: 0.77469
	loss_value_2: 0.07326
	loss_reward_2: 0.01404
	loss_policy_3: 0.04027
	accuracy_policy_3: 0.77844
	loss_value_3: 0.07474
	loss_reward_3: 0.01437
	loss_policy_4: 0.04019
	accuracy_policy_4: 0.78062
	loss_value_4: 0.07655
	loss_reward_4: 0.01629
	loss_policy_5: 0.04045
	accuracy_policy_5: 0.77641
	loss_value_5: 0.0782
	loss_reward_5: 0.01732
	loss_policy: 0.40234
	loss_value: 0.72296
	loss_reward: 0.07052
[2025-05-07 10:47:27] nn step 4950, lr: 0.1.
	loss_policy_0: 0.20824
	accuracy_policy_0: 0.77078
	loss_value_0: 0.36078
	loss_policy_1: 0.0417
	accuracy_policy_1: 0.77762
	loss_value_1: 0.07439
	loss_reward_1: 0.0093
	loss_policy_2: 0.04165
	accuracy_policy_2: 0.77855
	loss_value_2: 0.07623
	loss_reward_2: 0.01424
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.78203
	loss_value_3: 0.07857
	loss_reward_3: 0.01468
	loss_policy_4: 0.04203
	accuracy_policy_4: 0.7793
	loss_value_4: 0.08046
	loss_reward_4: 0.01725
	loss_policy_5: 0.04193
	accuracy_policy_5: 0.77855
	loss_value_5: 0.08204
	loss_reward_5: 0.01804
	loss_policy: 0.41717
	loss_value: 0.75248
	loss_reward: 0.07351
[2025-05-07 10:47:33] nn step 5000, lr: 0.1.
	loss_policy_0: 0.19667
	accuracy_policy_0: 0.77098
	loss_value_0: 0.33883
	loss_policy_1: 0.03956
	accuracy_policy_1: 0.77445
	loss_value_1: 0.06963
	loss_reward_1: 0.00859
	loss_policy_2: 0.0395
	accuracy_policy_2: 0.77914
	loss_value_2: 0.07159
	loss_reward_2: 0.01368
	loss_policy_3: 0.03943
	accuracy_policy_3: 0.78039
	loss_value_3: 0.07346
	loss_reward_3: 0.01424
	loss_policy_4: 0.03973
	accuracy_policy_4: 0.7798
	loss_value_4: 0.07552
	loss_reward_4: 0.01573
	loss_policy_5: 0.03946
	accuracy_policy_5: 0.7791
	loss_value_5: 0.07691
	loss_reward_5: 0.01701
	loss_policy: 0.39435
	loss_value: 0.70595
	loss_reward: 0.06924
Optimization_Done 5000
[2025-05-07 10:50:40] [command] train weight_iter_5000.pkl 7 26
[2025-05-07 10:50:49] nn step 5050, lr: 0.1.
	loss_policy_0: 0.19134
	accuracy_policy_0: 0.77289
	loss_value_0: 0.33971
	loss_policy_1: 0.03827
	accuracy_policy_1: 0.77617
	loss_value_1: 0.06955
	loss_reward_1: 0.00889
	loss_policy_2: 0.03846
	accuracy_policy_2: 0.77547
	loss_value_2: 0.07146
	loss_reward_2: 0.01339
	loss_policy_3: 0.0385
	accuracy_policy_3: 0.77719
	loss_value_3: 0.07322
	loss_reward_3: 0.014
	loss_policy_4: 0.03869
	accuracy_policy_4: 0.7791
	loss_value_4: 0.07479
	loss_reward_4: 0.01591
	loss_policy_5: 0.03852
	accuracy_policy_5: 0.77512
	loss_value_5: 0.07624
	loss_reward_5: 0.01668
	loss_policy: 0.38377
	loss_value: 0.70497
	loss_reward: 0.06887
[2025-05-07 10:50:56] nn step 5100, lr: 0.1.
	loss_policy_0: 0.20311
	accuracy_policy_0: 0.77168
	loss_value_0: 0.35095
	loss_policy_1: 0.04041
	accuracy_policy_1: 0.77617
	loss_value_1: 0.07186
	loss_reward_1: 0.00859
	loss_policy_2: 0.04071
	accuracy_policy_2: 0.77723
	loss_value_2: 0.07371
	loss_reward_2: 0.01384
	loss_policy_3: 0.04072
	accuracy_policy_3: 0.78391
	loss_value_3: 0.07538
	loss_reward_3: 0.01463
	loss_policy_4: 0.04091
	accuracy_policy_4: 0.78328
	loss_value_4: 0.07714
	loss_reward_4: 0.01652
	loss_policy_5: 0.0408
	accuracy_policy_5: 0.7816
	loss_value_5: 0.07875
	loss_reward_5: 0.01726
	loss_policy: 0.40666
	loss_value: 0.72778
	loss_reward: 0.07085
[2025-05-07 10:51:04] nn step 5150, lr: 0.1.
	loss_policy_0: 0.20335
	accuracy_policy_0: 0.77426
	loss_value_0: 0.35113
	loss_policy_1: 0.04054
	accuracy_policy_1: 0.77934
	loss_value_1: 0.0722
	loss_reward_1: 0.00892
	loss_policy_2: 0.04064
	accuracy_policy_2: 0.78031
	loss_value_2: 0.07423
	loss_reward_2: 0.01403
	loss_policy_3: 0.04073
	accuracy_policy_3: 0.78125
	loss_value_3: 0.07591
	loss_reward_3: 0.01468
	loss_policy_4: 0.04083
	accuracy_policy_4: 0.7834
	loss_value_4: 0.07787
	loss_reward_4: 0.01657
	loss_policy_5: 0.04082
	accuracy_policy_5: 0.77848
	loss_value_5: 0.07948
	loss_reward_5: 0.01725
	loss_policy: 0.40689
	loss_value: 0.73082
	loss_reward: 0.07144
[2025-05-07 10:51:12] nn step 5200, lr: 0.1.
	loss_policy_0: 0.21746
	accuracy_policy_0: 0.77129
	loss_value_0: 0.37689
	loss_policy_1: 0.04323
	accuracy_policy_1: 0.77609
	loss_value_1: 0.07716
	loss_reward_1: 0.00965
	loss_policy_2: 0.04355
	accuracy_policy_2: 0.77445
	loss_value_2: 0.07934
	loss_reward_2: 0.01502
	loss_policy_3: 0.04351
	accuracy_policy_3: 0.77789
	loss_value_3: 0.08121
	loss_reward_3: 0.01583
	loss_policy_4: 0.04337
	accuracy_policy_4: 0.77988
	loss_value_4: 0.08321
	loss_reward_4: 0.01777
	loss_policy_5: 0.04311
	accuracy_policy_5: 0.78648
	loss_value_5: 0.08486
	loss_reward_5: 0.01897
	loss_policy: 0.43423
	loss_value: 0.78268
	loss_reward: 0.07725
Optimization_Done 5200
[2025-05-07 10:54:16] [command] train weight_iter_5200.pkl 8 27
[2025-05-07 10:54:26] nn step 5250, lr: 0.1.
	loss_policy_0: 0.22908
	accuracy_policy_0: 0.75602
	loss_value_0: 0.40633
	loss_policy_1: 0.04537
	accuracy_policy_1: 0.76496
	loss_value_1: 0.08298
	loss_reward_1: 0.00933
	loss_policy_2: 0.04563
	accuracy_policy_2: 0.76555
	loss_value_2: 0.08468
	loss_reward_2: 0.01507
	loss_policy_3: 0.04608
	accuracy_policy_3: 0.76371
	loss_value_3: 0.08624
	loss_reward_3: 0.01649
	loss_policy_4: 0.04546
	accuracy_policy_4: 0.76969
	loss_value_4: 0.08806
	loss_reward_4: 0.01868
	loss_policy_5: 0.04547
	accuracy_policy_5: 0.77023
	loss_value_5: 0.0895
	loss_reward_5: 0.01912
	loss_policy: 0.45708
	loss_value: 0.83779
	loss_reward: 0.07869
[2025-05-07 10:54:34] nn step 5300, lr: 0.1.
	loss_policy_0: 0.2303
	accuracy_policy_0: 0.75793
	loss_value_0: 0.3951
	loss_policy_1: 0.04573
	accuracy_policy_1: 0.76258
	loss_value_1: 0.08118
	loss_reward_1: 0.00929
	loss_policy_2: 0.04584
	accuracy_policy_2: 0.76578
	loss_value_2: 0.08285
	loss_reward_2: 0.01504
	loss_policy_3: 0.04619
	accuracy_policy_3: 0.76582
	loss_value_3: 0.08484
	loss_reward_3: 0.01599
	loss_policy_4: 0.04603
	accuracy_policy_4: 0.76938
	loss_value_4: 0.08674
	loss_reward_4: 0.01815
	loss_policy_5: 0.0464
	accuracy_policy_5: 0.76418
	loss_value_5: 0.08867
	loss_reward_5: 0.01932
	loss_policy: 0.46048
	loss_value: 0.81938
	loss_reward: 0.0778
[2025-05-07 10:54:41] nn step 5350, lr: 0.1.
	loss_policy_0: 0.20413
	accuracy_policy_0: 0.76309
	loss_value_0: 0.35103
	loss_policy_1: 0.04092
	accuracy_policy_1: 0.76273
	loss_value_1: 0.07217
	loss_reward_1: 0.00851
	loss_policy_2: 0.04117
	accuracy_policy_2: 0.76895
	loss_value_2: 0.0736
	loss_reward_2: 0.01348
	loss_policy_3: 0.04107
	accuracy_policy_3: 0.7691
	loss_value_3: 0.07525
	loss_reward_3: 0.01456
	loss_policy_4: 0.04109
	accuracy_policy_4: 0.76938
	loss_value_4: 0.07664
	loss_reward_4: 0.01589
	loss_policy_5: 0.041
	accuracy_policy_5: 0.77195
	loss_value_5: 0.07833
	loss_reward_5: 0.01739
	loss_policy: 0.40939
	loss_value: 0.72702
	loss_reward: 0.06984
[2025-05-07 10:54:49] nn step 5400, lr: 0.1.
	loss_policy_0: 0.21535
	accuracy_policy_0: 0.7609
	loss_value_0: 0.3692
	loss_policy_1: 0.04296
	accuracy_policy_1: 0.76895
	loss_value_1: 0.0753
	loss_reward_1: 0.00876
	loss_policy_2: 0.04309
	accuracy_policy_2: 0.7702
	loss_value_2: 0.077
	loss_reward_2: 0.01401
	loss_policy_3: 0.04323
	accuracy_policy_3: 0.76668
	loss_value_3: 0.0788
	loss_reward_3: 0.0151
	loss_policy_4: 0.0433
	accuracy_policy_4: 0.77285
	loss_value_4: 0.0803
	loss_reward_4: 0.01721
	loss_policy_5: 0.04319
	accuracy_policy_5: 0.77281
	loss_value_5: 0.08182
	loss_reward_5: 0.01787
	loss_policy: 0.43113
	loss_value: 0.76242
	loss_reward: 0.07294
Optimization_Done 5400
[2025-05-07 10:57:56] [command] train weight_iter_5400.pkl 9 28
[2025-05-07 10:58:04] nn step 5450, lr: 0.1.
	loss_policy_0: 0.22375
	accuracy_policy_0: 0.74914
	loss_value_0: 0.38657
	loss_policy_1: 0.0444
	accuracy_policy_1: 0.75668
	loss_value_1: 0.07913
	loss_reward_1: 0.00944
	loss_policy_2: 0.04436
	accuracy_policy_2: 0.75809
	loss_value_2: 0.08091
	loss_reward_2: 0.0152
	loss_policy_3: 0.04419
	accuracy_policy_3: 0.76312
	loss_value_3: 0.08246
	loss_reward_3: 0.0153
	loss_policy_4: 0.04408
	accuracy_policy_4: 0.76559
	loss_value_4: 0.08442
	loss_reward_4: 0.01746
	loss_policy_5: 0.04434
	accuracy_policy_5: 0.76508
	loss_value_5: 0.08591
	loss_reward_5: 0.01877
	loss_policy: 0.44511
	loss_value: 0.7994
	loss_reward: 0.07616
[2025-05-07 10:58:12] nn step 5500, lr: 0.1.
	loss_policy_0: 0.21479
	accuracy_policy_0: 0.75309
	loss_value_0: 0.36514
	loss_policy_1: 0.0428
	accuracy_policy_1: 0.75828
	loss_value_1: 0.0749
	loss_reward_1: 0.00868
	loss_policy_2: 0.04268
	accuracy_policy_2: 0.75945
	loss_value_2: 0.07666
	loss_reward_2: 0.01415
	loss_policy_3: 0.04259
	accuracy_policy_3: 0.76723
	loss_value_3: 0.07863
	loss_reward_3: 0.01532
	loss_policy_4: 0.04291
	accuracy_policy_4: 0.765
	loss_value_4: 0.08007
	loss_reward_4: 0.01679
	loss_policy_5: 0.04246
	accuracy_policy_5: 0.76762
	loss_value_5: 0.08152
	loss_reward_5: 0.01755
	loss_policy: 0.42823
	loss_value: 0.75692
	loss_reward: 0.07248
[2025-05-07 10:58:20] nn step 5550, lr: 0.1.
	loss_policy_0: 0.20803
	accuracy_policy_0: 0.75691
	loss_value_0: 0.34992
	loss_policy_1: 0.04167
	accuracy_policy_1: 0.75918
	loss_value_1: 0.07179
	loss_reward_1: 0.00839
	loss_policy_2: 0.04158
	accuracy_policy_2: 0.76008
	loss_value_2: 0.07366
	loss_reward_2: 0.01334
	loss_policy_3: 0.04169
	accuracy_policy_3: 0.7666
	loss_value_3: 0.07553
	loss_reward_3: 0.01473
	loss_policy_4: 0.04145
	accuracy_policy_4: 0.76625
	loss_value_4: 0.07693
	loss_reward_4: 0.01629
	loss_policy_5: 0.04125
	accuracy_policy_5: 0.77047
	loss_value_5: 0.07863
	loss_reward_5: 0.0172
	loss_policy: 0.41567
	loss_value: 0.72646
	loss_reward: 0.06995
[2025-05-07 10:58:27] nn step 5600, lr: 0.1.
	loss_policy_0: 0.20244
	accuracy_policy_0: 0.75555
	loss_value_0: 0.34251
	loss_policy_1: 0.03998
	accuracy_policy_1: 0.76762
	loss_value_1: 0.06991
	loss_reward_1: 0.00888
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.76465
	loss_value_2: 0.07163
	loss_reward_2: 0.0138
	loss_policy_3: 0.03999
	accuracy_policy_3: 0.77438
	loss_value_3: 0.07366
	loss_reward_3: 0.01464
	loss_policy_4: 0.03987
	accuracy_policy_4: 0.7707
	loss_value_4: 0.07545
	loss_reward_4: 0.0157
	loss_policy_5: 0.04
	accuracy_policy_5: 0.7727
	loss_value_5: 0.07663
	loss_reward_5: 0.01707
	loss_policy: 0.40257
	loss_value: 0.70979
	loss_reward: 0.07008
Optimization_Done 5600
[2025-05-07 11:01:32] [command] train weight_iter_5600.pkl 10 29
[2025-05-07 11:01:42] nn step 5650, lr: 0.1.
	loss_policy_0: 0.20759
	accuracy_policy_0: 0.75941
	loss_value_0: 0.35714
	loss_policy_1: 0.04167
	accuracy_policy_1: 0.75762
	loss_value_1: 0.07279
	loss_reward_1: 0.00871
	loss_policy_2: 0.04152
	accuracy_policy_2: 0.76766
	loss_value_2: 0.07468
	loss_reward_2: 0.0134
	loss_policy_3: 0.04134
	accuracy_policy_3: 0.77129
	loss_value_3: 0.07611
	loss_reward_3: 0.01467
	loss_policy_4: 0.04149
	accuracy_policy_4: 0.76867
	loss_value_4: 0.07776
	loss_reward_4: 0.01688
	loss_policy_5: 0.04144
	accuracy_policy_5: 0.76887
	loss_value_5: 0.07946
	loss_reward_5: 0.01741
	loss_policy: 0.41505
	loss_value: 0.73794
	loss_reward: 0.07107
[2025-05-07 11:01:48] nn step 5700, lr: 0.1.
	loss_policy_0: 0.21587
	accuracy_policy_0: 0.75699
	loss_value_0: 0.36944
	loss_policy_1: 0.04321
	accuracy_policy_1: 0.75945
	loss_value_1: 0.07575
	loss_reward_1: 0.00939
	loss_policy_2: 0.04308
	accuracy_policy_2: 0.76332
	loss_value_2: 0.07747
	loss_reward_2: 0.01441
	loss_policy_3: 0.04314
	accuracy_policy_3: 0.76297
	loss_value_3: 0.07932
	loss_reward_3: 0.01545
	loss_policy_4: 0.04307
	accuracy_policy_4: 0.76668
	loss_value_4: 0.08147
	loss_reward_4: 0.01772
	loss_policy_5: 0.04326
	accuracy_policy_5: 0.76875
	loss_value_5: 0.08306
	loss_reward_5: 0.01802
	loss_policy: 0.43164
	loss_value: 0.76652
	loss_reward: 0.07498
[2025-05-07 11:01:56] nn step 5750, lr: 0.1.
	loss_policy_0: 0.19132
	accuracy_policy_0: 0.75969
	loss_value_0: 0.32179
	loss_policy_1: 0.03831
	accuracy_policy_1: 0.76137
	loss_value_1: 0.06626
	loss_reward_1: 0.00787
	loss_policy_2: 0.03837
	accuracy_policy_2: 0.76699
	loss_value_2: 0.06814
	loss_reward_2: 0.01251
	loss_policy_3: 0.03804
	accuracy_policy_3: 0.76859
	loss_value_3: 0.0697
	loss_reward_3: 0.01353
	loss_policy_4: 0.03784
	accuracy_policy_4: 0.77113
	loss_value_4: 0.07117
	loss_reward_4: 0.01518
	loss_policy_5: 0.03834
	accuracy_policy_5: 0.77047
	loss_value_5: 0.07281
	loss_reward_5: 0.01603
	loss_policy: 0.38222
	loss_value: 0.66988
	loss_reward: 0.06513
[2025-05-07 11:02:04] nn step 5800, lr: 0.1.
	loss_policy_0: 0.21774
	accuracy_policy_0: 0.75691
	loss_value_0: 0.37096
	loss_policy_1: 0.04327
	accuracy_policy_1: 0.76574
	loss_value_1: 0.07643
	loss_reward_1: 0.00881
	loss_policy_2: 0.04369
	accuracy_policy_2: 0.76539
	loss_value_2: 0.07828
	loss_reward_2: 0.01415
	loss_policy_3: 0.04301
	accuracy_policy_3: 0.77375
	loss_value_3: 0.0801
	loss_reward_3: 0.01538
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.77387
	loss_value_4: 0.08179
	loss_reward_4: 0.01749
	loss_policy_5: 0.04336
	accuracy_policy_5: 0.77055
	loss_value_5: 0.08326
	loss_reward_5: 0.01819
	loss_policy: 0.43429
	loss_value: 0.77081
	loss_reward: 0.07402
Optimization_Done 5800
[2025-05-07 11:05:10] [command] train weight_iter_5800.pkl 11 30
[2025-05-07 11:05:19] nn step 5850, lr: 0.1.
	loss_policy_0: 0.20812
	accuracy_policy_0: 0.75105
	loss_value_0: 0.35894
	loss_policy_1: 0.04176
	accuracy_policy_1: 0.75418
	loss_value_1: 0.07344
	loss_reward_1: 0.00904
	loss_policy_2: 0.04176
	accuracy_policy_2: 0.75684
	loss_value_2: 0.07499
	loss_reward_2: 0.01384
	loss_policy_3: 0.04169
	accuracy_policy_3: 0.76492
	loss_value_3: 0.07676
	loss_reward_3: 0.01512
	loss_policy_4: 0.0417
	accuracy_policy_4: 0.7609
	loss_value_4: 0.0784
	loss_reward_4: 0.01659
	loss_policy_5: 0.04141
	accuracy_policy_5: 0.76457
	loss_value_5: 0.07968
	loss_reward_5: 0.01747
	loss_policy: 0.41644
	loss_value: 0.7422
	loss_reward: 0.07206
[2025-05-07 11:05:27] nn step 5900, lr: 0.1.
	loss_policy_0: 0.21298
	accuracy_policy_0: 0.75164
	loss_value_0: 0.35808
	loss_policy_1: 0.04275
	accuracy_policy_1: 0.75562
	loss_value_1: 0.07362
	loss_reward_1: 0.00885
	loss_policy_2: 0.04241
	accuracy_policy_2: 0.76047
	loss_value_2: 0.0756
	loss_reward_2: 0.0138
	loss_policy_3: 0.04241
	accuracy_policy_3: 0.76715
	loss_value_3: 0.07725
	loss_reward_3: 0.01504
	loss_policy_4: 0.04226
	accuracy_policy_4: 0.76574
	loss_value_4: 0.07911
	loss_reward_4: 0.01711
	loss_policy_5: 0.04265
	accuracy_policy_5: 0.76402
	loss_value_5: 0.08076
	loss_reward_5: 0.01776
	loss_policy: 0.42545
	loss_value: 0.74442
	loss_reward: 0.07256
[2025-05-07 11:05:35] nn step 5950, lr: 0.1.
	loss_policy_0: 0.2154
	accuracy_policy_0: 0.74289
	loss_value_0: 0.34947
	loss_policy_1: 0.04142
	accuracy_policy_1: 0.75996
	loss_value_1: 0.07142
	loss_reward_1: 0.00861
	loss_policy_2: 0.04186
	accuracy_policy_2: 0.75906
	loss_value_2: 0.07333
	loss_reward_2: 0.01363
	loss_policy_3: 0.04194
	accuracy_policy_3: 0.76312
	loss_value_3: 0.07507
	loss_reward_3: 0.01439
	loss_policy_4: 0.0416
	accuracy_policy_4: 0.76457
	loss_value_4: 0.07654
	loss_reward_4: 0.01672
	loss_policy_5: 0.04192
	accuracy_policy_5: 0.76605
	loss_value_5: 0.07796
	loss_reward_5: 0.01756
	loss_policy: 0.42414
	loss_value: 0.72378
	loss_reward: 0.07091
[2025-05-07 11:05:41] nn step 6000, lr: 0.1.
	loss_policy_0: 0.20971
	accuracy_policy_0: 0.73996
	loss_value_0: 0.34634
	loss_policy_1: 0.04101
	accuracy_policy_1: 0.75723
	loss_value_1: 0.07133
	loss_reward_1: 0.00868
	loss_policy_2: 0.04076
	accuracy_policy_2: 0.76145
	loss_value_2: 0.07315
	loss_reward_2: 0.01355
	loss_policy_3: 0.04061
	accuracy_policy_3: 0.7634
	loss_value_3: 0.07458
	loss_reward_3: 0.01426
	loss_policy_4: 0.0407
	accuracy_policy_4: 0.76645
	loss_value_4: 0.07598
	loss_reward_4: 0.01649
	loss_policy_5: 0.04047
	accuracy_policy_5: 0.76914
	loss_value_5: 0.07793
	loss_reward_5: 0.01653
	loss_policy: 0.41327
	loss_value: 0.71931
	loss_reward: 0.06952
Optimization_Done 6000
[2025-05-07 11:08:46] [command] train weight_iter_6000.pkl 12 31
[2025-05-07 11:08:55] nn step 6050, lr: 0.1.
	loss_policy_0: 0.21677
	accuracy_policy_0: 0.73816
	loss_value_0: 0.36306
	loss_policy_1: 0.04313
	accuracy_policy_1: 0.74891
	loss_value_1: 0.07451
	loss_reward_1: 0.0088
	loss_policy_2: 0.04308
	accuracy_policy_2: 0.75301
	loss_value_2: 0.07631
	loss_reward_2: 0.01396
	loss_policy_3: 0.04292
	accuracy_policy_3: 0.75277
	loss_value_3: 0.07802
	loss_reward_3: 0.01474
	loss_policy_4: 0.04281
	accuracy_policy_4: 0.75848
	loss_value_4: 0.07951
	loss_reward_4: 0.01685
	loss_policy_5: 0.04296
	accuracy_policy_5: 0.75801
	loss_value_5: 0.08104
	loss_reward_5: 0.0173
	loss_policy: 0.43167
	loss_value: 0.75245
	loss_reward: 0.07165
[2025-05-07 11:09:02] nn step 6100, lr: 0.1.
	loss_policy_0: 0.20438
	accuracy_policy_0: 0.74902
	loss_value_0: 0.34277
	loss_policy_1: 0.04097
	accuracy_policy_1: 0.75676
	loss_value_1: 0.07019
	loss_reward_1: 0.00853
	loss_policy_2: 0.04078
	accuracy_policy_2: 0.75465
	loss_value_2: 0.07213
	loss_reward_2: 0.01417
	loss_policy_3: 0.04076
	accuracy_policy_3: 0.7566
	loss_value_3: 0.07385
	loss_reward_3: 0.01455
	loss_policy_4: 0.04049
	accuracy_policy_4: 0.76371
	loss_value_4: 0.07529
	loss_reward_4: 0.01626
	loss_policy_5: 0.04069
	accuracy_policy_5: 0.76379
	loss_value_5: 0.07666
	loss_reward_5: 0.01672
	loss_policy: 0.40808
	loss_value: 0.71089
	loss_reward: 0.07022
[2025-05-07 11:09:10] nn step 6150, lr: 0.1.
	loss_policy_0: 0.23094
	accuracy_policy_0: 0.74375
	loss_value_0: 0.3854
	loss_policy_1: 0.046
	accuracy_policy_1: 0.75391
	loss_value_1: 0.0791
	loss_reward_1: 0.00964
	loss_policy_2: 0.04592
	accuracy_policy_2: 0.75211
	loss_value_2: 0.08124
	loss_reward_2: 0.01529
	loss_policy_3: 0.04575
	accuracy_policy_3: 0.75133
	loss_value_3: 0.08328
	loss_reward_3: 0.01619
	loss_policy_4: 0.04578
	accuracy_policy_4: 0.76012
	loss_value_4: 0.08523
	loss_reward_4: 0.01832
	loss_policy_5: 0.04591
	accuracy_policy_5: 0.75758
	loss_value_5: 0.08648
	loss_reward_5: 0.01925
	loss_policy: 0.4603
	loss_value: 0.80073
	loss_reward: 0.07867
[2025-05-07 11:09:18] nn step 6200, lr: 0.1.
	loss_policy_0: 0.20975
	accuracy_policy_0: 0.7502
	loss_value_0: 0.34949
	loss_policy_1: 0.04251
	accuracy_policy_1: 0.75445
	loss_value_1: 0.07171
	loss_reward_1: 0.00839
	loss_policy_2: 0.04207
	accuracy_policy_2: 0.7575
	loss_value_2: 0.07361
	loss_reward_2: 0.01373
	loss_policy_3: 0.04167
	accuracy_policy_3: 0.76211
	loss_value_3: 0.07557
	loss_reward_3: 0.01472
	loss_policy_4: 0.04202
	accuracy_policy_4: 0.76035
	loss_value_4: 0.07712
	loss_reward_4: 0.01648
	loss_policy_5: 0.04217
	accuracy_policy_5: 0.7577
	loss_value_5: 0.07878
	loss_reward_5: 0.01745
	loss_policy: 0.42018
	loss_value: 0.72628
	loss_reward: 0.07076
Optimization_Done 6200
[2025-05-07 11:12:26] [command] train weight_iter_6200.pkl 13 32
[2025-05-07 11:12:35] nn step 6250, lr: 0.1.
	loss_policy_0: 0.22455
	accuracy_policy_0: 0.72789
	loss_value_0: 0.3666
	loss_policy_1: 0.04428
	accuracy_policy_1: 0.74277
	loss_value_1: 0.07502
	loss_reward_1: 0.00921
	loss_policy_2: 0.04429
	accuracy_policy_2: 0.74371
	loss_value_2: 0.07704
	loss_reward_2: 0.01424
	loss_policy_3: 0.04427
	accuracy_policy_3: 0.74418
	loss_value_3: 0.0787
	loss_reward_3: 0.0152
	loss_policy_4: 0.0444
	accuracy_policy_4: 0.74539
	loss_value_4: 0.08025
	loss_reward_4: 0.01773
	loss_policy_5: 0.04433
	accuracy_policy_5: 0.74379
	loss_value_5: 0.08147
	loss_reward_5: 0.01792
	loss_policy: 0.44612
	loss_value: 0.75907
	loss_reward: 0.07432
[2025-05-07 11:12:43] nn step 6300, lr: 0.1.
	loss_policy_0: 0.21293
	accuracy_policy_0: 0.73688
	loss_value_0: 0.34511
	loss_policy_1: 0.04249
	accuracy_policy_1: 0.7409
	loss_value_1: 0.071
	loss_reward_1: 0.00856
	loss_policy_2: 0.04241
	accuracy_policy_2: 0.74516
	loss_value_2: 0.07313
	loss_reward_2: 0.0133
	loss_policy_3: 0.04227
	accuracy_policy_3: 0.7493
	loss_value_3: 0.07458
	loss_reward_3: 0.01421
	loss_policy_4: 0.04237
	accuracy_policy_4: 0.7457
	loss_value_4: 0.07631
	loss_reward_4: 0.01635
	loss_policy_5: 0.04231
	accuracy_policy_5: 0.74883
	loss_value_5: 0.07785
	loss_reward_5: 0.01674
	loss_policy: 0.42477
	loss_value: 0.71796
	loss_reward: 0.06917
[2025-05-07 11:12:50] nn step 6350, lr: 0.1.
	loss_policy_0: 0.2285
	accuracy_policy_0: 0.73883
	loss_value_0: 0.37971
	loss_policy_1: 0.04604
	accuracy_policy_1: 0.73746
	loss_value_1: 0.07768
	loss_reward_1: 0.00929
	loss_policy_2: 0.04581
	accuracy_policy_2: 0.74203
	loss_value_2: 0.07959
	loss_reward_2: 0.01477
	loss_policy_3: 0.04588
	accuracy_policy_3: 0.74242
	loss_value_3: 0.08139
	loss_reward_3: 0.01608
	loss_policy_4: 0.04572
	accuracy_policy_4: 0.74848
	loss_value_4: 0.08331
	loss_reward_4: 0.01809
	loss_policy_5: 0.04592
	accuracy_policy_5: 0.74668
	loss_value_5: 0.08539
	loss_reward_5: 0.01871
	loss_policy: 0.45787
	loss_value: 0.78709
	loss_reward: 0.07694
[2025-05-07 11:12:57] nn step 6400, lr: 0.1.
	loss_policy_0: 0.23038
	accuracy_policy_0: 0.73648
	loss_value_0: 0.37448
	loss_policy_1: 0.04598
	accuracy_policy_1: 0.74262
	loss_value_1: 0.07706
	loss_reward_1: 0.00937
	loss_policy_2: 0.04585
	accuracy_policy_2: 0.74742
	loss_value_2: 0.0785
	loss_reward_2: 0.01452
	loss_policy_3: 0.04614
	accuracy_policy_3: 0.74789
	loss_value_3: 0.08037
	loss_reward_3: 0.01607
	loss_policy_4: 0.04618
	accuracy_policy_4: 0.74828
	loss_value_4: 0.08244
	loss_reward_4: 0.01794
	loss_policy_5: 0.0459
	accuracy_policy_5: 0.75422
	loss_value_5: 0.0839
	loss_reward_5: 0.01858
	loss_policy: 0.46042
	loss_value: 0.77675
	loss_reward: 0.07648
Optimization_Done 6400
[2025-05-07 11:15:59] [command] train weight_iter_6400.pkl 14 33
[2025-05-07 11:16:07] nn step 6450, lr: 0.1.
	loss_policy_0: 0.23676
	accuracy_policy_0: 0.72406
	loss_value_0: 0.39044
	loss_policy_1: 0.0475
	accuracy_policy_1: 0.73168
	loss_value_1: 0.08028
	loss_reward_1: 0.0096
	loss_policy_2: 0.04731
	accuracy_policy_2: 0.73352
	loss_value_2: 0.082
	loss_reward_2: 0.0148
	loss_policy_3: 0.04735
	accuracy_policy_3: 0.735
	loss_value_3: 0.08334
	loss_reward_3: 0.01608
	loss_policy_4: 0.04725
	accuracy_policy_4: 0.73445
	loss_value_4: 0.08564
	loss_reward_4: 0.01847
	loss_policy_5: 0.04738
	accuracy_policy_5: 0.73699
	loss_value_5: 0.087
	loss_reward_5: 0.0186
	loss_policy: 0.47356
	loss_value: 0.8087
	loss_reward: 0.07755
[2025-05-07 11:16:15] nn step 6500, lr: 0.1.
	loss_policy_0: 0.22377
	accuracy_policy_0: 0.72613
	loss_value_0: 0.36227
	loss_policy_1: 0.04494
	accuracy_policy_1: 0.7316
	loss_value_1: 0.07419
	loss_reward_1: 0.00888
	loss_policy_2: 0.04479
	accuracy_policy_2: 0.73871
	loss_value_2: 0.07618
	loss_reward_2: 0.01396
	loss_policy_3: 0.04447
	accuracy_policy_3: 0.73883
	loss_value_3: 0.07815
	loss_reward_3: 0.01509
	loss_policy_4: 0.0449
	accuracy_policy_4: 0.73781
	loss_value_4: 0.07982
	loss_reward_4: 0.0165
	loss_policy_5: 0.04491
	accuracy_policy_5: 0.73992
	loss_value_5: 0.0813
	loss_reward_5: 0.01817
	loss_policy: 0.44778
	loss_value: 0.7519
	loss_reward: 0.0726
[2025-05-07 11:16:23] nn step 6550, lr: 0.1.
	loss_policy_0: 0.22357
	accuracy_policy_0: 0.72891
	loss_value_0: 0.36915
	loss_policy_1: 0.04505
	accuracy_policy_1: 0.73707
	loss_value_1: 0.07565
	loss_reward_1: 0.00914
	loss_policy_2: 0.0449
	accuracy_policy_2: 0.74688
	loss_value_2: 0.07733
	loss_reward_2: 0.0146
	loss_policy_3: 0.04477
	accuracy_policy_3: 0.74188
	loss_value_3: 0.07943
	loss_reward_3: 0.01561
	loss_policy_4: 0.04519
	accuracy_policy_4: 0.74105
	loss_value_4: 0.08115
	loss_reward_4: 0.01745
	loss_policy_5: 0.04498
	accuracy_policy_5: 0.74449
	loss_value_5: 0.08293
	loss_reward_5: 0.01846
	loss_policy: 0.44846
	loss_value: 0.76566
	loss_reward: 0.07527
[2025-05-07 11:16:31] nn step 6600, lr: 0.1.
	loss_policy_0: 0.23692
	accuracy_policy_0: 0.72969
	loss_value_0: 0.38559
	loss_policy_1: 0.04705
	accuracy_policy_1: 0.73984
	loss_value_1: 0.0792
	loss_reward_1: 0.00936
	loss_policy_2: 0.04736
	accuracy_policy_2: 0.74176
	loss_value_2: 0.08118
	loss_reward_2: 0.01499
	loss_policy_3: 0.04739
	accuracy_policy_3: 0.74758
	loss_value_3: 0.08304
	loss_reward_3: 0.01585
	loss_policy_4: 0.04701
	accuracy_policy_4: 0.74168
	loss_value_4: 0.08472
	loss_reward_4: 0.01844
	loss_policy_5: 0.04675
	accuracy_policy_5: 0.74586
	loss_value_5: 0.08642
	loss_reward_5: 0.01871
	loss_policy: 0.47249
	loss_value: 0.80015
	loss_reward: 0.07736
Optimization_Done 6600
[2025-05-07 11:19:36] [command] train weight_iter_6600.pkl 15 34
[2025-05-07 11:19:46] nn step 6650, lr: 0.1.
	loss_policy_0: 0.25359
	accuracy_policy_0: 0.67348
	loss_value_0: 0.38735
	loss_policy_1: 0.04722
	accuracy_policy_1: 0.7184
	loss_value_1: 0.07897
	loss_reward_1: 0.00993
	loss_policy_2: 0.04709
	accuracy_policy_2: 0.72719
	loss_value_2: 0.08074
	loss_reward_2: 0.01551
	loss_policy_3: 0.04728
	accuracy_policy_3: 0.72898
	loss_value_3: 0.08254
	loss_reward_3: 0.0162
	loss_policy_4: 0.04714
	accuracy_policy_4: 0.72969
	loss_value_4: 0.08403
	loss_reward_4: 0.0177
	loss_policy_5: 0.04703
	accuracy_policy_5: 0.72891
	loss_value_5: 0.08574
	loss_reward_5: 0.01849
	loss_policy: 0.48935
	loss_value: 0.79938
	loss_reward: 0.07783
[2025-05-07 11:19:54] nn step 6700, lr: 0.1.
	loss_policy_0: 0.23461
	accuracy_policy_0: 0.70125
	loss_value_0: 0.36181
	loss_policy_1: 0.04486
	accuracy_policy_1: 0.72859
	loss_value_1: 0.07406
	loss_reward_1: 0.00929
	loss_policy_2: 0.04478
	accuracy_policy_2: 0.73484
	loss_value_2: 0.07624
	loss_reward_2: 0.01442
	loss_policy_3: 0.04439
	accuracy_policy_3: 0.73801
	loss_value_3: 0.07776
	loss_reward_3: 0.01509
	loss_policy_4: 0.04429
	accuracy_policy_4: 0.73805
	loss_value_4: 0.07942
	loss_reward_4: 0.01641
	loss_policy_5: 0.04442
	accuracy_policy_5: 0.73965
	loss_value_5: 0.08104
	loss_reward_5: 0.01739
	loss_policy: 0.45735
	loss_value: 0.75034
	loss_reward: 0.07259
[2025-05-07 11:20:00] nn step 6750, lr: 0.1.
	loss_policy_0: 0.2273
	accuracy_policy_0: 0.71625
	loss_value_0: 0.35899
	loss_policy_1: 0.04486
	accuracy_policy_1: 0.73262
	loss_value_1: 0.07324
	loss_reward_1: 0.00904
	loss_policy_2: 0.04501
	accuracy_policy_2: 0.73797
	loss_value_2: 0.07538
	loss_reward_2: 0.01415
	loss_policy_3: 0.04471
	accuracy_policy_3: 0.74203
	loss_value_3: 0.07726
	loss_reward_3: 0.01523
	loss_policy_4: 0.04491
	accuracy_policy_4: 0.73969
	loss_value_4: 0.0787
	loss_reward_4: 0.01721
	loss_policy_5: 0.04535
	accuracy_policy_5: 0.73859
	loss_value_5: 0.08077
	loss_reward_5: 0.01741
	loss_policy: 0.45214
	loss_value: 0.74433
	loss_reward: 0.07305
[2025-05-07 11:20:08] nn step 6800, lr: 0.1.
	loss_policy_0: 0.23249
	accuracy_policy_0: 0.72215
	loss_value_0: 0.37104
	loss_policy_1: 0.04616
	accuracy_policy_1: 0.73363
	loss_value_1: 0.07602
	loss_reward_1: 0.00908
	loss_policy_2: 0.04615
	accuracy_policy_2: 0.73797
	loss_value_2: 0.078
	loss_reward_2: 0.01394
	loss_policy_3: 0.04659
	accuracy_policy_3: 0.73301
	loss_value_3: 0.08024
	loss_reward_3: 0.01515
	loss_policy_4: 0.04627
	accuracy_policy_4: 0.73828
	loss_value_4: 0.08187
	loss_reward_4: 0.01774
	loss_policy_5: 0.04668
	accuracy_policy_5: 0.73434
	loss_value_5: 0.08353
	loss_reward_5: 0.01801
	loss_policy: 0.46434
	loss_value: 0.77068
	loss_reward: 0.07392
Optimization_Done 6800
[2025-05-07 11:23:07] [command] train weight_iter_6800.pkl 16 35
[2025-05-07 11:23:16] nn step 6850, lr: 0.1.
	loss_policy_0: 0.24255
	accuracy_policy_0: 0.70293
	loss_value_0: 0.37707
	loss_policy_1: 0.04693
	accuracy_policy_1: 0.72031
	loss_value_1: 0.07699
	loss_reward_1: 0.00908
	loss_policy_2: 0.04708
	accuracy_policy_2: 0.72902
	loss_value_2: 0.07894
	loss_reward_2: 0.01435
	loss_policy_3: 0.04688
	accuracy_policy_3: 0.72359
	loss_value_3: 0.08086
	loss_reward_3: 0.0155
	loss_policy_4: 0.04669
	accuracy_policy_4: 0.73203
	loss_value_4: 0.08217
	loss_reward_4: 0.01726
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.72906
	loss_value_5: 0.0834
	loss_reward_5: 0.01762
	loss_policy: 0.47674
	loss_value: 0.77943
	loss_reward: 0.07381
[2025-05-07 11:23:23] nn step 6900, lr: 0.1.
	loss_policy_0: 0.2437
	accuracy_policy_0: 0.71289
	loss_value_0: 0.37912
	loss_policy_1: 0.04817
	accuracy_policy_1: 0.7241
	loss_value_1: 0.07787
	loss_reward_1: 0.00935
	loss_policy_2: 0.04843
	accuracy_policy_2: 0.72609
	loss_value_2: 0.07972
	loss_reward_2: 0.01409
	loss_policy_3: 0.04824
	accuracy_policy_3: 0.72887
	loss_value_3: 0.08154
	loss_reward_3: 0.01592
	loss_policy_4: 0.04739
	accuracy_policy_4: 0.73477
	loss_value_4: 0.08304
	loss_reward_4: 0.01789
	loss_policy_5: 0.04831
	accuracy_policy_5: 0.73191
	loss_value_5: 0.08442
	loss_reward_5: 0.01821
	loss_policy: 0.48423
	loss_value: 0.7857
	loss_reward: 0.07545
[2025-05-07 11:23:31] nn step 6950, lr: 0.1.
	loss_policy_0: 0.22694
	accuracy_policy_0: 0.71406
	loss_value_0: 0.35366
	loss_policy_1: 0.04533
	accuracy_policy_1: 0.7234
	loss_value_1: 0.07242
	loss_reward_1: 0.00893
	loss_policy_2: 0.04499
	accuracy_policy_2: 0.73184
	loss_value_2: 0.07394
	loss_reward_2: 0.01387
	loss_policy_3: 0.04486
	accuracy_policy_3: 0.73434
	loss_value_3: 0.07574
	loss_reward_3: 0.01494
	loss_policy_4: 0.04543
	accuracy_policy_4: 0.7277
	loss_value_4: 0.0773
	loss_reward_4: 0.01672
	loss_policy_5: 0.04507
	accuracy_policy_5: 0.73523
	loss_value_5: 0.07857
	loss_reward_5: 0.01704
	loss_policy: 0.45263
	loss_value: 0.73163
	loss_reward: 0.07151
[2025-05-07 11:23:39] nn step 7000, lr: 0.1.
	loss_policy_0: 0.23835
	accuracy_policy_0: 0.72109
	loss_value_0: 0.36993
	loss_policy_1: 0.04734
	accuracy_policy_1: 0.72629
	loss_value_1: 0.07611
	loss_reward_1: 0.00902
	loss_policy_2: 0.04711
	accuracy_policy_2: 0.73559
	loss_value_2: 0.07807
	loss_reward_2: 0.0144
	loss_policy_3: 0.04744
	accuracy_policy_3: 0.73234
	loss_value_3: 0.08009
	loss_reward_3: 0.01496
	loss_policy_4: 0.04722
	accuracy_policy_4: 0.73523
	loss_value_4: 0.08181
	loss_reward_4: 0.0174
	loss_policy_5: 0.04711
	accuracy_policy_5: 0.73367
	loss_value_5: 0.08396
	loss_reward_5: 0.01822
	loss_policy: 0.47457
	loss_value: 0.76997
	loss_reward: 0.074
Optimization_Done 7000
[2025-05-07 11:26:47] [command] train weight_iter_7000.pkl 17 36
[2025-05-07 11:26:57] nn step 7050, lr: 0.1.
	loss_policy_0: 0.23918
	accuracy_policy_0: 0.69719
	loss_value_0: 0.37058
	loss_policy_1: 0.04737
	accuracy_policy_1: 0.71367
	loss_value_1: 0.07574
	loss_reward_1: 0.00904
	loss_policy_2: 0.04718
	accuracy_policy_2: 0.71773
	loss_value_2: 0.07738
	loss_reward_2: 0.01372
	loss_policy_3: 0.04753
	accuracy_policy_3: 0.71895
	loss_value_3: 0.07917
	loss_reward_3: 0.01539
	loss_policy_4: 0.04718
	accuracy_policy_4: 0.72176
	loss_value_4: 0.08076
	loss_reward_4: 0.01656
	loss_policy_5: 0.04725
	accuracy_policy_5: 0.7216
	loss_value_5: 0.08242
	loss_reward_5: 0.01752
	loss_policy: 0.47569
	loss_value: 0.76604
	loss_reward: 0.07222
[2025-05-07 11:27:05] nn step 7100, lr: 0.1.
	loss_policy_0: 0.24493
	accuracy_policy_0: 0.70723
	loss_value_0: 0.37726
	loss_policy_1: 0.04901
	accuracy_policy_1: 0.71203
	loss_value_1: 0.07697
	loss_reward_1: 0.00918
	loss_policy_2: 0.04883
	accuracy_policy_2: 0.72316
	loss_value_2: 0.07902
	loss_reward_2: 0.01447
	loss_policy_3: 0.04838
	accuracy_policy_3: 0.72668
	loss_value_3: 0.08058
	loss_reward_3: 0.01558
	loss_policy_4: 0.04849
	accuracy_policy_4: 0.72996
	loss_value_4: 0.08233
	loss_reward_4: 0.01727
	loss_policy_5: 0.04871
	accuracy_policy_5: 0.72824
	loss_value_5: 0.08422
	loss_reward_5: 0.01761
	loss_policy: 0.48835
	loss_value: 0.78037
	loss_reward: 0.07411
[2025-05-07 11:27:11] nn step 7150, lr: 0.1.
	loss_policy_0: 0.23494
	accuracy_policy_0: 0.70902
	loss_value_0: 0.35456
	loss_policy_1: 0.04664
	accuracy_policy_1: 0.71859
	loss_value_1: 0.07249
	loss_reward_1: 0.00872
	loss_policy_2: 0.04674
	accuracy_policy_2: 0.72344
	loss_value_2: 0.07473
	loss_reward_2: 0.01373
	loss_policy_3: 0.04669
	accuracy_policy_3: 0.72984
	loss_value_3: 0.07664
	loss_reward_3: 0.01451
	loss_policy_4: 0.04689
	accuracy_policy_4: 0.72973
	loss_value_4: 0.07849
	loss_reward_4: 0.01686
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.7316
	loss_value_5: 0.08042
	loss_reward_5: 0.01722
	loss_policy: 0.4683
	loss_value: 0.73733
	loss_reward: 0.07105
[2025-05-07 11:27:19] nn step 7200, lr: 0.1.
	loss_policy_0: 0.26032
	accuracy_policy_0: 0.70988
	loss_value_0: 0.39672
	loss_policy_1: 0.05164
	accuracy_policy_1: 0.71773
	loss_value_1: 0.0816
	loss_reward_1: 0.00991
	loss_policy_2: 0.05136
	accuracy_policy_2: 0.72426
	loss_value_2: 0.08383
	loss_reward_2: 0.01534
	loss_policy_3: 0.0519
	accuracy_policy_3: 0.7227
	loss_value_3: 0.08604
	loss_reward_3: 0.01598
	loss_policy_4: 0.05144
	accuracy_policy_4: 0.72328
	loss_value_4: 0.08773
	loss_reward_4: 0.01829
	loss_policy_5: 0.05165
	accuracy_policy_5: 0.72418
	loss_value_5: 0.08924
	loss_reward_5: 0.01925
	loss_policy: 0.51831
	loss_value: 0.82517
	loss_reward: 0.07878
Optimization_Done 7200
[2025-05-07 11:30:25] [command] train weight_iter_7200.pkl 18 37
[2025-05-07 11:30:33] nn step 7250, lr: 0.1.
	loss_policy_0: 0.25135
	accuracy_policy_0: 0.6991
	loss_value_0: 0.38651
	loss_policy_1: 0.04994
	accuracy_policy_1: 0.7118
	loss_value_1: 0.0789
	loss_reward_1: 0.00929
	loss_policy_2: 0.05004
	accuracy_policy_2: 0.71547
	loss_value_2: 0.08135
	loss_reward_2: 0.01488
	loss_policy_3: 0.04955
	accuracy_policy_3: 0.71762
	loss_value_3: 0.08302
	loss_reward_3: 0.01576
	loss_policy_4: 0.04998
	accuracy_policy_4: 0.71652
	loss_value_4: 0.08475
	loss_reward_4: 0.01777
	loss_policy_5: 0.04993
	accuracy_policy_5: 0.7168
	loss_value_5: 0.08628
	loss_reward_5: 0.01838
	loss_policy: 0.50078
	loss_value: 0.8008
	loss_reward: 0.07608
[2025-05-07 11:30:41] nn step 7300, lr: 0.1.
	loss_policy_0: 0.24735
	accuracy_policy_0: 0.70168
	loss_value_0: 0.37595
	loss_policy_1: 0.04906
	accuracy_policy_1: 0.71184
	loss_value_1: 0.07688
	loss_reward_1: 0.00904
	loss_policy_2: 0.04889
	accuracy_policy_2: 0.71805
	loss_value_2: 0.07903
	loss_reward_2: 0.01425
	loss_policy_3: 0.04892
	accuracy_policy_3: 0.71812
	loss_value_3: 0.081
	loss_reward_3: 0.01514
	loss_policy_4: 0.04836
	accuracy_policy_4: 0.7243
	loss_value_4: 0.08297
	loss_reward_4: 0.0173
	loss_policy_5: 0.04898
	accuracy_policy_5: 0.71895
	loss_value_5: 0.08445
	loss_reward_5: 0.01807
	loss_policy: 0.49156
	loss_value: 0.78027
	loss_reward: 0.07381
[2025-05-07 11:30:49] nn step 7350, lr: 0.1.
	loss_policy_0: 0.25708
	accuracy_policy_0: 0.70406
	loss_value_0: 0.39188
	loss_policy_1: 0.05114
	accuracy_policy_1: 0.71348
	loss_value_1: 0.0796
	loss_reward_1: 0.00954
	loss_policy_2: 0.05041
	accuracy_policy_2: 0.71781
	loss_value_2: 0.08205
	loss_reward_2: 0.015
	loss_policy_3: 0.0508
	accuracy_policy_3: 0.72145
	loss_value_3: 0.08332
	loss_reward_3: 0.01607
	loss_policy_4: 0.05085
	accuracy_policy_4: 0.72086
	loss_value_4: 0.08491
	loss_reward_4: 0.01846
	loss_policy_5: 0.05093
	accuracy_policy_5: 0.71965
	loss_value_5: 0.08679
	loss_reward_5: 0.0189
	loss_policy: 0.5112
	loss_value: 0.80855
	loss_reward: 0.07796
[2025-05-07 11:30:56] nn step 7400, lr: 0.1.
	loss_policy_0: 0.25759
	accuracy_policy_0: 0.7075
	loss_value_0: 0.39294
	loss_policy_1: 0.05085
	accuracy_policy_1: 0.72086
	loss_value_1: 0.08044
	loss_reward_1: 0.00958
	loss_policy_2: 0.05094
	accuracy_policy_2: 0.72207
	loss_value_2: 0.08254
	loss_reward_2: 0.01558
	loss_policy_3: 0.05117
	accuracy_policy_3: 0.71816
	loss_value_3: 0.08455
	loss_reward_3: 0.01664
	loss_policy_4: 0.05103
	accuracy_policy_4: 0.72031
	loss_value_4: 0.08641
	loss_reward_4: 0.01838
	loss_policy_5: 0.05095
	accuracy_policy_5: 0.72305
	loss_value_5: 0.08787
	loss_reward_5: 0.01891
	loss_policy: 0.51254
	loss_value: 0.81475
	loss_reward: 0.0791
Optimization_Done 7400
[2025-05-07 11:33:54] [command] train weight_iter_7400.pkl 19 38
[2025-05-07 11:34:04] nn step 7450, lr: 0.1.
	loss_policy_0: 0.25859
	accuracy_policy_0: 0.69719
	loss_value_0: 0.39383
	loss_policy_1: 0.05104
	accuracy_policy_1: 0.70891
	loss_value_1: 0.08079
	loss_reward_1: 0.00933
	loss_policy_2: 0.05126
	accuracy_policy_2: 0.71395
	loss_value_2: 0.08264
	loss_reward_2: 0.01489
	loss_policy_3: 0.05128
	accuracy_policy_3: 0.7132
	loss_value_3: 0.08447
	loss_reward_3: 0.01603
	loss_policy_4: 0.05089
	accuracy_policy_4: 0.71918
	loss_value_4: 0.08598
	loss_reward_4: 0.01836
	loss_policy_5: 0.05129
	accuracy_policy_5: 0.71676
	loss_value_5: 0.08753
	loss_reward_5: 0.01857
	loss_policy: 0.51435
	loss_value: 0.81525
	loss_reward: 0.07718
[2025-05-07 11:34:12] nn step 7500, lr: 0.1.
	loss_policy_0: 0.2531
	accuracy_policy_0: 0.69738
	loss_value_0: 0.38195
	loss_policy_1: 0.05044
	accuracy_policy_1: 0.70871
	loss_value_1: 0.078
	loss_reward_1: 0.00956
	loss_policy_2: 0.04992
	accuracy_policy_2: 0.71105
	loss_value_2: 0.08019
	loss_reward_2: 0.01464
	loss_policy_3: 0.05017
	accuracy_policy_3: 0.71418
	loss_value_3: 0.08194
	loss_reward_3: 0.01559
	loss_policy_4: 0.04959
	accuracy_policy_4: 0.72246
	loss_value_4: 0.0837
	loss_reward_4: 0.01745
	loss_policy_5: 0.04989
	accuracy_policy_5: 0.71824
	loss_value_5: 0.08533
	loss_reward_5: 0.01877
	loss_policy: 0.5031
	loss_value: 0.79111
	loss_reward: 0.076
[2025-05-07 11:34:19] nn step 7550, lr: 0.1.
	loss_policy_0: 0.24892
	accuracy_policy_0: 0.70062
	loss_value_0: 0.37302
	loss_policy_1: 0.04938
	accuracy_policy_1: 0.70828
	loss_value_1: 0.07633
	loss_reward_1: 0.00908
	loss_policy_2: 0.04924
	accuracy_policy_2: 0.71262
	loss_value_2: 0.07827
	loss_reward_2: 0.01425
	loss_policy_3: 0.04919
	accuracy_policy_3: 0.71742
	loss_value_3: 0.08032
	loss_reward_3: 0.01538
	loss_policy_4: 0.04904
	accuracy_policy_4: 0.71773
	loss_value_4: 0.08219
	loss_reward_4: 0.01707
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.71699
	loss_value_5: 0.08408
	loss_reward_5: 0.01783
	loss_policy: 0.49521
	loss_value: 0.77421
	loss_reward: 0.07361
[2025-05-07 11:34:27] nn step 7600, lr: 0.1.
	loss_policy_0: 0.24473
	accuracy_policy_0: 0.6991
	loss_value_0: 0.36198
	loss_policy_1: 0.0482
	accuracy_policy_1: 0.71203
	loss_value_1: 0.07429
	loss_reward_1: 0.00894
	loss_policy_2: 0.04792
	accuracy_policy_2: 0.70758
	loss_value_2: 0.0764
	loss_reward_2: 0.01404
	loss_policy_3: 0.04807
	accuracy_policy_3: 0.71531
	loss_value_3: 0.07855
	loss_reward_3: 0.01506
	loss_policy_4: 0.04776
	accuracy_policy_4: 0.71898
	loss_value_4: 0.08028
	loss_reward_4: 0.01717
	loss_policy_5: 0.04822
	accuracy_policy_5: 0.7193
	loss_value_5: 0.08197
	loss_reward_5: 0.01766
	loss_policy: 0.4849
	loss_value: 0.75346
	loss_reward: 0.07287
Optimization_Done 7600
[2025-05-07 11:37:30] [command] train weight_iter_7600.pkl 20 39
[2025-05-07 11:37:39] nn step 7650, lr: 0.1.
	loss_policy_0: 0.26055
	accuracy_policy_0: 0.69352
	loss_value_0: 0.39741
	loss_policy_1: 0.05186
	accuracy_policy_1: 0.70434
	loss_value_1: 0.08095
	loss_reward_1: 0.00951
	loss_policy_2: 0.05156
	accuracy_policy_2: 0.70926
	loss_value_2: 0.08312
	loss_reward_2: 0.01463
	loss_policy_3: 0.05161
	accuracy_policy_3: 0.71277
	loss_value_3: 0.08493
	loss_reward_3: 0.01651
	loss_policy_4: 0.05142
	accuracy_policy_4: 0.7152
	loss_value_4: 0.08644
	loss_reward_4: 0.01842
	loss_policy_5: 0.05162
	accuracy_policy_5: 0.70934
	loss_value_5: 0.0881
	loss_reward_5: 0.01857
	loss_policy: 0.51861
	loss_value: 0.82095
	loss_reward: 0.07764
[2025-05-07 11:37:47] nn step 7700, lr: 0.1.
	loss_policy_0: 0.24495
	accuracy_policy_0: 0.69949
	loss_value_0: 0.36817
	loss_policy_1: 0.04846
	accuracy_policy_1: 0.70809
	loss_value_1: 0.07518
	loss_reward_1: 0.00892
	loss_policy_2: 0.04833
	accuracy_policy_2: 0.71367
	loss_value_2: 0.07676
	loss_reward_2: 0.01408
	loss_policy_3: 0.04832
	accuracy_policy_3: 0.70785
	loss_value_3: 0.07843
	loss_reward_3: 0.01526
	loss_policy_4: 0.0481
	accuracy_policy_4: 0.71293
	loss_value_4: 0.08006
	loss_reward_4: 0.01715
	loss_policy_5: 0.04818
	accuracy_policy_5: 0.71547
	loss_value_5: 0.0816
	loss_reward_5: 0.01782
	loss_policy: 0.48633
	loss_value: 0.7602
	loss_reward: 0.07323
[2025-05-07 11:37:55] nn step 7750, lr: 0.1.
	loss_policy_0: 0.24464
	accuracy_policy_0: 0.69816
	loss_value_0: 0.36221
	loss_policy_1: 0.04868
	accuracy_policy_1: 0.7098
	loss_value_1: 0.07446
	loss_reward_1: 0.00893
	loss_policy_2: 0.04886
	accuracy_policy_2: 0.70938
	loss_value_2: 0.07667
	loss_reward_2: 0.01444
	loss_policy_3: 0.04863
	accuracy_policy_3: 0.71289
	loss_value_3: 0.07866
	loss_reward_3: 0.01541
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.71262
	loss_value_4: 0.08042
	loss_reward_4: 0.01755
	loss_policy_5: 0.04864
	accuracy_policy_5: 0.71629
	loss_value_5: 0.08197
	loss_reward_5: 0.01754
	loss_policy: 0.48805
	loss_value: 0.75439
	loss_reward: 0.07387
[2025-05-07 11:38:03] nn step 7800, lr: 0.1.
	loss_policy_0: 0.25853
	accuracy_policy_0: 0.69676
	loss_value_0: 0.38335
	loss_policy_1: 0.05139
	accuracy_policy_1: 0.70727
	loss_value_1: 0.0786
	loss_reward_1: 0.00929
	loss_policy_2: 0.05092
	accuracy_policy_2: 0.71371
	loss_value_2: 0.08056
	loss_reward_2: 0.01524
	loss_policy_3: 0.051
	accuracy_policy_3: 0.71168
	loss_value_3: 0.08255
	loss_reward_3: 0.01613
	loss_policy_4: 0.05068
	accuracy_policy_4: 0.71965
	loss_value_4: 0.08417
	loss_reward_4: 0.01845
	loss_policy_5: 0.05096
	accuracy_policy_5: 0.71723
	loss_value_5: 0.0858
	loss_reward_5: 0.01957
	loss_policy: 0.51349
	loss_value: 0.79502
	loss_reward: 0.07868
Optimization_Done 7800
[2025-05-07 11:41:00] [command] train weight_iter_7800.pkl 21 40
[2025-05-07 11:41:09] nn step 7850, lr: 0.1.
	loss_policy_0: 0.23874
	accuracy_policy_0: 0.69484
	loss_value_0: 0.36358
	loss_policy_1: 0.04776
	accuracy_policy_1: 0.69996
	loss_value_1: 0.07451
	loss_reward_1: 0.00878
	loss_policy_2: 0.04787
	accuracy_policy_2: 0.70613
	loss_value_2: 0.07657
	loss_reward_2: 0.01356
	loss_policy_3: 0.04782
	accuracy_policy_3: 0.70391
	loss_value_3: 0.07809
	loss_reward_3: 0.01517
	loss_policy_4: 0.04774
	accuracy_policy_4: 0.70891
	loss_value_4: 0.07962
	loss_reward_4: 0.01694
	loss_policy_5: 0.04779
	accuracy_policy_5: 0.71016
	loss_value_5: 0.0814
	loss_reward_5: 0.01732
	loss_policy: 0.47771
	loss_value: 0.75376
	loss_reward: 0.07177
[2025-05-07 11:41:17] nn step 7900, lr: 0.1.
	loss_policy_0: 0.24229
	accuracy_policy_0: 0.69668
	loss_value_0: 0.35919
	loss_policy_1: 0.04782
	accuracy_policy_1: 0.70246
	loss_value_1: 0.0739
	loss_reward_1: 0.00872
	loss_policy_2: 0.04818
	accuracy_policy_2: 0.70539
	loss_value_2: 0.0757
	loss_reward_2: 0.01369
	loss_policy_3: 0.04805
	accuracy_policy_3: 0.70633
	loss_value_3: 0.07752
	loss_reward_3: 0.01528
	loss_policy_4: 0.0479
	accuracy_policy_4: 0.70781
	loss_value_4: 0.07923
	loss_reward_4: 0.0168
	loss_policy_5: 0.04826
	accuracy_policy_5: 0.71113
	loss_value_5: 0.08109
	loss_reward_5: 0.01726
	loss_policy: 0.4825
	loss_value: 0.74663
	loss_reward: 0.07174
[2025-05-07 11:41:25] nn step 7950, lr: 0.1.
	loss_policy_0: 0.24941
	accuracy_policy_0: 0.69477
	loss_value_0: 0.36789
	loss_policy_1: 0.04939
	accuracy_policy_1: 0.70438
	loss_value_1: 0.07564
	loss_reward_1: 0.0094
	loss_policy_2: 0.04936
	accuracy_policy_2: 0.70457
	loss_value_2: 0.07777
	loss_reward_2: 0.01379
	loss_policy_3: 0.04933
	accuracy_policy_3: 0.70754
	loss_value_3: 0.07999
	loss_reward_3: 0.01526
	loss_policy_4: 0.04949
	accuracy_policy_4: 0.70941
	loss_value_4: 0.08178
	loss_reward_4: 0.01753
	loss_policy_5: 0.04953
	accuracy_policy_5: 0.70805
	loss_value_5: 0.08354
	loss_reward_5: 0.01732
	loss_policy: 0.49651
	loss_value: 0.7666
	loss_reward: 0.07331
[2025-05-07 11:41:32] nn step 8000, lr: 0.1.
	loss_policy_0: 0.25062
	accuracy_policy_0: 0.69594
	loss_value_0: 0.37087
	loss_policy_1: 0.04993
	accuracy_policy_1: 0.70266
	loss_value_1: 0.07622
	loss_reward_1: 0.0093
	loss_policy_2: 0.05007
	accuracy_policy_2: 0.70445
	loss_value_2: 0.07793
	loss_reward_2: 0.01452
	loss_policy_3: 0.04972
	accuracy_policy_3: 0.7116
	loss_value_3: 0.08012
	loss_reward_3: 0.01544
	loss_policy_4: 0.04946
	accuracy_policy_4: 0.71156
	loss_value_4: 0.08188
	loss_reward_4: 0.01746
	loss_policy_5: 0.04969
	accuracy_policy_5: 0.71023
	loss_value_5: 0.08384
	loss_reward_5: 0.01828
	loss_policy: 0.49948
	loss_value: 0.77086
	loss_reward: 0.07501
Optimization_Done 8000
[2025-05-07 11:44:43] [command] train weight_iter_8000.pkl 22 41
[2025-05-07 11:44:51] nn step 8050, lr: 0.1.
	loss_policy_0: 0.25236
	accuracy_policy_0: 0.66586
	loss_value_0: 0.36613
	loss_policy_1: 0.04873
	accuracy_policy_1: 0.68363
	loss_value_1: 0.07532
	loss_reward_1: 0.00899
	loss_policy_2: 0.04847
	accuracy_policy_2: 0.68809
	loss_value_2: 0.07685
	loss_reward_2: 0.01429
	loss_policy_3: 0.04862
	accuracy_policy_3: 0.69152
	loss_value_3: 0.07855
	loss_reward_3: 0.01507
	loss_policy_4: 0.04886
	accuracy_policy_4: 0.69473
	loss_value_4: 0.08002
	loss_reward_4: 0.01718
	loss_policy_5: 0.04879
	accuracy_policy_5: 0.69363
	loss_value_5: 0.08163
	loss_reward_5: 0.01779
	loss_policy: 0.49582
	loss_value: 0.7585
	loss_reward: 0.07332
[2025-05-07 11:44:59] nn step 8100, lr: 0.1.
	loss_policy_0: 0.25635
	accuracy_policy_0: 0.68871
	loss_value_0: 0.38273
	loss_policy_1: 0.05149
	accuracy_policy_1: 0.6957
	loss_value_1: 0.07878
	loss_reward_1: 0.00963
	loss_policy_2: 0.05145
	accuracy_policy_2: 0.69457
	loss_value_2: 0.08078
	loss_reward_2: 0.01468
	loss_policy_3: 0.05135
	accuracy_policy_3: 0.69863
	loss_value_3: 0.08236
	loss_reward_3: 0.01623
	loss_policy_4: 0.051
	accuracy_policy_4: 0.6975
	loss_value_4: 0.08414
	loss_reward_4: 0.01851
	loss_policy_5: 0.05156
	accuracy_policy_5: 0.69801
	loss_value_5: 0.08547
	loss_reward_5: 0.01891
	loss_policy: 0.51319
	loss_value: 0.79426
	loss_reward: 0.07796
[2025-05-07 11:45:08] nn step 8150, lr: 0.1.
	loss_policy_0: 0.26365
	accuracy_policy_0: 0.6923
	loss_value_0: 0.39281
	loss_policy_1: 0.05286
	accuracy_policy_1: 0.69027
	loss_value_1: 0.08015
	loss_reward_1: 0.0094
	loss_policy_2: 0.05268
	accuracy_policy_2: 0.69512
	loss_value_2: 0.08263
	loss_reward_2: 0.01503
	loss_policy_3: 0.05246
	accuracy_policy_3: 0.69652
	loss_value_3: 0.08478
	loss_reward_3: 0.0161
	loss_policy_4: 0.05264
	accuracy_policy_4: 0.69797
	loss_value_4: 0.08702
	loss_reward_4: 0.01863
	loss_policy_5: 0.05225
	accuracy_policy_5: 0.70387
	loss_value_5: 0.08864
	loss_reward_5: 0.01878
	loss_policy: 0.52653
	loss_value: 0.81603
	loss_reward: 0.07793
[2025-05-07 11:45:14] nn step 8200, lr: 0.1.
	loss_policy_0: 0.26637
	accuracy_policy_0: 0.66863
	loss_value_0: 0.38334
	loss_policy_1: 0.05151
	accuracy_policy_1: 0.69191
	loss_value_1: 0.07871
	loss_reward_1: 0.00974
	loss_policy_2: 0.05159
	accuracy_policy_2: 0.69078
	loss_value_2: 0.08085
	loss_reward_2: 0.01525
	loss_policy_3: 0.05191
	accuracy_policy_3: 0.69363
	loss_value_3: 0.08296
	loss_reward_3: 0.01598
	loss_policy_4: 0.05161
	accuracy_policy_4: 0.69793
	loss_value_4: 0.08456
	loss_reward_4: 0.01787
	loss_policy_5: 0.05212
	accuracy_policy_5: 0.69109
	loss_value_5: 0.08632
	loss_reward_5: 0.01864
	loss_policy: 0.52511
	loss_value: 0.79673
	loss_reward: 0.07748
Optimization_Done 8200
[2025-05-07 11:48:20] [command] train weight_iter_8200.pkl 23 42
[2025-05-07 11:48:30] nn step 8250, lr: 0.1.
	loss_policy_0: 0.26617
	accuracy_policy_0: 0.67449
	loss_value_0: 0.39091
	loss_policy_1: 0.05319
	accuracy_policy_1: 0.67664
	loss_value_1: 0.08015
	loss_reward_1: 0.00901
	loss_policy_2: 0.0533
	accuracy_policy_2: 0.67953
	loss_value_2: 0.08204
	loss_reward_2: 0.01459
	loss_policy_3: 0.05315
	accuracy_policy_3: 0.68594
	loss_value_3: 0.08394
	loss_reward_3: 0.01608
	loss_policy_4: 0.05297
	accuracy_policy_4: 0.68535
	loss_value_4: 0.0856
	loss_reward_4: 0.01755
	loss_policy_5: 0.05299
	accuracy_policy_5: 0.6918
	loss_value_5: 0.08742
	loss_reward_5: 0.01815
	loss_policy: 0.53176
	loss_value: 0.81007
	loss_reward: 0.07539
[2025-05-07 11:48:37] nn step 8300, lr: 0.1.
	loss_policy_0: 0.27582
	accuracy_policy_0: 0.67867
	loss_value_0: 0.40455
	loss_policy_1: 0.05505
	accuracy_policy_1: 0.68812
	loss_value_1: 0.08311
	loss_reward_1: 0.00966
	loss_policy_2: 0.05493
	accuracy_policy_2: 0.68797
	loss_value_2: 0.08485
	loss_reward_2: 0.01558
	loss_policy_3: 0.0546
	accuracy_policy_3: 0.69391
	loss_value_3: 0.08662
	loss_reward_3: 0.01639
	loss_policy_4: 0.05484
	accuracy_policy_4: 0.69156
	loss_value_4: 0.08827
	loss_reward_4: 0.01888
	loss_policy_5: 0.05533
	accuracy_policy_5: 0.69223
	loss_value_5: 0.09016
	loss_reward_5: 0.01907
	loss_policy: 0.55056
	loss_value: 0.83757
	loss_reward: 0.07959
[2025-05-07 11:48:45] nn step 8350, lr: 0.1.
	loss_policy_0: 0.2748
	accuracy_policy_0: 0.68039
	loss_value_0: 0.40109
	loss_policy_1: 0.05492
	accuracy_policy_1: 0.68047
	loss_value_1: 0.08243
	loss_reward_1: 0.00985
	loss_policy_2: 0.05461
	accuracy_policy_2: 0.68711
	loss_value_2: 0.08456
	loss_reward_2: 0.01587
	loss_policy_3: 0.05441
	accuracy_policy_3: 0.69156
	loss_value_3: 0.08674
	loss_reward_3: 0.01691
	loss_policy_4: 0.05473
	accuracy_policy_4: 0.69383
	loss_value_4: 0.08852
	loss_reward_4: 0.0187
	loss_policy_5: 0.05464
	accuracy_policy_5: 0.69285
	loss_value_5: 0.09036
	loss_reward_5: 0.01957
	loss_policy: 0.5481
	loss_value: 0.83371
	loss_reward: 0.0809
[2025-05-07 11:48:53] nn step 8400, lr: 0.1.
	loss_policy_0: 0.24634
	accuracy_policy_0: 0.68516
	loss_value_0: 0.3615
	loss_policy_1: 0.04961
	accuracy_policy_1: 0.68387
	loss_value_1: 0.07386
	loss_reward_1: 0.00889
	loss_policy_2: 0.05004
	accuracy_policy_2: 0.68262
	loss_value_2: 0.07592
	loss_reward_2: 0.01385
	loss_policy_3: 0.04956
	accuracy_policy_3: 0.68863
	loss_value_3: 0.07788
	loss_reward_3: 0.01538
	loss_policy_4: 0.04942
	accuracy_policy_4: 0.69324
	loss_value_4: 0.07946
	loss_reward_4: 0.01685
	loss_policy_5: 0.04946
	accuracy_policy_5: 0.69508
	loss_value_5: 0.08128
	loss_reward_5: 0.01672
	loss_policy: 0.49443
	loss_value: 0.7499
	loss_reward: 0.07168
Optimization_Done 8400
[2025-05-07 11:51:57] [command] train weight_iter_8400.pkl 24 43
[2025-05-07 11:52:07] nn step 8450, lr: 0.1.
	loss_policy_0: 0.25941
	accuracy_policy_0: 0.68023
	loss_value_0: 0.38288
	loss_policy_1: 0.05226
	accuracy_policy_1: 0.67953
	loss_value_1: 0.07854
	loss_reward_1: 0.00907
	loss_policy_2: 0.05194
	accuracy_policy_2: 0.68738
	loss_value_2: 0.08058
	loss_reward_2: 0.01443
	loss_policy_3: 0.05193
	accuracy_policy_3: 0.68754
	loss_value_3: 0.08246
	loss_reward_3: 0.01559
	loss_policy_4: 0.05225
	accuracy_policy_4: 0.68484
	loss_value_4: 0.08421
	loss_reward_4: 0.0175
	loss_policy_5: 0.0524
	accuracy_policy_5: 0.68039
	loss_value_5: 0.0862
	loss_reward_5: 0.01805
	loss_policy: 0.52019
	loss_value: 0.79487
	loss_reward: 0.07464
[2025-05-07 11:52:15] nn step 8500, lr: 0.1.
	loss_policy_0: 0.27375
	accuracy_policy_0: 0.67305
	loss_value_0: 0.39795
	loss_policy_1: 0.05452
	accuracy_policy_1: 0.68145
	loss_value_1: 0.08133
	loss_reward_1: 0.00942
	loss_policy_2: 0.05449
	accuracy_policy_2: 0.6793
	loss_value_2: 0.08311
	loss_reward_2: 0.01464
	loss_policy_3: 0.05458
	accuracy_policy_3: 0.68375
	loss_value_3: 0.08549
	loss_reward_3: 0.01595
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.68742
	loss_value_4: 0.08695
	loss_reward_4: 0.0183
	loss_policy_5: 0.05465
	accuracy_policy_5: 0.68488
	loss_value_5: 0.08868
	loss_reward_5: 0.01893
	loss_policy: 0.54647
	loss_value: 0.8235
	loss_reward: 0.07724
[2025-05-07 11:52:23] nn step 8550, lr: 0.1.
	loss_policy_0: 0.26209
	accuracy_policy_0: 0.68195
	loss_value_0: 0.38135
	loss_policy_1: 0.05245
	accuracy_policy_1: 0.68035
	loss_value_1: 0.07811
	loss_reward_1: 0.00935
	loss_policy_2: 0.05261
	accuracy_policy_2: 0.68652
	loss_value_2: 0.07997
	loss_reward_2: 0.01416
	loss_policy_3: 0.05259
	accuracy_policy_3: 0.68383
	loss_value_3: 0.08215
	loss_reward_3: 0.01576
	loss_policy_4: 0.05251
	accuracy_policy_4: 0.68754
	loss_value_4: 0.08405
	loss_reward_4: 0.01778
	loss_policy_5: 0.05239
	accuracy_policy_5: 0.68898
	loss_value_5: 0.08572
	loss_reward_5: 0.01799
	loss_policy: 0.52464
	loss_value: 0.79135
	loss_reward: 0.07503
[2025-05-07 11:52:29] nn step 8600, lr: 0.1.
	loss_policy_0: 0.24621
	accuracy_policy_0: 0.68363
	loss_value_0: 0.3561
	loss_policy_1: 0.04933
	accuracy_policy_1: 0.68406
	loss_value_1: 0.07316
	loss_reward_1: 0.00848
	loss_policy_2: 0.04934
	accuracy_policy_2: 0.68426
	loss_value_2: 0.07503
	loss_reward_2: 0.01368
	loss_policy_3: 0.04915
	accuracy_policy_3: 0.68758
	loss_value_3: 0.0768
	loss_reward_3: 0.01479
	loss_policy_4: 0.04938
	accuracy_policy_4: 0.68906
	loss_value_4: 0.07853
	loss_reward_4: 0.01653
	loss_policy_5: 0.04917
	accuracy_policy_5: 0.68949
	loss_value_5: 0.08047
	loss_reward_5: 0.01692
	loss_policy: 0.49258
	loss_value: 0.74009
	loss_reward: 0.07041
Optimization_Done 8600
[2025-05-07 11:55:34] [command] train weight_iter_8600.pkl 25 44
[2025-05-07 11:55:43] nn step 8650, lr: 0.1.
	loss_policy_0: 0.2629
	accuracy_policy_0: 0.66672
	loss_value_0: 0.38032
	loss_policy_1: 0.0525
	accuracy_policy_1: 0.67207
	loss_value_1: 0.07775
	loss_reward_1: 0.00902
	loss_policy_2: 0.05283
	accuracy_policy_2: 0.67254
	loss_value_2: 0.0797
	loss_reward_2: 0.01469
	loss_policy_3: 0.05228
	accuracy_policy_3: 0.67926
	loss_value_3: 0.08118
	loss_reward_3: 0.01567
	loss_policy_4: 0.05252
	accuracy_policy_4: 0.6793
	loss_value_4: 0.08301
	loss_reward_4: 0.01738
	loss_policy_5: 0.05254
	accuracy_policy_5: 0.67539
	loss_value_5: 0.08464
	loss_reward_5: 0.01742
	loss_policy: 0.52557
	loss_value: 0.7866
	loss_reward: 0.07418
[2025-05-07 11:55:50] nn step 8700, lr: 0.1.
	loss_policy_0: 0.27572
	accuracy_policy_0: 0.6725
	loss_value_0: 0.39234
	loss_policy_1: 0.05478
	accuracy_policy_1: 0.67934
	loss_value_1: 0.08081
	loss_reward_1: 0.00916
	loss_policy_2: 0.05529
	accuracy_policy_2: 0.67586
	loss_value_2: 0.0832
	loss_reward_2: 0.01512
	loss_policy_3: 0.05502
	accuracy_policy_3: 0.68344
	loss_value_3: 0.08484
	loss_reward_3: 0.01634
	loss_policy_4: 0.0555
	accuracy_policy_4: 0.67793
	loss_value_4: 0.08687
	loss_reward_4: 0.01837
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.67828
	loss_value_5: 0.08858
	loss_reward_5: 0.01895
	loss_policy: 0.55158
	loss_value: 0.81665
	loss_reward: 0.07793
[2025-05-07 11:55:58] nn step 8750, lr: 0.1.
	loss_policy_0: 0.26149
	accuracy_policy_0: 0.67246
	loss_value_0: 0.37116
	loss_policy_1: 0.05195
	accuracy_policy_1: 0.67324
	loss_value_1: 0.07603
	loss_reward_1: 0.00873
	loss_policy_2: 0.05156
	accuracy_policy_2: 0.67902
	loss_value_2: 0.07793
	loss_reward_2: 0.01393
	loss_policy_3: 0.05202
	accuracy_policy_3: 0.67496
	loss_value_3: 0.07981
	loss_reward_3: 0.01544
	loss_policy_4: 0.05185
	accuracy_policy_4: 0.68242
	loss_value_4: 0.08163
	loss_reward_4: 0.01753
	loss_policy_5: 0.05228
	accuracy_policy_5: 0.68164
	loss_value_5: 0.08341
	loss_reward_5: 0.01752
	loss_policy: 0.52115
	loss_value: 0.76997
	loss_reward: 0.07314
[2025-05-07 11:56:06] nn step 8800, lr: 0.1.
	loss_policy_0: 0.26538
	accuracy_policy_0: 0.66621
	loss_value_0: 0.37617
	loss_policy_1: 0.05281
	accuracy_policy_1: 0.67512
	loss_value_1: 0.07735
	loss_reward_1: 0.00908
	loss_policy_2: 0.05286
	accuracy_policy_2: 0.67652
	loss_value_2: 0.07958
	loss_reward_2: 0.01442
	loss_policy_3: 0.05324
	accuracy_policy_3: 0.67609
	loss_value_3: 0.08143
	loss_reward_3: 0.01537
	loss_policy_4: 0.05244
	accuracy_policy_4: 0.68266
	loss_value_4: 0.08343
	loss_reward_4: 0.01724
	loss_policy_5: 0.05291
	accuracy_policy_5: 0.67855
	loss_value_5: 0.08509
	loss_reward_5: 0.01765
	loss_policy: 0.52963
	loss_value: 0.78305
	loss_reward: 0.07376
Optimization_Done 8800
[2025-05-07 11:59:07] [command] train weight_iter_8800.pkl 26 45
[2025-05-07 11:59:16] nn step 8850, lr: 0.1.
	loss_policy_0: 0.28266
	accuracy_policy_0: 0.6627
	loss_value_0: 0.4078
	loss_policy_1: 0.05654
	accuracy_policy_1: 0.6632
	loss_value_1: 0.08341
	loss_reward_1: 0.00977
	loss_policy_2: 0.05677
	accuracy_policy_2: 0.66289
	loss_value_2: 0.08563
	loss_reward_2: 0.01561
	loss_policy_3: 0.05704
	accuracy_policy_3: 0.66754
	loss_value_3: 0.0873
	loss_reward_3: 0.01678
	loss_policy_4: 0.05669
	accuracy_policy_4: 0.67078
	loss_value_4: 0.08905
	loss_reward_4: 0.01821
	loss_policy_5: 0.05736
	accuracy_policy_5: 0.6652
	loss_value_5: 0.09053
	loss_reward_5: 0.01926
	loss_policy: 0.56706
	loss_value: 0.84373
	loss_reward: 0.07963
[2025-05-07 11:59:24] nn step 8900, lr: 0.1.
	loss_policy_0: 0.26139
	accuracy_policy_0: 0.66699
	loss_value_0: 0.37054
	loss_policy_1: 0.05227
	accuracy_policy_1: 0.66656
	loss_value_1: 0.07556
	loss_reward_1: 0.00916
	loss_policy_2: 0.05238
	accuracy_policy_2: 0.66891
	loss_value_2: 0.07771
	loss_reward_2: 0.01413
	loss_policy_3: 0.05274
	accuracy_policy_3: 0.6657
	loss_value_3: 0.07969
	loss_reward_3: 0.01562
	loss_policy_4: 0.05235
	accuracy_policy_4: 0.67406
	loss_value_4: 0.08093
	loss_reward_4: 0.01759
	loss_policy_5: 0.05277
	accuracy_policy_5: 0.67434
	loss_value_5: 0.08266
	loss_reward_5: 0.01778
	loss_policy: 0.5239
	loss_value: 0.7671
	loss_reward: 0.07428
[2025-05-07 11:59:32] nn step 8950, lr: 0.1.
	loss_policy_0: 0.29404
	accuracy_policy_0: 0.66227
	loss_value_0: 0.41208
	loss_policy_1: 0.05874
	accuracy_policy_1: 0.66328
	loss_value_1: 0.08467
	loss_reward_1: 0.00983
	loss_policy_2: 0.0586
	accuracy_policy_2: 0.67055
	loss_value_2: 0.08708
	loss_reward_2: 0.01582
	loss_policy_3: 0.05877
	accuracy_policy_3: 0.66703
	loss_value_3: 0.08886
	loss_reward_3: 0.01742
	loss_policy_4: 0.05853
	accuracy_policy_4: 0.67719
	loss_value_4: 0.09037
	loss_reward_4: 0.01939
	loss_policy_5: 0.05841
	accuracy_policy_5: 0.67531
	loss_value_5: 0.09243
	loss_reward_5: 0.02
	loss_policy: 0.5871
	loss_value: 0.85549
	loss_reward: 0.08246
[2025-05-07 11:59:38] nn step 9000, lr: 0.1.
	loss_policy_0: 0.28294
	accuracy_policy_0: 0.66277
	loss_value_0: 0.4003
	loss_policy_1: 0.05674
	accuracy_policy_1: 0.66352
	loss_value_1: 0.08222
	loss_reward_1: 0.00975
	loss_policy_2: 0.05661
	accuracy_policy_2: 0.66883
	loss_value_2: 0.08435
	loss_reward_2: 0.0151
	loss_policy_3: 0.05633
	accuracy_policy_3: 0.67098
	loss_value_3: 0.08608
	loss_reward_3: 0.01702
	loss_policy_4: 0.05559
	accuracy_policy_4: 0.68219
	loss_value_4: 0.08815
	loss_reward_4: 0.01918
	loss_policy_5: 0.05627
	accuracy_policy_5: 0.67633
	loss_value_5: 0.08984
	loss_reward_5: 0.01967
	loss_policy: 0.56448
	loss_value: 0.83094
	loss_reward: 0.08071
Optimization_Done 9000
[2025-05-07 12:02:49] [command] train weight_iter_9000.pkl 27 46
[2025-05-07 12:02:58] nn step 9050, lr: 0.1.
	loss_policy_0: 0.2783
	accuracy_policy_0: 0.65723
	loss_value_0: 0.40032
	loss_policy_1: 0.05537
	accuracy_policy_1: 0.66371
	loss_value_1: 0.08196
	loss_reward_1: 0.00939
	loss_policy_2: 0.0555
	accuracy_policy_2: 0.66617
	loss_value_2: 0.084
	loss_reward_2: 0.01513
	loss_policy_3: 0.0554
	accuracy_policy_3: 0.66844
	loss_value_3: 0.08561
	loss_reward_3: 0.01602
	loss_policy_4: 0.05546
	accuracy_policy_4: 0.66668
	loss_value_4: 0.08756
	loss_reward_4: 0.01837
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.66684
	loss_value_5: 0.0893
	loss_reward_5: 0.01892
	loss_policy: 0.55594
	loss_value: 0.82874
	loss_reward: 0.07784
[2025-05-07 12:03:06] nn step 9100, lr: 0.1.
	loss_policy_0: 0.27163
	accuracy_policy_0: 0.65934
	loss_value_0: 0.38207
	loss_policy_1: 0.05423
	accuracy_policy_1: 0.66465
	loss_value_1: 0.07857
	loss_reward_1: 0.00884
	loss_policy_2: 0.05437
	accuracy_policy_2: 0.66199
	loss_value_2: 0.08069
	loss_reward_2: 0.01444
	loss_policy_3: 0.05442
	accuracy_policy_3: 0.66922
	loss_value_3: 0.08266
	loss_reward_3: 0.01573
	loss_policy_4: 0.05398
	accuracy_policy_4: 0.66988
	loss_value_4: 0.0842
	loss_reward_4: 0.01772
	loss_policy_5: 0.05429
	accuracy_policy_5: 0.67129
	loss_value_5: 0.08587
	loss_reward_5: 0.01787
	loss_policy: 0.54293
	loss_value: 0.79406
	loss_reward: 0.07458
[2025-05-07 12:03:13] nn step 9150, lr: 0.1.
	loss_policy_0: 0.29092
	accuracy_policy_0: 0.65781
	loss_value_0: 0.41318
	loss_policy_1: 0.05847
	accuracy_policy_1: 0.65609
	loss_value_1: 0.08484
	loss_reward_1: 0.00938
	loss_policy_2: 0.05807
	accuracy_policy_2: 0.66973
	loss_value_2: 0.08732
	loss_reward_2: 0.01568
	loss_policy_3: 0.05826
	accuracy_policy_3: 0.66371
	loss_value_3: 0.08964
	loss_reward_3: 0.01736
	loss_policy_4: 0.0582
	accuracy_policy_4: 0.67078
	loss_value_4: 0.09157
	loss_reward_4: 0.01898
	loss_policy_5: 0.05815
	accuracy_policy_5: 0.67156
	loss_value_5: 0.09293
	loss_reward_5: 0.01961
	loss_policy: 0.58207
	loss_value: 0.85947
	loss_reward: 0.08102
[2025-05-07 12:03:21] nn step 9200, lr: 0.1.
	loss_policy_0: 0.31295
	accuracy_policy_0: 0.64121
	loss_value_0: 0.43193
	loss_policy_1: 0.06138
	accuracy_policy_1: 0.66188
	loss_value_1: 0.08854
	loss_reward_1: 0.01038
	loss_policy_2: 0.06114
	accuracy_policy_2: 0.66348
	loss_value_2: 0.09104
	loss_reward_2: 0.01707
	loss_policy_3: 0.06083
	accuracy_policy_3: 0.66574
	loss_value_3: 0.09294
	loss_reward_3: 0.01798
	loss_policy_4: 0.06059
	accuracy_policy_4: 0.67062
	loss_value_4: 0.09483
	loss_reward_4: 0.01983
	loss_policy_5: 0.06052
	accuracy_policy_5: 0.66969
	loss_value_5: 0.09688
	loss_reward_5: 0.02048
	loss_policy: 0.61742
	loss_value: 0.89615
	loss_reward: 0.08573
Optimization_Done 9200
[2025-05-07 12:06:30] [command] train weight_iter_9200.pkl 28 47
[2025-05-07 12:06:39] nn step 9250, lr: 0.1.
	loss_policy_0: 0.27933
	accuracy_policy_0: 0.65352
	loss_value_0: 0.39969
	loss_policy_1: 0.05603
	accuracy_policy_1: 0.65859
	loss_value_1: 0.08166
	loss_reward_1: 0.00922
	loss_policy_2: 0.05583
	accuracy_policy_2: 0.6582
	loss_value_2: 0.08392
	loss_reward_2: 0.01486
	loss_policy_3: 0.05579
	accuracy_policy_3: 0.65863
	loss_value_3: 0.08567
	loss_reward_3: 0.01665
	loss_policy_4: 0.05563
	accuracy_policy_4: 0.66523
	loss_value_4: 0.0873
	loss_reward_4: 0.01872
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.66242
	loss_value_5: 0.08871
	loss_reward_5: 0.01855
	loss_policy: 0.55841
	loss_value: 0.82694
	loss_reward: 0.078
[2025-05-07 12:06:46] nn step 9300, lr: 0.1.
	loss_policy_0: 0.27363
	accuracy_policy_0: 0.6493
	loss_value_0: 0.38882
	loss_policy_1: 0.05476
	accuracy_policy_1: 0.65727
	loss_value_1: 0.07961
	loss_reward_1: 0.00898
	loss_policy_2: 0.0549
	accuracy_policy_2: 0.65773
	loss_value_2: 0.08155
	loss_reward_2: 0.015
	loss_policy_3: 0.0545
	accuracy_policy_3: 0.66086
	loss_value_3: 0.08331
	loss_reward_3: 0.01583
	loss_policy_4: 0.05424
	accuracy_policy_4: 0.66871
	loss_value_4: 0.08482
	loss_reward_4: 0.01792
	loss_policy_5: 0.05456
	accuracy_policy_5: 0.66547
	loss_value_5: 0.08648
	loss_reward_5: 0.01821
	loss_policy: 0.5466
	loss_value: 0.80459
	loss_reward: 0.07594
[2025-05-07 12:06:54] nn step 9350, lr: 0.1.
	loss_policy_0: 0.27371
	accuracy_policy_0: 0.66125
	loss_value_0: 0.39164
	loss_policy_1: 0.0554
	accuracy_policy_1: 0.66043
	loss_value_1: 0.08018
	loss_reward_1: 0.00926
	loss_policy_2: 0.055
	accuracy_policy_2: 0.66391
	loss_value_2: 0.08212
	loss_reward_2: 0.01474
	loss_policy_3: 0.05495
	accuracy_policy_3: 0.66828
	loss_value_3: 0.08397
	loss_reward_3: 0.01612
	loss_policy_4: 0.05549
	accuracy_policy_4: 0.66543
	loss_value_4: 0.08548
	loss_reward_4: 0.01814
	loss_policy_5: 0.05469
	accuracy_policy_5: 0.67203
	loss_value_5: 0.08733
	loss_reward_5: 0.01816
	loss_policy: 0.54923
	loss_value: 0.81072
	loss_reward: 0.07642
[2025-05-07 12:07:01] nn step 9400, lr: 0.1.
	loss_policy_0: 0.28678
	accuracy_policy_0: 0.6577
	loss_value_0: 0.40398
	loss_policy_1: 0.05692
	accuracy_policy_1: 0.66363
	loss_value_1: 0.08282
	loss_reward_1: 0.00962
	loss_policy_2: 0.05734
	accuracy_policy_2: 0.65656
	loss_value_2: 0.08501
	loss_reward_2: 0.01575
	loss_policy_3: 0.05689
	accuracy_policy_3: 0.66203
	loss_value_3: 0.08704
	loss_reward_3: 0.01699
	loss_policy_4: 0.0571
	accuracy_policy_4: 0.6657
	loss_value_4: 0.08882
	loss_reward_4: 0.01854
	loss_policy_5: 0.05699
	accuracy_policy_5: 0.66684
	loss_value_5: 0.09036
	loss_reward_5: 0.01942
	loss_policy: 0.57203
	loss_value: 0.83803
	loss_reward: 0.08032
Optimization_Done 9400
[2025-05-07 12:10:05] [command] train weight_iter_9400.pkl 29 48
[2025-05-07 12:10:15] nn step 9450, lr: 0.1.
	loss_policy_0: 0.27686
	accuracy_policy_0: 0.65309
	loss_value_0: 0.39691
	loss_policy_1: 0.05576
	accuracy_policy_1: 0.65539
	loss_value_1: 0.08133
	loss_reward_1: 0.00971
	loss_policy_2: 0.0558
	accuracy_policy_2: 0.65531
	loss_value_2: 0.08327
	loss_reward_2: 0.0153
	loss_policy_3: 0.05574
	accuracy_policy_3: 0.66141
	loss_value_3: 0.0851
	loss_reward_3: 0.01642
	loss_policy_4: 0.05553
	accuracy_policy_4: 0.66305
	loss_value_4: 0.0869
	loss_reward_4: 0.01828
	loss_policy_5: 0.05564
	accuracy_policy_5: 0.665
	loss_value_5: 0.08851
	loss_reward_5: 0.01873
	loss_policy: 0.55533
	loss_value: 0.82202
	loss_reward: 0.07844
[2025-05-07 12:10:23] nn step 9500, lr: 0.1.
	loss_policy_0: 0.28488
	accuracy_policy_0: 0.64824
	loss_value_0: 0.40258
	loss_policy_1: 0.0569
	accuracy_policy_1: 0.65637
	loss_value_1: 0.08279
	loss_reward_1: 0.00954
	loss_policy_2: 0.05738
	accuracy_policy_2: 0.6548
	loss_value_2: 0.08484
	loss_reward_2: 0.01543
	loss_policy_3: 0.05732
	accuracy_policy_3: 0.65531
	loss_value_3: 0.08665
	loss_reward_3: 0.01681
	loss_policy_4: 0.05711
	accuracy_policy_4: 0.66047
	loss_value_4: 0.08834
	loss_reward_4: 0.01851
	loss_policy_5: 0.05715
	accuracy_policy_5: 0.66227
	loss_value_5: 0.09031
	loss_reward_5: 0.01853
	loss_policy: 0.57075
	loss_value: 0.83553
	loss_reward: 0.07882
[2025-05-07 12:10:31] nn step 9550, lr: 0.1.
	loss_policy_0: 0.28387
	accuracy_policy_0: 0.64777
	loss_value_0: 0.39688
	loss_policy_1: 0.05692
	accuracy_policy_1: 0.65266
	loss_value_1: 0.0812
	loss_reward_1: 0.00947
	loss_policy_2: 0.0567
	accuracy_policy_2: 0.6557
	loss_value_2: 0.08335
	loss_reward_2: 0.01514
	loss_policy_3: 0.05712
	accuracy_policy_3: 0.65488
	loss_value_3: 0.08503
	loss_reward_3: 0.01635
	loss_policy_4: 0.0567
	accuracy_policy_4: 0.65645
	loss_value_4: 0.08696
	loss_reward_4: 0.01798
	loss_policy_5: 0.05658
	accuracy_policy_5: 0.65914
	loss_value_5: 0.08863
	loss_reward_5: 0.01869
	loss_policy: 0.56788
	loss_value: 0.82207
	loss_reward: 0.07762
[2025-05-07 12:10:38] nn step 9600, lr: 0.1.
	loss_policy_0: 0.28495
	accuracy_policy_0: 0.64945
	loss_value_0: 0.39536
	loss_policy_1: 0.05668
	accuracy_policy_1: 0.64688
	loss_value_1: 0.08115
	loss_reward_1: 0.00925
	loss_policy_2: 0.05572
	accuracy_policy_2: 0.65648
	loss_value_2: 0.08331
	loss_reward_2: 0.01502
	loss_policy_3: 0.05662
	accuracy_policy_3: 0.65953
	loss_value_3: 0.08528
	loss_reward_3: 0.01602
	loss_policy_4: 0.05623
	accuracy_policy_4: 0.65762
	loss_value_4: 0.08714
	loss_reward_4: 0.01793
	loss_policy_5: 0.05657
	accuracy_policy_5: 0.66219
	loss_value_5: 0.08887
	loss_reward_5: 0.01898
	loss_policy: 0.56678
	loss_value: 0.82112
	loss_reward: 0.0772
Optimization_Done 9600
[2025-05-07 12:13:49] [command] train weight_iter_9600.pkl 30 49
[2025-05-07 12:13:57] nn step 9650, lr: 0.1.
	loss_policy_0: 0.2981
	accuracy_policy_0: 0.63766
	loss_value_0: 0.42341
	loss_policy_1: 0.06009
	accuracy_policy_1: 0.63895
	loss_value_1: 0.08686
	loss_reward_1: 0.00952
	loss_policy_2: 0.05964
	accuracy_policy_2: 0.6459
	loss_value_2: 0.08901
	loss_reward_2: 0.01516
	loss_policy_3: 0.05978
	accuracy_policy_3: 0.64316
	loss_value_3: 0.09092
	loss_reward_3: 0.01651
	loss_policy_4: 0.05966
	accuracy_policy_4: 0.64879
	loss_value_4: 0.09287
	loss_reward_4: 0.01922
	loss_policy_5: 0.05954
	accuracy_policy_5: 0.64898
	loss_value_5: 0.09405
	loss_reward_5: 0.0189
	loss_policy: 0.59682
	loss_value: 0.87712
	loss_reward: 0.07931
[2025-05-07 12:14:05] nn step 9700, lr: 0.1.
	loss_policy_0: 0.2958
	accuracy_policy_0: 0.64344
	loss_value_0: 0.41498
	loss_policy_1: 0.05922
	accuracy_policy_1: 0.6491
	loss_value_1: 0.08523
	loss_reward_1: 0.00979
	loss_policy_2: 0.05938
	accuracy_policy_2: 0.64734
	loss_value_2: 0.08726
	loss_reward_2: 0.01527
	loss_policy_3: 0.05921
	accuracy_policy_3: 0.64988
	loss_value_3: 0.08907
	loss_reward_3: 0.01699
	loss_policy_4: 0.05913
	accuracy_policy_4: 0.65812
	loss_value_4: 0.09091
	loss_reward_4: 0.01907
	loss_policy_5: 0.05933
	accuracy_policy_5: 0.65734
	loss_value_5: 0.09303
	loss_reward_5: 0.01939
	loss_policy: 0.59207
	loss_value: 0.86048
	loss_reward: 0.08051
[2025-05-07 12:14:13] nn step 9750, lr: 0.1.
	loss_policy_0: 0.29821
	accuracy_policy_0: 0.64781
	loss_value_0: 0.41906
	loss_policy_1: 0.05971
	accuracy_policy_1: 0.64863
	loss_value_1: 0.08602
	loss_reward_1: 0.01
	loss_policy_2: 0.05953
	accuracy_policy_2: 0.64863
	loss_value_2: 0.08826
	loss_reward_2: 0.0154
	loss_policy_3: 0.05972
	accuracy_policy_3: 0.65129
	loss_value_3: 0.09054
	loss_reward_3: 0.01722
	loss_policy_4: 0.05986
	accuracy_policy_4: 0.65387
	loss_value_4: 0.09271
	loss_reward_4: 0.01964
	loss_policy_5: 0.05964
	accuracy_policy_5: 0.65746
	loss_value_5: 0.09444
	loss_reward_5: 0.0199
	loss_policy: 0.59667
	loss_value: 0.87103
	loss_reward: 0.08215
[2025-05-07 12:14:21] nn step 9800, lr: 0.1.
	loss_policy_0: 0.27718
	accuracy_policy_0: 0.64324
	loss_value_0: 0.38546
	loss_policy_1: 0.05509
	accuracy_policy_1: 0.64613
	loss_value_1: 0.07904
	loss_reward_1: 0.00916
	loss_policy_2: 0.05523
	accuracy_policy_2: 0.64887
	loss_value_2: 0.08093
	loss_reward_2: 0.01492
	loss_policy_3: 0.05519
	accuracy_policy_3: 0.65121
	loss_value_3: 0.08265
	loss_reward_3: 0.01597
	loss_policy_4: 0.05513
	accuracy_policy_4: 0.65973
	loss_value_4: 0.08457
	loss_reward_4: 0.01814
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.6532
	loss_value_5: 0.08632
	loss_reward_5: 0.01855
	loss_policy: 0.55308
	loss_value: 0.79898
	loss_reward: 0.07673
Optimization_Done 9800
[2025-05-07 12:17:22] [command] train weight_iter_9800.pkl 31 50
[2025-05-07 12:17:32] nn step 9850, lr: 0.1.
	loss_policy_0: 0.27923
	accuracy_policy_0: 0.64879
	loss_value_0: 0.39894
	loss_policy_1: 0.05606
	accuracy_policy_1: 0.64844
	loss_value_1: 0.08125
	loss_reward_1: 0.0092
	loss_policy_2: 0.05605
	accuracy_policy_2: 0.6532
	loss_value_2: 0.08314
	loss_reward_2: 0.01438
	loss_policy_3: 0.05612
	accuracy_policy_3: 0.65227
	loss_value_3: 0.08504
	loss_reward_3: 0.01572
	loss_policy_4: 0.05576
	accuracy_policy_4: 0.66352
	loss_value_4: 0.08677
	loss_reward_4: 0.01813
	loss_policy_5: 0.05605
	accuracy_policy_5: 0.66242
	loss_value_5: 0.08809
	loss_reward_5: 0.01902
	loss_policy: 0.55927
	loss_value: 0.82324
	loss_reward: 0.07646
[2025-05-07 12:17:40] nn step 9900, lr: 0.1.
	loss_policy_0: 0.29114
	accuracy_policy_0: 0.64207
	loss_value_0: 0.40617
	loss_policy_1: 0.05798
	accuracy_policy_1: 0.64746
	loss_value_1: 0.08361
	loss_reward_1: 0.00973
	loss_policy_2: 0.0582
	accuracy_policy_2: 0.64582
	loss_value_2: 0.08594
	loss_reward_2: 0.01497
	loss_policy_3: 0.05786
	accuracy_policy_3: 0.64797
	loss_value_3: 0.08824
	loss_reward_3: 0.01666
	loss_policy_4: 0.05806
	accuracy_policy_4: 0.65281
	loss_value_4: 0.08967
	loss_reward_4: 0.01905
	loss_policy_5: 0.05807
	accuracy_policy_5: 0.65844
	loss_value_5: 0.0917
	loss_reward_5: 0.01915
	loss_policy: 0.58132
	loss_value: 0.84533
	loss_reward: 0.07957
[2025-05-07 12:17:46] nn step 9950, lr: 0.1.
	loss_policy_0: 0.29916
	accuracy_policy_0: 0.6477
	loss_value_0: 0.42244
	loss_policy_1: 0.05993
	accuracy_policy_1: 0.65043
	loss_value_1: 0.08674
	loss_reward_1: 0.01014
	loss_policy_2: 0.06039
	accuracy_policy_2: 0.65137
	loss_value_2: 0.08893
	loss_reward_2: 0.01605
	loss_policy_3: 0.06019
	accuracy_policy_3: 0.65512
	loss_value_3: 0.09081
	loss_reward_3: 0.0174
	loss_policy_4: 0.05985
	accuracy_policy_4: 0.65688
	loss_value_4: 0.0929
	loss_reward_4: 0.01975
	loss_policy_5: 0.05962
	accuracy_policy_5: 0.66035
	loss_value_5: 0.0946
	loss_reward_5: 0.01991
	loss_policy: 0.59914
	loss_value: 0.87642
	loss_reward: 0.08325
[2025-05-07 12:17:54] nn step 10000, lr: 0.1.
	loss_policy_0: 0.29723
	accuracy_policy_0: 0.64539
	loss_value_0: 0.42198
	loss_policy_1: 0.0592
	accuracy_policy_1: 0.65539
	loss_value_1: 0.08665
	loss_reward_1: 0.01009
	loss_policy_2: 0.05897
	accuracy_policy_2: 0.65113
	loss_value_2: 0.08902
	loss_reward_2: 0.01622
	loss_policy_3: 0.05944
	accuracy_policy_3: 0.64988
	loss_value_3: 0.09077
	loss_reward_3: 0.0176
	loss_policy_4: 0.05935
	accuracy_policy_4: 0.65586
	loss_value_4: 0.09259
	loss_reward_4: 0.01951
	loss_policy_5: 0.05951
	accuracy_policy_5: 0.65512
	loss_value_5: 0.09428
	loss_reward_5: 0.01999
	loss_policy: 0.59371
	loss_value: 0.87529
	loss_reward: 0.08341
Optimization_Done 10000
[2025-05-07 12:21:02] [command] train weight_iter_10000.pkl 32 51
[2025-05-07 12:21:11] nn step 10050, lr: 0.1.
	loss_policy_0: 0.28362
	accuracy_policy_0: 0.63777
	loss_value_0: 0.40221
	loss_policy_1: 0.05663
	accuracy_policy_1: 0.64137
	loss_value_1: 0.08243
	loss_reward_1: 0.00907
	loss_policy_2: 0.05705
	accuracy_policy_2: 0.64508
	loss_value_2: 0.08468
	loss_reward_2: 0.01432
	loss_policy_3: 0.05655
	accuracy_policy_3: 0.6473
	loss_value_3: 0.08623
	loss_reward_3: 0.01624
	loss_policy_4: 0.05596
	accuracy_policy_4: 0.65738
	loss_value_4: 0.08789
	loss_reward_4: 0.01844
	loss_policy_5: 0.05603
	accuracy_policy_5: 0.65891
	loss_value_5: 0.08924
	loss_reward_5: 0.0183
	loss_policy: 0.56585
	loss_value: 0.83269
	loss_reward: 0.07637
[2025-05-07 12:21:19] nn step 10100, lr: 0.1.
	loss_policy_0: 0.26956
	accuracy_policy_0: 0.64258
	loss_value_0: 0.38104
	loss_policy_1: 0.05416
	accuracy_policy_1: 0.64426
	loss_value_1: 0.0781
	loss_reward_1: 0.00872
	loss_policy_2: 0.05404
	accuracy_policy_2: 0.64891
	loss_value_2: 0.07996
	loss_reward_2: 0.014
	loss_policy_3: 0.05368
	accuracy_policy_3: 0.65781
	loss_value_3: 0.08178
	loss_reward_3: 0.01536
	loss_policy_4: 0.05422
	accuracy_policy_4: 0.65523
	loss_value_4: 0.08336
	loss_reward_4: 0.0174
	loss_policy_5: 0.05353
	accuracy_policy_5: 0.65926
	loss_value_5: 0.08508
	loss_reward_5: 0.01757
	loss_policy: 0.5392
	loss_value: 0.78932
	loss_reward: 0.07305
[2025-05-07 12:21:26] nn step 10150, lr: 0.1.
	loss_policy_0: 0.27793
	accuracy_policy_0: 0.64984
	loss_value_0: 0.39309
	loss_policy_1: 0.05562
	accuracy_policy_1: 0.64828
	loss_value_1: 0.08105
	loss_reward_1: 0.00884
	loss_policy_2: 0.05596
	accuracy_policy_2: 0.65062
	loss_value_2: 0.08312
	loss_reward_2: 0.01442
	loss_policy_3: 0.05608
	accuracy_policy_3: 0.65301
	loss_value_3: 0.08498
	loss_reward_3: 0.01647
	loss_policy_4: 0.05566
	accuracy_policy_4: 0.66223
	loss_value_4: 0.08699
	loss_reward_4: 0.01833
	loss_policy_5: 0.05607
	accuracy_policy_5: 0.65863
	loss_value_5: 0.08874
	loss_reward_5: 0.01829
	loss_policy: 0.55733
	loss_value: 0.81796
	loss_reward: 0.07636
[2025-05-07 12:21:33] nn step 10200, lr: 0.1.
	loss_policy_0: 0.28826
	accuracy_policy_0: 0.64293
	loss_value_0: 0.40524
	loss_policy_1: 0.05764
	accuracy_policy_1: 0.64672
	loss_value_1: 0.08335
	loss_reward_1: 0.00924
	loss_policy_2: 0.05776
	accuracy_policy_2: 0.65312
	loss_value_2: 0.08568
	loss_reward_2: 0.01513
	loss_policy_3: 0.05833
	accuracy_policy_3: 0.64883
	loss_value_3: 0.0875
	loss_reward_3: 0.01641
	loss_policy_4: 0.05759
	accuracy_policy_4: 0.65684
	loss_value_4: 0.08917
	loss_reward_4: 0.01874
	loss_policy_5: 0.05744
	accuracy_policy_5: 0.65637
	loss_value_5: 0.0908
	loss_reward_5: 0.0188
	loss_policy: 0.57702
	loss_value: 0.84174
	loss_reward: 0.07831
Optimization_Done 10200
[2025-05-07 12:24:38] [command] train weight_iter_10200.pkl 33 52
[2025-05-07 12:24:47] nn step 10250, lr: 0.1.
	loss_policy_0: 0.29514
	accuracy_policy_0: 0.64602
	loss_value_0: 0.42427
	loss_policy_1: 0.05921
	accuracy_policy_1: 0.65062
	loss_value_1: 0.08675
	loss_reward_1: 0.00945
	loss_policy_2: 0.05952
	accuracy_policy_2: 0.64828
	loss_value_2: 0.08903
	loss_reward_2: 0.01526
	loss_policy_3: 0.05962
	accuracy_policy_3: 0.65094
	loss_value_3: 0.09099
	loss_reward_3: 0.01692
	loss_policy_4: 0.05934
	accuracy_policy_4: 0.65672
	loss_value_4: 0.09253
	loss_reward_4: 0.0193
	loss_policy_5: 0.05942
	accuracy_policy_5: 0.65898
	loss_value_5: 0.0939
	loss_reward_5: 0.01934
	loss_policy: 0.59224
	loss_value: 0.87747
	loss_reward: 0.08026
[2025-05-07 12:24:54] nn step 10300, lr: 0.1.
	loss_policy_0: 0.28425
	accuracy_policy_0: 0.64273
	loss_value_0: 0.40038
	loss_policy_1: 0.05691
	accuracy_policy_1: 0.6448
	loss_value_1: 0.08225
	loss_reward_1: 0.00933
	loss_policy_2: 0.05671
	accuracy_policy_2: 0.6484
	loss_value_2: 0.08435
	loss_reward_2: 0.01493
	loss_policy_3: 0.05694
	accuracy_policy_3: 0.6473
	loss_value_3: 0.08627
	loss_reward_3: 0.01649
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.65375
	loss_value_4: 0.08797
	loss_reward_4: 0.01828
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.65746
	loss_value_5: 0.08929
	loss_reward_5: 0.01889
	loss_policy: 0.56806
	loss_value: 0.83051
	loss_reward: 0.07793
[2025-05-07 12:25:02] nn step 10350, lr: 0.1.
	loss_policy_0: 0.30885
	accuracy_policy_0: 0.64875
	loss_value_0: 0.43705
	loss_policy_1: 0.06249
	accuracy_policy_1: 0.64422
	loss_value_1: 0.0894
	loss_reward_1: 0.01034
	loss_policy_2: 0.06198
	accuracy_policy_2: 0.64582
	loss_value_2: 0.09142
	loss_reward_2: 0.01651
	loss_policy_3: 0.06138
	accuracy_policy_3: 0.65344
	loss_value_3: 0.09326
	loss_reward_3: 0.0183
	loss_policy_4: 0.06195
	accuracy_policy_4: 0.65902
	loss_value_4: 0.09527
	loss_reward_4: 0.02007
	loss_policy_5: 0.06194
	accuracy_policy_5: 0.65395
	loss_value_5: 0.09712
	loss_reward_5: 0.0205
	loss_policy: 0.61858
	loss_value: 0.90353
	loss_reward: 0.08572
[2025-05-07 12:25:10] nn step 10400, lr: 0.1.
	loss_policy_0: 0.27626
	accuracy_policy_0: 0.64805
	loss_value_0: 0.39139
	loss_policy_1: 0.05533
	accuracy_policy_1: 0.64602
	loss_value_1: 0.08068
	loss_reward_1: 0.00927
	loss_policy_2: 0.05503
	accuracy_policy_2: 0.65375
	loss_value_2: 0.08261
	loss_reward_2: 0.01465
	loss_policy_3: 0.05535
	accuracy_policy_3: 0.6541
	loss_value_3: 0.08461
	loss_reward_3: 0.01619
	loss_policy_4: 0.05526
	accuracy_policy_4: 0.65688
	loss_value_4: 0.08636
	loss_reward_4: 0.01781
	loss_policy_5: 0.05523
	accuracy_policy_5: 0.65441
	loss_value_5: 0.08761
	loss_reward_5: 0.01871
	loss_policy: 0.55245
	loss_value: 0.81325
	loss_reward: 0.07664
Optimization_Done 10400
[2025-05-07 12:28:12] [command] train weight_iter_10400.pkl 34 53
[2025-05-07 12:28:21] nn step 10450, lr: 0.1.
	loss_policy_0: 0.30689
	accuracy_policy_0: 0.64152
	loss_value_0: 0.43655
	loss_policy_1: 0.0609
	accuracy_policy_1: 0.64789
	loss_value_1: 0.08901
	loss_reward_1: 0.01019
	loss_policy_2: 0.06104
	accuracy_policy_2: 0.65109
	loss_value_2: 0.09117
	loss_reward_2: 0.01635
	loss_policy_3: 0.06077
	accuracy_policy_3: 0.65312
	loss_value_3: 0.09325
	loss_reward_3: 0.0178
	loss_policy_4: 0.06052
	accuracy_policy_4: 0.65578
	loss_value_4: 0.09506
	loss_reward_4: 0.01987
	loss_policy_5: 0.06045
	accuracy_policy_5: 0.65938
	loss_value_5: 0.09659
	loss_reward_5: 0.02054
	loss_policy: 0.61058
	loss_value: 0.90162
	loss_reward: 0.08474
[2025-05-07 12:28:30] nn step 10500, lr: 0.1.
	loss_policy_0: 0.30047
	accuracy_policy_0: 0.63719
	loss_value_0: 0.42128
	loss_policy_1: 0.05902
	accuracy_policy_1: 0.64977
	loss_value_1: 0.08682
	loss_reward_1: 0.00982
	loss_policy_2: 0.05903
	accuracy_policy_2: 0.65363
	loss_value_2: 0.08881
	loss_reward_2: 0.01564
	loss_policy_3: 0.05917
	accuracy_policy_3: 0.65211
	loss_value_3: 0.09087
	loss_reward_3: 0.01736
	loss_policy_4: 0.05923
	accuracy_policy_4: 0.65969
	loss_value_4: 0.09257
	loss_reward_4: 0.01961
	loss_policy_5: 0.05933
	accuracy_policy_5: 0.65789
	loss_value_5: 0.09407
	loss_reward_5: 0.01999
	loss_policy: 0.59625
	loss_value: 0.87441
	loss_reward: 0.08243
[2025-05-07 12:28:38] nn step 10550, lr: 0.1.
	loss_policy_0: 0.30247
	accuracy_policy_0: 0.64461
	loss_value_0: 0.42812
	loss_policy_1: 0.06093
	accuracy_policy_1: 0.65
	loss_value_1: 0.08816
	loss_reward_1: 0.01014
	loss_policy_2: 0.0612
	accuracy_policy_2: 0.64719
	loss_value_2: 0.09055
	loss_reward_2: 0.01638
	loss_policy_3: 0.06087
	accuracy_policy_3: 0.65324
	loss_value_3: 0.09275
	loss_reward_3: 0.01746
	loss_policy_4: 0.06055
	accuracy_policy_4: 0.65969
	loss_value_4: 0.0947
	loss_reward_4: 0.01988
	loss_policy_5: 0.06065
	accuracy_policy_5: 0.65586
	loss_value_5: 0.09652
	loss_reward_5: 0.01969
	loss_policy: 0.60667
	loss_value: 0.8908
	loss_reward: 0.08356
[2025-05-07 12:28:44] nn step 10600, lr: 0.1.
	loss_policy_0: 0.28611
	accuracy_policy_0: 0.64766
	loss_value_0: 0.40556
	loss_policy_1: 0.05749
	accuracy_policy_1: 0.64691
	loss_value_1: 0.08337
	loss_reward_1: 0.0093
	loss_policy_2: 0.0577
	accuracy_policy_2: 0.64605
	loss_value_2: 0.08556
	loss_reward_2: 0.01491
	loss_policy_3: 0.05773
	accuracy_policy_3: 0.6523
	loss_value_3: 0.08736
	loss_reward_3: 0.01657
	loss_policy_4: 0.05754
	accuracy_policy_4: 0.65844
	loss_value_4: 0.08959
	loss_reward_4: 0.01874
	loss_policy_5: 0.0574
	accuracy_policy_5: 0.65867
	loss_value_5: 0.09148
	loss_reward_5: 0.01922
	loss_policy: 0.57397
	loss_value: 0.84292
	loss_reward: 0.07874
Optimization_Done 10600
[2025-05-07 12:31:48] [command] train weight_iter_10600.pkl 35 54
[2025-05-07 12:31:58] nn step 10650, lr: 0.1.
	loss_policy_0: 0.27759
	accuracy_policy_0: 0.64688
	loss_value_0: 0.40106
	loss_policy_1: 0.05543
	accuracy_policy_1: 0.64746
	loss_value_1: 0.08231
	loss_reward_1: 0.00891
	loss_policy_2: 0.05557
	accuracy_policy_2: 0.64629
	loss_value_2: 0.08449
	loss_reward_2: 0.01494
	loss_policy_3: 0.0558
	accuracy_policy_3: 0.65184
	loss_value_3: 0.0864
	loss_reward_3: 0.01638
	loss_policy_4: 0.05578
	accuracy_policy_4: 0.64828
	loss_value_4: 0.0879
	loss_reward_4: 0.01774
	loss_policy_5: 0.05578
	accuracy_policy_5: 0.65035
	loss_value_5: 0.08922
	loss_reward_5: 0.01823
	loss_policy: 0.55594
	loss_value: 0.83139
	loss_reward: 0.07621
[2025-05-07 12:32:04] nn step 10700, lr: 0.1.
	loss_policy_0: 0.29304
	accuracy_policy_0: 0.64402
	loss_value_0: 0.41453
	loss_policy_1: 0.05846
	accuracy_policy_1: 0.64277
	loss_value_1: 0.08496
	loss_reward_1: 0.00948
	loss_policy_2: 0.05849
	accuracy_policy_2: 0.64637
	loss_value_2: 0.08739
	loss_reward_2: 0.01523
	loss_policy_3: 0.05817
	accuracy_policy_3: 0.65395
	loss_value_3: 0.08926
	loss_reward_3: 0.01688
	loss_policy_4: 0.05854
	accuracy_policy_4: 0.6516
	loss_value_4: 0.09106
	loss_reward_4: 0.01872
	loss_policy_5: 0.05831
	accuracy_policy_5: 0.65363
	loss_value_5: 0.09261
	loss_reward_5: 0.01888
	loss_policy: 0.58501
	loss_value: 0.85981
	loss_reward: 0.07918
[2025-05-07 12:32:12] nn step 10750, lr: 0.1.
	loss_policy_0: 0.26839
	accuracy_policy_0: 0.64363
	loss_value_0: 0.37457
	loss_policy_1: 0.05375
	accuracy_policy_1: 0.64703
	loss_value_1: 0.07693
	loss_reward_1: 0.00876
	loss_policy_2: 0.0537
	accuracy_policy_2: 0.64469
	loss_value_2: 0.07902
	loss_reward_2: 0.01362
	loss_policy_3: 0.05378
	accuracy_policy_3: 0.64637
	loss_value_3: 0.08087
	loss_reward_3: 0.01533
	loss_policy_4: 0.05369
	accuracy_policy_4: 0.64922
	loss_value_4: 0.08241
	loss_reward_4: 0.01736
	loss_policy_5: 0.05393
	accuracy_policy_5: 0.65309
	loss_value_5: 0.08373
	loss_reward_5: 0.01728
	loss_policy: 0.53724
	loss_value: 0.77754
	loss_reward: 0.07235
[2025-05-07 12:32:20] nn step 10800, lr: 0.1.
	loss_policy_0: 0.31667
	accuracy_policy_0: 0.63996
	loss_value_0: 0.446
	loss_policy_1: 0.06317
	accuracy_policy_1: 0.64637
	loss_value_1: 0.09184
	loss_reward_1: 0.0104
	loss_policy_2: 0.06331
	accuracy_policy_2: 0.64547
	loss_value_2: 0.09441
	loss_reward_2: 0.01635
	loss_policy_3: 0.06287
	accuracy_policy_3: 0.64695
	loss_value_3: 0.09614
	loss_reward_3: 0.01803
	loss_policy_4: 0.06278
	accuracy_policy_4: 0.65465
	loss_value_4: 0.0981
	loss_reward_4: 0.02051
	loss_policy_5: 0.06336
	accuracy_policy_5: 0.65074
	loss_value_5: 0.09988
	loss_reward_5: 0.02068
	loss_policy: 0.63216
	loss_value: 0.92637
	loss_reward: 0.08598
Optimization_Done 10800
[2025-05-07 12:35:20] [command] train weight_iter_10800.pkl 36 55
[2025-05-07 12:35:29] nn step 10850, lr: 0.1.
	loss_policy_0: 0.27915
	accuracy_policy_0: 0.63926
	loss_value_0: 0.40276
	loss_policy_1: 0.05587
	accuracy_policy_1: 0.64332
	loss_value_1: 0.08277
	loss_reward_1: 0.00887
	loss_policy_2: 0.05586
	accuracy_policy_2: 0.64469
	loss_value_2: 0.085
	loss_reward_2: 0.0148
	loss_policy_3: 0.05622
	accuracy_policy_3: 0.64754
	loss_value_3: 0.08674
	loss_reward_3: 0.01602
	loss_policy_4: 0.05613
	accuracy_policy_4: 0.64941
	loss_value_4: 0.08859
	loss_reward_4: 0.01799
	loss_policy_5: 0.05594
	accuracy_policy_5: 0.65582
	loss_value_5: 0.08986
	loss_reward_5: 0.01825
	loss_policy: 0.55917
	loss_value: 0.83573
	loss_reward: 0.07592
[2025-05-07 12:35:37] nn step 10900, lr: 0.1.
	loss_policy_0: 0.30007
	accuracy_policy_0: 0.64297
	loss_value_0: 0.42854
	loss_policy_1: 0.06012
	accuracy_policy_1: 0.63969
	loss_value_1: 0.08762
	loss_reward_1: 0.00947
	loss_policy_2: 0.06061
	accuracy_policy_2: 0.64383
	loss_value_2: 0.09013
	loss_reward_2: 0.01554
	loss_policy_3: 0.06026
	accuracy_policy_3: 0.64605
	loss_value_3: 0.09184
	loss_reward_3: 0.01737
	loss_policy_4: 0.06036
	accuracy_policy_4: 0.65082
	loss_value_4: 0.09356
	loss_reward_4: 0.01924
	loss_policy_5: 0.06016
	accuracy_policy_5: 0.65449
	loss_value_5: 0.09512
	loss_reward_5: 0.01978
	loss_policy: 0.60158
	loss_value: 0.8868
	loss_reward: 0.0814
[2025-05-07 12:35:44] nn step 10950, lr: 0.1.
	loss_policy_0: 0.30882
	accuracy_policy_0: 0.60566
	loss_value_0: 0.41547
	loss_policy_1: 0.05884
	accuracy_policy_1: 0.63477
	loss_value_1: 0.08529
	loss_reward_1: 0.00952
	loss_policy_2: 0.05863
	accuracy_policy_2: 0.64301
	loss_value_2: 0.08785
	loss_reward_2: 0.01603
	loss_policy_3: 0.05854
	accuracy_policy_3: 0.64656
	loss_value_3: 0.08984
	loss_reward_3: 0.01677
	loss_policy_4: 0.05829
	accuracy_policy_4: 0.65238
	loss_value_4: 0.09144
	loss_reward_4: 0.01913
	loss_policy_5: 0.05852
	accuracy_policy_5: 0.64668
	loss_value_5: 0.09307
	loss_reward_5: 0.0194
	loss_policy: 0.60163
	loss_value: 0.86296
	loss_reward: 0.08086
[2025-05-07 12:35:51] nn step 11000, lr: 0.1.
	loss_policy_0: 0.30418
	accuracy_policy_0: 0.62836
	loss_value_0: 0.41786
	loss_policy_1: 0.05871
	accuracy_policy_1: 0.64301
	loss_value_1: 0.08587
	loss_reward_1: 0.00973
	loss_policy_2: 0.05889
	accuracy_policy_2: 0.65391
	loss_value_2: 0.08796
	loss_reward_2: 0.01543
	loss_policy_3: 0.05949
	accuracy_policy_3: 0.64758
	loss_value_3: 0.08994
	loss_reward_3: 0.01747
	loss_policy_4: 0.05942
	accuracy_policy_4: 0.65141
	loss_value_4: 0.09133
	loss_reward_4: 0.01979
	loss_policy_5: 0.05932
	accuracy_policy_5: 0.65289
	loss_value_5: 0.09303
	loss_reward_5: 0.01913
	loss_policy: 0.60001
	loss_value: 0.86599
	loss_reward: 0.08154
Optimization_Done 11000
[2025-05-07 12:38:57] [command] train weight_iter_11000.pkl 37 56
[2025-05-07 12:39:06] nn step 11050, lr: 0.1.
	loss_policy_0: 0.30396
	accuracy_policy_0: 0.63223
	loss_value_0: 0.43238
	loss_policy_1: 0.06048
	accuracy_policy_1: 0.64215
	loss_value_1: 0.08834
	loss_reward_1: 0.00998
	loss_policy_2: 0.05988
	accuracy_policy_2: 0.64293
	loss_value_2: 0.09065
	loss_reward_2: 0.01581
	loss_policy_3: 0.05987
	accuracy_policy_3: 0.6434
	loss_value_3: 0.09268
	loss_reward_3: 0.01759
	loss_policy_4: 0.05953
	accuracy_policy_4: 0.65199
	loss_value_4: 0.09449
	loss_reward_4: 0.02
	loss_policy_5: 0.06001
	accuracy_policy_5: 0.65148
	loss_value_5: 0.09592
	loss_reward_5: 0.01978
	loss_policy: 0.60372
	loss_value: 0.89446
	loss_reward: 0.08317
[2025-05-07 12:39:12] nn step 11100, lr: 0.1.
	loss_policy_0: 0.30792
	accuracy_policy_0: 0.63254
	loss_value_0: 0.43456
	loss_policy_1: 0.06125
	accuracy_policy_1: 0.63883
	loss_value_1: 0.08878
	loss_reward_1: 0.0097
	loss_policy_2: 0.06099
	accuracy_policy_2: 0.6441
	loss_value_2: 0.09115
	loss_reward_2: 0.01611
	loss_policy_3: 0.06086
	accuracy_policy_3: 0.64863
	loss_value_3: 0.09329
	loss_reward_3: 0.0176
	loss_policy_4: 0.06057
	accuracy_policy_4: 0.65652
	loss_value_4: 0.09493
	loss_reward_4: 0.01954
	loss_policy_5: 0.06138
	accuracy_policy_5: 0.645
	loss_value_5: 0.09638
	loss_reward_5: 0.01934
	loss_policy: 0.61297
	loss_value: 0.8991
	loss_reward: 0.08229
[2025-05-07 12:39:20] nn step 11150, lr: 0.1.
	loss_policy_0: 0.2775
	accuracy_policy_0: 0.64086
	loss_value_0: 0.39488
	loss_policy_1: 0.05561
	accuracy_policy_1: 0.64539
	loss_value_1: 0.08124
	loss_reward_1: 0.00847
	loss_policy_2: 0.0553
	accuracy_policy_2: 0.64539
	loss_value_2: 0.08325
	loss_reward_2: 0.01436
	loss_policy_3: 0.05537
	accuracy_policy_3: 0.65043
	loss_value_3: 0.08488
	loss_reward_3: 0.01671
	loss_policy_4: 0.05532
	accuracy_policy_4: 0.65254
	loss_value_4: 0.08648
	loss_reward_4: 0.01787
	loss_policy_5: 0.05532
	accuracy_policy_5: 0.65656
	loss_value_5: 0.08824
	loss_reward_5: 0.01791
	loss_policy: 0.55442
	loss_value: 0.81897
	loss_reward: 0.07532
[2025-05-07 12:39:28] nn step 11200, lr: 0.1.
	loss_policy_0: 0.29263
	accuracy_policy_0: 0.62434
	loss_value_0: 0.40261
	loss_policy_1: 0.05784
	accuracy_policy_1: 0.6384
	loss_value_1: 0.0826
	loss_reward_1: 0.00919
	loss_policy_2: 0.0576
	accuracy_policy_2: 0.64395
	loss_value_2: 0.08511
	loss_reward_2: 0.01514
	loss_policy_3: 0.05735
	accuracy_policy_3: 0.64566
	loss_value_3: 0.08689
	loss_reward_3: 0.01692
	loss_policy_4: 0.0572
	accuracy_policy_4: 0.64926
	loss_value_4: 0.08856
	loss_reward_4: 0.01871
	loss_policy_5: 0.05741
	accuracy_policy_5: 0.65258
	loss_value_5: 0.09025
	loss_reward_5: 0.0191
	loss_policy: 0.58003
	loss_value: 0.83601
	loss_reward: 0.07906
Optimization_Done 11200
[2025-05-07 12:42:34] [command] train weight_iter_11200.pkl 38 57
[2025-05-07 12:42:43] nn step 11250, lr: 0.1.
	loss_policy_0: 0.29175
	accuracy_policy_0: 0.64289
	loss_value_0: 0.42231
	loss_policy_1: 0.05831
	accuracy_policy_1: 0.64926
	loss_value_1: 0.08644
	loss_reward_1: 0.00947
	loss_policy_2: 0.05832
	accuracy_policy_2: 0.65312
	loss_value_2: 0.08847
	loss_reward_2: 0.01503
	loss_policy_3: 0.05849
	accuracy_policy_3: 0.65109
	loss_value_3: 0.09011
	loss_reward_3: 0.01646
	loss_policy_4: 0.05848
	accuracy_policy_4: 0.65242
	loss_value_4: 0.09196
	loss_reward_4: 0.0188
	loss_policy_5: 0.05911
	accuracy_policy_5: 0.65
	loss_value_5: 0.09341
	loss_reward_5: 0.01914
	loss_policy: 0.58446
	loss_value: 0.8727
	loss_reward: 0.0789
[2025-05-07 12:42:51] nn step 11300, lr: 0.1.
	loss_policy_0: 0.31396
	accuracy_policy_0: 0.63398
	loss_value_0: 0.43778
	loss_policy_1: 0.06203
	accuracy_policy_1: 0.64332
	loss_value_1: 0.08964
	loss_reward_1: 0.00956
	loss_policy_2: 0.06228
	accuracy_policy_2: 0.65
	loss_value_2: 0.09202
	loss_reward_2: 0.01596
	loss_policy_3: 0.06241
	accuracy_policy_3: 0.64578
	loss_value_3: 0.09403
	loss_reward_3: 0.01748
	loss_policy_4: 0.0622
	accuracy_policy_4: 0.65301
	loss_value_4: 0.0959
	loss_reward_4: 0.02
	loss_policy_5: 0.06212
	accuracy_policy_5: 0.65473
	loss_value_5: 0.09751
	loss_reward_5: 0.02003
	loss_policy: 0.625
	loss_value: 0.90689
	loss_reward: 0.08303
[2025-05-07 12:42:59] nn step 11350, lr: 0.1.
	loss_policy_0: 0.29579
	accuracy_policy_0: 0.64336
	loss_value_0: 0.41637
	loss_policy_1: 0.05937
	accuracy_policy_1: 0.64297
	loss_value_1: 0.08549
	loss_reward_1: 0.00952
	loss_policy_2: 0.05929
	accuracy_policy_2: 0.65
	loss_value_2: 0.08828
	loss_reward_2: 0.01506
	loss_policy_3: 0.05932
	accuracy_policy_3: 0.65141
	loss_value_3: 0.09018
	loss_reward_3: 0.01672
	loss_policy_4: 0.05956
	accuracy_policy_4: 0.64762
	loss_value_4: 0.09175
	loss_reward_4: 0.01927
	loss_policy_5: 0.05982
	accuracy_policy_5: 0.65195
	loss_value_5: 0.09339
	loss_reward_5: 0.01892
	loss_policy: 0.59314
	loss_value: 0.86546
	loss_reward: 0.07949
[2025-05-07 12:43:05] nn step 11400, lr: 0.1.
	loss_policy_0: 0.29765
	accuracy_policy_0: 0.64418
	loss_value_0: 0.41702
	loss_policy_1: 0.05978
	accuracy_policy_1: 0.64102
	loss_value_1: 0.08568
	loss_reward_1: 0.00913
	loss_policy_2: 0.05927
	accuracy_policy_2: 0.64965
	loss_value_2: 0.08803
	loss_reward_2: 0.01535
	loss_policy_3: 0.05967
	accuracy_policy_3: 0.65035
	loss_value_3: 0.09011
	loss_reward_3: 0.01693
	loss_policy_4: 0.0598
	accuracy_policy_4: 0.65082
	loss_value_4: 0.09188
	loss_reward_4: 0.01901
	loss_policy_5: 0.05974
	accuracy_policy_5: 0.65203
	loss_value_5: 0.09364
	loss_reward_5: 0.01874
	loss_policy: 0.59591
	loss_value: 0.86636
	loss_reward: 0.07915
Optimization_Done 11400
[2025-05-07 12:46:15] [command] train weight_iter_11400.pkl 39 58
[2025-05-07 12:46:23] nn step 11450, lr: 0.1.
	loss_policy_0: 0.29052
	accuracy_policy_0: 0.63051
	loss_value_0: 0.41044
	loss_policy_1: 0.05819
	accuracy_policy_1: 0.63367
	loss_value_1: 0.08456
	loss_reward_1: 0.0088
	loss_policy_2: 0.05795
	accuracy_policy_2: 0.64145
	loss_value_2: 0.08669
	loss_reward_2: 0.01443
	loss_policy_3: 0.05813
	accuracy_policy_3: 0.64043
	loss_value_3: 0.08864
	loss_reward_3: 0.01613
	loss_policy_4: 0.05815
	accuracy_policy_4: 0.64703
	loss_value_4: 0.09021
	loss_reward_4: 0.0178
	loss_policy_5: 0.05819
	accuracy_policy_5: 0.6475
	loss_value_5: 0.09177
	loss_reward_5: 0.01869
	loss_policy: 0.58112
	loss_value: 0.8523
	loss_reward: 0.07584
[2025-05-07 12:46:31] nn step 11500, lr: 0.1.
	loss_policy_0: 0.29738
	accuracy_policy_0: 0.63848
	loss_value_0: 0.41696
	loss_policy_1: 0.05946
	accuracy_policy_1: 0.63996
	loss_value_1: 0.08585
	loss_reward_1: 0.00931
	loss_policy_2: 0.05954
	accuracy_policy_2: 0.63918
	loss_value_2: 0.08796
	loss_reward_2: 0.01571
	loss_policy_3: 0.0597
	accuracy_policy_3: 0.64449
	loss_value_3: 0.08999
	loss_reward_3: 0.01636
	loss_policy_4: 0.05971
	accuracy_policy_4: 0.64863
	loss_value_4: 0.09184
	loss_reward_4: 0.01883
	loss_policy_5: 0.05987
	accuracy_policy_5: 0.64375
	loss_value_5: 0.0934
	loss_reward_5: 0.01963
	loss_policy: 0.59566
	loss_value: 0.86599
	loss_reward: 0.07983
[2025-05-07 12:46:39] nn step 11550, lr: 0.1.
	loss_policy_0: 0.29966
	accuracy_policy_0: 0.63008
	loss_value_0: 0.41889
	loss_policy_1: 0.0599
	accuracy_policy_1: 0.63207
	loss_value_1: 0.08597
	loss_reward_1: 0.00878
	loss_policy_2: 0.06
	accuracy_policy_2: 0.64289
	loss_value_2: 0.08844
	loss_reward_2: 0.01565
	loss_policy_3: 0.05991
	accuracy_policy_3: 0.64852
	loss_value_3: 0.09041
	loss_reward_3: 0.0169
	loss_policy_4: 0.05976
	accuracy_policy_4: 0.64715
	loss_value_4: 0.09202
	loss_reward_4: 0.01872
	loss_policy_5: 0.06019
	accuracy_policy_5: 0.64898
	loss_value_5: 0.09332
	loss_reward_5: 0.01962
	loss_policy: 0.59943
	loss_value: 0.86905
	loss_reward: 0.07966
[2025-05-07 12:46:47] nn step 11600, lr: 0.1.
	loss_policy_0: 0.2929
	accuracy_policy_0: 0.62648
	loss_value_0: 0.40173
	loss_policy_1: 0.0583
	accuracy_policy_1: 0.63352
	loss_value_1: 0.08287
	loss_reward_1: 0.00878
	loss_policy_2: 0.05769
	accuracy_policy_2: 0.64406
	loss_value_2: 0.08501
	loss_reward_2: 0.0147
	loss_policy_3: 0.05771
	accuracy_policy_3: 0.645
	loss_value_3: 0.08662
	loss_reward_3: 0.01625
	loss_policy_4: 0.05741
	accuracy_policy_4: 0.65082
	loss_value_4: 0.0886
	loss_reward_4: 0.01808
	loss_policy_5: 0.05766
	accuracy_policy_5: 0.65121
	loss_value_5: 0.09019
	loss_reward_5: 0.01863
	loss_policy: 0.58168
	loss_value: 0.83502
	loss_reward: 0.07644
Optimization_Done 11600
[2025-05-07 12:49:48] [command] train weight_iter_11600.pkl 40 59
[2025-05-07 12:49:58] nn step 11650, lr: 0.1.
	loss_policy_0: 0.30532
	accuracy_policy_0: 0.62719
	loss_value_0: 0.42843
	loss_policy_1: 0.06066
	accuracy_policy_1: 0.63176
	loss_value_1: 0.08792
	loss_reward_1: 0.00932
	loss_policy_2: 0.06081
	accuracy_policy_2: 0.63645
	loss_value_2: 0.08997
	loss_reward_2: 0.01577
	loss_policy_3: 0.06124
	accuracy_policy_3: 0.63324
	loss_value_3: 0.09193
	loss_reward_3: 0.01726
	loss_policy_4: 0.06091
	accuracy_policy_4: 0.64254
	loss_value_4: 0.09362
	loss_reward_4: 0.01903
	loss_policy_5: 0.06109
	accuracy_policy_5: 0.63973
	loss_value_5: 0.09495
	loss_reward_5: 0.01943
	loss_policy: 0.61002
	loss_value: 0.88682
	loss_reward: 0.08082
[2025-05-07 12:50:06] nn step 11700, lr: 0.1.
	loss_policy_0: 0.31664
	accuracy_policy_0: 0.62453
	loss_value_0: 0.43609
	loss_policy_1: 0.06252
	accuracy_policy_1: 0.63328
	loss_value_1: 0.08933
	loss_reward_1: 0.00957
	loss_policy_2: 0.06306
	accuracy_policy_2: 0.63047
	loss_value_2: 0.09169
	loss_reward_2: 0.01628
	loss_policy_3: 0.06303
	accuracy_policy_3: 0.63523
	loss_value_3: 0.09392
	loss_reward_3: 0.01698
	loss_policy_4: 0.06302
	accuracy_policy_4: 0.63594
	loss_value_4: 0.0955
	loss_reward_4: 0.01964
	loss_policy_5: 0.06301
	accuracy_policy_5: 0.63934
	loss_value_5: 0.09714
	loss_reward_5: 0.02014
	loss_policy: 0.63129
	loss_value: 0.90367
	loss_reward: 0.08261
[2025-05-07 12:50:12] nn step 11750, lr: 0.1.
	loss_policy_0: 0.30903
	accuracy_policy_0: 0.63273
	loss_value_0: 0.4316
	loss_policy_1: 0.06179
	accuracy_policy_1: 0.63352
	loss_value_1: 0.08877
	loss_reward_1: 0.00973
	loss_policy_2: 0.06182
	accuracy_policy_2: 0.63398
	loss_value_2: 0.09097
	loss_reward_2: 0.01579
	loss_policy_3: 0.06188
	accuracy_policy_3: 0.64113
	loss_value_3: 0.09298
	loss_reward_3: 0.01715
	loss_policy_4: 0.06243
	accuracy_policy_4: 0.63359
	loss_value_4: 0.09523
	loss_reward_4: 0.01948
	loss_policy_5: 0.06192
	accuracy_policy_5: 0.64727
	loss_value_5: 0.09688
	loss_reward_5: 0.01997
	loss_policy: 0.61887
	loss_value: 0.89643
	loss_reward: 0.08211
[2025-05-07 12:50:20] nn step 11800, lr: 0.1.
	loss_policy_0: 0.30298
	accuracy_policy_0: 0.63012
	loss_value_0: 0.41751
	loss_policy_1: 0.06017
	accuracy_policy_1: 0.63461
	loss_value_1: 0.08566
	loss_reward_1: 0.00907
	loss_policy_2: 0.06007
	accuracy_policy_2: 0.64098
	loss_value_2: 0.08779
	loss_reward_2: 0.01499
	loss_policy_3: 0.06013
	accuracy_policy_3: 0.64176
	loss_value_3: 0.09017
	loss_reward_3: 0.01672
	loss_policy_4: 0.06055
	accuracy_policy_4: 0.64188
	loss_value_4: 0.09221
	loss_reward_4: 0.01872
	loss_policy_5: 0.06086
	accuracy_policy_5: 0.63535
	loss_value_5: 0.09365
	loss_reward_5: 0.01939
	loss_policy: 0.60475
	loss_value: 0.867
	loss_reward: 0.0789
Optimization_Done 11800
[2025-05-07 12:53:31] [command] train weight_iter_11800.pkl 41 60
[2025-05-07 12:53:41] nn step 11850, lr: 0.1.
	loss_policy_0: 0.3055
	accuracy_policy_0: 0.62363
	loss_value_0: 0.42925
	loss_policy_1: 0.06121
	accuracy_policy_1: 0.62816
	loss_value_1: 0.08832
	loss_reward_1: 0.00943
	loss_policy_2: 0.06149
	accuracy_policy_2: 0.62703
	loss_value_2: 0.09027
	loss_reward_2: 0.01541
	loss_policy_3: 0.06109
	accuracy_policy_3: 0.63309
	loss_value_3: 0.09232
	loss_reward_3: 0.01677
	loss_policy_4: 0.06112
	accuracy_policy_4: 0.63758
	loss_value_4: 0.09419
	loss_reward_4: 0.01936
	loss_policy_5: 0.0611
	accuracy_policy_5: 0.63711
	loss_value_5: 0.09598
	loss_reward_5: 0.019
	loss_policy: 0.61151
	loss_value: 0.89033
	loss_reward: 0.07997
[2025-05-07 12:53:49] nn step 11900, lr: 0.1.
	loss_policy_0: 0.3088
	accuracy_policy_0: 0.62266
	loss_value_0: 0.42364
	loss_policy_1: 0.06144
	accuracy_policy_1: 0.63137
	loss_value_1: 0.08687
	loss_reward_1: 0.00909
	loss_policy_2: 0.06132
	accuracy_policy_2: 0.63184
	loss_value_2: 0.08946
	loss_reward_2: 0.01474
	loss_policy_3: 0.06145
	accuracy_policy_3: 0.63484
	loss_value_3: 0.09147
	loss_reward_3: 0.01688
	loss_policy_4: 0.06162
	accuracy_policy_4: 0.63879
	loss_value_4: 0.09306
	loss_reward_4: 0.01925
	loss_policy_5: 0.06141
	accuracy_policy_5: 0.64172
	loss_value_5: 0.09473
	loss_reward_5: 0.01918
	loss_policy: 0.61604
	loss_value: 0.87923
	loss_reward: 0.07915
[2025-05-07 12:53:57] nn step 11950, lr: 0.1.
	loss_policy_0: 0.30947
	accuracy_policy_0: 0.62992
	loss_value_0: 0.42612
	loss_policy_1: 0.06213
	accuracy_policy_1: 0.62719
	loss_value_1: 0.08805
	loss_reward_1: 0.01001
	loss_policy_2: 0.06216
	accuracy_policy_2: 0.63309
	loss_value_2: 0.09015
	loss_reward_2: 0.0157
	loss_policy_3: 0.06234
	accuracy_policy_3: 0.63402
	loss_value_3: 0.09228
	loss_reward_3: 0.01734
	loss_policy_4: 0.06217
	accuracy_policy_4: 0.63617
	loss_value_4: 0.09399
	loss_reward_4: 0.01905
	loss_policy_5: 0.0623
	accuracy_policy_5: 0.6384
	loss_value_5: 0.09568
	loss_reward_5: 0.01933
	loss_policy: 0.62056
	loss_value: 0.88629
	loss_reward: 0.08142
[2025-05-07 12:54:03] nn step 12000, lr: 0.1.
	loss_policy_0: 0.29806
	accuracy_policy_0: 0.62562
	loss_value_0: 0.40793
	loss_policy_1: 0.05908
	accuracy_policy_1: 0.63367
	loss_value_1: 0.08404
	loss_reward_1: 0.00887
	loss_policy_2: 0.05904
	accuracy_policy_2: 0.62867
	loss_value_2: 0.08626
	loss_reward_2: 0.01481
	loss_policy_3: 0.05906
	accuracy_policy_3: 0.63695
	loss_value_3: 0.08818
	loss_reward_3: 0.01593
	loss_policy_4: 0.05923
	accuracy_policy_4: 0.63992
	loss_value_4: 0.08985
	loss_reward_4: 0.01835
	loss_policy_5: 0.05945
	accuracy_policy_5: 0.63914
	loss_value_5: 0.09125
	loss_reward_5: 0.01853
	loss_policy: 0.59392
	loss_value: 0.84751
	loss_reward: 0.07649
Optimization_Done 12000
[2025-05-07 12:57:12] [command] train weight_iter_12000.pkl 42 61
[2025-05-07 12:57:19] nn step 12050, lr: 0.1.
	loss_policy_0: 0.30608
	accuracy_policy_0: 0.62684
	loss_value_0: 0.4277
	loss_policy_1: 0.06126
	accuracy_policy_1: 0.62961
	loss_value_1: 0.08773
	loss_reward_1: 0.00936
	loss_policy_2: 0.06165
	accuracy_policy_2: 0.63273
	loss_value_2: 0.08974
	loss_reward_2: 0.01529
	loss_policy_3: 0.06145
	accuracy_policy_3: 0.63395
	loss_value_3: 0.09165
	loss_reward_3: 0.01698
	loss_policy_4: 0.0614
	accuracy_policy_4: 0.63582
	loss_value_4: 0.09304
	loss_reward_4: 0.01845
	loss_policy_5: 0.06147
	accuracy_policy_5: 0.64008
	loss_value_5: 0.09445
	loss_reward_5: 0.01897
	loss_policy: 0.6133
	loss_value: 0.88431
	loss_reward: 0.07905
[2025-05-07 12:57:27] nn step 12100, lr: 0.1.
	loss_policy_0: 0.30497
	accuracy_policy_0: 0.63625
	loss_value_0: 0.42366
	loss_policy_1: 0.06116
	accuracy_policy_1: 0.63629
	loss_value_1: 0.08744
	loss_reward_1: 0.00944
	loss_policy_2: 0.06097
	accuracy_policy_2: 0.63453
	loss_value_2: 0.08971
	loss_reward_2: 0.0153
	loss_policy_3: 0.06129
	accuracy_policy_3: 0.6382
	loss_value_3: 0.09145
	loss_reward_3: 0.0169
	loss_policy_4: 0.06146
	accuracy_policy_4: 0.64422
	loss_value_4: 0.09316
	loss_reward_4: 0.01888
	loss_policy_5: 0.06148
	accuracy_policy_5: 0.64445
	loss_value_5: 0.09476
	loss_reward_5: 0.01889
	loss_policy: 0.61133
	loss_value: 0.88017
	loss_reward: 0.07941
[2025-05-07 12:57:35] nn step 12150, lr: 0.1.
	loss_policy_0: 0.30763
	accuracy_policy_0: 0.6341
	loss_value_0: 0.42589
	loss_policy_1: 0.06185
	accuracy_policy_1: 0.62844
	loss_value_1: 0.08772
	loss_reward_1: 0.00945
	loss_policy_2: 0.06173
	accuracy_policy_2: 0.63242
	loss_value_2: 0.08986
	loss_reward_2: 0.01524
	loss_policy_3: 0.06141
	accuracy_policy_3: 0.63938
	loss_value_3: 0.0921
	loss_reward_3: 0.01641
	loss_policy_4: 0.06211
	accuracy_policy_4: 0.63813
	loss_value_4: 0.09388
	loss_reward_4: 0.01895
	loss_policy_5: 0.0617
	accuracy_policy_5: 0.64203
	loss_value_5: 0.0956
	loss_reward_5: 0.01999
	loss_policy: 0.61644
	loss_value: 0.88505
	loss_reward: 0.08005
[2025-05-07 12:57:43] nn step 12200, lr: 0.1.
	loss_policy_0: 0.3183
	accuracy_policy_0: 0.62121
	loss_value_0: 0.43159
	loss_policy_1: 0.0625
	accuracy_policy_1: 0.63629
	loss_value_1: 0.08837
	loss_reward_1: 0.00974
	loss_policy_2: 0.06236
	accuracy_policy_2: 0.6368
	loss_value_2: 0.09061
	loss_reward_2: 0.01568
	loss_policy_3: 0.06236
	accuracy_policy_3: 0.64133
	loss_value_3: 0.09284
	loss_reward_3: 0.01687
	loss_policy_4: 0.0626
	accuracy_policy_4: 0.64777
	loss_value_4: 0.09461
	loss_reward_4: 0.01943
	loss_policy_5: 0.06304
	accuracy_policy_5: 0.64273
	loss_value_5: 0.09647
	loss_reward_5: 0.02009
	loss_policy: 0.63115
	loss_value: 0.8945
	loss_reward: 0.08181
Optimization_Done 12200
[2025-05-07 13:00:40] [command] train weight_iter_12200.pkl 43 62
[2025-05-07 13:00:49] nn step 12250, lr: 0.1.
	loss_policy_0: 0.29893
	accuracy_policy_0: 0.61449
	loss_value_0: 0.41058
	loss_policy_1: 0.05942
	accuracy_policy_1: 0.62867
	loss_value_1: 0.08383
	loss_reward_1: 0.0094
	loss_policy_2: 0.05935
	accuracy_policy_2: 0.6232
	loss_value_2: 0.08585
	loss_reward_2: 0.01487
	loss_policy_3: 0.05923
	accuracy_policy_3: 0.62297
	loss_value_3: 0.08801
	loss_reward_3: 0.0164
	loss_policy_4: 0.05944
	accuracy_policy_4: 0.6377
	loss_value_4: 0.08962
	loss_reward_4: 0.01852
	loss_policy_5: 0.05921
	accuracy_policy_5: 0.63836
	loss_value_5: 0.09135
	loss_reward_5: 0.01877
	loss_policy: 0.59559
	loss_value: 0.84923
	loss_reward: 0.07797
[2025-05-07 13:00:57] nn step 12300, lr: 0.1.
	loss_policy_0: 0.31115
	accuracy_policy_0: 0.62844
	loss_value_0: 0.42934
	loss_policy_1: 0.06216
	accuracy_policy_1: 0.63227
	loss_value_1: 0.08802
	loss_reward_1: 0.0096
	loss_policy_2: 0.06236
	accuracy_policy_2: 0.62562
	loss_value_2: 0.09004
	loss_reward_2: 0.01494
	loss_policy_3: 0.06224
	accuracy_policy_3: 0.63031
	loss_value_3: 0.09183
	loss_reward_3: 0.01658
	loss_policy_4: 0.06259
	accuracy_policy_4: 0.6368
	loss_value_4: 0.09412
	loss_reward_4: 0.01912
	loss_policy_5: 0.06255
	accuracy_policy_5: 0.63621
	loss_value_5: 0.0956
	loss_reward_5: 0.01975
	loss_policy: 0.62305
	loss_value: 0.88894
	loss_reward: 0.08
[2025-05-07 13:01:05] nn step 12350, lr: 0.1.
	loss_policy_0: 0.28939
	accuracy_policy_0: 0.62641
	loss_value_0: 0.39851
	loss_policy_1: 0.05802
	accuracy_policy_1: 0.63125
	loss_value_1: 0.08172
	loss_reward_1: 0.00899
	loss_policy_2: 0.05778
	accuracy_policy_2: 0.63707
	loss_value_2: 0.08386
	loss_reward_2: 0.01427
	loss_policy_3: 0.05775
	accuracy_policy_3: 0.63543
	loss_value_3: 0.08561
	loss_reward_3: 0.01549
	loss_policy_4: 0.0583
	accuracy_policy_4: 0.63734
	loss_value_4: 0.08743
	loss_reward_4: 0.01804
	loss_policy_5: 0.05807
	accuracy_policy_5: 0.64207
	loss_value_5: 0.08865
	loss_reward_5: 0.01826
	loss_policy: 0.57931
	loss_value: 0.82577
	loss_reward: 0.07506
[2025-05-07 13:01:12] nn step 12400, lr: 0.1.
	loss_policy_0: 0.29723
	accuracy_policy_0: 0.63234
	loss_value_0: 0.41275
	loss_policy_1: 0.05987
	accuracy_policy_1: 0.63109
	loss_value_1: 0.08496
	loss_reward_1: 0.00886
	loss_policy_2: 0.06001
	accuracy_policy_2: 0.63109
	loss_value_2: 0.08705
	loss_reward_2: 0.01461
	loss_policy_3: 0.06002
	accuracy_policy_3: 0.63297
	loss_value_3: 0.08876
	loss_reward_3: 0.01673
	loss_policy_4: 0.06004
	accuracy_policy_4: 0.63938
	loss_value_4: 0.09023
	loss_reward_4: 0.0189
	loss_policy_5: 0.06036
	accuracy_policy_5: 0.63867
	loss_value_5: 0.09182
	loss_reward_5: 0.01856
	loss_policy: 0.59753
	loss_value: 0.85558
	loss_reward: 0.07766
Optimization_Done 12400
[2025-05-07 13:04:23] [command] train weight_iter_12400.pkl 44 63
[2025-05-07 13:04:31] nn step 12450, lr: 0.1.
	loss_policy_0: 0.28902
	accuracy_policy_0: 0.62656
	loss_value_0: 0.40166
	loss_policy_1: 0.058
	accuracy_policy_1: 0.62617
	loss_value_1: 0.08232
	loss_reward_1: 0.00871
	loss_policy_2: 0.05822
	accuracy_policy_2: 0.62898
	loss_value_2: 0.08406
	loss_reward_2: 0.0139
	loss_policy_3: 0.0584
	accuracy_policy_3: 0.62488
	loss_value_3: 0.08562
	loss_reward_3: 0.01546
	loss_policy_4: 0.05854
	accuracy_policy_4: 0.63047
	loss_value_4: 0.08741
	loss_reward_4: 0.01771
	loss_policy_5: 0.05833
	accuracy_policy_5: 0.63262
	loss_value_5: 0.08874
	loss_reward_5: 0.01816
	loss_policy: 0.5805
	loss_value: 0.82981
	loss_reward: 0.07393
[2025-05-07 13:04:39] nn step 12500, lr: 0.1.
	loss_policy_0: 0.27392
	accuracy_policy_0: 0.63309
	loss_value_0: 0.37556
	loss_policy_1: 0.05504
	accuracy_policy_1: 0.63039
	loss_value_1: 0.07721
	loss_reward_1: 0.00825
	loss_policy_2: 0.05548
	accuracy_policy_2: 0.62859
	loss_value_2: 0.07925
	loss_reward_2: 0.01345
	loss_policy_3: 0.05522
	accuracy_policy_3: 0.63641
	loss_value_3: 0.08118
	loss_reward_3: 0.01496
	loss_policy_4: 0.0553
	accuracy_policy_4: 0.64062
	loss_value_4: 0.08294
	loss_reward_4: 0.01718
	loss_policy_5: 0.05584
	accuracy_policy_5: 0.6309
	loss_value_5: 0.08419
	loss_reward_5: 0.01703
	loss_policy: 0.55079
	loss_value: 0.78034
	loss_reward: 0.07088
[2025-05-07 13:04:47] nn step 12550, lr: 0.1.
	loss_policy_0: 0.30282
	accuracy_policy_0: 0.62816
	loss_value_0: 0.42003
	loss_policy_1: 0.06051
	accuracy_policy_1: 0.63387
	loss_value_1: 0.08643
	loss_reward_1: 0.00894
	loss_policy_2: 0.06115
	accuracy_policy_2: 0.62742
	loss_value_2: 0.08883
	loss_reward_2: 0.01491
	loss_policy_3: 0.0613
	accuracy_policy_3: 0.62848
	loss_value_3: 0.09097
	loss_reward_3: 0.01689
	loss_policy_4: 0.06078
	accuracy_policy_4: 0.63285
	loss_value_4: 0.09256
	loss_reward_4: 0.01879
	loss_policy_5: 0.06117
	accuracy_policy_5: 0.63449
	loss_value_5: 0.0942
	loss_reward_5: 0.01889
	loss_policy: 0.60774
	loss_value: 0.87303
	loss_reward: 0.07842
[2025-05-07 13:04:54] nn step 12600, lr: 0.1.
	loss_policy_0: 0.29653
	accuracy_policy_0: 0.62277
	loss_value_0: 0.40258
	loss_policy_1: 0.05896
	accuracy_policy_1: 0.63414
	loss_value_1: 0.08242
	loss_reward_1: 0.00944
	loss_policy_2: 0.05896
	accuracy_policy_2: 0.63383
	loss_value_2: 0.08482
	loss_reward_2: 0.01461
	loss_policy_3: 0.05914
	accuracy_policy_3: 0.62859
	loss_value_3: 0.08678
	loss_reward_3: 0.01589
	loss_policy_4: 0.05894
	accuracy_policy_4: 0.62941
	loss_value_4: 0.0885
	loss_reward_4: 0.01764
	loss_policy_5: 0.05854
	accuracy_policy_5: 0.64297
	loss_value_5: 0.08971
	loss_reward_5: 0.01832
	loss_policy: 0.59107
	loss_value: 0.83482
	loss_reward: 0.0759
Optimization_Done 12600
[2025-05-07 13:07:51] [command] train weight_iter_12600.pkl 45 64
[2025-05-07 13:08:00] nn step 12650, lr: 0.1.
	loss_policy_0: 0.28677
	accuracy_policy_0: 0.6384
	loss_value_0: 0.409
	loss_policy_1: 0.05747
	accuracy_policy_1: 0.63258
	loss_value_1: 0.08389
	loss_reward_1: 0.00913
	loss_policy_2: 0.05755
	accuracy_policy_2: 0.63332
	loss_value_2: 0.08565
	loss_reward_2: 0.01409
	loss_policy_3: 0.05768
	accuracy_policy_3: 0.63824
	loss_value_3: 0.08717
	loss_reward_3: 0.01577
	loss_policy_4: 0.05725
	accuracy_policy_4: 0.6409
	loss_value_4: 0.08889
	loss_reward_4: 0.01791
	loss_policy_5: 0.05751
	accuracy_policy_5: 0.64402
	loss_value_5: 0.09024
	loss_reward_5: 0.01839
	loss_policy: 0.57423
	loss_value: 0.84484
	loss_reward: 0.0753
[2025-05-07 13:08:08] nn step 12700, lr: 0.1.
	loss_policy_0: 0.28062
	accuracy_policy_0: 0.63922
	loss_value_0: 0.3958
	loss_policy_1: 0.05643
	accuracy_policy_1: 0.63352
	loss_value_1: 0.08123
	loss_reward_1: 0.00882
	loss_policy_2: 0.05664
	accuracy_policy_2: 0.63211
	loss_value_2: 0.08343
	loss_reward_2: 0.01414
	loss_policy_3: 0.05633
	accuracy_policy_3: 0.64188
	loss_value_3: 0.08512
	loss_reward_3: 0.0157
	loss_policy_4: 0.05639
	accuracy_policy_4: 0.64281
	loss_value_4: 0.08676
	loss_reward_4: 0.01807
	loss_policy_5: 0.05702
	accuracy_policy_5: 0.64125
	loss_value_5: 0.08833
	loss_reward_5: 0.01738
	loss_policy: 0.56343
	loss_value: 0.82067
	loss_reward: 0.0741
[2025-05-07 13:08:15] nn step 12750, lr: 0.1.
	loss_policy_0: 0.29537
	accuracy_policy_0: 0.63805
	loss_value_0: 0.41425
	loss_policy_1: 0.05916
	accuracy_policy_1: 0.64012
	loss_value_1: 0.08471
	loss_reward_1: 0.00935
	loss_policy_2: 0.0594
	accuracy_policy_2: 0.63289
	loss_value_2: 0.08699
	loss_reward_2: 0.01479
	loss_policy_3: 0.05947
	accuracy_policy_3: 0.64016
	loss_value_3: 0.08904
	loss_reward_3: 0.01644
	loss_policy_4: 0.05951
	accuracy_policy_4: 0.64168
	loss_value_4: 0.09079
	loss_reward_4: 0.0193
	loss_policy_5: 0.05906
	accuracy_policy_5: 0.64266
	loss_value_5: 0.09243
	loss_reward_5: 0.01877
	loss_policy: 0.59196
	loss_value: 0.85821
	loss_reward: 0.07865
[2025-05-07 13:08:23] nn step 12800, lr: 0.1.
	loss_policy_0: 0.28028
	accuracy_policy_0: 0.63469
	loss_value_0: 0.38998
	loss_policy_1: 0.05643
	accuracy_policy_1: 0.63031
	loss_value_1: 0.08024
	loss_reward_1: 0.00853
	loss_policy_2: 0.05624
	accuracy_policy_2: 0.63836
	loss_value_2: 0.08229
	loss_reward_2: 0.01388
	loss_policy_3: 0.05665
	accuracy_policy_3: 0.63168
	loss_value_3: 0.08463
	loss_reward_3: 0.01521
	loss_policy_4: 0.05633
	accuracy_policy_4: 0.64199
	loss_value_4: 0.08648
	loss_reward_4: 0.01793
	loss_policy_5: 0.05632
	accuracy_policy_5: 0.6441
	loss_value_5: 0.08797
	loss_reward_5: 0.01764
	loss_policy: 0.56225
	loss_value: 0.81158
	loss_reward: 0.07318
Optimization_Done 12800
[2025-05-07 13:11:30] [command] train weight_iter_12800.pkl 46 65
[2025-05-07 13:11:38] nn step 12850, lr: 0.1.
	loss_policy_0: 0.30265
	accuracy_policy_0: 0.64367
	loss_value_0: 0.43553
	loss_policy_1: 0.0616
	accuracy_policy_1: 0.64008
	loss_value_1: 0.08939
	loss_reward_1: 0.00951
	loss_policy_2: 0.06151
	accuracy_policy_2: 0.64059
	loss_value_2: 0.09175
	loss_reward_2: 0.0154
	loss_policy_3: 0.0617
	accuracy_policy_3: 0.6398
	loss_value_3: 0.09376
	loss_reward_3: 0.01717
	loss_policy_4: 0.06128
	accuracy_policy_4: 0.64371
	loss_value_4: 0.09538
	loss_reward_4: 0.01967
	loss_policy_5: 0.06157
	accuracy_policy_5: 0.64555
	loss_value_5: 0.09699
	loss_reward_5: 0.01932
	loss_policy: 0.61032
	loss_value: 0.9028
	loss_reward: 0.08107
[2025-05-07 13:11:46] nn step 12900, lr: 0.1.
	loss_policy_0: 0.2964
	accuracy_policy_0: 0.64172
	loss_value_0: 0.41999
	loss_policy_1: 0.05931
	accuracy_policy_1: 0.64203
	loss_value_1: 0.08605
	loss_reward_1: 0.00907
	loss_policy_2: 0.05929
	accuracy_policy_2: 0.64504
	loss_value_2: 0.08844
	loss_reward_2: 0.01436
	loss_policy_3: 0.05975
	accuracy_policy_3: 0.64227
	loss_value_3: 0.09066
	loss_reward_3: 0.01672
	loss_policy_4: 0.05953
	accuracy_policy_4: 0.64754
	loss_value_4: 0.09217
	loss_reward_4: 0.01863
	loss_policy_5: 0.05993
	accuracy_policy_5: 0.64883
	loss_value_5: 0.09374
	loss_reward_5: 0.01909
	loss_policy: 0.59421
	loss_value: 0.87105
	loss_reward: 0.07787
[2025-05-07 13:11:54] nn step 12950, lr: 0.1.
	loss_policy_0: 0.29435
	accuracy_policy_0: 0.6398
	loss_value_0: 0.4197
	loss_policy_1: 0.05939
	accuracy_policy_1: 0.63984
	loss_value_1: 0.08628
	loss_reward_1: 0.00886
	loss_policy_2: 0.05969
	accuracy_policy_2: 0.64094
	loss_value_2: 0.08866
	loss_reward_2: 0.01451
	loss_policy_3: 0.05987
	accuracy_policy_3: 0.63539
	loss_value_3: 0.0906
	loss_reward_3: 0.01644
	loss_policy_4: 0.06016
	accuracy_policy_4: 0.64398
	loss_value_4: 0.09248
	loss_reward_4: 0.01866
	loss_policy_5: 0.06033
	accuracy_policy_5: 0.64234
	loss_value_5: 0.09437
	loss_reward_5: 0.01867
	loss_policy: 0.59379
	loss_value: 0.8721
	loss_reward: 0.07715
[2025-05-07 13:12:02] nn step 13000, lr: 0.1.
	loss_policy_0: 0.30462
	accuracy_policy_0: 0.63457
	loss_value_0: 0.42714
	loss_policy_1: 0.06117
	accuracy_policy_1: 0.63895
	loss_value_1: 0.08745
	loss_reward_1: 0.00975
	loss_policy_2: 0.06154
	accuracy_policy_2: 0.63348
	loss_value_2: 0.08992
	loss_reward_2: 0.01546
	loss_policy_3: 0.06186
	accuracy_policy_3: 0.63488
	loss_value_3: 0.09193
	loss_reward_3: 0.01719
	loss_policy_4: 0.06195
	accuracy_policy_4: 0.64199
	loss_value_4: 0.09396
	loss_reward_4: 0.01948
	loss_policy_5: 0.06196
	accuracy_policy_5: 0.64453
	loss_value_5: 0.09598
	loss_reward_5: 0.0191
	loss_policy: 0.6131
	loss_value: 0.88638
	loss_reward: 0.08098
Optimization_Done 13000
[2025-05-07 13:15:01] [command] train weight_iter_13000.pkl 47 66
[2025-05-07 13:15:11] nn step 13050, lr: 0.1.
	loss_policy_0: 0.30292
	accuracy_policy_0: 0.63039
	loss_value_0: 0.42388
	loss_policy_1: 0.06021
	accuracy_policy_1: 0.63598
	loss_value_1: 0.08724
	loss_reward_1: 0.00933
	loss_policy_2: 0.06048
	accuracy_policy_2: 0.63984
	loss_value_2: 0.08946
	loss_reward_2: 0.01497
	loss_policy_3: 0.0607
	accuracy_policy_3: 0.63535
	loss_value_3: 0.09135
	loss_reward_3: 0.01693
	loss_policy_4: 0.06113
	accuracy_policy_4: 0.63938
	loss_value_4: 0.09276
	loss_reward_4: 0.01923
	loss_policy_5: 0.06109
	accuracy_policy_5: 0.63562
	loss_value_5: 0.09439
	loss_reward_5: 0.01881
	loss_policy: 0.60653
	loss_value: 0.87908
	loss_reward: 0.07927
[2025-05-07 13:15:19] nn step 13100, lr: 0.1.
	loss_policy_0: 0.28683
	accuracy_policy_0: 0.64582
	loss_value_0: 0.40619
	loss_policy_1: 0.05768
	accuracy_policy_1: 0.63973
	loss_value_1: 0.08274
	loss_reward_1: 0.00895
	loss_policy_2: 0.05757
	accuracy_policy_2: 0.64004
	loss_value_2: 0.08512
	loss_reward_2: 0.0143
	loss_policy_3: 0.05775
	accuracy_policy_3: 0.64332
	loss_value_3: 0.08714
	loss_reward_3: 0.01635
	loss_policy_4: 0.05796
	accuracy_policy_4: 0.6452
	loss_value_4: 0.08887
	loss_reward_4: 0.01792
	loss_policy_5: 0.0579
	accuracy_policy_5: 0.64824
	loss_value_5: 0.09018
	loss_reward_5: 0.01841
	loss_policy: 0.57569
	loss_value: 0.84024
	loss_reward: 0.07594
[2025-05-07 13:15:25] nn step 13150, lr: 0.1.
	loss_policy_0: 0.29028
	accuracy_policy_0: 0.63617
	loss_value_0: 0.39786
	loss_policy_1: 0.05833
	accuracy_policy_1: 0.63398
	loss_value_1: 0.08194
	loss_reward_1: 0.00887
	loss_policy_2: 0.05818
	accuracy_policy_2: 0.63566
	loss_value_2: 0.08442
	loss_reward_2: 0.01441
	loss_policy_3: 0.05834
	accuracy_policy_3: 0.63754
	loss_value_3: 0.08639
	loss_reward_3: 0.01553
	loss_policy_4: 0.0579
	accuracy_policy_4: 0.64172
	loss_value_4: 0.0879
	loss_reward_4: 0.01797
	loss_policy_5: 0.05808
	accuracy_policy_5: 0.64301
	loss_value_5: 0.08958
	loss_reward_5: 0.01801
	loss_policy: 0.58111
	loss_value: 0.82808
	loss_reward: 0.07479
[2025-05-07 13:15:34] nn step 13200, lr: 0.1.
	loss_policy_0: 0.28564
	accuracy_policy_0: 0.63902
	loss_value_0: 0.39502
	loss_policy_1: 0.05719
	accuracy_policy_1: 0.63859
	loss_value_1: 0.08115
	loss_reward_1: 0.00871
	loss_policy_2: 0.0578
	accuracy_policy_2: 0.63227
	loss_value_2: 0.08334
	loss_reward_2: 0.01436
	loss_policy_3: 0.05727
	accuracy_policy_3: 0.63902
	loss_value_3: 0.085
	loss_reward_3: 0.01595
	loss_policy_4: 0.05731
	accuracy_policy_4: 0.63602
	loss_value_4: 0.08685
	loss_reward_4: 0.01803
	loss_policy_5: 0.05771
	accuracy_policy_5: 0.63707
	loss_value_5: 0.08826
	loss_reward_5: 0.01855
	loss_policy: 0.57292
	loss_value: 0.81961
	loss_reward: 0.0756
Optimization_Done 13200
[2025-05-07 13:18:41] [command] train weight_iter_13200.pkl 48 67
[2025-05-07 13:18:48] nn step 13250, lr: 0.1.
	loss_policy_0: 0.27995
	accuracy_policy_0: 0.64516
	loss_value_0: 0.39647
	loss_policy_1: 0.05633
	accuracy_policy_1: 0.64332
	loss_value_1: 0.08114
	loss_reward_1: 0.00863
	loss_policy_2: 0.05652
	accuracy_policy_2: 0.63926
	loss_value_2: 0.08321
	loss_reward_2: 0.01385
	loss_policy_3: 0.05668
	accuracy_policy_3: 0.63914
	loss_value_3: 0.0848
	loss_reward_3: 0.01554
	loss_policy_4: 0.05686
	accuracy_policy_4: 0.63844
	loss_value_4: 0.08652
	loss_reward_4: 0.01776
	loss_policy_5: 0.05644
	accuracy_policy_5: 0.64414
	loss_value_5: 0.08807
	loss_reward_5: 0.0171
	loss_policy: 0.56279
	loss_value: 0.8202
	loss_reward: 0.07287
[2025-05-07 13:18:56] nn step 13300, lr: 0.1.
	loss_policy_0: 0.28869
	accuracy_policy_0: 0.64137
	loss_value_0: 0.40811
	loss_policy_1: 0.05773
	accuracy_policy_1: 0.64023
	loss_value_1: 0.08386
	loss_reward_1: 0.00907
	loss_policy_2: 0.05848
	accuracy_policy_2: 0.63895
	loss_value_2: 0.08597
	loss_reward_2: 0.01409
	loss_policy_3: 0.05853
	accuracy_policy_3: 0.63633
	loss_value_3: 0.08803
	loss_reward_3: 0.01612
	loss_policy_4: 0.05845
	accuracy_policy_4: 0.63895
	loss_value_4: 0.0898
	loss_reward_4: 0.01863
	loss_policy_5: 0.05832
	accuracy_policy_5: 0.64535
	loss_value_5: 0.09151
	loss_reward_5: 0.01862
	loss_policy: 0.58021
	loss_value: 0.84727
	loss_reward: 0.07653
[2025-05-07 13:19:04] nn step 13350, lr: 0.1.
	loss_policy_0: 0.29841
	accuracy_policy_0: 0.63266
	loss_value_0: 0.41131
	loss_policy_1: 0.05936
	accuracy_policy_1: 0.63688
	loss_value_1: 0.08461
	loss_reward_1: 0.00886
	loss_policy_2: 0.05941
	accuracy_policy_2: 0.63945
	loss_value_2: 0.0872
	loss_reward_2: 0.01461
	loss_policy_3: 0.0597
	accuracy_policy_3: 0.63898
	loss_value_3: 0.08878
	loss_reward_3: 0.01625
	loss_policy_4: 0.05943
	accuracy_policy_4: 0.64109
	loss_value_4: 0.09022
	loss_reward_4: 0.01826
	loss_policy_5: 0.05988
	accuracy_policy_5: 0.64277
	loss_value_5: 0.09208
	loss_reward_5: 0.01855
	loss_policy: 0.59618
	loss_value: 0.85421
	loss_reward: 0.07653
[2025-05-07 13:19:12] nn step 13400, lr: 0.1.
	loss_policy_0: 0.27871
	accuracy_policy_0: 0.64953
	loss_value_0: 0.39019
	loss_policy_1: 0.05618
	accuracy_policy_1: 0.6416
	loss_value_1: 0.08002
	loss_reward_1: 0.00882
	loss_policy_2: 0.05611
	accuracy_policy_2: 0.64188
	loss_value_2: 0.0821
	loss_reward_2: 0.01387
	loss_policy_3: 0.05653
	accuracy_policy_3: 0.63793
	loss_value_3: 0.08385
	loss_reward_3: 0.01565
	loss_policy_4: 0.0562
	accuracy_policy_4: 0.64328
	loss_value_4: 0.0857
	loss_reward_4: 0.01793
	loss_policy_5: 0.05638
	accuracy_policy_5: 0.6491
	loss_value_5: 0.08769
	loss_reward_5: 0.01769
	loss_policy: 0.56011
	loss_value: 0.80954
	loss_reward: 0.07396
Optimization_Done 13400
[2025-05-07 13:22:19] [command] train weight_iter_13400.pkl 49 68
[2025-05-07 13:22:28] nn step 13450, lr: 0.1.
	loss_policy_0: 0.28708
	accuracy_policy_0: 0.65078
	loss_value_0: 0.41113
	loss_policy_1: 0.05758
	accuracy_policy_1: 0.65227
	loss_value_1: 0.08414
	loss_reward_1: 0.00911
	loss_policy_2: 0.05826
	accuracy_policy_2: 0.64926
	loss_value_2: 0.0865
	loss_reward_2: 0.01418
	loss_policy_3: 0.05828
	accuracy_policy_3: 0.64336
	loss_value_3: 0.08843
	loss_reward_3: 0.01642
	loss_policy_4: 0.05827
	accuracy_policy_4: 0.64297
	loss_value_4: 0.08998
	loss_reward_4: 0.01848
	loss_policy_5: 0.05834
	accuracy_policy_5: 0.64703
	loss_value_5: 0.0915
	loss_reward_5: 0.01856
	loss_policy: 0.57782
	loss_value: 0.85166
	loss_reward: 0.07676
[2025-05-07 13:22:35] nn step 13500, lr: 0.1.
	loss_policy_0: 0.30267
	accuracy_policy_0: 0.64316
	loss_value_0: 0.42502
	loss_policy_1: 0.06109
	accuracy_policy_1: 0.64496
	loss_value_1: 0.08729
	loss_reward_1: 0.0094
	loss_policy_2: 0.06102
	accuracy_policy_2: 0.64398
	loss_value_2: 0.08936
	loss_reward_2: 0.0148
	loss_policy_3: 0.06098
	accuracy_policy_3: 0.64539
	loss_value_3: 0.09126
	loss_reward_3: 0.01666
	loss_policy_4: 0.0615
	accuracy_policy_4: 0.63887
	loss_value_4: 0.09381
	loss_reward_4: 0.01896
	loss_policy_5: 0.0613
	accuracy_policy_5: 0.64738
	loss_value_5: 0.09543
	loss_reward_5: 0.01905
	loss_policy: 0.60856
	loss_value: 0.88217
	loss_reward: 0.07887
[2025-05-07 13:22:43] nn step 13550, lr: 0.1.
	loss_policy_0: 0.30424
	accuracy_policy_0: 0.64242
	loss_value_0: 0.42412
	loss_policy_1: 0.06091
	accuracy_policy_1: 0.64172
	loss_value_1: 0.08719
	loss_reward_1: 0.0093
	loss_policy_2: 0.06125
	accuracy_policy_2: 0.64012
	loss_value_2: 0.08971
	loss_reward_2: 0.01459
	loss_policy_3: 0.0613
	accuracy_policy_3: 0.63816
	loss_value_3: 0.09169
	loss_reward_3: 0.01689
	loss_policy_4: 0.06109
	accuracy_policy_4: 0.6432
	loss_value_4: 0.09348
	loss_reward_4: 0.01908
	loss_policy_5: 0.06087
	accuracy_policy_5: 0.64312
	loss_value_5: 0.09512
	loss_reward_5: 0.01917
	loss_policy: 0.60967
	loss_value: 0.88131
	loss_reward: 0.07903
[2025-05-07 13:22:51] nn step 13600, lr: 0.1.
	loss_policy_0: 0.30164
	accuracy_policy_0: 0.64047
	loss_value_0: 0.41321
	loss_policy_1: 0.06032
	accuracy_policy_1: 0.64234
	loss_value_1: 0.08523
	loss_reward_1: 0.00949
	loss_policy_2: 0.06057
	accuracy_policy_2: 0.64211
	loss_value_2: 0.08777
	loss_reward_2: 0.01465
	loss_policy_3: 0.06055
	accuracy_policy_3: 0.63938
	loss_value_3: 0.09001
	loss_reward_3: 0.01656
	loss_policy_4: 0.06048
	accuracy_policy_4: 0.64641
	loss_value_4: 0.09184
	loss_reward_4: 0.01917
	loss_policy_5: 0.06059
	accuracy_policy_5: 0.64555
	loss_value_5: 0.09342
	loss_reward_5: 0.0188
	loss_policy: 0.60415
	loss_value: 0.86147
	loss_reward: 0.07867
Optimization_Done 13600
[2025-05-07 13:25:56] [command] train weight_iter_13600.pkl 50 69
[2025-05-07 13:26:05] nn step 13650, lr: 0.1.
	loss_policy_0: 0.28902
	accuracy_policy_0: 0.6527
	loss_value_0: 0.41213
	loss_policy_1: 0.05835
	accuracy_policy_1: 0.64668
	loss_value_1: 0.0843
	loss_reward_1: 0.00924
	loss_policy_2: 0.05842
	accuracy_policy_2: 0.64434
	loss_value_2: 0.08641
	loss_reward_2: 0.0148
	loss_policy_3: 0.05859
	accuracy_policy_3: 0.64352
	loss_value_3: 0.08855
	loss_reward_3: 0.01639
	loss_policy_4: 0.05834
	accuracy_policy_4: 0.65344
	loss_value_4: 0.09021
	loss_reward_4: 0.0184
	loss_policy_5: 0.05893
	accuracy_policy_5: 0.655
	loss_value_5: 0.09194
	loss_reward_5: 0.01874
	loss_policy: 0.58164
	loss_value: 0.85354
	loss_reward: 0.07757
[2025-05-07 13:26:13] nn step 13700, lr: 0.1.
	loss_policy_0: 0.28669
	accuracy_policy_0: 0.65406
	loss_value_0: 0.40397
	loss_policy_1: 0.05796
	accuracy_policy_1: 0.64793
	loss_value_1: 0.08257
	loss_reward_1: 0.00903
	loss_policy_2: 0.0578
	accuracy_policy_2: 0.65016
	loss_value_2: 0.08514
	loss_reward_2: 0.01416
	loss_policy_3: 0.0582
	accuracy_policy_3: 0.64848
	loss_value_3: 0.08703
	loss_reward_3: 0.01573
	loss_policy_4: 0.0584
	accuracy_policy_4: 0.64715
	loss_value_4: 0.08881
	loss_reward_4: 0.01873
	loss_policy_5: 0.05835
	accuracy_policy_5: 0.64738
	loss_value_5: 0.09018
	loss_reward_5: 0.01842
	loss_policy: 0.5774
	loss_value: 0.83769
	loss_reward: 0.07606
[2025-05-07 13:26:21] nn step 13750, lr: 0.1.
	loss_policy_0: 0.29268
	accuracy_policy_0: 0.65023
	loss_value_0: 0.40934
	loss_policy_1: 0.05948
	accuracy_policy_1: 0.64312
	loss_value_1: 0.08426
	loss_reward_1: 0.00895
	loss_policy_2: 0.0596
	accuracy_policy_2: 0.64496
	loss_value_2: 0.08664
	loss_reward_2: 0.01411
	loss_policy_3: 0.05971
	accuracy_policy_3: 0.64527
	loss_value_3: 0.08849
	loss_reward_3: 0.01636
	loss_policy_4: 0.05932
	accuracy_policy_4: 0.64777
	loss_value_4: 0.09024
	loss_reward_4: 0.01873
	loss_policy_5: 0.06016
	accuracy_policy_5: 0.64445
	loss_value_5: 0.09199
	loss_reward_5: 0.01881
	loss_policy: 0.59095
	loss_value: 0.85096
	loss_reward: 0.07697
[2025-05-07 13:26:28] nn step 13800, lr: 0.1.
	loss_policy_0: 0.28963
	accuracy_policy_0: 0.64938
	loss_value_0: 0.4024
	loss_policy_1: 0.05813
	accuracy_policy_1: 0.64887
	loss_value_1: 0.08257
	loss_reward_1: 0.00888
	loss_policy_2: 0.05871
	accuracy_policy_2: 0.64473
	loss_value_2: 0.08463
	loss_reward_2: 0.0142
	loss_policy_3: 0.05887
	accuracy_policy_3: 0.64836
	loss_value_3: 0.08663
	loss_reward_3: 0.01618
	loss_policy_4: 0.05864
	accuracy_policy_4: 0.64988
	loss_value_4: 0.08857
	loss_reward_4: 0.01774
	loss_policy_5: 0.05836
	accuracy_policy_5: 0.65473
	loss_value_5: 0.09036
	loss_reward_5: 0.01817
	loss_policy: 0.58236
	loss_value: 0.83516
	loss_reward: 0.07517
Optimization_Done 13800
[2025-05-07 13:29:33] [command] train weight_iter_13800.pkl 51 70
[2025-05-07 13:29:42] nn step 13850, lr: 0.1.
	loss_policy_0: 0.29848
	accuracy_policy_0: 0.6441
	loss_value_0: 0.4224
	loss_policy_1: 0.05985
	accuracy_policy_1: 0.65195
	loss_value_1: 0.08651
	loss_reward_1: 0.00908
	loss_policy_2: 0.06014
	accuracy_policy_2: 0.64234
	loss_value_2: 0.08849
	loss_reward_2: 0.01479
	loss_policy_3: 0.06029
	accuracy_policy_3: 0.6398
	loss_value_3: 0.09036
	loss_reward_3: 0.01643
	loss_policy_4: 0.06022
	accuracy_policy_4: 0.64551
	loss_value_4: 0.09192
	loss_reward_4: 0.01903
	loss_policy_5: 0.06068
	accuracy_policy_5: 0.64332
	loss_value_5: 0.09348
	loss_reward_5: 0.01936
	loss_policy: 0.59966
	loss_value: 0.87317
	loss_reward: 0.07868
[2025-05-07 13:29:48] nn step 13900, lr: 0.1.
	loss_policy_0: 0.28913
	accuracy_policy_0: 0.65102
	loss_value_0: 0.40514
	loss_policy_1: 0.05791
	accuracy_policy_1: 0.65203
	loss_value_1: 0.0831
	loss_reward_1: 0.00883
	loss_policy_2: 0.05858
	accuracy_policy_2: 0.6475
	loss_value_2: 0.08542
	loss_reward_2: 0.01393
	loss_policy_3: 0.05853
	accuracy_policy_3: 0.65078
	loss_value_3: 0.08736
	loss_reward_3: 0.0159
	loss_policy_4: 0.05834
	accuracy_policy_4: 0.65488
	loss_value_4: 0.08917
	loss_reward_4: 0.01845
	loss_policy_5: 0.05856
	accuracy_policy_5: 0.65137
	loss_value_5: 0.09063
	loss_reward_5: 0.01812
	loss_policy: 0.58106
	loss_value: 0.84082
	loss_reward: 0.07523
[2025-05-07 13:29:56] nn step 13950, lr: 0.1.
	loss_policy_0: 0.28669
	accuracy_policy_0: 0.65578
	loss_value_0: 0.40691
	loss_policy_1: 0.05827
	accuracy_policy_1: 0.64938
	loss_value_1: 0.08352
	loss_reward_1: 0.00876
	loss_policy_2: 0.05857
	accuracy_policy_2: 0.64562
	loss_value_2: 0.08651
	loss_reward_2: 0.01421
	loss_policy_3: 0.05846
	accuracy_policy_3: 0.64695
	loss_value_3: 0.08843
	loss_reward_3: 0.01662
	loss_policy_4: 0.05901
	accuracy_policy_4: 0.6443
	loss_value_4: 0.09032
	loss_reward_4: 0.01842
	loss_policy_5: 0.05916
	accuracy_policy_5: 0.64992
	loss_value_5: 0.09226
	loss_reward_5: 0.01821
	loss_policy: 0.58017
	loss_value: 0.84795
	loss_reward: 0.07623
[2025-05-07 13:30:04] nn step 14000, lr: 0.1.
	loss_policy_0: 0.29972
	accuracy_policy_0: 0.65117
	loss_value_0: 0.41774
	loss_policy_1: 0.06078
	accuracy_policy_1: 0.64402
	loss_value_1: 0.08574
	loss_reward_1: 0.00921
	loss_policy_2: 0.06067
	accuracy_policy_2: 0.64086
	loss_value_2: 0.08808
	loss_reward_2: 0.01511
	loss_policy_3: 0.06085
	accuracy_policy_3: 0.64469
	loss_value_3: 0.09018
	loss_reward_3: 0.01656
	loss_policy_4: 0.06095
	accuracy_policy_4: 0.64508
	loss_value_4: 0.09165
	loss_reward_4: 0.01906
	loss_policy_5: 0.06119
	accuracy_policy_5: 0.64301
	loss_value_5: 0.09348
	loss_reward_5: 0.01919
	loss_policy: 0.60417
	loss_value: 0.86687
	loss_reward: 0.07912
Optimization_Done 14000
[2025-05-07 13:33:15] [command] train weight_iter_14000.pkl 52 71
[2025-05-07 13:33:24] nn step 14050, lr: 0.1.
	loss_policy_0: 0.28283
	accuracy_policy_0: 0.64535
	loss_value_0: 0.40312
	loss_policy_1: 0.057
	accuracy_policy_1: 0.64301
	loss_value_1: 0.08265
	loss_reward_1: 0.00861
	loss_policy_2: 0.05724
	accuracy_policy_2: 0.64055
	loss_value_2: 0.0849
	loss_reward_2: 0.01437
	loss_policy_3: 0.05701
	accuracy_policy_3: 0.64418
	loss_value_3: 0.08667
	loss_reward_3: 0.01573
	loss_policy_4: 0.05711
	accuracy_policy_4: 0.645
	loss_value_4: 0.08812
	loss_reward_4: 0.0174
	loss_policy_5: 0.05726
	accuracy_policy_5: 0.65098
	loss_value_5: 0.08957
	loss_reward_5: 0.01832
	loss_policy: 0.56844
	loss_value: 0.83502
	loss_reward: 0.07442
[2025-05-07 13:33:30] nn step 14100, lr: 0.1.
	loss_policy_0: 0.3055
	accuracy_policy_0: 0.65156
	loss_value_0: 0.431
	loss_policy_1: 0.06181
	accuracy_policy_1: 0.64473
	loss_value_1: 0.08893
	loss_reward_1: 0.00969
	loss_policy_2: 0.06206
	accuracy_policy_2: 0.63988
	loss_value_2: 0.091
	loss_reward_2: 0.01576
	loss_policy_3: 0.06191
	accuracy_policy_3: 0.6468
	loss_value_3: 0.09306
	loss_reward_3: 0.01728
	loss_policy_4: 0.06253
	accuracy_policy_4: 0.64551
	loss_value_4: 0.09489
	loss_reward_4: 0.01931
	loss_policy_5: 0.06263
	accuracy_policy_5: 0.64699
	loss_value_5: 0.09676
	loss_reward_5: 0.02007
	loss_policy: 0.61644
	loss_value: 0.89564
	loss_reward: 0.08211
[2025-05-07 13:33:38] nn step 14150, lr: 0.1.
	loss_policy_0: 0.27472
	accuracy_policy_0: 0.64828
	loss_value_0: 0.38585
	loss_policy_1: 0.05575
	accuracy_policy_1: 0.64285
	loss_value_1: 0.07918
	loss_reward_1: 0.00856
	loss_policy_2: 0.05556
	accuracy_policy_2: 0.64512
	loss_value_2: 0.08118
	loss_reward_2: 0.01354
	loss_policy_3: 0.05636
	accuracy_policy_3: 0.64098
	loss_value_3: 0.08322
	loss_reward_3: 0.01557
	loss_policy_4: 0.05612
	accuracy_policy_4: 0.64391
	loss_value_4: 0.08481
	loss_reward_4: 0.01705
	loss_policy_5: 0.0562
	accuracy_policy_5: 0.65023
	loss_value_5: 0.08654
	loss_reward_5: 0.01745
	loss_policy: 0.55472
	loss_value: 0.80078
	loss_reward: 0.07217
[2025-05-07 13:33:46] nn step 14200, lr: 0.1.
	loss_policy_0: 0.27033
	accuracy_policy_0: 0.65301
	loss_value_0: 0.38043
	loss_policy_1: 0.05494
	accuracy_policy_1: 0.64508
	loss_value_1: 0.07824
	loss_reward_1: 0.00825
	loss_policy_2: 0.05527
	accuracy_policy_2: 0.64664
	loss_value_2: 0.08043
	loss_reward_2: 0.01285
	loss_policy_3: 0.05516
	accuracy_policy_3: 0.64254
	loss_value_3: 0.08216
	loss_reward_3: 0.01492
	loss_policy_4: 0.05524
	accuracy_policy_4: 0.64422
	loss_value_4: 0.08356
	loss_reward_4: 0.01767
	loss_policy_5: 0.05532
	accuracy_policy_5: 0.65062
	loss_value_5: 0.08483
	loss_reward_5: 0.01753
	loss_policy: 0.54627
	loss_value: 0.78965
	loss_reward: 0.07124
Optimization_Done 14200
[2025-05-07 13:36:51] [command] train weight_iter_14200.pkl 53 72
[2025-05-07 13:37:00] nn step 14250, lr: 0.1.
	loss_policy_0: 0.28464
	accuracy_policy_0: 0.64117
	loss_value_0: 0.40183
	loss_policy_1: 0.05699
	accuracy_policy_1: 0.64176
	loss_value_1: 0.08213
	loss_reward_1: 0.00901
	loss_policy_2: 0.05762
	accuracy_policy_2: 0.63766
	loss_value_2: 0.08452
	loss_reward_2: 0.01367
	loss_policy_3: 0.05773
	accuracy_policy_3: 0.64051
	loss_value_3: 0.08641
	loss_reward_3: 0.01578
	loss_policy_4: 0.05772
	accuracy_policy_4: 0.64055
	loss_value_4: 0.08809
	loss_reward_4: 0.01834
	loss_policy_5: 0.05764
	accuracy_policy_5: 0.64613
	loss_value_5: 0.08963
	loss_reward_5: 0.01814
	loss_policy: 0.57235
	loss_value: 0.8326
	loss_reward: 0.07494
[2025-05-07 13:37:08] nn step 14300, lr: 0.1.
	loss_policy_0: 0.28854
	accuracy_policy_0: 0.64508
	loss_value_0: 0.40539
	loss_policy_1: 0.05818
	accuracy_policy_1: 0.64238
	loss_value_1: 0.08311
	loss_reward_1: 0.00886
	loss_policy_2: 0.05831
	accuracy_policy_2: 0.6384
	loss_value_2: 0.08496
	loss_reward_2: 0.01374
	loss_policy_3: 0.05835
	accuracy_policy_3: 0.63824
	loss_value_3: 0.08712
	loss_reward_3: 0.016
	loss_policy_4: 0.05878
	accuracy_policy_4: 0.64043
	loss_value_4: 0.08888
	loss_reward_4: 0.01835
	loss_policy_5: 0.05838
	accuracy_policy_5: 0.64551
	loss_value_5: 0.09065
	loss_reward_5: 0.01829
	loss_policy: 0.58054
	loss_value: 0.8401
	loss_reward: 0.07523
[2025-05-07 13:37:16] nn step 14350, lr: 0.1.
	loss_policy_0: 0.29292
	accuracy_policy_0: 0.64594
	loss_value_0: 0.40914
	loss_policy_1: 0.05902
	accuracy_policy_1: 0.64203
	loss_value_1: 0.08428
	loss_reward_1: 0.00904
	loss_policy_2: 0.05926
	accuracy_policy_2: 0.64383
	loss_value_2: 0.0862
	loss_reward_2: 0.01436
	loss_policy_3: 0.05887
	accuracy_policy_3: 0.64562
	loss_value_3: 0.08821
	loss_reward_3: 0.01621
	loss_policy_4: 0.05891
	accuracy_policy_4: 0.64609
	loss_value_4: 0.09002
	loss_reward_4: 0.0185
	loss_policy_5: 0.05936
	accuracy_policy_5: 0.64781
	loss_value_5: 0.0916
	loss_reward_5: 0.0189
	loss_policy: 0.58834
	loss_value: 0.84945
	loss_reward: 0.07701
[2025-05-07 13:37:22] nn step 14400, lr: 0.1.
	loss_policy_0: 0.28423
	accuracy_policy_0: 0.64895
	loss_value_0: 0.39636
	loss_policy_1: 0.05772
	accuracy_policy_1: 0.64258
	loss_value_1: 0.08161
	loss_reward_1: 0.00858
	loss_policy_2: 0.05761
	accuracy_policy_2: 0.64074
	loss_value_2: 0.0839
	loss_reward_2: 0.01422
	loss_policy_3: 0.05756
	accuracy_policy_3: 0.64391
	loss_value_3: 0.08594
	loss_reward_3: 0.01579
	loss_policy_4: 0.05754
	accuracy_policy_4: 0.6484
	loss_value_4: 0.08741
	loss_reward_4: 0.01859
	loss_policy_5: 0.05764
	accuracy_policy_5: 0.64855
	loss_value_5: 0.08931
	loss_reward_5: 0.01813
	loss_policy: 0.57231
	loss_value: 0.82453
	loss_reward: 0.07532
Optimization_Done 14400
[2025-05-07 13:40:24] [command] train weight_iter_14400.pkl 54 73
[2025-05-07 13:40:33] nn step 14450, lr: 0.1.
	loss_policy_0: 0.29914
	accuracy_policy_0: 0.64156
	loss_value_0: 0.42129
	loss_policy_1: 0.0603
	accuracy_policy_1: 0.63844
	loss_value_1: 0.08625
	loss_reward_1: 0.00922
	loss_policy_2: 0.06018
	accuracy_policy_2: 0.63547
	loss_value_2: 0.08859
	loss_reward_2: 0.01439
	loss_policy_3: 0.06023
	accuracy_policy_3: 0.63641
	loss_value_3: 0.09049
	loss_reward_3: 0.01661
	loss_policy_4: 0.06063
	accuracy_policy_4: 0.63746
	loss_value_4: 0.09236
	loss_reward_4: 0.01883
	loss_policy_5: 0.06087
	accuracy_policy_5: 0.6384
	loss_value_5: 0.09424
	loss_reward_5: 0.01874
	loss_policy: 0.60134
	loss_value: 0.87322
	loss_reward: 0.0778
[2025-05-07 13:40:40] nn step 14500, lr: 0.1.
	loss_policy_0: 0.28303
	accuracy_policy_0: 0.65457
	loss_value_0: 0.39679
	loss_policy_1: 0.05733
	accuracy_policy_1: 0.64332
	loss_value_1: 0.08182
	loss_reward_1: 0.00877
	loss_policy_2: 0.05768
	accuracy_policy_2: 0.64281
	loss_value_2: 0.0837
	loss_reward_2: 0.01394
	loss_policy_3: 0.05781
	accuracy_policy_3: 0.6398
	loss_value_3: 0.08572
	loss_reward_3: 0.01612
	loss_policy_4: 0.05801
	accuracy_policy_4: 0.63539
	loss_value_4: 0.08771
	loss_reward_4: 0.01772
	loss_policy_5: 0.05823
	accuracy_policy_5: 0.64227
	loss_value_5: 0.08913
	loss_reward_5: 0.01768
	loss_policy: 0.57208
	loss_value: 0.82487
	loss_reward: 0.07423
[2025-05-07 13:40:48] nn step 14550, lr: 0.1.
	loss_policy_0: 0.29625
	accuracy_policy_0: 0.64285
	loss_value_0: 0.41123
	loss_policy_1: 0.0598
	accuracy_policy_1: 0.63941
	loss_value_1: 0.08453
	loss_reward_1: 0.00895
	loss_policy_2: 0.05993
	accuracy_policy_2: 0.63699
	loss_value_2: 0.08671
	loss_reward_2: 0.01436
	loss_policy_3: 0.06058
	accuracy_policy_3: 0.6393
	loss_value_3: 0.08893
	loss_reward_3: 0.01639
	loss_policy_4: 0.06019
	accuracy_policy_4: 0.64188
	loss_value_4: 0.09087
	loss_reward_4: 0.01821
	loss_policy_5: 0.06019
	accuracy_policy_5: 0.64469
	loss_value_5: 0.09276
	loss_reward_5: 0.01837
	loss_policy: 0.59694
	loss_value: 0.85503
	loss_reward: 0.07629
[2025-05-07 13:40:56] nn step 14600, lr: 0.1.
	loss_policy_0: 0.29386
	accuracy_policy_0: 0.64039
	loss_value_0: 0.40575
	loss_policy_1: 0.05903
	accuracy_policy_1: 0.64688
	loss_value_1: 0.08343
	loss_reward_1: 0.00953
	loss_policy_2: 0.05944
	accuracy_policy_2: 0.63352
	loss_value_2: 0.08564
	loss_reward_2: 0.01405
	loss_policy_3: 0.05945
	accuracy_policy_3: 0.63449
	loss_value_3: 0.08741
	loss_reward_3: 0.01569
	loss_policy_4: 0.05939
	accuracy_policy_4: 0.64367
	loss_value_4: 0.08955
	loss_reward_4: 0.01856
	loss_policy_5: 0.05959
	accuracy_policy_5: 0.64332
	loss_value_5: 0.09136
	loss_reward_5: 0.01862
	loss_policy: 0.59076
	loss_value: 0.84315
	loss_reward: 0.07646
Optimization_Done 14600
[2025-05-07 13:44:11] [command] train weight_iter_14600.pkl 55 74
[2025-05-07 13:44:18] nn step 14650, lr: 0.1.
	loss_policy_0: 0.2871
	accuracy_policy_0: 0.64305
	loss_value_0: 0.40426
	loss_policy_1: 0.05761
	accuracy_policy_1: 0.64137
	loss_value_1: 0.08293
	loss_reward_1: 0.00905
	loss_policy_2: 0.05794
	accuracy_policy_2: 0.64121
	loss_value_2: 0.08513
	loss_reward_2: 0.01427
	loss_policy_3: 0.05803
	accuracy_policy_3: 0.6407
	loss_value_3: 0.087
	loss_reward_3: 0.01561
	loss_policy_4: 0.05796
	accuracy_policy_4: 0.64566
	loss_value_4: 0.0887
	loss_reward_4: 0.01819
	loss_policy_5: 0.0582
	accuracy_policy_5: 0.6441
	loss_value_5: 0.08994
	loss_reward_5: 0.01848
	loss_policy: 0.57684
	loss_value: 0.83797
	loss_reward: 0.07559
[2025-05-07 13:44:26] nn step 14700, lr: 0.1.
	loss_policy_0: 0.27095
	accuracy_policy_0: 0.64508
	loss_value_0: 0.37262
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.63938
	loss_value_1: 0.07713
	loss_reward_1: 0.00843
	loss_policy_2: 0.05513
	accuracy_policy_2: 0.63945
	loss_value_2: 0.07967
	loss_reward_2: 0.01293
	loss_policy_3: 0.05498
	accuracy_policy_3: 0.64227
	loss_value_3: 0.08113
	loss_reward_3: 0.01463
	loss_policy_4: 0.05523
	accuracy_policy_4: 0.64062
	loss_value_4: 0.08239
	loss_reward_4: 0.01728
	loss_policy_5: 0.05521
	accuracy_policy_5: 0.63965
	loss_value_5: 0.08408
	loss_reward_5: 0.01689
	loss_policy: 0.54594
	loss_value: 0.77701
	loss_reward: 0.07017
[2025-05-07 13:44:33] nn step 14750, lr: 0.1.
	loss_policy_0: 0.29152
	accuracy_policy_0: 0.64891
	loss_value_0: 0.40433
	loss_policy_1: 0.05882
	accuracy_policy_1: 0.64129
	loss_value_1: 0.08313
	loss_reward_1: 0.00929
	loss_policy_2: 0.05908
	accuracy_policy_2: 0.64422
	loss_value_2: 0.08574
	loss_reward_2: 0.01414
	loss_policy_3: 0.05917
	accuracy_policy_3: 0.64285
	loss_value_3: 0.08734
	loss_reward_3: 0.01572
	loss_policy_4: 0.05893
	accuracy_policy_4: 0.64215
	loss_value_4: 0.08892
	loss_reward_4: 0.01835
	loss_policy_5: 0.05912
	accuracy_policy_5: 0.64844
	loss_value_5: 0.09081
	loss_reward_5: 0.01822
	loss_policy: 0.58664
	loss_value: 0.84027
	loss_reward: 0.07571
[2025-05-07 13:44:41] nn step 14800, lr: 0.1.
	loss_policy_0: 0.30843
	accuracy_policy_0: 0.64359
	loss_value_0: 0.42662
	loss_policy_1: 0.062
	accuracy_policy_1: 0.6375
	loss_value_1: 0.08789
	loss_reward_1: 0.00935
	loss_policy_2: 0.06234
	accuracy_policy_2: 0.63715
	loss_value_2: 0.09015
	loss_reward_2: 0.01506
	loss_policy_3: 0.0622
	accuracy_policy_3: 0.64344
	loss_value_3: 0.09216
	loss_reward_3: 0.01692
	loss_policy_4: 0.06261
	accuracy_policy_4: 0.64008
	loss_value_4: 0.09419
	loss_reward_4: 0.01924
	loss_policy_5: 0.06245
	accuracy_policy_5: 0.64188
	loss_value_5: 0.09569
	loss_reward_5: 0.01966
	loss_policy: 0.62004
	loss_value: 0.88669
	loss_reward: 0.08023
Optimization_Done 14800
[2025-05-07 13:47:45] [command] train weight_iter_14800.pkl 56 75
[2025-05-07 13:47:53] nn step 14850, lr: 0.1.
	loss_policy_0: 0.30064
	accuracy_policy_0: 0.64363
	loss_value_0: 0.42214
	loss_policy_1: 0.06067
	accuracy_policy_1: 0.63828
	loss_value_1: 0.08637
	loss_reward_1: 0.0092
	loss_policy_2: 0.06046
	accuracy_policy_2: 0.64234
	loss_value_2: 0.08834
	loss_reward_2: 0.01477
	loss_policy_3: 0.06075
	accuracy_policy_3: 0.63891
	loss_value_3: 0.09012
	loss_reward_3: 0.01606
	loss_policy_4: 0.06072
	accuracy_policy_4: 0.64031
	loss_value_4: 0.09195
	loss_reward_4: 0.01839
	loss_policy_5: 0.06059
	accuracy_policy_5: 0.64473
	loss_value_5: 0.09377
	loss_reward_5: 0.01895
	loss_policy: 0.60383
	loss_value: 0.87269
	loss_reward: 0.07739
[2025-05-07 13:48:00] nn step 14900, lr: 0.1.
	loss_policy_0: 0.28827
	accuracy_policy_0: 0.65359
	loss_value_0: 0.40472
	loss_policy_1: 0.05812
	accuracy_policy_1: 0.64715
	loss_value_1: 0.08319
	loss_reward_1: 0.00854
	loss_policy_2: 0.0583
	accuracy_policy_2: 0.64395
	loss_value_2: 0.08519
	loss_reward_2: 0.01376
	loss_policy_3: 0.05852
	accuracy_policy_3: 0.64301
	loss_value_3: 0.08726
	loss_reward_3: 0.01542
	loss_policy_4: 0.05886
	accuracy_policy_4: 0.64828
	loss_value_4: 0.08909
	loss_reward_4: 0.01836
	loss_policy_5: 0.05865
	accuracy_policy_5: 0.65
	loss_value_5: 0.09082
	loss_reward_5: 0.0181
	loss_policy: 0.58072
	loss_value: 0.84027
	loss_reward: 0.07418
[2025-05-07 13:48:08] nn step 14950, lr: 0.1.
	loss_policy_0: 0.28582
	accuracy_policy_0: 0.64914
	loss_value_0: 0.39828
	loss_policy_1: 0.05787
	accuracy_policy_1: 0.64688
	loss_value_1: 0.08201
	loss_reward_1: 0.00882
	loss_policy_2: 0.0581
	accuracy_policy_2: 0.6391
	loss_value_2: 0.08428
	loss_reward_2: 0.01386
	loss_policy_3: 0.05839
	accuracy_policy_3: 0.64129
	loss_value_3: 0.08643
	loss_reward_3: 0.01599
	loss_policy_4: 0.05849
	accuracy_policy_4: 0.64195
	loss_value_4: 0.08774
	loss_reward_4: 0.0181
	loss_policy_5: 0.05822
	accuracy_policy_5: 0.64875
	loss_value_5: 0.0893
	loss_reward_5: 0.01835
	loss_policy: 0.57689
	loss_value: 0.82804
	loss_reward: 0.07513
[2025-05-07 13:48:16] nn step 15000, lr: 0.1.
	loss_policy_0: 0.29521
	accuracy_policy_0: 0.64875
	loss_value_0: 0.41031
	loss_policy_1: 0.05949
	accuracy_policy_1: 0.6443
	loss_value_1: 0.08424
	loss_reward_1: 0.00887
	loss_policy_2: 0.0599
	accuracy_policy_2: 0.63961
	loss_value_2: 0.08643
	loss_reward_2: 0.01444
	loss_policy_3: 0.06021
	accuracy_policy_3: 0.64043
	loss_value_3: 0.08824
	loss_reward_3: 0.01606
	loss_policy_4: 0.06034
	accuracy_policy_4: 0.64355
	loss_value_4: 0.09025
	loss_reward_4: 0.01819
	loss_policy_5: 0.06045
	accuracy_policy_5: 0.64793
	loss_value_5: 0.09192
	loss_reward_5: 0.01853
	loss_policy: 0.59559
	loss_value: 0.85139
	loss_reward: 0.07608
Optimization_Done 15000
[2025-05-07 13:51:23] [command] train weight_iter_15000.pkl 57 76
[2025-05-07 13:51:32] nn step 15050, lr: 0.1.
	loss_policy_0: 0.28972
	accuracy_policy_0: 0.6416
	loss_value_0: 0.41226
	loss_policy_1: 0.05853
	accuracy_policy_1: 0.64051
	loss_value_1: 0.08428
	loss_reward_1: 0.00913
	loss_policy_2: 0.05885
	accuracy_policy_2: 0.63914
	loss_value_2: 0.08648
	loss_reward_2: 0.01414
	loss_policy_3: 0.0592
	accuracy_policy_3: 0.63383
	loss_value_3: 0.08845
	loss_reward_3: 0.01596
	loss_policy_4: 0.05938
	accuracy_policy_4: 0.64086
	loss_value_4: 0.09009
	loss_reward_4: 0.01821
	loss_policy_5: 0.05899
	accuracy_policy_5: 0.64312
	loss_value_5: 0.09142
	loss_reward_5: 0.01872
	loss_policy: 0.58467
	loss_value: 0.85299
	loss_reward: 0.07615
[2025-05-07 13:51:38] nn step 15100, lr: 0.1.
	loss_policy_0: 0.27922
	accuracy_policy_0: 0.64574
	loss_value_0: 0.38567
	loss_policy_1: 0.05606
	accuracy_policy_1: 0.63535
	loss_value_1: 0.07923
	loss_reward_1: 0.00869
	loss_policy_2: 0.05619
	accuracy_policy_2: 0.63957
	loss_value_2: 0.0813
	loss_reward_2: 0.01317
	loss_policy_3: 0.05651
	accuracy_policy_3: 0.63621
	loss_value_3: 0.08322
	loss_reward_3: 0.01544
	loss_policy_4: 0.05659
	accuracy_policy_4: 0.64355
	loss_value_4: 0.08479
	loss_reward_4: 0.01734
	loss_policy_5: 0.05673
	accuracy_policy_5: 0.64457
	loss_value_5: 0.08644
	loss_reward_5: 0.01767
	loss_policy: 0.56131
	loss_value: 0.80065
	loss_reward: 0.07232
[2025-05-07 13:51:46] nn step 15150, lr: 0.1.
	loss_policy_0: 0.27587
	accuracy_policy_0: 0.64422
	loss_value_0: 0.38054
	loss_policy_1: 0.05499
	accuracy_policy_1: 0.64656
	loss_value_1: 0.07856
	loss_reward_1: 0.00795
	loss_policy_2: 0.05521
	accuracy_policy_2: 0.63906
	loss_value_2: 0.08047
	loss_reward_2: 0.01254
	loss_policy_3: 0.05541
	accuracy_policy_3: 0.64652
	loss_value_3: 0.08239
	loss_reward_3: 0.0151
	loss_policy_4: 0.05568
	accuracy_policy_4: 0.64539
	loss_value_4: 0.08393
	loss_reward_4: 0.01725
	loss_policy_5: 0.05596
	accuracy_policy_5: 0.64777
	loss_value_5: 0.08546
	loss_reward_5: 0.01751
	loss_policy: 0.55312
	loss_value: 0.79134
	loss_reward: 0.07035
[2025-05-07 13:51:54] nn step 15200, lr: 0.1.
	loss_policy_0: 0.30102
	accuracy_policy_0: 0.64555
	loss_value_0: 0.41489
	loss_policy_1: 0.06062
	accuracy_policy_1: 0.64137
	loss_value_1: 0.08529
	loss_reward_1: 0.00955
	loss_policy_2: 0.06038
	accuracy_policy_2: 0.63809
	loss_value_2: 0.08773
	loss_reward_2: 0.01452
	loss_policy_3: 0.0605
	accuracy_policy_3: 0.64035
	loss_value_3: 0.08977
	loss_reward_3: 0.01657
	loss_policy_4: 0.06055
	accuracy_policy_4: 0.64145
	loss_value_4: 0.09178
	loss_reward_4: 0.0191
	loss_policy_5: 0.06107
	accuracy_policy_5: 0.64277
	loss_value_5: 0.09366
	loss_reward_5: 0.01918
	loss_policy: 0.60414
	loss_value: 0.86312
	loss_reward: 0.07891
Optimization_Done 15200
[2025-05-07 13:55:01] [command] train weight_iter_15200.pkl 58 77
[2025-05-07 13:55:10] nn step 15250, lr: 0.1.
	loss_policy_0: 0.29834
	accuracy_policy_0: 0.63953
	loss_value_0: 0.42124
	loss_policy_1: 0.05973
	accuracy_policy_1: 0.63906
	loss_value_1: 0.0859
	loss_reward_1: 0.0092
	loss_policy_2: 0.06009
	accuracy_policy_2: 0.63645
	loss_value_2: 0.08799
	loss_reward_2: 0.01393
	loss_policy_3: 0.05992
	accuracy_policy_3: 0.63938
	loss_value_3: 0.08986
	loss_reward_3: 0.01598
	loss_policy_4: 0.05996
	accuracy_policy_4: 0.64012
	loss_value_4: 0.09139
	loss_reward_4: 0.01893
	loss_policy_5: 0.06011
	accuracy_policy_5: 0.64277
	loss_value_5: 0.0927
	loss_reward_5: 0.01917
	loss_policy: 0.59815
	loss_value: 0.86907
	loss_reward: 0.07721
[2025-05-07 13:55:18] nn step 15300, lr: 0.1.
	loss_policy_0: 0.29505
	accuracy_policy_0: 0.64871
	loss_value_0: 0.41032
	loss_policy_1: 0.05901
	accuracy_policy_1: 0.64898
	loss_value_1: 0.08396
	loss_reward_1: 0.00853
	loss_policy_2: 0.05956
	accuracy_policy_2: 0.63797
	loss_value_2: 0.08582
	loss_reward_2: 0.01362
	loss_policy_3: 0.05986
	accuracy_policy_3: 0.63871
	loss_value_3: 0.08815
	loss_reward_3: 0.01654
	loss_policy_4: 0.05982
	accuracy_policy_4: 0.64438
	loss_value_4: 0.08978
	loss_reward_4: 0.01818
	loss_policy_5: 0.05994
	accuracy_policy_5: 0.64633
	loss_value_5: 0.09141
	loss_reward_5: 0.01837
	loss_policy: 0.59323
	loss_value: 0.84944
	loss_reward: 0.07524
[2025-05-07 13:55:26] nn step 15350, lr: 0.1.
	loss_policy_0: 0.29958
	accuracy_policy_0: 0.64582
	loss_value_0: 0.41645
	loss_policy_1: 0.06016
	accuracy_policy_1: 0.64547
	loss_value_1: 0.08555
	loss_reward_1: 0.00939
	loss_policy_2: 0.06085
	accuracy_policy_2: 0.64207
	loss_value_2: 0.08768
	loss_reward_2: 0.01413
	loss_policy_3: 0.06069
	accuracy_policy_3: 0.64191
	loss_value_3: 0.08975
	loss_reward_3: 0.01647
	loss_policy_4: 0.06083
	accuracy_policy_4: 0.64398
	loss_value_4: 0.0916
	loss_reward_4: 0.01852
	loss_policy_5: 0.06122
	accuracy_policy_5: 0.64504
	loss_value_5: 0.09332
	loss_reward_5: 0.01921
	loss_policy: 0.60332
	loss_value: 0.86436
	loss_reward: 0.07771
[2025-05-07 13:55:32] nn step 15400, lr: 0.1.
	loss_policy_0: 0.28242
	accuracy_policy_0: 0.64527
	loss_value_0: 0.39364
	loss_policy_1: 0.05695
	accuracy_policy_1: 0.64129
	loss_value_1: 0.0806
	loss_reward_1: 0.00853
	loss_policy_2: 0.05723
	accuracy_policy_2: 0.63984
	loss_value_2: 0.08291
	loss_reward_2: 0.01363
	loss_policy_3: 0.05722
	accuracy_policy_3: 0.6477
	loss_value_3: 0.08523
	loss_reward_3: 0.01542
	loss_policy_4: 0.05726
	accuracy_policy_4: 0.64238
	loss_value_4: 0.08657
	loss_reward_4: 0.01761
	loss_policy_5: 0.05737
	accuracy_policy_5: 0.64223
	loss_value_5: 0.08857
	loss_reward_5: 0.01779
	loss_policy: 0.56845
	loss_value: 0.81752
	loss_reward: 0.07297
Optimization_Done 15400
[2025-05-07 13:58:42] [command] train weight_iter_15400.pkl 59 78
[2025-05-07 13:58:49] nn step 15450, lr: 0.1.
	loss_policy_0: 0.27605
	accuracy_policy_0: 0.64531
	loss_value_0: 0.39198
	loss_policy_1: 0.05594
	accuracy_policy_1: 0.64176
	loss_value_1: 0.08038
	loss_reward_1: 0.00832
	loss_policy_2: 0.05615
	accuracy_policy_2: 0.63766
	loss_value_2: 0.0821
	loss_reward_2: 0.01304
	loss_policy_3: 0.05636
	accuracy_policy_3: 0.63766
	loss_value_3: 0.08392
	loss_reward_3: 0.015
	loss_policy_4: 0.05628
	accuracy_policy_4: 0.64336
	loss_value_4: 0.08566
	loss_reward_4: 0.0171
	loss_policy_5: 0.05636
	accuracy_policy_5: 0.64457
	loss_value_5: 0.08719
	loss_reward_5: 0.01754
	loss_policy: 0.55714
	loss_value: 0.81123
	loss_reward: 0.07101
[2025-05-07 13:58:57] nn step 15500, lr: 0.1.
	loss_policy_0: 0.26935
	accuracy_policy_0: 0.64602
	loss_value_0: 0.37531
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.64141
	loss_value_1: 0.07734
	loss_reward_1: 0.00801
	loss_policy_2: 0.05466
	accuracy_policy_2: 0.6375
	loss_value_2: 0.07925
	loss_reward_2: 0.01259
	loss_policy_3: 0.05433
	accuracy_policy_3: 0.64051
	loss_value_3: 0.08137
	loss_reward_3: 0.01459
	loss_policy_4: 0.05466
	accuracy_policy_4: 0.64363
	loss_value_4: 0.08285
	loss_reward_4: 0.01689
	loss_policy_5: 0.05486
	accuracy_policy_5: 0.64145
	loss_value_5: 0.08435
	loss_reward_5: 0.01688
	loss_policy: 0.5423
	loss_value: 0.78046
	loss_reward: 0.06897
[2025-05-07 13:59:05] nn step 15550, lr: 0.1.
	loss_policy_0: 0.27376
	accuracy_policy_0: 0.64875
	loss_value_0: 0.38234
	loss_policy_1: 0.05501
	accuracy_policy_1: 0.63945
	loss_value_1: 0.07846
	loss_reward_1: 0.00819
	loss_policy_2: 0.0558
	accuracy_policy_2: 0.63379
	loss_value_2: 0.08052
	loss_reward_2: 0.01267
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.64016
	loss_value_3: 0.08265
	loss_reward_3: 0.01475
	loss_policy_4: 0.05549
	accuracy_policy_4: 0.64336
	loss_value_4: 0.08421
	loss_reward_4: 0.01714
	loss_policy_5: 0.05531
	accuracy_policy_5: 0.64664
	loss_value_5: 0.08573
	loss_reward_5: 0.01704
	loss_policy: 0.55119
	loss_value: 0.7939
	loss_reward: 0.06978
[2025-05-07 13:59:13] nn step 15600, lr: 0.1.
	loss_policy_0: 0.30431
	accuracy_policy_0: 0.64953
	loss_value_0: 0.42545
	loss_policy_1: 0.06171
	accuracy_policy_1: 0.64203
	loss_value_1: 0.08721
	loss_reward_1: 0.00913
	loss_policy_2: 0.06184
	accuracy_policy_2: 0.63977
	loss_value_2: 0.08968
	loss_reward_2: 0.01476
	loss_policy_3: 0.06122
	accuracy_policy_3: 0.64895
	loss_value_3: 0.09165
	loss_reward_3: 0.0173
	loss_policy_4: 0.06144
	accuracy_policy_4: 0.65113
	loss_value_4: 0.09333
	loss_reward_4: 0.01899
	loss_policy_5: 0.06176
	accuracy_policy_5: 0.64605
	loss_value_5: 0.09534
	loss_reward_5: 0.01927
	loss_policy: 0.61228
	loss_value: 0.88266
	loss_reward: 0.07944
Optimization_Done 15600
[2025-05-07 14:02:10] [command] train weight_iter_15600.pkl 60 79
[2025-05-07 14:02:19] nn step 15650, lr: 0.1.
	loss_policy_0: 0.30223
	accuracy_policy_0: 0.62566
	loss_value_0: 0.41514
	loss_policy_1: 0.05988
	accuracy_policy_1: 0.63285
	loss_value_1: 0.08477
	loss_reward_1: 0.00918
	loss_policy_2: 0.0596
	accuracy_policy_2: 0.63438
	loss_value_2: 0.08683
	loss_reward_2: 0.01414
	loss_policy_3: 0.05973
	accuracy_policy_3: 0.63707
	loss_value_3: 0.08855
	loss_reward_3: 0.01621
	loss_policy_4: 0.05973
	accuracy_policy_4: 0.6398
	loss_value_4: 0.09042
	loss_reward_4: 0.0181
	loss_policy_5: 0.05999
	accuracy_policy_5: 0.63891
	loss_value_5: 0.09229
	loss_reward_5: 0.01866
	loss_policy: 0.60116
	loss_value: 0.85801
	loss_reward: 0.07629
[2025-05-07 14:02:26] nn step 15700, lr: 0.1.
	loss_policy_0: 0.29069
	accuracy_policy_0: 0.63652
	loss_value_0: 0.39916
	loss_policy_1: 0.05774
	accuracy_policy_1: 0.64043
	loss_value_1: 0.08205
	loss_reward_1: 0.00862
	loss_policy_2: 0.05834
	accuracy_policy_2: 0.6359
	loss_value_2: 0.08407
	loss_reward_2: 0.01385
	loss_policy_3: 0.05762
	accuracy_policy_3: 0.63848
	loss_value_3: 0.0858
	loss_reward_3: 0.01551
	loss_policy_4: 0.05775
	accuracy_policy_4: 0.64297
	loss_value_4: 0.0877
	loss_reward_4: 0.01777
	loss_policy_5: 0.05806
	accuracy_policy_5: 0.64168
	loss_value_5: 0.08945
	loss_reward_5: 0.0183
	loss_policy: 0.5802
	loss_value: 0.82824
	loss_reward: 0.07405
[2025-05-07 14:02:34] nn step 15750, lr: 0.1.
	loss_policy_0: 0.27486
	accuracy_policy_0: 0.64664
	loss_value_0: 0.38203
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.64129
	loss_value_1: 0.07852
	loss_reward_1: 0.00844
	loss_policy_2: 0.05556
	accuracy_policy_2: 0.64176
	loss_value_2: 0.08054
	loss_reward_2: 0.01335
	loss_policy_3: 0.05555
	accuracy_policy_3: 0.64324
	loss_value_3: 0.08256
	loss_reward_3: 0.01492
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.6432
	loss_value_4: 0.08436
	loss_reward_4: 0.01701
	loss_policy_5: 0.05566
	accuracy_policy_5: 0.64613
	loss_value_5: 0.08603
	loss_reward_5: 0.0172
	loss_policy: 0.55262
	loss_value: 0.79403
	loss_reward: 0.07091
[2025-05-07 14:02:41] nn step 15800, lr: 0.1.
	loss_policy_0: 0.27563
	accuracy_policy_0: 0.64512
	loss_value_0: 0.37557
	loss_policy_1: 0.055
	accuracy_policy_1: 0.64445
	loss_value_1: 0.07734
	loss_reward_1: 0.00821
	loss_policy_2: 0.05531
	accuracy_policy_2: 0.64309
	loss_value_2: 0.07928
	loss_reward_2: 0.01303
	loss_policy_3: 0.05561
	accuracy_policy_3: 0.64359
	loss_value_3: 0.08156
	loss_reward_3: 0.01476
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.6416
	loss_value_4: 0.08319
	loss_reward_4: 0.01675
	loss_policy_5: 0.05554
	accuracy_policy_5: 0.64664
	loss_value_5: 0.08487
	loss_reward_5: 0.01758
	loss_policy: 0.55272
	loss_value: 0.78182
	loss_reward: 0.07034
Optimization_Done 15800
[2025-05-07 14:05:54] [command] train weight_iter_15800.pkl 61 80
[2025-05-07 14:06:01] nn step 15850, lr: 0.1.
	loss_policy_0: 0.27741
	accuracy_policy_0: 0.63754
	loss_value_0: 0.38417
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.63582
	loss_value_1: 0.07891
	loss_reward_1: 0.00832
	loss_policy_2: 0.05536
	accuracy_policy_2: 0.63629
	loss_value_2: 0.08071
	loss_reward_2: 0.01286
	loss_policy_3: 0.05552
	accuracy_policy_3: 0.64094
	loss_value_3: 0.08245
	loss_reward_3: 0.01477
	loss_policy_4: 0.05513
	accuracy_policy_4: 0.64422
	loss_value_4: 0.08411
	loss_reward_4: 0.01716
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.64727
	loss_value_5: 0.08578
	loss_reward_5: 0.017
	loss_policy: 0.55421
	loss_value: 0.79613
	loss_reward: 0.07011
[2025-05-07 14:06:09] nn step 15900, lr: 0.1.
	loss_policy_0: 0.309
	accuracy_policy_0: 0.63918
	loss_value_0: 0.4263
	loss_policy_1: 0.06216
	accuracy_policy_1: 0.64488
	loss_value_1: 0.08758
	loss_reward_1: 0.00917
	loss_policy_2: 0.06236
	accuracy_policy_2: 0.63945
	loss_value_2: 0.09015
	loss_reward_2: 0.01443
	loss_policy_3: 0.06277
	accuracy_policy_3: 0.64234
	loss_value_3: 0.09186
	loss_reward_3: 0.01665
	loss_policy_4: 0.06263
	accuracy_policy_4: 0.64496
	loss_value_4: 0.0938
	loss_reward_4: 0.01899
	loss_policy_5: 0.06285
	accuracy_policy_5: 0.6427
	loss_value_5: 0.09558
	loss_reward_5: 0.01928
	loss_policy: 0.62176
	loss_value: 0.88527
	loss_reward: 0.07852
[2025-05-07 14:06:17] nn step 15950, lr: 0.1.
	loss_policy_0: 0.2938
	accuracy_policy_0: 0.64234
	loss_value_0: 0.40358
	loss_policy_1: 0.05887
	accuracy_policy_1: 0.6407
	loss_value_1: 0.08301
	loss_reward_1: 0.00887
	loss_policy_2: 0.05911
	accuracy_policy_2: 0.64195
	loss_value_2: 0.08569
	loss_reward_2: 0.01331
	loss_policy_3: 0.05912
	accuracy_policy_3: 0.64109
	loss_value_3: 0.08768
	loss_reward_3: 0.01612
	loss_policy_4: 0.05906
	accuracy_policy_4: 0.64438
	loss_value_4: 0.0895
	loss_reward_4: 0.01823
	loss_policy_5: 0.05935
	accuracy_policy_5: 0.64258
	loss_value_5: 0.09093
	loss_reward_5: 0.01806
	loss_policy: 0.58931
	loss_value: 0.84039
	loss_reward: 0.0746
[2025-05-07 14:06:24] nn step 16000, lr: 0.1.
	loss_policy_0: 0.26115
	accuracy_policy_0: 0.65109
	loss_value_0: 0.36079
	loss_policy_1: 0.05259
	accuracy_policy_1: 0.64059
	loss_value_1: 0.07412
	loss_reward_1: 0.00779
	loss_policy_2: 0.05294
	accuracy_policy_2: 0.64246
	loss_value_2: 0.07602
	loss_reward_2: 0.01254
	loss_policy_3: 0.05294
	accuracy_policy_3: 0.64922
	loss_value_3: 0.07775
	loss_reward_3: 0.0142
	loss_policy_4: 0.05321
	accuracy_policy_4: 0.64484
	loss_value_4: 0.07898
	loss_reward_4: 0.01614
	loss_policy_5: 0.05311
	accuracy_policy_5: 0.64785
	loss_value_5: 0.08075
	loss_reward_5: 0.01635
	loss_policy: 0.52594
	loss_value: 0.74841
	loss_reward: 0.06703
Optimization_Done 16000
[2025-05-07 14:09:23] [command] train weight_iter_16000.pkl 62 81
[2025-05-07 14:09:32] nn step 16050, lr: 0.1.
	loss_policy_0: 0.26082
	accuracy_policy_0: 0.65086
	loss_value_0: 0.36944
	loss_policy_1: 0.05279
	accuracy_policy_1: 0.63855
	loss_value_1: 0.07568
	loss_reward_1: 0.00792
	loss_policy_2: 0.05312
	accuracy_policy_2: 0.63965
	loss_value_2: 0.07768
	loss_reward_2: 0.01174
	loss_policy_3: 0.05297
	accuracy_policy_3: 0.64758
	loss_value_3: 0.07931
	loss_reward_3: 0.01421
	loss_policy_4: 0.05316
	accuracy_policy_4: 0.64438
	loss_value_4: 0.08062
	loss_reward_4: 0.01612
	loss_policy_5: 0.05337
	accuracy_policy_5: 0.64727
	loss_value_5: 0.08204
	loss_reward_5: 0.0166
	loss_policy: 0.52623
	loss_value: 0.76478
	loss_reward: 0.0666
[2025-05-07 14:09:39] nn step 16100, lr: 0.1.
	loss_policy_0: 0.29364
	accuracy_policy_0: 0.64922
	loss_value_0: 0.40316
	loss_policy_1: 0.05909
	accuracy_policy_1: 0.64352
	loss_value_1: 0.08232
	loss_reward_1: 0.0089
	loss_policy_2: 0.05869
	accuracy_policy_2: 0.64605
	loss_value_2: 0.08462
	loss_reward_2: 0.01376
	loss_policy_3: 0.0591
	accuracy_policy_3: 0.64844
	loss_value_3: 0.08689
	loss_reward_3: 0.01576
	loss_policy_4: 0.05897
	accuracy_policy_4: 0.65211
	loss_value_4: 0.08872
	loss_reward_4: 0.01807
	loss_policy_5: 0.05969
	accuracy_policy_5: 0.64102
	loss_value_5: 0.09038
	loss_reward_5: 0.01808
	loss_policy: 0.58918
	loss_value: 0.83609
	loss_reward: 0.07457
[2025-05-07 14:09:46] nn step 16150, lr: 0.1.
	loss_policy_0: 0.28566
	accuracy_policy_0: 0.65008
	loss_value_0: 0.39197
	loss_policy_1: 0.05748
	accuracy_policy_1: 0.64305
	loss_value_1: 0.08049
	loss_reward_1: 0.00833
	loss_policy_2: 0.05756
	accuracy_policy_2: 0.64074
	loss_value_2: 0.08283
	loss_reward_2: 0.01323
	loss_policy_3: 0.05778
	accuracy_policy_3: 0.64363
	loss_value_3: 0.08472
	loss_reward_3: 0.01545
	loss_policy_4: 0.05815
	accuracy_policy_4: 0.64234
	loss_value_4: 0.08673
	loss_reward_4: 0.0175
	loss_policy_5: 0.05844
	accuracy_policy_5: 0.6382
	loss_value_5: 0.08811
	loss_reward_5: 0.01797
	loss_policy: 0.57506
	loss_value: 0.81485
	loss_reward: 0.07248
[2025-05-07 14:09:54] nn step 16200, lr: 0.1.
	loss_policy_0: 0.29964
	accuracy_policy_0: 0.6452
	loss_value_0: 0.40913
	loss_policy_1: 0.06015
	accuracy_policy_1: 0.64254
	loss_value_1: 0.08429
	loss_reward_1: 0.00889
	loss_policy_2: 0.06027
	accuracy_policy_2: 0.64496
	loss_value_2: 0.08669
	loss_reward_2: 0.01393
	loss_policy_3: 0.06036
	accuracy_policy_3: 0.6452
	loss_value_3: 0.08901
	loss_reward_3: 0.01606
	loss_policy_4: 0.0604
	accuracy_policy_4: 0.64777
	loss_value_4: 0.09079
	loss_reward_4: 0.01817
	loss_policy_5: 0.06085
	accuracy_policy_5: 0.64402
	loss_value_5: 0.09263
	loss_reward_5: 0.01863
	loss_policy: 0.60167
	loss_value: 0.85253
	loss_reward: 0.07567
Optimization_Done 16200
[2025-05-07 14:13:06] [command] train weight_iter_16200.pkl 63 82
[2025-05-07 14:13:15] nn step 16250, lr: 0.1.
	loss_policy_0: 0.31225
	accuracy_policy_0: 0.64133
	loss_value_0: 0.43295
	loss_policy_1: 0.06264
	accuracy_policy_1: 0.64129
	loss_value_1: 0.08855
	loss_reward_1: 0.00926
	loss_policy_2: 0.06273
	accuracy_policy_2: 0.64113
	loss_value_2: 0.09071
	loss_reward_2: 0.01398
	loss_policy_3: 0.06229
	accuracy_policy_3: 0.64207
	loss_value_3: 0.09297
	loss_reward_3: 0.01598
	loss_policy_4: 0.06245
	accuracy_policy_4: 0.63922
	loss_value_4: 0.09457
	loss_reward_4: 0.019
	loss_policy_5: 0.06281
	accuracy_policy_5: 0.64395
	loss_value_5: 0.09642
	loss_reward_5: 0.01943
	loss_policy: 0.62517
	loss_value: 0.89617
	loss_reward: 0.07765
[2025-05-07 14:13:23] nn step 16300, lr: 0.1.
	loss_policy_0: 0.29351
	accuracy_policy_0: 0.63266
	loss_value_0: 0.39407
	loss_policy_1: 0.05828
	accuracy_policy_1: 0.63969
	loss_value_1: 0.08087
	loss_reward_1: 0.00879
	loss_policy_2: 0.05824
	accuracy_policy_2: 0.64066
	loss_value_2: 0.08346
	loss_reward_2: 0.01334
	loss_policy_3: 0.05847
	accuracy_policy_3: 0.63785
	loss_value_3: 0.08552
	loss_reward_3: 0.01533
	loss_policy_4: 0.05827
	accuracy_policy_4: 0.64008
	loss_value_4: 0.08722
	loss_reward_4: 0.01769
	loss_policy_5: 0.05848
	accuracy_policy_5: 0.64383
	loss_value_5: 0.08871
	loss_reward_5: 0.01792
	loss_policy: 0.58525
	loss_value: 0.81986
	loss_reward: 0.07308
[2025-05-07 14:13:31] nn step 16350, lr: 0.1.
	loss_policy_0: 0.29859
	accuracy_policy_0: 0.6398
	loss_value_0: 0.40638
	loss_policy_1: 0.05995
	accuracy_policy_1: 0.63797
	loss_value_1: 0.08343
	loss_reward_1: 0.00862
	loss_policy_2: 0.06039
	accuracy_policy_2: 0.63738
	loss_value_2: 0.08581
	loss_reward_2: 0.01332
	loss_policy_3: 0.06016
	accuracy_policy_3: 0.63844
	loss_value_3: 0.08762
	loss_reward_3: 0.01566
	loss_policy_4: 0.06003
	accuracy_policy_4: 0.64211
	loss_value_4: 0.08961
	loss_reward_4: 0.01812
	loss_policy_5: 0.06024
	accuracy_policy_5: 0.63984
	loss_value_5: 0.09122
	loss_reward_5: 0.01849
	loss_policy: 0.59937
	loss_value: 0.84406
	loss_reward: 0.07421
[2025-05-07 14:13:37] nn step 16400, lr: 0.1.
	loss_policy_0: 0.29737
	accuracy_policy_0: 0.64613
	loss_value_0: 0.40522
	loss_policy_1: 0.05963
	accuracy_policy_1: 0.64383
	loss_value_1: 0.08328
	loss_reward_1: 0.0089
	loss_policy_2: 0.05962
	accuracy_policy_2: 0.6459
	loss_value_2: 0.08575
	loss_reward_2: 0.01381
	loss_policy_3: 0.05962
	accuracy_policy_3: 0.6452
	loss_value_3: 0.08783
	loss_reward_3: 0.01643
	loss_policy_4: 0.05957
	accuracy_policy_4: 0.64492
	loss_value_4: 0.08992
	loss_reward_4: 0.01873
	loss_policy_5: 0.0603
	accuracy_policy_5: 0.64543
	loss_value_5: 0.0917
	loss_reward_5: 0.01854
	loss_policy: 0.59611
	loss_value: 0.8437
	loss_reward: 0.0764
Optimization_Done 16400
[2025-05-07 14:16:43] [command] train weight_iter_16400.pkl 64 83
[2025-05-07 14:16:52] nn step 16450, lr: 0.1.
	loss_policy_0: 0.30272
	accuracy_policy_0: 0.64465
	loss_value_0: 0.4142
	loss_policy_1: 0.06132
	accuracy_policy_1: 0.63352
	loss_value_1: 0.08527
	loss_reward_1: 0.00904
	loss_policy_2: 0.06108
	accuracy_policy_2: 0.64207
	loss_value_2: 0.08763
	loss_reward_2: 0.01369
	loss_policy_3: 0.06111
	accuracy_policy_3: 0.64402
	loss_value_3: 0.08952
	loss_reward_3: 0.01607
	loss_policy_4: 0.06162
	accuracy_policy_4: 0.6393
	loss_value_4: 0.09126
	loss_reward_4: 0.01864
	loss_policy_5: 0.0616
	accuracy_policy_5: 0.64211
	loss_value_5: 0.09297
	loss_reward_5: 0.01889
	loss_policy: 0.60946
	loss_value: 0.86087
	loss_reward: 0.07633
[2025-05-07 14:16:58] nn step 16500, lr: 0.1.
	loss_policy_0: 0.28055
	accuracy_policy_0: 0.64711
	loss_value_0: 0.38122
	loss_policy_1: 0.05636
	accuracy_policy_1: 0.64219
	loss_value_1: 0.07818
	loss_reward_1: 0.00842
	loss_policy_2: 0.05678
	accuracy_policy_2: 0.64328
	loss_value_2: 0.0805
	loss_reward_2: 0.01278
	loss_policy_3: 0.05647
	accuracy_policy_3: 0.64684
	loss_value_3: 0.08269
	loss_reward_3: 0.01499
	loss_policy_4: 0.05644
	accuracy_policy_4: 0.64824
	loss_value_4: 0.08416
	loss_reward_4: 0.01742
	loss_policy_5: 0.05684
	accuracy_policy_5: 0.64812
	loss_value_5: 0.08582
	loss_reward_5: 0.01707
	loss_policy: 0.56344
	loss_value: 0.79257
	loss_reward: 0.07068
[2025-05-07 14:17:06] nn step 16550, lr: 0.1.
	loss_policy_0: 0.28744
	accuracy_policy_0: 0.64406
	loss_value_0: 0.3877
	loss_policy_1: 0.05753
	accuracy_policy_1: 0.6375
	loss_value_1: 0.08003
	loss_reward_1: 0.00848
	loss_policy_2: 0.0578
	accuracy_policy_2: 0.6359
	loss_value_2: 0.08195
	loss_reward_2: 0.01303
	loss_policy_3: 0.05784
	accuracy_policy_3: 0.64188
	loss_value_3: 0.08396
	loss_reward_3: 0.01503
	loss_policy_4: 0.05739
	accuracy_policy_4: 0.64445
	loss_value_4: 0.08573
	loss_reward_4: 0.01741
	loss_policy_5: 0.05771
	accuracy_policy_5: 0.64699
	loss_value_5: 0.08732
	loss_reward_5: 0.0182
	loss_policy: 0.57571
	loss_value: 0.8067
	loss_reward: 0.07215
[2025-05-07 14:17:14] nn step 16600, lr: 0.1.
	loss_policy_0: 0.28097
	accuracy_policy_0: 0.64461
	loss_value_0: 0.37973
	loss_policy_1: 0.05691
	accuracy_policy_1: 0.63562
	loss_value_1: 0.07815
	loss_reward_1: 0.00868
	loss_policy_2: 0.0571
	accuracy_policy_2: 0.63441
	loss_value_2: 0.0806
	loss_reward_2: 0.01273
	loss_policy_3: 0.05651
	accuracy_policy_3: 0.64254
	loss_value_3: 0.08257
	loss_reward_3: 0.01465
	loss_policy_4: 0.0572
	accuracy_policy_4: 0.64508
	loss_value_4: 0.08404
	loss_reward_4: 0.01754
	loss_policy_5: 0.05765
	accuracy_policy_5: 0.64211
	loss_value_5: 0.08538
	loss_reward_5: 0.01755
	loss_policy: 0.56635
	loss_value: 0.79046
	loss_reward: 0.07114
Optimization_Done 16600
[2025-05-07 14:20:16] [command] train weight_iter_16600.pkl 65 84
[2025-05-07 14:20:25] nn step 16650, lr: 0.1.
	loss_policy_0: 0.29329
	accuracy_policy_0: 0.64371
	loss_value_0: 0.40484
	loss_policy_1: 0.05945
	accuracy_policy_1: 0.63242
	loss_value_1: 0.08248
	loss_reward_1: 0.00868
	loss_policy_2: 0.05888
	accuracy_policy_2: 0.64109
	loss_value_2: 0.08455
	loss_reward_2: 0.01341
	loss_policy_3: 0.05918
	accuracy_policy_3: 0.6382
	loss_value_3: 0.08654
	loss_reward_3: 0.01529
	loss_policy_4: 0.0595
	accuracy_policy_4: 0.6352
	loss_value_4: 0.08825
	loss_reward_4: 0.0179
	loss_policy_5: 0.0597
	accuracy_policy_5: 0.63742
	loss_value_5: 0.08968
	loss_reward_5: 0.01858
	loss_policy: 0.59
	loss_value: 0.83634
	loss_reward: 0.07386
[2025-05-07 14:20:33] nn step 16700, lr: 0.1.
	loss_policy_0: 0.28434
	accuracy_policy_0: 0.64164
	loss_value_0: 0.38343
	loss_policy_1: 0.05717
	accuracy_policy_1: 0.6307
	loss_value_1: 0.07873
	loss_reward_1: 0.0085
	loss_policy_2: 0.05709
	accuracy_policy_2: 0.63438
	loss_value_2: 0.0809
	loss_reward_2: 0.01333
	loss_policy_3: 0.05728
	accuracy_policy_3: 0.63898
	loss_value_3: 0.08302
	loss_reward_3: 0.01456
	loss_policy_4: 0.05737
	accuracy_policy_4: 0.64117
	loss_value_4: 0.08466
	loss_reward_4: 0.01723
	loss_policy_5: 0.05727
	accuracy_policy_5: 0.64219
	loss_value_5: 0.08633
	loss_reward_5: 0.01768
	loss_policy: 0.57053
	loss_value: 0.79709
	loss_reward: 0.07129
[2025-05-07 14:20:41] nn step 16750, lr: 0.1.
	loss_policy_0: 0.28074
	accuracy_policy_0: 0.64984
	loss_value_0: 0.3818
	loss_policy_1: 0.0564
	accuracy_policy_1: 0.64223
	loss_value_1: 0.07826
	loss_reward_1: 0.00817
	loss_policy_2: 0.05637
	accuracy_policy_2: 0.64078
	loss_value_2: 0.08039
	loss_reward_2: 0.0129
	loss_policy_3: 0.05637
	accuracy_policy_3: 0.64309
	loss_value_3: 0.08213
	loss_reward_3: 0.01418
	loss_policy_4: 0.05676
	accuracy_policy_4: 0.64344
	loss_value_4: 0.08407
	loss_reward_4: 0.01693
	loss_policy_5: 0.05688
	accuracy_policy_5: 0.64293
	loss_value_5: 0.0857
	loss_reward_5: 0.01755
	loss_policy: 0.56352
	loss_value: 0.79235
	loss_reward: 0.06972
[2025-05-07 14:20:47] nn step 16800, lr: 0.1.
	loss_policy_0: 0.28747
	accuracy_policy_0: 0.64023
	loss_value_0: 0.38188
	loss_policy_1: 0.05758
	accuracy_policy_1: 0.63508
	loss_value_1: 0.07804
	loss_reward_1: 0.00849
	loss_policy_2: 0.05746
	accuracy_policy_2: 0.63922
	loss_value_2: 0.08017
	loss_reward_2: 0.0129
	loss_policy_3: 0.05713
	accuracy_policy_3: 0.64613
	loss_value_3: 0.08237
	loss_reward_3: 0.01493
	loss_policy_4: 0.05677
	accuracy_policy_4: 0.64828
	loss_value_4: 0.08424
	loss_reward_4: 0.01763
	loss_policy_5: 0.05708
	accuracy_policy_5: 0.65246
	loss_value_5: 0.08617
	loss_reward_5: 0.01767
	loss_policy: 0.5735
	loss_value: 0.79286
	loss_reward: 0.07163
Optimization_Done 16800
[2025-05-07 14:23:55] [command] train weight_iter_16800.pkl 66 85
[2025-05-07 14:24:02] nn step 16850, lr: 0.1.
	loss_policy_0: 0.30186
	accuracy_policy_0: 0.6525
	loss_value_0: 0.42519
	loss_policy_1: 0.06099
	accuracy_policy_1: 0.64727
	loss_value_1: 0.08694
	loss_reward_1: 0.00889
	loss_policy_2: 0.06179
	accuracy_policy_2: 0.63781
	loss_value_2: 0.08902
	loss_reward_2: 0.01381
	loss_policy_3: 0.06108
	accuracy_policy_3: 0.64441
	loss_value_3: 0.09085
	loss_reward_3: 0.01593
	loss_policy_4: 0.06161
	accuracy_policy_4: 0.64414
	loss_value_4: 0.09273
	loss_reward_4: 0.01891
	loss_policy_5: 0.0613
	accuracy_policy_5: 0.64863
	loss_value_5: 0.09416
	loss_reward_5: 0.01892
	loss_policy: 0.60862
	loss_value: 0.8789
	loss_reward: 0.07647
[2025-05-07 14:24:09] nn step 16900, lr: 0.1.
	loss_policy_0: 0.26362
	accuracy_policy_0: 0.64109
	loss_value_0: 0.36258
	loss_policy_1: 0.05324
	accuracy_policy_1: 0.63988
	loss_value_1: 0.07438
	loss_reward_1: 0.00775
	loss_policy_2: 0.05306
	accuracy_policy_2: 0.63699
	loss_value_2: 0.07617
	loss_reward_2: 0.0115
	loss_policy_3: 0.05298
	accuracy_policy_3: 0.64277
	loss_value_3: 0.07784
	loss_reward_3: 0.01376
	loss_policy_4: 0.05319
	accuracy_policy_4: 0.64578
	loss_value_4: 0.0792
	loss_reward_4: 0.0162
	loss_policy_5: 0.05351
	accuracy_policy_5: 0.64203
	loss_value_5: 0.08049
	loss_reward_5: 0.01624
	loss_policy: 0.5296
	loss_value: 0.75066
	loss_reward: 0.06546
[2025-05-07 14:24:17] nn step 16950, lr: 0.1.
	loss_policy_0: 0.29817
	accuracy_policy_0: 0.65336
	loss_value_0: 0.4052
	loss_policy_1: 0.06017
	accuracy_policy_1: 0.63809
	loss_value_1: 0.08312
	loss_reward_1: 0.00898
	loss_policy_2: 0.06007
	accuracy_policy_2: 0.63949
	loss_value_2: 0.08562
	loss_reward_2: 0.01346
	loss_policy_3: 0.05998
	accuracy_policy_3: 0.64816
	loss_value_3: 0.0875
	loss_reward_3: 0.01615
	loss_policy_4: 0.0604
	accuracy_policy_4: 0.64387
	loss_value_4: 0.08932
	loss_reward_4: 0.0185
	loss_policy_5: 0.06058
	accuracy_policy_5: 0.64586
	loss_value_5: 0.0912
	loss_reward_5: 0.01808
	loss_policy: 0.59937
	loss_value: 0.84197
	loss_reward: 0.07518
[2025-05-07 14:24:25] nn step 17000, lr: 0.1.
	loss_policy_0: 0.28917
	accuracy_policy_0: 0.64273
	loss_value_0: 0.38835
	loss_policy_1: 0.05818
	accuracy_policy_1: 0.63715
	loss_value_1: 0.0802
	loss_reward_1: 0.00815
	loss_policy_2: 0.05834
	accuracy_policy_2: 0.63469
	loss_value_2: 0.08237
	loss_reward_2: 0.01351
	loss_policy_3: 0.05836
	accuracy_policy_3: 0.64
	loss_value_3: 0.08434
	loss_reward_3: 0.01549
	loss_policy_4: 0.05842
	accuracy_policy_4: 0.63707
	loss_value_4: 0.08655
	loss_reward_4: 0.01786
	loss_policy_5: 0.05815
	accuracy_policy_5: 0.64328
	loss_value_5: 0.08806
	loss_reward_5: 0.01785
	loss_policy: 0.58061
	loss_value: 0.80987
	loss_reward: 0.07286
Optimization_Done 17000
[2025-05-07 14:27:29] [command] train weight_iter_17000.pkl 67 86
[2025-05-07 14:27:38] nn step 17050, lr: 0.1.
	loss_policy_0: 0.27813
	accuracy_policy_0: 0.64684
	loss_value_0: 0.38459
	loss_policy_1: 0.05612
	accuracy_policy_1: 0.64145
	loss_value_1: 0.07897
	loss_reward_1: 0.00813
	loss_policy_2: 0.05603
	accuracy_policy_2: 0.6393
	loss_value_2: 0.08069
	loss_reward_2: 0.01212
	loss_policy_3: 0.05587
	accuracy_policy_3: 0.64535
	loss_value_3: 0.08237
	loss_reward_3: 0.0144
	loss_policy_4: 0.05636
	accuracy_policy_4: 0.64266
	loss_value_4: 0.08381
	loss_reward_4: 0.01685
	loss_policy_5: 0.05646
	accuracy_policy_5: 0.64125
	loss_value_5: 0.08534
	loss_reward_5: 0.01719
	loss_policy: 0.55898
	loss_value: 0.79576
	loss_reward: 0.06868
[2025-05-07 14:27:46] nn step 17100, lr: 0.1.
	loss_policy_0: 0.27779
	accuracy_policy_0: 0.65285
	loss_value_0: 0.38377
	loss_policy_1: 0.05633
	accuracy_policy_1: 0.64094
	loss_value_1: 0.07881
	loss_reward_1: 0.00822
	loss_policy_2: 0.05677
	accuracy_policy_2: 0.63891
	loss_value_2: 0.08083
	loss_reward_2: 0.01265
	loss_policy_3: 0.05639
	accuracy_policy_3: 0.64039
	loss_value_3: 0.08268
	loss_reward_3: 0.01484
	loss_policy_4: 0.05667
	accuracy_policy_4: 0.64223
	loss_value_4: 0.08419
	loss_reward_4: 0.01714
	loss_policy_5: 0.05675
	accuracy_policy_5: 0.64719
	loss_value_5: 0.08591
	loss_reward_5: 0.01759
	loss_policy: 0.5607
	loss_value: 0.79619
	loss_reward: 0.07043
[2025-05-07 14:27:52] nn step 17150, lr: 0.1.
	loss_policy_0: 0.29993
	accuracy_policy_0: 0.65176
	loss_value_0: 0.40542
	loss_policy_1: 0.06025
	accuracy_policy_1: 0.64461
	loss_value_1: 0.08348
	loss_reward_1: 0.00884
	loss_policy_2: 0.06045
	accuracy_policy_2: 0.64172
	loss_value_2: 0.0859
	loss_reward_2: 0.01358
	loss_policy_3: 0.06017
	accuracy_policy_3: 0.64523
	loss_value_3: 0.08781
	loss_reward_3: 0.01518
	loss_policy_4: 0.06047
	accuracy_policy_4: 0.64285
	loss_value_4: 0.08958
	loss_reward_4: 0.01804
	loss_policy_5: 0.06068
	accuracy_policy_5: 0.64492
	loss_value_5: 0.09132
	loss_reward_5: 0.01845
	loss_policy: 0.60195
	loss_value: 0.84351
	loss_reward: 0.07411
[2025-05-07 14:28:00] nn step 17200, lr: 0.1.
	loss_policy_0: 0.30826
	accuracy_policy_0: 0.64629
	loss_value_0: 0.42238
	loss_policy_1: 0.06193
	accuracy_policy_1: 0.64012
	loss_value_1: 0.08693
	loss_reward_1: 0.00958
	loss_policy_2: 0.06223
	accuracy_policy_2: 0.63816
	loss_value_2: 0.08914
	loss_reward_2: 0.01383
	loss_policy_3: 0.06242
	accuracy_policy_3: 0.64578
	loss_value_3: 0.09128
	loss_reward_3: 0.01666
	loss_policy_4: 0.06238
	accuracy_policy_4: 0.6457
	loss_value_4: 0.09316
	loss_reward_4: 0.01898
	loss_policy_5: 0.06263
	accuracy_policy_5: 0.64297
	loss_value_5: 0.09502
	loss_reward_5: 0.01945
	loss_policy: 0.61986
	loss_value: 0.8779
	loss_reward: 0.0785
Optimization_Done 17200
[2025-05-07 14:31:03] [command] train weight_iter_17200.pkl 68 87
[2025-05-07 14:31:10] nn step 17250, lr: 0.1.
	loss_policy_0: 0.29054
	accuracy_policy_0: 0.65648
	loss_value_0: 0.4124
	loss_policy_1: 0.05822
	accuracy_policy_1: 0.64812
	loss_value_1: 0.08445
	loss_reward_1: 0.00903
	loss_policy_2: 0.05846
	accuracy_policy_2: 0.64742
	loss_value_2: 0.08625
	loss_reward_2: 0.01309
	loss_policy_3: 0.05817
	accuracy_policy_3: 0.64855
	loss_value_3: 0.08787
	loss_reward_3: 0.01519
	loss_policy_4: 0.05835
	accuracy_policy_4: 0.65047
	loss_value_4: 0.08951
	loss_reward_4: 0.01739
	loss_policy_5: 0.05843
	accuracy_policy_5: 0.65238
	loss_value_5: 0.09096
	loss_reward_5: 0.01812
	loss_policy: 0.58217
	loss_value: 0.85145
	loss_reward: 0.07281
[2025-05-07 14:31:18] nn step 17300, lr: 0.1.
	loss_policy_0: 0.28783
	accuracy_policy_0: 0.64625
	loss_value_0: 0.404
	loss_policy_1: 0.05811
	accuracy_policy_1: 0.63953
	loss_value_1: 0.08286
	loss_reward_1: 0.00872
	loss_policy_2: 0.05785
	accuracy_policy_2: 0.6443
	loss_value_2: 0.08501
	loss_reward_2: 0.01286
	loss_policy_3: 0.05771
	accuracy_policy_3: 0.64984
	loss_value_3: 0.08696
	loss_reward_3: 0.01516
	loss_policy_4: 0.05799
	accuracy_policy_4: 0.64969
	loss_value_4: 0.08867
	loss_reward_4: 0.01772
	loss_policy_5: 0.0581
	accuracy_policy_5: 0.65703
	loss_value_5: 0.09012
	loss_reward_5: 0.01812
	loss_policy: 0.57758
	loss_value: 0.83762
	loss_reward: 0.07259
[2025-05-07 14:31:26] nn step 17350, lr: 0.1.
	loss_policy_0: 0.27834
	accuracy_policy_0: 0.65637
	loss_value_0: 0.39018
	loss_policy_1: 0.05637
	accuracy_policy_1: 0.64504
	loss_value_1: 0.07997
	loss_reward_1: 0.0084
	loss_policy_2: 0.05638
	accuracy_policy_2: 0.64844
	loss_value_2: 0.08216
	loss_reward_2: 0.01253
	loss_policy_3: 0.05675
	accuracy_policy_3: 0.64895
	loss_value_3: 0.08386
	loss_reward_3: 0.01506
	loss_policy_4: 0.05638
	accuracy_policy_4: 0.65355
	loss_value_4: 0.08565
	loss_reward_4: 0.01697
	loss_policy_5: 0.0567
	accuracy_policy_5: 0.64902
	loss_value_5: 0.08746
	loss_reward_5: 0.01765
	loss_policy: 0.56093
	loss_value: 0.80928
	loss_reward: 0.07061
[2025-05-07 14:31:34] nn step 17400, lr: 0.1.
	loss_policy_0: 0.30662
	accuracy_policy_0: 0.65301
	loss_value_0: 0.426
	loss_policy_1: 0.06159
	accuracy_policy_1: 0.64434
	loss_value_1: 0.08725
	loss_reward_1: 0.0091
	loss_policy_2: 0.062
	accuracy_policy_2: 0.64395
	loss_value_2: 0.08952
	loss_reward_2: 0.01372
	loss_policy_3: 0.06182
	accuracy_policy_3: 0.64719
	loss_value_3: 0.09164
	loss_reward_3: 0.01607
	loss_policy_4: 0.0625
	accuracy_policy_4: 0.64781
	loss_value_4: 0.09369
	loss_reward_4: 0.01864
	loss_policy_5: 0.06223
	accuracy_policy_5: 0.6507
	loss_value_5: 0.09564
	loss_reward_5: 0.01904
	loss_policy: 0.61676
	loss_value: 0.88372
	loss_reward: 0.07657
Optimization_Done 17400
[2025-05-07 14:34:37] [command] train weight_iter_17400.pkl 69 88
[2025-05-07 14:34:46] nn step 17450, lr: 0.1.
	loss_policy_0: 0.27469
	accuracy_policy_0: 0.66438
	loss_value_0: 0.39216
	loss_policy_1: 0.05547
	accuracy_policy_1: 0.65059
	loss_value_1: 0.08062
	loss_reward_1: 0.00824
	loss_policy_2: 0.0555
	accuracy_policy_2: 0.65129
	loss_value_2: 0.08247
	loss_reward_2: 0.0128
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.65566
	loss_value_3: 0.08432
	loss_reward_3: 0.01449
	loss_policy_4: 0.05594
	accuracy_policy_4: 0.65496
	loss_value_4: 0.08578
	loss_reward_4: 0.01731
	loss_policy_5: 0.05605
	accuracy_policy_5: 0.66035
	loss_value_5: 0.08765
	loss_reward_5: 0.01717
	loss_policy: 0.55345
	loss_value: 0.81301
	loss_reward: 0.07001
[2025-05-07 14:34:54] nn step 17500, lr: 0.1.
	loss_policy_0: 0.28435
	accuracy_policy_0: 0.65941
	loss_value_0: 0.39929
	loss_policy_1: 0.05736
	accuracy_policy_1: 0.65582
	loss_value_1: 0.0821
	loss_reward_1: 0.0082
	loss_policy_2: 0.05774
	accuracy_policy_2: 0.65254
	loss_value_2: 0.084
	loss_reward_2: 0.01312
	loss_policy_3: 0.05786
	accuracy_policy_3: 0.65441
	loss_value_3: 0.08583
	loss_reward_3: 0.01506
	loss_policy_4: 0.05772
	accuracy_policy_4: 0.65738
	loss_value_4: 0.088
	loss_reward_4: 0.01771
	loss_policy_5: 0.05802
	accuracy_policy_5: 0.66039
	loss_value_5: 0.08966
	loss_reward_5: 0.01761
	loss_policy: 0.57305
	loss_value: 0.82887
	loss_reward: 0.07171
[2025-05-07 14:35:00] nn step 17550, lr: 0.1.
	loss_policy_0: 0.2684
	accuracy_policy_0: 0.66059
	loss_value_0: 0.37442
	loss_policy_1: 0.05437
	accuracy_policy_1: 0.65
	loss_value_1: 0.07701
	loss_reward_1: 0.00824
	loss_policy_2: 0.05415
	accuracy_policy_2: 0.65062
	loss_value_2: 0.07922
	loss_reward_2: 0.01185
	loss_policy_3: 0.05429
	accuracy_policy_3: 0.65293
	loss_value_3: 0.081
	loss_reward_3: 0.01411
	loss_policy_4: 0.05444
	accuracy_policy_4: 0.65383
	loss_value_4: 0.08257
	loss_reward_4: 0.01682
	loss_policy_5: 0.05435
	accuracy_policy_5: 0.65406
	loss_value_5: 0.08403
	loss_reward_5: 0.01629
	loss_policy: 0.54001
	loss_value: 0.77825
	loss_reward: 0.06732
[2025-05-07 14:35:08] nn step 17600, lr: 0.1.
	loss_policy_0: 0.2809
	accuracy_policy_0: 0.66406
	loss_value_0: 0.3916
	loss_policy_1: 0.05678
	accuracy_policy_1: 0.65285
	loss_value_1: 0.08059
	loss_reward_1: 0.00819
	loss_policy_2: 0.05711
	accuracy_policy_2: 0.65277
	loss_value_2: 0.0833
	loss_reward_2: 0.01245
	loss_policy_3: 0.05702
	accuracy_policy_3: 0.65414
	loss_value_3: 0.08511
	loss_reward_3: 0.01463
	loss_policy_4: 0.05714
	accuracy_policy_4: 0.65469
	loss_value_4: 0.08671
	loss_reward_4: 0.01682
	loss_policy_5: 0.05714
	accuracy_policy_5: 0.65598
	loss_value_5: 0.08844
	loss_reward_5: 0.01755
	loss_policy: 0.56609
	loss_value: 0.81574
	loss_reward: 0.06963
Optimization_Done 17600
[2025-05-07 14:38:18] [command] train weight_iter_17600.pkl 70 89
[2025-05-07 14:38:26] nn step 17650, lr: 0.1.
	loss_policy_0: 0.28912
	accuracy_policy_0: 0.65855
	loss_value_0: 0.41564
	loss_policy_1: 0.05872
	accuracy_policy_1: 0.64961
	loss_value_1: 0.0851
	loss_reward_1: 0.00839
	loss_policy_2: 0.05886
	accuracy_policy_2: 0.6509
	loss_value_2: 0.08698
	loss_reward_2: 0.01337
	loss_policy_3: 0.05913
	accuracy_policy_3: 0.65277
	loss_value_3: 0.08887
	loss_reward_3: 0.01531
	loss_policy_4: 0.05885
	accuracy_policy_4: 0.64961
	loss_value_4: 0.09058
	loss_reward_4: 0.01768
	loss_policy_5: 0.05925
	accuracy_policy_5: 0.6507
	loss_value_5: 0.09193
	loss_reward_5: 0.01817
	loss_policy: 0.58393
	loss_value: 0.8591
	loss_reward: 0.0729
[2025-05-07 14:38:34] nn step 17700, lr: 0.1.
	loss_policy_0: 0.29117
	accuracy_policy_0: 0.65812
	loss_value_0: 0.41221
	loss_policy_1: 0.0584
	accuracy_policy_1: 0.65316
	loss_value_1: 0.08446
	loss_reward_1: 0.00897
	loss_policy_2: 0.05891
	accuracy_policy_2: 0.64676
	loss_value_2: 0.08666
	loss_reward_2: 0.01348
	loss_policy_3: 0.05894
	accuracy_policy_3: 0.65176
	loss_value_3: 0.08794
	loss_reward_3: 0.01555
	loss_policy_4: 0.05887
	accuracy_policy_4: 0.65344
	loss_value_4: 0.08934
	loss_reward_4: 0.01805
	loss_policy_5: 0.05909
	accuracy_policy_5: 0.65574
	loss_value_5: 0.09125
	loss_reward_5: 0.0183
	loss_policy: 0.58537
	loss_value: 0.85188
	loss_reward: 0.07435
[2025-05-07 14:38:42] nn step 17750, lr: 0.1.
	loss_policy_0: 0.28367
	accuracy_policy_0: 0.65582
	loss_value_0: 0.39586
	loss_policy_1: 0.05695
	accuracy_policy_1: 0.65062
	loss_value_1: 0.08104
	loss_reward_1: 0.00846
	loss_policy_2: 0.05751
	accuracy_policy_2: 0.6441
	loss_value_2: 0.08331
	loss_reward_2: 0.01278
	loss_policy_3: 0.05719
	accuracy_policy_3: 0.65188
	loss_value_3: 0.08501
	loss_reward_3: 0.01511
	loss_policy_4: 0.05757
	accuracy_policy_4: 0.6523
	loss_value_4: 0.08693
	loss_reward_4: 0.01722
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.65797
	loss_value_5: 0.08848
	loss_reward_5: 0.01728
	loss_policy: 0.5701
	loss_value: 0.82064
	loss_reward: 0.07085
[2025-05-07 14:38:49] nn step 17800, lr: 0.1.
	loss_policy_0: 0.29316
	accuracy_policy_0: 0.66273
	loss_value_0: 0.40773
	loss_policy_1: 0.05891
	accuracy_policy_1: 0.65215
	loss_value_1: 0.08395
	loss_reward_1: 0.00875
	loss_policy_2: 0.05951
	accuracy_policy_2: 0.6525
	loss_value_2: 0.08644
	loss_reward_2: 0.0133
	loss_policy_3: 0.05903
	accuracy_policy_3: 0.65633
	loss_value_3: 0.0883
	loss_reward_3: 0.01539
	loss_policy_4: 0.05904
	accuracy_policy_4: 0.65895
	loss_value_4: 0.0906
	loss_reward_4: 0.01799
	loss_policy_5: 0.05942
	accuracy_policy_5: 0.65914
	loss_value_5: 0.09169
	loss_reward_5: 0.01845
	loss_policy: 0.58907
	loss_value: 0.84871
	loss_reward: 0.07387
Optimization_Done 17800
[2025-05-07 14:41:56] [command] train weight_iter_17800.pkl 71 90
[2025-05-07 14:42:05] nn step 17850, lr: 0.1.
	loss_policy_0: 0.27919
	accuracy_policy_0: 0.64754
	loss_value_0: 0.38853
	loss_policy_1: 0.05598
	accuracy_policy_1: 0.64574
	loss_value_1: 0.07949
	loss_reward_1: 0.00823
	loss_policy_2: 0.0559
	accuracy_policy_2: 0.64465
	loss_value_2: 0.08164
	loss_reward_2: 0.01277
	loss_policy_3: 0.05632
	accuracy_policy_3: 0.64703
	loss_value_3: 0.08308
	loss_reward_3: 0.01446
	loss_policy_4: 0.05609
	accuracy_policy_4: 0.64926
	loss_value_4: 0.08501
	loss_reward_4: 0.01675
	loss_policy_5: 0.0566
	accuracy_policy_5: 0.64836
	loss_value_5: 0.0867
	loss_reward_5: 0.01746
	loss_policy: 0.56008
	loss_value: 0.80445
	loss_reward: 0.06967
[2025-05-07 14:42:12] nn step 17900, lr: 0.1.
	loss_policy_0: 0.2742
	accuracy_policy_0: 0.6584
	loss_value_0: 0.38162
	loss_policy_1: 0.05565
	accuracy_policy_1: 0.64508
	loss_value_1: 0.0783
	loss_reward_1: 0.00799
	loss_policy_2: 0.05585
	accuracy_policy_2: 0.6443
	loss_value_2: 0.08048
	loss_reward_2: 0.01196
	loss_policy_3: 0.05614
	accuracy_policy_3: 0.65199
	loss_value_3: 0.08225
	loss_reward_3: 0.01424
	loss_policy_4: 0.05613
	accuracy_policy_4: 0.65043
	loss_value_4: 0.08389
	loss_reward_4: 0.01662
	loss_policy_5: 0.05626
	accuracy_policy_5: 0.64875
	loss_value_5: 0.08528
	loss_reward_5: 0.01716
	loss_policy: 0.55423
	loss_value: 0.79181
	loss_reward: 0.06797
[2025-05-07 14:42:20] nn step 17950, lr: 0.1.
	loss_policy_0: 0.28625
	accuracy_policy_0: 0.65867
	loss_value_0: 0.39353
	loss_policy_1: 0.05781
	accuracy_policy_1: 0.64535
	loss_value_1: 0.08119
	loss_reward_1: 0.0085
	loss_policy_2: 0.05801
	accuracy_policy_2: 0.6434
	loss_value_2: 0.08338
	loss_reward_2: 0.01267
	loss_policy_3: 0.05761
	accuracy_policy_3: 0.6591
	loss_value_3: 0.08503
	loss_reward_3: 0.01533
	loss_policy_4: 0.05791
	accuracy_policy_4: 0.65355
	loss_value_4: 0.08668
	loss_reward_4: 0.01776
	loss_policy_5: 0.05824
	accuracy_policy_5: 0.64883
	loss_value_5: 0.08843
	loss_reward_5: 0.01743
	loss_policy: 0.57584
	loss_value: 0.81824
	loss_reward: 0.07169
[2025-05-07 14:42:28] nn step 18000, lr: 0.1.
	loss_policy_0: 0.29181
	accuracy_policy_0: 0.65664
	loss_value_0: 0.40075
	loss_policy_1: 0.05875
	accuracy_policy_1: 0.64477
	loss_value_1: 0.08235
	loss_reward_1: 0.00829
	loss_policy_2: 0.05919
	accuracy_policy_2: 0.64484
	loss_value_2: 0.08465
	loss_reward_2: 0.01298
	loss_policy_3: 0.05962
	accuracy_policy_3: 0.64617
	loss_value_3: 0.08666
	loss_reward_3: 0.01547
	loss_policy_4: 0.05937
	accuracy_policy_4: 0.64949
	loss_value_4: 0.08784
	loss_reward_4: 0.017
	loss_policy_5: 0.05928
	accuracy_policy_5: 0.65801
	loss_value_5: 0.08989
	loss_reward_5: 0.0178
	loss_policy: 0.58802
	loss_value: 0.83214
	loss_reward: 0.07153
Optimization_Done 18000
[2025-05-07 14:45:43] [command] train weight_iter_18000.pkl 72 91
[2025-05-07 14:45:51] nn step 18050, lr: 0.1.
	loss_policy_0: 0.2894
	accuracy_policy_0: 0.64988
	loss_value_0: 0.39423
	loss_policy_1: 0.0578
	accuracy_policy_1: 0.64777
	loss_value_1: 0.08088
	loss_reward_1: 0.00837
	loss_policy_2: 0.05769
	accuracy_policy_2: 0.64832
	loss_value_2: 0.08311
	loss_reward_2: 0.0124
	loss_policy_3: 0.05789
	accuracy_policy_3: 0.64852
	loss_value_3: 0.08481
	loss_reward_3: 0.01445
	loss_policy_4: 0.05759
	accuracy_policy_4: 0.65117
	loss_value_4: 0.08635
	loss_reward_4: 0.01672
	loss_policy_5: 0.0576
	accuracy_policy_5: 0.65547
	loss_value_5: 0.08749
	loss_reward_5: 0.01708
	loss_policy: 0.57797
	loss_value: 0.81685
	loss_reward: 0.06902
[2025-05-07 14:45:59] nn step 18100, lr: 0.1.
	loss_policy_0: 0.29688
	accuracy_policy_0: 0.65242
	loss_value_0: 0.40829
	loss_policy_1: 0.05982
	accuracy_policy_1: 0.65203
	loss_value_1: 0.08372
	loss_reward_1: 0.0085
	loss_policy_2: 0.06006
	accuracy_policy_2: 0.64914
	loss_value_2: 0.08609
	loss_reward_2: 0.01338
	loss_policy_3: 0.06009
	accuracy_policy_3: 0.65395
	loss_value_3: 0.08783
	loss_reward_3: 0.01543
	loss_policy_4: 0.06034
	accuracy_policy_4: 0.65258
	loss_value_4: 0.08968
	loss_reward_4: 0.01773
	loss_policy_5: 0.06022
	accuracy_policy_5: 0.65512
	loss_value_5: 0.09082
	loss_reward_5: 0.01797
	loss_policy: 0.59742
	loss_value: 0.84642
	loss_reward: 0.073
[2025-05-07 14:46:07] nn step 18150, lr: 0.1.
	loss_policy_0: 0.26891
	accuracy_policy_0: 0.65809
	loss_value_0: 0.36606
	loss_policy_1: 0.05439
	accuracy_policy_1: 0.64879
	loss_value_1: 0.07529
	loss_reward_1: 0.00756
	loss_policy_2: 0.05417
	accuracy_policy_2: 0.6443
	loss_value_2: 0.0773
	loss_reward_2: 0.01191
	loss_policy_3: 0.05419
	accuracy_policy_3: 0.6482
	loss_value_3: 0.07876
	loss_reward_3: 0.01399
	loss_policy_4: 0.05437
	accuracy_policy_4: 0.65117
	loss_value_4: 0.08052
	loss_reward_4: 0.01526
	loss_policy_5: 0.05423
	accuracy_policy_5: 0.6518
	loss_value_5: 0.08193
	loss_reward_5: 0.01609
	loss_policy: 0.54025
	loss_value: 0.75986
	loss_reward: 0.06481
[2025-05-07 14:46:13] nn step 18200, lr: 0.1.
	loss_policy_0: 0.31009
	accuracy_policy_0: 0.65586
	loss_value_0: 0.42187
	loss_policy_1: 0.06248
	accuracy_policy_1: 0.64805
	loss_value_1: 0.08692
	loss_reward_1: 0.00916
	loss_policy_2: 0.06272
	accuracy_policy_2: 0.64598
	loss_value_2: 0.08918
	loss_reward_2: 0.01372
	loss_policy_3: 0.06266
	accuracy_policy_3: 0.65062
	loss_value_3: 0.09125
	loss_reward_3: 0.01575
	loss_policy_4: 0.06309
	accuracy_policy_4: 0.64793
	loss_value_4: 0.09307
	loss_reward_4: 0.01869
	loss_policy_5: 0.06295
	accuracy_policy_5: 0.64746
	loss_value_5: 0.095
	loss_reward_5: 0.0186
	loss_policy: 0.624
	loss_value: 0.87728
	loss_reward: 0.07592
Optimization_Done 18200
[2025-05-07 14:49:09] [command] train weight_iter_18200.pkl 73 92
[2025-05-07 14:49:16] nn step 18250, lr: 0.1.
	loss_policy_0: 0.2805
	accuracy_policy_0: 0.65617
	loss_value_0: 0.39067
	loss_policy_1: 0.05648
	accuracy_policy_1: 0.645
	loss_value_1: 0.07983
	loss_reward_1: 0.0081
	loss_policy_2: 0.05657
	accuracy_policy_2: 0.64613
	loss_value_2: 0.08184
	loss_reward_2: 0.01233
	loss_policy_3: 0.0571
	accuracy_policy_3: 0.64578
	loss_value_3: 0.08344
	loss_reward_3: 0.01454
	loss_policy_4: 0.05644
	accuracy_policy_4: 0.6507
	loss_value_4: 0.08498
	loss_reward_4: 0.01746
	loss_policy_5: 0.05689
	accuracy_policy_5: 0.65082
	loss_value_5: 0.08679
	loss_reward_5: 0.01716
	loss_policy: 0.56399
	loss_value: 0.80755
	loss_reward: 0.06959
[2025-05-07 14:49:24] nn step 18300, lr: 0.1.
	loss_policy_0: 0.27925
	accuracy_policy_0: 0.6593
	loss_value_0: 0.38727
	loss_policy_1: 0.05628
	accuracy_policy_1: 0.65105
	loss_value_1: 0.07958
	loss_reward_1: 0.00784
	loss_policy_2: 0.05695
	accuracy_policy_2: 0.64984
	loss_value_2: 0.08159
	loss_reward_2: 0.01213
	loss_policy_3: 0.05737
	accuracy_policy_3: 0.65023
	loss_value_3: 0.08385
	loss_reward_3: 0.01486
	loss_policy_4: 0.05729
	accuracy_policy_4: 0.65238
	loss_value_4: 0.08557
	loss_reward_4: 0.01688
	loss_policy_5: 0.05727
	accuracy_policy_5: 0.65133
	loss_value_5: 0.08722
	loss_reward_5: 0.01708
	loss_policy: 0.56441
	loss_value: 0.80507
	loss_reward: 0.06878
[2025-05-07 14:49:32] nn step 18350, lr: 0.1.
	loss_policy_0: 0.28994
	accuracy_policy_0: 0.65906
	loss_value_0: 0.39126
	loss_policy_1: 0.05814
	accuracy_policy_1: 0.65051
	loss_value_1: 0.08069
	loss_reward_1: 0.00853
	loss_policy_2: 0.05824
	accuracy_policy_2: 0.65223
	loss_value_2: 0.08266
	loss_reward_2: 0.01239
	loss_policy_3: 0.05836
	accuracy_policy_3: 0.65066
	loss_value_3: 0.08469
	loss_reward_3: 0.01454
	loss_policy_4: 0.05879
	accuracy_policy_4: 0.65176
	loss_value_4: 0.08636
	loss_reward_4: 0.01714
	loss_policy_5: 0.05835
	accuracy_policy_5: 0.65527
	loss_value_5: 0.08814
	loss_reward_5: 0.01787
	loss_policy: 0.58182
	loss_value: 0.8138
	loss_reward: 0.07047
[2025-05-07 14:49:40] nn step 18400, lr: 0.1.
	loss_policy_0: 0.27001
	accuracy_policy_0: 0.65914
	loss_value_0: 0.36991
	loss_policy_1: 0.0546
	accuracy_policy_1: 0.64996
	loss_value_1: 0.07612
	loss_reward_1: 0.00786
	loss_policy_2: 0.05481
	accuracy_policy_2: 0.65062
	loss_value_2: 0.07794
	loss_reward_2: 0.01208
	loss_policy_3: 0.05523
	accuracy_policy_3: 0.64746
	loss_value_3: 0.07983
	loss_reward_3: 0.01364
	loss_policy_4: 0.05524
	accuracy_policy_4: 0.64945
	loss_value_4: 0.08137
	loss_reward_4: 0.0162
	loss_policy_5: 0.05491
	accuracy_policy_5: 0.65977
	loss_value_5: 0.08289
	loss_reward_5: 0.0169
	loss_policy: 0.54481
	loss_value: 0.76806
	loss_reward: 0.06668
Optimization_Done 18400
[2025-05-07 14:52:50] [command] train weight_iter_18400.pkl 74 93
[2025-05-07 14:52:59] nn step 18450, lr: 0.1.
	loss_policy_0: 0.28605
	accuracy_policy_0: 0.65504
	loss_value_0: 0.39507
	loss_policy_1: 0.05817
	accuracy_policy_1: 0.64418
	loss_value_1: 0.08095
	loss_reward_1: 0.00822
	loss_policy_2: 0.05781
	accuracy_policy_2: 0.64832
	loss_value_2: 0.08282
	loss_reward_2: 0.01239
	loss_policy_3: 0.05851
	accuracy_policy_3: 0.65066
	loss_value_3: 0.08458
	loss_reward_3: 0.01454
	loss_policy_4: 0.05842
	accuracy_policy_4: 0.64871
	loss_value_4: 0.08626
	loss_reward_4: 0.0173
	loss_policy_5: 0.05836
	accuracy_policy_5: 0.64832
	loss_value_5: 0.08794
	loss_reward_5: 0.01722
	loss_policy: 0.57732
	loss_value: 0.81762
	loss_reward: 0.06967
[2025-05-07 14:53:06] nn step 18500, lr: 0.1.
	loss_policy_0: 0.28157
	accuracy_policy_0: 0.65406
	loss_value_0: 0.37995
	loss_policy_1: 0.05705
	accuracy_policy_1: 0.64328
	loss_value_1: 0.07825
	loss_reward_1: 0.00796
	loss_policy_2: 0.05693
	accuracy_policy_2: 0.6498
	loss_value_2: 0.08034
	loss_reward_2: 0.01186
	loss_policy_3: 0.05711
	accuracy_policy_3: 0.65023
	loss_value_3: 0.08255
	loss_reward_3: 0.01444
	loss_policy_4: 0.05728
	accuracy_policy_4: 0.65219
	loss_value_4: 0.08443
	loss_reward_4: 0.017
	loss_policy_5: 0.0573
	accuracy_policy_5: 0.65125
	loss_value_5: 0.08604
	loss_reward_5: 0.01696
	loss_policy: 0.56723
	loss_value: 0.79157
	loss_reward: 0.06822
[2025-05-07 14:53:14] nn step 18550, lr: 0.1.
	loss_policy_0: 0.28101
	accuracy_policy_0: 0.65449
	loss_value_0: 0.38128
	loss_policy_1: 0.05651
	accuracy_policy_1: 0.65016
	loss_value_1: 0.07813
	loss_reward_1: 0.008
	loss_policy_2: 0.05653
	accuracy_policy_2: 0.6484
	loss_value_2: 0.08028
	loss_reward_2: 0.01211
	loss_policy_3: 0.05658
	accuracy_policy_3: 0.65145
	loss_value_3: 0.08229
	loss_reward_3: 0.01426
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.65359
	loss_value_4: 0.08377
	loss_reward_4: 0.01661
	loss_policy_5: 0.05696
	accuracy_policy_5: 0.65359
	loss_value_5: 0.08528
	loss_reward_5: 0.01693
	loss_policy: 0.56423
	loss_value: 0.79104
	loss_reward: 0.06792
[2025-05-07 14:53:22] nn step 18600, lr: 0.1.
	loss_policy_0: 0.28924
	accuracy_policy_0: 0.65578
	loss_value_0: 0.39657
	loss_policy_1: 0.05845
	accuracy_policy_1: 0.64809
	loss_value_1: 0.0818
	loss_reward_1: 0.00845
	loss_policy_2: 0.05908
	accuracy_policy_2: 0.64691
	loss_value_2: 0.08412
	loss_reward_2: 0.01246
	loss_policy_3: 0.05898
	accuracy_policy_3: 0.65125
	loss_value_3: 0.08602
	loss_reward_3: 0.01477
	loss_policy_4: 0.05902
	accuracy_policy_4: 0.65648
	loss_value_4: 0.08772
	loss_reward_4: 0.01778
	loss_policy_5: 0.05874
	accuracy_policy_5: 0.65363
	loss_value_5: 0.08939
	loss_reward_5: 0.01753
	loss_policy: 0.58352
	loss_value: 0.82563
	loss_reward: 0.07098
Optimization_Done 18600
[2025-05-07 14:56:25] [command] train weight_iter_18600.pkl 75 94
[2025-05-07 14:56:32] nn step 18650, lr: 0.1.
	loss_policy_0: 0.29168
	accuracy_policy_0: 0.65629
	loss_value_0: 0.40244
	loss_policy_1: 0.05876
	accuracy_policy_1: 0.64301
	loss_value_1: 0.08286
	loss_reward_1: 0.00825
	loss_policy_2: 0.05868
	accuracy_policy_2: 0.64848
	loss_value_2: 0.08435
	loss_reward_2: 0.01272
	loss_policy_3: 0.0595
	accuracy_policy_3: 0.6373
	loss_value_3: 0.08649
	loss_reward_3: 0.01527
	loss_policy_4: 0.05915
	accuracy_policy_4: 0.6482
	loss_value_4: 0.08786
	loss_reward_4: 0.01724
	loss_policy_5: 0.05895
	accuracy_policy_5: 0.65301
	loss_value_5: 0.08939
	loss_reward_5: 0.01736
	loss_policy: 0.58672
	loss_value: 0.8334
	loss_reward: 0.07084
[2025-05-07 14:56:40] nn step 18700, lr: 0.1.
	loss_policy_0: 0.27266
	accuracy_policy_0: 0.65211
	loss_value_0: 0.36914
	loss_policy_1: 0.05526
	accuracy_policy_1: 0.64211
	loss_value_1: 0.07572
	loss_reward_1: 0.00756
	loss_policy_2: 0.05512
	accuracy_policy_2: 0.64613
	loss_value_2: 0.07795
	loss_reward_2: 0.01167
	loss_policy_3: 0.05533
	accuracy_policy_3: 0.64336
	loss_value_3: 0.07983
	loss_reward_3: 0.01343
	loss_policy_4: 0.05521
	accuracy_policy_4: 0.6509
	loss_value_4: 0.08137
	loss_reward_4: 0.0158
	loss_policy_5: 0.05519
	accuracy_policy_5: 0.65625
	loss_value_5: 0.08291
	loss_reward_5: 0.01618
	loss_policy: 0.54877
	loss_value: 0.76692
	loss_reward: 0.06465
[2025-05-07 14:56:48] nn step 18750, lr: 0.1.
	loss_policy_0: 0.28308
	accuracy_policy_0: 0.65387
	loss_value_0: 0.38502
	loss_policy_1: 0.05747
	accuracy_policy_1: 0.645
	loss_value_1: 0.07922
	loss_reward_1: 0.00833
	loss_policy_2: 0.05723
	accuracy_policy_2: 0.65086
	loss_value_2: 0.08134
	loss_reward_2: 0.01281
	loss_policy_3: 0.05728
	accuracy_policy_3: 0.65359
	loss_value_3: 0.08343
	loss_reward_3: 0.01438
	loss_policy_4: 0.05747
	accuracy_policy_4: 0.65512
	loss_value_4: 0.08501
	loss_reward_4: 0.01749
	loss_policy_5: 0.05749
	accuracy_policy_5: 0.65551
	loss_value_5: 0.08682
	loss_reward_5: 0.01734
	loss_policy: 0.57002
	loss_value: 0.80083
	loss_reward: 0.07034
[2025-05-07 14:56:55] nn step 18800, lr: 0.1.
	loss_policy_0: 0.30342
	accuracy_policy_0: 0.65227
	loss_value_0: 0.40772
	loss_policy_1: 0.06088
	accuracy_policy_1: 0.64336
	loss_value_1: 0.08353
	loss_reward_1: 0.00877
	loss_policy_2: 0.06102
	accuracy_policy_2: 0.64496
	loss_value_2: 0.08622
	loss_reward_2: 0.01308
	loss_policy_3: 0.06109
	accuracy_policy_3: 0.6468
	loss_value_3: 0.08808
	loss_reward_3: 0.01516
	loss_policy_4: 0.06131
	accuracy_policy_4: 0.65184
	loss_value_4: 0.09025
	loss_reward_4: 0.01829
	loss_policy_5: 0.06139
	accuracy_policy_5: 0.65215
	loss_value_5: 0.09183
	loss_reward_5: 0.01838
	loss_policy: 0.60912
	loss_value: 0.84764
	loss_reward: 0.07369
Optimization_Done 18800
[2025-05-07 15:00:04] [command] train weight_iter_18800.pkl 76 95
[2025-05-07 15:00:13] nn step 18850, lr: 0.1.
	loss_policy_0: 0.27084
	accuracy_policy_0: 0.66055
	loss_value_0: 0.37254
	loss_policy_1: 0.05486
	accuracy_policy_1: 0.64863
	loss_value_1: 0.0767
	loss_reward_1: 0.00799
	loss_policy_2: 0.05493
	accuracy_policy_2: 0.65168
	loss_value_2: 0.07867
	loss_reward_2: 0.01175
	loss_policy_3: 0.05515
	accuracy_policy_3: 0.64848
	loss_value_3: 0.08024
	loss_reward_3: 0.01407
	loss_policy_4: 0.05528
	accuracy_policy_4: 0.65352
	loss_value_4: 0.08198
	loss_reward_4: 0.01618
	loss_policy_5: 0.05541
	accuracy_policy_5: 0.65285
	loss_value_5: 0.08351
	loss_reward_5: 0.01618
	loss_policy: 0.54646
	loss_value: 0.77363
	loss_reward: 0.06616
[2025-05-07 15:00:20] nn step 18900, lr: 0.1.
	loss_policy_0: 0.29545
	accuracy_policy_0: 0.64773
	loss_value_0: 0.39401
	loss_policy_1: 0.05927
	accuracy_policy_1: 0.64461
	loss_value_1: 0.08109
	loss_reward_1: 0.00801
	loss_policy_2: 0.05909
	accuracy_policy_2: 0.64867
	loss_value_2: 0.08278
	loss_reward_2: 0.01258
	loss_policy_3: 0.05916
	accuracy_policy_3: 0.65
	loss_value_3: 0.08474
	loss_reward_3: 0.01504
	loss_policy_4: 0.05904
	accuracy_policy_4: 0.6477
	loss_value_4: 0.08667
	loss_reward_4: 0.0171
	loss_policy_5: 0.05924
	accuracy_policy_5: 0.65254
	loss_value_5: 0.08854
	loss_reward_5: 0.01791
	loss_policy: 0.59125
	loss_value: 0.81782
	loss_reward: 0.07064
[2025-05-07 15:00:27] nn step 18950, lr: 0.1.
	loss_policy_0: 0.28216
	accuracy_policy_0: 0.65941
	loss_value_0: 0.38947
	loss_policy_1: 0.05733
	accuracy_policy_1: 0.65004
	loss_value_1: 0.07961
	loss_reward_1: 0.00829
	loss_policy_2: 0.0572
	accuracy_policy_2: 0.65289
	loss_value_2: 0.08202
	loss_reward_2: 0.01204
	loss_policy_3: 0.05766
	accuracy_policy_3: 0.64957
	loss_value_3: 0.0839
	loss_reward_3: 0.01395
	loss_policy_4: 0.05771
	accuracy_policy_4: 0.65543
	loss_value_4: 0.08551
	loss_reward_4: 0.01729
	loss_policy_5: 0.05769
	accuracy_policy_5: 0.65523
	loss_value_5: 0.08719
	loss_reward_5: 0.01693
	loss_policy: 0.56975
	loss_value: 0.80769
	loss_reward: 0.0685
[2025-05-07 15:00:35] nn step 19000, lr: 0.1.
	loss_policy_0: 0.29185
	accuracy_policy_0: 0.6593
	loss_value_0: 0.39695
	loss_policy_1: 0.05896
	accuracy_policy_1: 0.64957
	loss_value_1: 0.08173
	loss_reward_1: 0.00849
	loss_policy_2: 0.05911
	accuracy_policy_2: 0.65051
	loss_value_2: 0.08377
	loss_reward_2: 0.01277
	loss_policy_3: 0.05931
	accuracy_policy_3: 0.65527
	loss_value_3: 0.08586
	loss_reward_3: 0.01503
	loss_policy_4: 0.05938
	accuracy_policy_4: 0.65039
	loss_value_4: 0.08761
	loss_reward_4: 0.0179
	loss_policy_5: 0.05951
	accuracy_policy_5: 0.65379
	loss_value_5: 0.08916
	loss_reward_5: 0.01754
	loss_policy: 0.58813
	loss_value: 0.82507
	loss_reward: 0.07174
Optimization_Done 19000
[2025-05-07 15:03:39] [command] train weight_iter_19000.pkl 77 96
[2025-05-07 15:03:48] nn step 19050, lr: 0.1.
	loss_policy_0: 0.26022
	accuracy_policy_0: 0.66562
	loss_value_0: 0.36396
	loss_policy_1: 0.05309
	accuracy_policy_1: 0.65637
	loss_value_1: 0.07471
	loss_reward_1: 0.00782
	loss_policy_2: 0.05299
	accuracy_policy_2: 0.65707
	loss_value_2: 0.07691
	loss_reward_2: 0.01153
	loss_policy_3: 0.0532
	accuracy_policy_3: 0.6532
	loss_value_3: 0.07831
	loss_reward_3: 0.01323
	loss_policy_4: 0.05348
	accuracy_policy_4: 0.65176
	loss_value_4: 0.07959
	loss_reward_4: 0.01525
	loss_policy_5: 0.05317
	accuracy_policy_5: 0.65961
	loss_value_5: 0.0812
	loss_reward_5: 0.01544
	loss_policy: 0.52615
	loss_value: 0.75468
	loss_reward: 0.06328
[2025-05-07 15:03:56] nn step 19100, lr: 0.1.
	loss_policy_0: 0.28376
	accuracy_policy_0: 0.65594
	loss_value_0: 0.39014
	loss_policy_1: 0.05723
	accuracy_policy_1: 0.64852
	loss_value_1: 0.07992
	loss_reward_1: 0.00794
	loss_policy_2: 0.0581
	accuracy_policy_2: 0.6443
	loss_value_2: 0.08174
	loss_reward_2: 0.01233
	loss_policy_3: 0.05754
	accuracy_policy_3: 0.64832
	loss_value_3: 0.08384
	loss_reward_3: 0.0144
	loss_policy_4: 0.0574
	accuracy_policy_4: 0.65242
	loss_value_4: 0.08531
	loss_reward_4: 0.01683
	loss_policy_5: 0.05794
	accuracy_policy_5: 0.65355
	loss_value_5: 0.08708
	loss_reward_5: 0.01753
	loss_policy: 0.57197
	loss_value: 0.80803
	loss_reward: 0.06903
[2025-05-07 15:04:02] nn step 19150, lr: 0.1.
	loss_policy_0: 0.28435
	accuracy_policy_0: 0.65695
	loss_value_0: 0.38873
	loss_policy_1: 0.05747
	accuracy_policy_1: 0.65164
	loss_value_1: 0.07959
	loss_reward_1: 0.00816
	loss_policy_2: 0.05715
	accuracy_policy_2: 0.65656
	loss_value_2: 0.08151
	loss_reward_2: 0.01238
	loss_policy_3: 0.05776
	accuracy_policy_3: 0.6502
	loss_value_3: 0.08343
	loss_reward_3: 0.01428
	loss_policy_4: 0.05775
	accuracy_policy_4: 0.65715
	loss_value_4: 0.08517
	loss_reward_4: 0.01671
	loss_policy_5: 0.05776
	accuracy_policy_5: 0.65719
	loss_value_5: 0.08659
	loss_reward_5: 0.01701
	loss_policy: 0.57224
	loss_value: 0.80503
	loss_reward: 0.06853
[2025-05-07 15:04:10] nn step 19200, lr: 0.1.
	loss_policy_0: 0.277
	accuracy_policy_0: 0.65578
	loss_value_0: 0.37121
	loss_policy_1: 0.05499
	accuracy_policy_1: 0.65457
	loss_value_1: 0.07638
	loss_reward_1: 0.00799
	loss_policy_2: 0.05548
	accuracy_policy_2: 0.6491
	loss_value_2: 0.07873
	loss_reward_2: 0.01169
	loss_policy_3: 0.05528
	accuracy_policy_3: 0.6532
	loss_value_3: 0.0801
	loss_reward_3: 0.01392
	loss_policy_4: 0.05532
	accuracy_policy_4: 0.65543
	loss_value_4: 0.08195
	loss_reward_4: 0.01623
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.65613
	loss_value_5: 0.08323
	loss_reward_5: 0.01648
	loss_policy: 0.55387
	loss_value: 0.7716
	loss_reward: 0.06631
Optimization_Done 19200
[2025-05-07 15:07:20] [command] train weight_iter_19200.pkl 78 97
[2025-05-07 15:07:29] nn step 19250, lr: 0.1.
	loss_policy_0: 0.28718
	accuracy_policy_0: 0.64801
	loss_value_0: 0.39299
	loss_policy_1: 0.05741
	accuracy_policy_1: 0.64316
	loss_value_1: 0.08024
	loss_reward_1: 0.00805
	loss_policy_2: 0.05727
	accuracy_policy_2: 0.65062
	loss_value_2: 0.08188
	loss_reward_2: 0.01228
	loss_policy_3: 0.05768
	accuracy_policy_3: 0.64402
	loss_value_3: 0.08353
	loss_reward_3: 0.01421
	loss_policy_4: 0.0576
	accuracy_policy_4: 0.64691
	loss_value_4: 0.08495
	loss_reward_4: 0.01633
	loss_policy_5: 0.05758
	accuracy_policy_5: 0.64812
	loss_value_5: 0.0863
	loss_reward_5: 0.01684
	loss_policy: 0.57473
	loss_value: 0.80989
	loss_reward: 0.06771
[2025-05-07 15:07:37] nn step 19300, lr: 0.1.
	loss_policy_0: 0.27745
	accuracy_policy_0: 0.66809
	loss_value_0: 0.37623
	loss_policy_1: 0.05627
	accuracy_policy_1: 0.65543
	loss_value_1: 0.07712
	loss_reward_1: 0.00835
	loss_policy_2: 0.05604
	accuracy_policy_2: 0.65723
	loss_value_2: 0.07919
	loss_reward_2: 0.01243
	loss_policy_3: 0.05664
	accuracy_policy_3: 0.65125
	loss_value_3: 0.0808
	loss_reward_3: 0.01371
	loss_policy_4: 0.05611
	accuracy_policy_4: 0.6543
	loss_value_4: 0.08287
	loss_reward_4: 0.0161
	loss_policy_5: 0.05634
	accuracy_policy_5: 0.66051
	loss_value_5: 0.08416
	loss_reward_5: 0.0163
	loss_policy: 0.55884
	loss_value: 0.78037
	loss_reward: 0.06688
[2025-05-07 15:07:45] nn step 19350, lr: 0.1.
	loss_policy_0: 0.25716
	accuracy_policy_0: 0.66078
	loss_value_0: 0.34527
	loss_policy_1: 0.05187
	accuracy_policy_1: 0.64742
	loss_value_1: 0.07115
	loss_reward_1: 0.00704
	loss_policy_2: 0.05186
	accuracy_policy_2: 0.65203
	loss_value_2: 0.07327
	loss_reward_2: 0.01085
	loss_policy_3: 0.05211
	accuracy_policy_3: 0.65098
	loss_value_3: 0.07513
	loss_reward_3: 0.01271
	loss_policy_4: 0.05242
	accuracy_policy_4: 0.65109
	loss_value_4: 0.07685
	loss_reward_4: 0.015
	loss_policy_5: 0.05205
	accuracy_policy_5: 0.65895
	loss_value_5: 0.07829
	loss_reward_5: 0.01488
	loss_policy: 0.51748
	loss_value: 0.71996
	loss_reward: 0.06049
[2025-05-07 15:07:51] nn step 19400, lr: 0.1.
	loss_policy_0: 0.28733
	accuracy_policy_0: 0.66012
	loss_value_0: 0.38436
	loss_policy_1: 0.05774
	accuracy_policy_1: 0.65062
	loss_value_1: 0.07908
	loss_reward_1: 0.00818
	loss_policy_2: 0.05794
	accuracy_policy_2: 0.64996
	loss_value_2: 0.08113
	loss_reward_2: 0.0119
	loss_policy_3: 0.05816
	accuracy_policy_3: 0.64855
	loss_value_3: 0.083
	loss_reward_3: 0.01459
	loss_policy_4: 0.05795
	accuracy_policy_4: 0.65109
	loss_value_4: 0.08446
	loss_reward_4: 0.01671
	loss_policy_5: 0.05798
	accuracy_policy_5: 0.65316
	loss_value_5: 0.08601
	loss_reward_5: 0.01693
	loss_policy: 0.5771
	loss_value: 0.79803
	loss_reward: 0.06831
Optimization_Done 19400
[2025-05-07 15:11:05] [command] train weight_iter_19400.pkl 79 98
[2025-05-07 15:11:12] nn step 19450, lr: 0.1.
	loss_policy_0: 0.26113
	accuracy_policy_0: 0.67359
	loss_value_0: 0.36041
	loss_policy_1: 0.05287
	accuracy_policy_1: 0.6541
	loss_value_1: 0.07365
	loss_reward_1: 0.00774
	loss_policy_2: 0.0532
	accuracy_policy_2: 0.65973
	loss_value_2: 0.07545
	loss_reward_2: 0.01126
	loss_policy_3: 0.05322
	accuracy_policy_3: 0.65699
	loss_value_3: 0.07711
	loss_reward_3: 0.01337
	loss_policy_4: 0.05318
	accuracy_policy_4: 0.66172
	loss_value_4: 0.0785
	loss_reward_4: 0.01605
	loss_policy_5: 0.05301
	accuracy_policy_5: 0.66816
	loss_value_5: 0.07956
	loss_reward_5: 0.01543
	loss_policy: 0.52661
	loss_value: 0.74467
	loss_reward: 0.06384
[2025-05-07 15:11:20] nn step 19500, lr: 0.1.
	loss_policy_0: 0.28882
	accuracy_policy_0: 0.67145
	loss_value_0: 0.39824
	loss_policy_1: 0.0589
	accuracy_policy_1: 0.6548
	loss_value_1: 0.08217
	loss_reward_1: 0.00832
	loss_policy_2: 0.05843
	accuracy_policy_2: 0.6607
	loss_value_2: 0.08442
	loss_reward_2: 0.0123
	loss_policy_3: 0.05874
	accuracy_policy_3: 0.65914
	loss_value_3: 0.08624
	loss_reward_3: 0.015
	loss_policy_4: 0.05897
	accuracy_policy_4: 0.66035
	loss_value_4: 0.08797
	loss_reward_4: 0.01711
	loss_policy_5: 0.05903
	accuracy_policy_5: 0.66168
	loss_value_5: 0.08952
	loss_reward_5: 0.01681
	loss_policy: 0.5829
	loss_value: 0.82855
	loss_reward: 0.06953
[2025-05-07 15:11:28] nn step 19550, lr: 0.1.
	loss_policy_0: 0.27069
	accuracy_policy_0: 0.67348
	loss_value_0: 0.37217
	loss_policy_1: 0.05469
	accuracy_policy_1: 0.66098
	loss_value_1: 0.07638
	loss_reward_1: 0.00773
	loss_policy_2: 0.05532
	accuracy_policy_2: 0.65629
	loss_value_2: 0.07835
	loss_reward_2: 0.01167
	loss_policy_3: 0.05484
	accuracy_policy_3: 0.66422
	loss_value_3: 0.08014
	loss_reward_3: 0.01379
	loss_policy_4: 0.05517
	accuracy_policy_4: 0.6627
	loss_value_4: 0.0815
	loss_reward_4: 0.016
	loss_policy_5: 0.05486
	accuracy_policy_5: 0.66633
	loss_value_5: 0.08291
	loss_reward_5: 0.01633
	loss_policy: 0.54556
	loss_value: 0.77146
	loss_reward: 0.06552
[2025-05-07 15:11:36] nn step 19600, lr: 0.1.
	loss_policy_0: 0.29791
	accuracy_policy_0: 0.66066
	loss_value_0: 0.39535
	loss_policy_1: 0.05985
	accuracy_policy_1: 0.65086
	loss_value_1: 0.08147
	loss_reward_1: 0.00848
	loss_policy_2: 0.0596
	accuracy_policy_2: 0.6527
	loss_value_2: 0.08372
	loss_reward_2: 0.01248
	loss_policy_3: 0.0595
	accuracy_policy_3: 0.65773
	loss_value_3: 0.08587
	loss_reward_3: 0.01493
	loss_policy_4: 0.05981
	accuracy_policy_4: 0.66102
	loss_value_4: 0.08734
	loss_reward_4: 0.01746
	loss_policy_5: 0.05941
	accuracy_policy_5: 0.66227
	loss_value_5: 0.08903
	loss_reward_5: 0.01718
	loss_policy: 0.59609
	loss_value: 0.82278
	loss_reward: 0.07052
Optimization_Done 19600
[2025-05-07 15:14:34] [command] train weight_iter_19600.pkl 80 99
[2025-05-07 15:14:43] nn step 19650, lr: 0.1.
	loss_policy_0: 0.27705
	accuracy_policy_0: 0.67688
	loss_value_0: 0.39056
	loss_policy_1: 0.05576
	accuracy_policy_1: 0.66152
	loss_value_1: 0.07994
	loss_reward_1: 0.00852
	loss_policy_2: 0.05598
	accuracy_policy_2: 0.66449
	loss_value_2: 0.08192
	loss_reward_2: 0.0127
	loss_policy_3: 0.05628
	accuracy_policy_3: 0.66414
	loss_value_3: 0.08368
	loss_reward_3: 0.01432
	loss_policy_4: 0.05611
	accuracy_policy_4: 0.66793
	loss_value_4: 0.08508
	loss_reward_4: 0.01718
	loss_policy_5: 0.0562
	accuracy_policy_5: 0.66758
	loss_value_5: 0.08642
	loss_reward_5: 0.0167
	loss_policy: 0.55738
	loss_value: 0.80761
	loss_reward: 0.06942
[2025-05-07 15:14:51] nn step 19700, lr: 0.1.
	loss_policy_0: 0.26476
	accuracy_policy_0: 0.67062
	loss_value_0: 0.36625
	loss_policy_1: 0.05323
	accuracy_policy_1: 0.6668
	loss_value_1: 0.0748
	loss_reward_1: 0.00768
	loss_policy_2: 0.05348
	accuracy_policy_2: 0.66457
	loss_value_2: 0.07675
	loss_reward_2: 0.01151
	loss_policy_3: 0.05378
	accuracy_policy_3: 0.66426
	loss_value_3: 0.07838
	loss_reward_3: 0.01372
	loss_policy_4: 0.05357
	accuracy_policy_4: 0.66535
	loss_value_4: 0.07975
	loss_reward_4: 0.01583
	loss_policy_5: 0.05357
	accuracy_policy_5: 0.66836
	loss_value_5: 0.08102
	loss_reward_5: 0.01608
	loss_policy: 0.53239
	loss_value: 0.75695
	loss_reward: 0.06482
[2025-05-07 15:14:58] nn step 19750, lr: 0.1.
	loss_policy_0: 0.27222
	accuracy_policy_0: 0.67848
	loss_value_0: 0.37032
	loss_policy_1: 0.05541
	accuracy_policy_1: 0.66227
	loss_value_1: 0.07611
	loss_reward_1: 0.00776
	loss_policy_2: 0.05558
	accuracy_policy_2: 0.66688
	loss_value_2: 0.07798
	loss_reward_2: 0.01176
	loss_policy_3: 0.05545
	accuracy_policy_3: 0.66188
	loss_value_3: 0.08031
	loss_reward_3: 0.0135
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.66418
	loss_value_4: 0.08204
	loss_reward_4: 0.0159
	loss_policy_5: 0.05566
	accuracy_policy_5: 0.66645
	loss_value_5: 0.08331
	loss_reward_5: 0.01631
	loss_policy: 0.54995
	loss_value: 0.77007
	loss_reward: 0.06523
[2025-05-07 15:15:06] nn step 19800, lr: 0.1.
	loss_policy_0: 0.28725
	accuracy_policy_0: 0.67293
	loss_value_0: 0.39436
	loss_policy_1: 0.0579
	accuracy_policy_1: 0.66957
	loss_value_1: 0.08134
	loss_reward_1: 0.00795
	loss_policy_2: 0.05807
	accuracy_policy_2: 0.66891
	loss_value_2: 0.08328
	loss_reward_2: 0.01227
	loss_policy_3: 0.05844
	accuracy_policy_3: 0.66555
	loss_value_3: 0.08496
	loss_reward_3: 0.01473
	loss_policy_4: 0.05817
	accuracy_policy_4: 0.66695
	loss_value_4: 0.08663
	loss_reward_4: 0.01672
	loss_policy_5: 0.0582
	accuracy_policy_5: 0.66973
	loss_value_5: 0.08838
	loss_reward_5: 0.0169
	loss_policy: 0.57804
	loss_value: 0.81894
	loss_reward: 0.06858
Optimization_Done 19800
[2025-05-07 15:18:13] [command] train weight_iter_19800.pkl 81 100
[2025-05-07 15:18:21] nn step 19850, lr: 0.1.
	loss_policy_0: 0.27477
	accuracy_policy_0: 0.67418
	loss_value_0: 0.38232
	loss_policy_1: 0.05565
	accuracy_policy_1: 0.66543
	loss_value_1: 0.07824
	loss_reward_1: 0.00789
	loss_policy_2: 0.05608
	accuracy_policy_2: 0.6625
	loss_value_2: 0.08013
	loss_reward_2: 0.0117
	loss_policy_3: 0.05618
	accuracy_policy_3: 0.66309
	loss_value_3: 0.08165
	loss_reward_3: 0.01428
	loss_policy_4: 0.05593
	accuracy_policy_4: 0.66766
	loss_value_4: 0.08308
	loss_reward_4: 0.01609
	loss_policy_5: 0.05562
	accuracy_policy_5: 0.67078
	loss_value_5: 0.08441
	loss_reward_5: 0.01675
	loss_policy: 0.55423
	loss_value: 0.78983
	loss_reward: 0.06671
[2025-05-07 15:18:29] nn step 19900, lr: 0.1.
	loss_policy_0: 0.26653
	accuracy_policy_0: 0.68129
	loss_value_0: 0.37142
	loss_policy_1: 0.05392
	accuracy_policy_1: 0.66598
	loss_value_1: 0.0764
	loss_reward_1: 0.00749
	loss_policy_2: 0.05446
	accuracy_policy_2: 0.66191
	loss_value_2: 0.0785
	loss_reward_2: 0.0113
	loss_policy_3: 0.05425
	accuracy_policy_3: 0.66879
	loss_value_3: 0.08035
	loss_reward_3: 0.01318
	loss_policy_4: 0.0544
	accuracy_policy_4: 0.66742
	loss_value_4: 0.08224
	loss_reward_4: 0.01593
	loss_policy_5: 0.05438
	accuracy_policy_5: 0.66926
	loss_value_5: 0.08354
	loss_reward_5: 0.01614
	loss_policy: 0.53794
	loss_value: 0.77245
	loss_reward: 0.06405
[2025-05-07 15:18:37] nn step 19950, lr: 0.1.
	loss_policy_0: 0.28206
	accuracy_policy_0: 0.67316
	loss_value_0: 0.38131
	loss_policy_1: 0.05721
	accuracy_policy_1: 0.66328
	loss_value_1: 0.07865
	loss_reward_1: 0.00804
	loss_policy_2: 0.05706
	accuracy_policy_2: 0.66535
	loss_value_2: 0.08046
	loss_reward_2: 0.01187
	loss_policy_3: 0.05719
	accuracy_policy_3: 0.65953
	loss_value_3: 0.08234
	loss_reward_3: 0.01421
	loss_policy_4: 0.05734
	accuracy_policy_4: 0.66719
	loss_value_4: 0.08381
	loss_reward_4: 0.01669
	loss_policy_5: 0.05753
	accuracy_policy_5: 0.66844
	loss_value_5: 0.08525
	loss_reward_5: 0.01657
	loss_policy: 0.56839
	loss_value: 0.79181
	loss_reward: 0.06738
[2025-05-07 15:18:45] nn step 20000, lr: 0.1.
	loss_policy_0: 0.26424
	accuracy_policy_0: 0.68254
	loss_value_0: 0.36742
	loss_policy_1: 0.05373
	accuracy_policy_1: 0.66699
	loss_value_1: 0.07607
	loss_reward_1: 0.00779
	loss_policy_2: 0.05409
	accuracy_policy_2: 0.6668
	loss_value_2: 0.07797
	loss_reward_2: 0.01112
	loss_policy_3: 0.05441
	accuracy_policy_3: 0.6698
	loss_value_3: 0.07963
	loss_reward_3: 0.01371
	loss_policy_4: 0.05478
	accuracy_policy_4: 0.67188
	loss_value_4: 0.08122
	loss_reward_4: 0.01623
	loss_policy_5: 0.05449
	accuracy_policy_5: 0.67711
	loss_value_5: 0.0825
	loss_reward_5: 0.01587
	loss_policy: 0.53573
	loss_value: 0.76482
	loss_reward: 0.06473
Optimization_Done 20000
[2025-05-07 15:21:46] [command] train weight_iter_20000.pkl 82 101
[2025-05-07 15:21:55] nn step 20050, lr: 0.1.
	loss_policy_0: 0.26772
	accuracy_policy_0: 0.66684
	loss_value_0: 0.36745
	loss_policy_1: 0.054
	accuracy_policy_1: 0.6566
	loss_value_1: 0.07557
	loss_reward_1: 0.00733
	loss_policy_2: 0.05436
	accuracy_policy_2: 0.65543
	loss_value_2: 0.07729
	loss_reward_2: 0.01176
	loss_policy_3: 0.05453
	accuracy_policy_3: 0.65934
	loss_value_3: 0.07882
	loss_reward_3: 0.01327
	loss_policy_4: 0.05425
	accuracy_policy_4: 0.66262
	loss_value_4: 0.08006
	loss_reward_4: 0.01567
	loss_policy_5: 0.05443
	accuracy_policy_5: 0.66605
	loss_value_5: 0.08094
	loss_reward_5: 0.01588
	loss_policy: 0.53928
	loss_value: 0.76014
	loss_reward: 0.06391
[2025-05-07 15:22:03] nn step 20100, lr: 0.1.
	loss_policy_0: 0.29478
	accuracy_policy_0: 0.66871
	loss_value_0: 0.40198
	loss_policy_1: 0.05945
	accuracy_policy_1: 0.65977
	loss_value_1: 0.08261
	loss_reward_1: 0.00901
	loss_policy_2: 0.06006
	accuracy_policy_2: 0.65723
	loss_value_2: 0.0848
	loss_reward_2: 0.01274
	loss_policy_3: 0.05971
	accuracy_policy_3: 0.65949
	loss_value_3: 0.08667
	loss_reward_3: 0.01476
	loss_policy_4: 0.06008
	accuracy_policy_4: 0.66082
	loss_value_4: 0.08845
	loss_reward_4: 0.01778
	loss_policy_5: 0.06039
	accuracy_policy_5: 0.66316
	loss_value_5: 0.09001
	loss_reward_5: 0.01761
	loss_policy: 0.59447
	loss_value: 0.83452
	loss_reward: 0.07191
[2025-05-07 15:22:10] nn step 20150, lr: 0.1.
	loss_policy_0: 0.25596
	accuracy_policy_0: 0.67297
	loss_value_0: 0.34997
	loss_policy_1: 0.05217
	accuracy_policy_1: 0.66746
	loss_value_1: 0.07234
	loss_reward_1: 0.00716
	loss_policy_2: 0.05259
	accuracy_policy_2: 0.65652
	loss_value_2: 0.0745
	loss_reward_2: 0.0113
	loss_policy_3: 0.05245
	accuracy_policy_3: 0.66094
	loss_value_3: 0.07617
	loss_reward_3: 0.01265
	loss_policy_4: 0.05246
	accuracy_policy_4: 0.6634
	loss_value_4: 0.07752
	loss_reward_4: 0.01514
	loss_policy_5: 0.05234
	accuracy_policy_5: 0.66605
	loss_value_5: 0.07883
	loss_reward_5: 0.01539
	loss_policy: 0.51798
	loss_value: 0.72932
	loss_reward: 0.06165
[2025-05-07 15:22:17] nn step 20200, lr: 0.1.
	loss_policy_0: 0.29012
	accuracy_policy_0: 0.67039
	loss_value_0: 0.39704
	loss_policy_1: 0.0584
	accuracy_policy_1: 0.66078
	loss_value_1: 0.08142
	loss_reward_1: 0.00823
	loss_policy_2: 0.05829
	accuracy_policy_2: 0.66652
	loss_value_2: 0.08355
	loss_reward_2: 0.01235
	loss_policy_3: 0.05883
	accuracy_policy_3: 0.65996
	loss_value_3: 0.08571
	loss_reward_3: 0.01476
	loss_policy_4: 0.0591
	accuracy_policy_4: 0.66262
	loss_value_4: 0.08722
	loss_reward_4: 0.01724
	loss_policy_5: 0.05885
	accuracy_policy_5: 0.66707
	loss_value_5: 0.08887
	loss_reward_5: 0.01836
	loss_policy: 0.58359
	loss_value: 0.82381
	loss_reward: 0.07094
Optimization_Done 20200
[2025-05-07 15:25:31] [command] train weight_iter_20200.pkl 83 102
[2025-05-07 15:25:39] nn step 20250, lr: 0.1.
	loss_policy_0: 0.25488
	accuracy_policy_0: 0.66895
	loss_value_0: 0.35416
	loss_policy_1: 0.05127
	accuracy_policy_1: 0.66242
	loss_value_1: 0.07239
	loss_reward_1: 0.00708
	loss_policy_2: 0.05155
	accuracy_policy_2: 0.66492
	loss_value_2: 0.07402
	loss_reward_2: 0.01061
	loss_policy_3: 0.05165
	accuracy_policy_3: 0.66207
	loss_value_3: 0.07533
	loss_reward_3: 0.0128
	loss_policy_4: 0.05171
	accuracy_policy_4: 0.66691
	loss_value_4: 0.07694
	loss_reward_4: 0.01508
	loss_policy_5: 0.05146
	accuracy_policy_5: 0.66969
	loss_value_5: 0.07783
	loss_reward_5: 0.01537
	loss_policy: 0.51252
	loss_value: 0.73068
	loss_reward: 0.06094
[2025-05-07 15:25:47] nn step 20300, lr: 0.1.
	loss_policy_0: 0.27819
	accuracy_policy_0: 0.67039
	loss_value_0: 0.38162
	loss_policy_1: 0.05578
	accuracy_policy_1: 0.66895
	loss_value_1: 0.07841
	loss_reward_1: 0.00782
	loss_policy_2: 0.05612
	accuracy_policy_2: 0.66555
	loss_value_2: 0.08056
	loss_reward_2: 0.0115
	loss_policy_3: 0.05639
	accuracy_policy_3: 0.66051
	loss_value_3: 0.08245
	loss_reward_3: 0.01373
	loss_policy_4: 0.05648
	accuracy_policy_4: 0.6627
	loss_value_4: 0.08431
	loss_reward_4: 0.01646
	loss_policy_5: 0.05651
	accuracy_policy_5: 0.66648
	loss_value_5: 0.08588
	loss_reward_5: 0.01631
	loss_policy: 0.55945
	loss_value: 0.79323
	loss_reward: 0.06582
[2025-05-07 15:25:55] nn step 20350, lr: 0.1.
	loss_policy_0: 0.27374
	accuracy_policy_0: 0.66812
	loss_value_0: 0.37146
	loss_policy_1: 0.055
	accuracy_policy_1: 0.66211
	loss_value_1: 0.07642
	loss_reward_1: 0.00744
	loss_policy_2: 0.05491
	accuracy_policy_2: 0.66469
	loss_value_2: 0.07834
	loss_reward_2: 0.01132
	loss_policy_3: 0.05517
	accuracy_policy_3: 0.66668
	loss_value_3: 0.08026
	loss_reward_3: 0.01375
	loss_policy_4: 0.05511
	accuracy_policy_4: 0.66461
	loss_value_4: 0.08179
	loss_reward_4: 0.01636
	loss_policy_5: 0.05524
	accuracy_policy_5: 0.67438
	loss_value_5: 0.0832
	loss_reward_5: 0.01615
	loss_policy: 0.54919
	loss_value: 0.77146
	loss_reward: 0.06501
[2025-05-07 15:26:01] nn step 20400, lr: 0.1.
	loss_policy_0: 0.27845
	accuracy_policy_0: 0.6675
	loss_value_0: 0.37384
	loss_policy_1: 0.05596
	accuracy_policy_1: 0.66719
	loss_value_1: 0.07687
	loss_reward_1: 0.00789
	loss_policy_2: 0.05606
	accuracy_policy_2: 0.66168
	loss_value_2: 0.07886
	loss_reward_2: 0.01141
	loss_policy_3: 0.05604
	accuracy_policy_3: 0.66207
	loss_value_3: 0.08063
	loss_reward_3: 0.01386
	loss_policy_4: 0.05597
	accuracy_policy_4: 0.6641
	loss_value_4: 0.08213
	loss_reward_4: 0.01671
	loss_policy_5: 0.05596
	accuracy_policy_5: 0.66855
	loss_value_5: 0.08356
	loss_reward_5: 0.01632
	loss_policy: 0.55844
	loss_value: 0.77589
	loss_reward: 0.06619
Optimization_Done 20400
[2025-05-07 15:29:13] [command] train weight_iter_20400.pkl 84 103
[2025-05-07 15:29:21] nn step 20450, lr: 0.1.
	loss_policy_0: 0.26189
	accuracy_policy_0: 0.67121
	loss_value_0: 0.37139
	loss_policy_1: 0.05342
	accuracy_policy_1: 0.65871
	loss_value_1: 0.07567
	loss_reward_1: 0.00723
	loss_policy_2: 0.05329
	accuracy_policy_2: 0.66258
	loss_value_2: 0.07758
	loss_reward_2: 0.01133
	loss_policy_3: 0.05398
	accuracy_policy_3: 0.65352
	loss_value_3: 0.07921
	loss_reward_3: 0.01312
	loss_policy_4: 0.05358
	accuracy_policy_4: 0.65969
	loss_value_4: 0.08045
	loss_reward_4: 0.01507
	loss_policy_5: 0.05379
	accuracy_policy_5: 0.66352
	loss_value_5: 0.08173
	loss_reward_5: 0.01543
	loss_policy: 0.52995
	loss_value: 0.76602
	loss_reward: 0.06218
[2025-05-07 15:29:28] nn step 20500, lr: 0.1.
	loss_policy_0: 0.26345
	accuracy_policy_0: 0.67477
	loss_value_0: 0.36743
	loss_policy_1: 0.05318
	accuracy_policy_1: 0.66184
	loss_value_1: 0.07564
	loss_reward_1: 0.00745
	loss_policy_2: 0.05347
	accuracy_policy_2: 0.66391
	loss_value_2: 0.07756
	loss_reward_2: 0.01102
	loss_policy_3: 0.05387
	accuracy_policy_3: 0.65875
	loss_value_3: 0.07936
	loss_reward_3: 0.01324
	loss_policy_4: 0.05374
	accuracy_policy_4: 0.66609
	loss_value_4: 0.08084
	loss_reward_4: 0.01565
	loss_policy_5: 0.05379
	accuracy_policy_5: 0.66973
	loss_value_5: 0.08208
	loss_reward_5: 0.01584
	loss_policy: 0.5315
	loss_value: 0.76291
	loss_reward: 0.0632
[2025-05-07 15:29:36] nn step 20550, lr: 0.1.
	loss_policy_0: 0.27857
	accuracy_policy_0: 0.67488
	loss_value_0: 0.38383
	loss_policy_1: 0.05604
	accuracy_policy_1: 0.66152
	loss_value_1: 0.07878
	loss_reward_1: 0.00812
	loss_policy_2: 0.05628
	accuracy_policy_2: 0.66324
	loss_value_2: 0.08056
	loss_reward_2: 0.01174
	loss_policy_3: 0.05698
	accuracy_policy_3: 0.65543
	loss_value_3: 0.08249
	loss_reward_3: 0.01406
	loss_policy_4: 0.05697
	accuracy_policy_4: 0.66043
	loss_value_4: 0.0843
	loss_reward_4: 0.01606
	loss_policy_5: 0.05675
	accuracy_policy_5: 0.66773
	loss_value_5: 0.08576
	loss_reward_5: 0.0166
	loss_policy: 0.5616
	loss_value: 0.79572
	loss_reward: 0.06659
[2025-05-07 15:29:44] nn step 20600, lr: 0.1.
	loss_policy_0: 0.27835
	accuracy_policy_0: 0.66887
	loss_value_0: 0.38134
	loss_policy_1: 0.05631
	accuracy_policy_1: 0.65902
	loss_value_1: 0.0783
	loss_reward_1: 0.00815
	loss_policy_2: 0.05633
	accuracy_policy_2: 0.65719
	loss_value_2: 0.08039
	loss_reward_2: 0.01182
	loss_policy_3: 0.05621
	accuracy_policy_3: 0.65965
	loss_value_3: 0.0824
	loss_reward_3: 0.01375
	loss_policy_4: 0.05685
	accuracy_policy_4: 0.65703
	loss_value_4: 0.0841
	loss_reward_4: 0.01651
	loss_policy_5: 0.05674
	accuracy_policy_5: 0.66699
	loss_value_5: 0.08537
	loss_reward_5: 0.01693
	loss_policy: 0.5608
	loss_value: 0.79191
	loss_reward: 0.06716
Optimization_Done 20600
[2025-05-07 15:32:48] [command] train weight_iter_20600.pkl 85 104
[2025-05-07 15:32:57] nn step 20650, lr: 0.1.
	loss_policy_0: 0.26704
	accuracy_policy_0: 0.66594
	loss_value_0: 0.37478
	loss_policy_1: 0.05398
	accuracy_policy_1: 0.65836
	loss_value_1: 0.07617
	loss_reward_1: 0.00765
	loss_policy_2: 0.05403
	accuracy_policy_2: 0.65953
	loss_value_2: 0.07819
	loss_reward_2: 0.01046
	loss_policy_3: 0.05456
	accuracy_policy_3: 0.66164
	loss_value_3: 0.07978
	loss_reward_3: 0.01346
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.65863
	loss_value_4: 0.08119
	loss_reward_4: 0.01586
	loss_policy_5: 0.05468
	accuracy_policy_5: 0.65906
	loss_value_5: 0.08257
	loss_reward_5: 0.01589
	loss_policy: 0.53876
	loss_value: 0.77268
	loss_reward: 0.06331
[2025-05-07 15:33:04] nn step 20700, lr: 0.1.
	loss_policy_0: 0.29444
	accuracy_policy_0: 0.66355
	loss_value_0: 0.40593
	loss_policy_1: 0.05966
	accuracy_policy_1: 0.66012
	loss_value_1: 0.08338
	loss_reward_1: 0.00883
	loss_policy_2: 0.05998
	accuracy_policy_2: 0.65289
	loss_value_2: 0.08509
	loss_reward_2: 0.01293
	loss_policy_3: 0.06013
	accuracy_policy_3: 0.65887
	loss_value_3: 0.08679
	loss_reward_3: 0.01495
	loss_policy_4: 0.06012
	accuracy_policy_4: 0.65695
	loss_value_4: 0.08857
	loss_reward_4: 0.01766
	loss_policy_5: 0.05989
	accuracy_policy_5: 0.66348
	loss_value_5: 0.0896
	loss_reward_5: 0.01804
	loss_policy: 0.59422
	loss_value: 0.83938
	loss_reward: 0.0724
[2025-05-07 15:33:12] nn step 20750, lr: 0.1.
	loss_policy_0: 0.27224
	accuracy_policy_0: 0.67121
	loss_value_0: 0.37012
	loss_policy_1: 0.05461
	accuracy_policy_1: 0.66277
	loss_value_1: 0.07616
	loss_reward_1: 0.00778
	loss_policy_2: 0.05508
	accuracy_policy_2: 0.66145
	loss_value_2: 0.07842
	loss_reward_2: 0.01139
	loss_policy_3: 0.05543
	accuracy_policy_3: 0.66051
	loss_value_3: 0.08003
	loss_reward_3: 0.0137
	loss_policy_4: 0.05582
	accuracy_policy_4: 0.6582
	loss_value_4: 0.08189
	loss_reward_4: 0.01602
	loss_policy_5: 0.05562
	accuracy_policy_5: 0.66957
	loss_value_5: 0.08332
	loss_reward_5: 0.01655
	loss_policy: 0.54881
	loss_value: 0.76993
	loss_reward: 0.06544
[2025-05-07 15:33:20] nn step 20800, lr: 0.1.
	loss_policy_0: 0.27939
	accuracy_policy_0: 0.67387
	loss_value_0: 0.3801
	loss_policy_1: 0.05618
	accuracy_policy_1: 0.65973
	loss_value_1: 0.07789
	loss_reward_1: 0.00778
	loss_policy_2: 0.0565
	accuracy_policy_2: 0.66004
	loss_value_2: 0.07975
	loss_reward_2: 0.0116
	loss_policy_3: 0.05623
	accuracy_policy_3: 0.6675
	loss_value_3: 0.08183
	loss_reward_3: 0.01433
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.66438
	loss_value_4: 0.08339
	loss_reward_4: 0.01681
	loss_policy_5: 0.05671
	accuracy_policy_5: 0.66945
	loss_value_5: 0.08496
	loss_reward_5: 0.0165
	loss_policy: 0.56165
	loss_value: 0.78793
	loss_reward: 0.06703
Optimization_Done 20800
[2025-05-07 15:36:25] [command] train weight_iter_20800.pkl 86 105
[2025-05-07 15:36:33] nn step 20850, lr: 0.1.
	loss_policy_0: 0.29923
	accuracy_policy_0: 0.66133
	loss_value_0: 0.40943
	loss_policy_1: 0.06044
	accuracy_policy_1: 0.65234
	loss_value_1: 0.08394
	loss_reward_1: 0.00803
	loss_policy_2: 0.0609
	accuracy_policy_2: 0.65246
	loss_value_2: 0.08569
	loss_reward_2: 0.01233
	loss_policy_3: 0.06073
	accuracy_policy_3: 0.65617
	loss_value_3: 0.0875
	loss_reward_3: 0.01504
	loss_policy_4: 0.06104
	accuracy_policy_4: 0.65613
	loss_value_4: 0.08918
	loss_reward_4: 0.01773
	loss_policy_5: 0.06056
	accuracy_policy_5: 0.65762
	loss_value_5: 0.09064
	loss_reward_5: 0.01782
	loss_policy: 0.60291
	loss_value: 0.84638
	loss_reward: 0.07094
[2025-05-07 15:36:41] nn step 20900, lr: 0.1.
	loss_policy_0: 0.27968
	accuracy_policy_0: 0.66531
	loss_value_0: 0.38396
	loss_policy_1: 0.05622
	accuracy_policy_1: 0.65645
	loss_value_1: 0.07878
	loss_reward_1: 0.00794
	loss_policy_2: 0.05681
	accuracy_policy_2: 0.6532
	loss_value_2: 0.08066
	loss_reward_2: 0.01175
	loss_policy_3: 0.05684
	accuracy_policy_3: 0.65738
	loss_value_3: 0.08242
	loss_reward_3: 0.01416
	loss_policy_4: 0.05687
	accuracy_policy_4: 0.65926
	loss_value_4: 0.08362
	loss_reward_4: 0.01721
	loss_policy_5: 0.05705
	accuracy_policy_5: 0.66398
	loss_value_5: 0.08507
	loss_reward_5: 0.01663
	loss_policy: 0.56347
	loss_value: 0.79452
	loss_reward: 0.06769
[2025-05-07 15:36:50] nn step 20950, lr: 0.1.
	loss_policy_0: 0.28243
	accuracy_policy_0: 0.67176
	loss_value_0: 0.38103
	loss_policy_1: 0.05731
	accuracy_policy_1: 0.6625
	loss_value_1: 0.07853
	loss_reward_1: 0.00785
	loss_policy_2: 0.05719
	accuracy_policy_2: 0.6593
	loss_value_2: 0.08074
	loss_reward_2: 0.01156
	loss_policy_3: 0.05756
	accuracy_policy_3: 0.66078
	loss_value_3: 0.08221
	loss_reward_3: 0.01408
	loss_policy_4: 0.05771
	accuracy_policy_4: 0.6616
	loss_value_4: 0.08406
	loss_reward_4: 0.01652
	loss_policy_5: 0.05764
	accuracy_policy_5: 0.66715
	loss_value_5: 0.08534
	loss_reward_5: 0.01647
	loss_policy: 0.56984
	loss_value: 0.79191
	loss_reward: 0.06649
[2025-05-07 15:36:56] nn step 21000, lr: 0.1.
	loss_policy_0: 0.27623
	accuracy_policy_0: 0.66438
	loss_value_0: 0.37112
	loss_policy_1: 0.05539
	accuracy_policy_1: 0.65973
	loss_value_1: 0.07648
	loss_reward_1: 0.00804
	loss_policy_2: 0.05637
	accuracy_policy_2: 0.66078
	loss_value_2: 0.0786
	loss_reward_2: 0.01195
	loss_policy_3: 0.05589
	accuracy_policy_3: 0.66477
	loss_value_3: 0.08066
	loss_reward_3: 0.01385
	loss_policy_4: 0.05593
	accuracy_policy_4: 0.66672
	loss_value_4: 0.0827
	loss_reward_4: 0.01631
	loss_policy_5: 0.05603
	accuracy_policy_5: 0.66746
	loss_value_5: 0.08361
	loss_reward_5: 0.01664
	loss_policy: 0.55584
	loss_value: 0.77317
	loss_reward: 0.06678
Optimization_Done 21000
[2025-05-07 15:39:55] [command] train weight_iter_21000.pkl 87 106
[2025-05-07 15:40:04] nn step 21050, lr: 0.1.
	loss_policy_0: 0.27921
	accuracy_policy_0: 0.65012
	loss_value_0: 0.37503
	loss_policy_1: 0.05592
	accuracy_policy_1: 0.65035
	loss_value_1: 0.07683
	loss_reward_1: 0.00755
	loss_policy_2: 0.05614
	accuracy_policy_2: 0.64773
	loss_value_2: 0.07877
	loss_reward_2: 0.01114
	loss_policy_3: 0.05629
	accuracy_policy_3: 0.64535
	loss_value_3: 0.08076
	loss_reward_3: 0.01381
	loss_policy_4: 0.05636
	accuracy_policy_4: 0.65176
	loss_value_4: 0.08239
	loss_reward_4: 0.01617
	loss_policy_5: 0.05627
	accuracy_policy_5: 0.65512
	loss_value_5: 0.08334
	loss_reward_5: 0.01623
	loss_policy: 0.56019
	loss_value: 0.77711
	loss_reward: 0.0649
[2025-05-07 15:40:12] nn step 21100, lr: 0.1.
	loss_policy_0: 0.2764
	accuracy_policy_0: 0.66
	loss_value_0: 0.36951
	loss_policy_1: 0.05564
	accuracy_policy_1: 0.65316
	loss_value_1: 0.07612
	loss_reward_1: 0.00765
	loss_policy_2: 0.05621
	accuracy_policy_2: 0.64535
	loss_value_2: 0.07841
	loss_reward_2: 0.01102
	loss_policy_3: 0.05606
	accuracy_policy_3: 0.65129
	loss_value_3: 0.08008
	loss_reward_3: 0.01361
	loss_policy_4: 0.05615
	accuracy_policy_4: 0.65012
	loss_value_4: 0.08175
	loss_reward_4: 0.01571
	loss_policy_5: 0.05612
	accuracy_policy_5: 0.65887
	loss_value_5: 0.08312
	loss_reward_5: 0.0159
	loss_policy: 0.55659
	loss_value: 0.769
	loss_reward: 0.06388
[2025-05-07 15:40:18] nn step 21150, lr: 0.1.
	loss_policy_0: 0.28354
	accuracy_policy_0: 0.66629
	loss_value_0: 0.3775
	loss_policy_1: 0.05723
	accuracy_policy_1: 0.65121
	loss_value_1: 0.07799
	loss_reward_1: 0.00765
	loss_policy_2: 0.05687
	accuracy_policy_2: 0.65773
	loss_value_2: 0.07966
	loss_reward_2: 0.01114
	loss_policy_3: 0.05663
	accuracy_policy_3: 0.66332
	loss_value_3: 0.08154
	loss_reward_3: 0.01389
	loss_policy_4: 0.05694
	accuracy_policy_4: 0.65914
	loss_value_4: 0.08329
	loss_reward_4: 0.01633
	loss_policy_5: 0.05745
	accuracy_policy_5: 0.66203
	loss_value_5: 0.08464
	loss_reward_5: 0.01649
	loss_policy: 0.56865
	loss_value: 0.78463
	loss_reward: 0.06549
[2025-05-07 15:40:26] nn step 21200, lr: 0.1.
	loss_policy_0: 0.28683
	accuracy_policy_0: 0.66645
	loss_value_0: 0.38469
	loss_policy_1: 0.05833
	accuracy_policy_1: 0.64918
	loss_value_1: 0.07938
	loss_reward_1: 0.00843
	loss_policy_2: 0.05836
	accuracy_policy_2: 0.6523
	loss_value_2: 0.08151
	loss_reward_2: 0.012
	loss_policy_3: 0.05846
	accuracy_policy_3: 0.65488
	loss_value_3: 0.0835
	loss_reward_3: 0.01399
	loss_policy_4: 0.05868
	accuracy_policy_4: 0.65641
	loss_value_4: 0.08561
	loss_reward_4: 0.01707
	loss_policy_5: 0.05907
	accuracy_policy_5: 0.65922
	loss_value_5: 0.08702
	loss_reward_5: 0.0174
	loss_policy: 0.57973
	loss_value: 0.80171
	loss_reward: 0.06888
Optimization_Done 21200
[2025-05-07 15:43:32] [command] train weight_iter_21200.pkl 88 107
[2025-05-07 15:43:40] nn step 21250, lr: 0.1.
	loss_policy_0: 0.29489
	accuracy_policy_0: 0.66301
	loss_value_0: 0.40551
	loss_policy_1: 0.05974
	accuracy_policy_1: 0.65199
	loss_value_1: 0.08338
	loss_reward_1: 0.0082
	loss_policy_2: 0.06
	accuracy_policy_2: 0.65289
	loss_value_2: 0.08529
	loss_reward_2: 0.01236
	loss_policy_3: 0.06036
	accuracy_policy_3: 0.65426
	loss_value_3: 0.08718
	loss_reward_3: 0.01514
	loss_policy_4: 0.06046
	accuracy_policy_4: 0.65207
	loss_value_4: 0.08868
	loss_reward_4: 0.01756
	loss_policy_5: 0.06083
	accuracy_policy_5: 0.65434
	loss_value_5: 0.08993
	loss_reward_5: 0.01776
	loss_policy: 0.59628
	loss_value: 0.83996
	loss_reward: 0.07101
[2025-05-07 15:43:48] nn step 21300, lr: 0.1.
	loss_policy_0: 0.28874
	accuracy_policy_0: 0.66289
	loss_value_0: 0.38921
	loss_policy_1: 0.05798
	accuracy_policy_1: 0.65223
	loss_value_1: 0.07971
	loss_reward_1: 0.00812
	loss_policy_2: 0.05844
	accuracy_policy_2: 0.65094
	loss_value_2: 0.0817
	loss_reward_2: 0.0115
	loss_policy_3: 0.05843
	accuracy_policy_3: 0.6566
	loss_value_3: 0.08367
	loss_reward_3: 0.01417
	loss_policy_4: 0.05831
	accuracy_policy_4: 0.65461
	loss_value_4: 0.08488
	loss_reward_4: 0.01678
	loss_policy_5: 0.05847
	accuracy_policy_5: 0.66477
	loss_value_5: 0.08599
	loss_reward_5: 0.01703
	loss_policy: 0.58037
	loss_value: 0.80516
	loss_reward: 0.06759
[2025-05-07 15:43:56] nn step 21350, lr: 0.1.
	loss_policy_0: 0.2996
	accuracy_policy_0: 0.66043
	loss_value_0: 0.39838
	loss_policy_1: 0.06041
	accuracy_policy_1: 0.64949
	loss_value_1: 0.08232
	loss_reward_1: 0.00848
	loss_policy_2: 0.06097
	accuracy_policy_2: 0.64855
	loss_value_2: 0.08457
	loss_reward_2: 0.01201
	loss_policy_3: 0.06082
	accuracy_policy_3: 0.65195
	loss_value_3: 0.08651
	loss_reward_3: 0.01471
	loss_policy_4: 0.06085
	accuracy_policy_4: 0.6552
	loss_value_4: 0.08775
	loss_reward_4: 0.01777
	loss_policy_5: 0.06096
	accuracy_policy_5: 0.65918
	loss_value_5: 0.08932
	loss_reward_5: 0.01752
	loss_policy: 0.6036
	loss_value: 0.82885
	loss_reward: 0.07049
[2025-05-07 15:44:04] nn step 21400, lr: 0.1.
	loss_policy_0: 0.29057
	accuracy_policy_0: 0.66543
	loss_value_0: 0.39207
	loss_policy_1: 0.05879
	accuracy_policy_1: 0.65133
	loss_value_1: 0.08073
	loss_reward_1: 0.00822
	loss_policy_2: 0.05912
	accuracy_policy_2: 0.65238
	loss_value_2: 0.08275
	loss_reward_2: 0.01152
	loss_policy_3: 0.05938
	accuracy_policy_3: 0.6543
	loss_value_3: 0.08503
	loss_reward_3: 0.01449
	loss_policy_4: 0.05968
	accuracy_policy_4: 0.65273
	loss_value_4: 0.08669
	loss_reward_4: 0.017
	loss_policy_5: 0.05981
	accuracy_policy_5: 0.65738
	loss_value_5: 0.08804
	loss_reward_5: 0.01749
	loss_policy: 0.58735
	loss_value: 0.81531
	loss_reward: 0.06872
Optimization_Done 21400
[2025-05-07 15:47:17] [command] train weight_iter_21400.pkl 89 108
[2025-05-07 15:47:24] nn step 21450, lr: 0.1.
	loss_policy_0: 0.29746
	accuracy_policy_0: 0.65363
	loss_value_0: 0.39797
	loss_policy_1: 0.06009
	accuracy_policy_1: 0.64711
	loss_value_1: 0.08178
	loss_reward_1: 0.00813
	loss_policy_2: 0.0601
	accuracy_policy_2: 0.64758
	loss_value_2: 0.08386
	loss_reward_2: 0.01184
	loss_policy_3: 0.06005
	accuracy_policy_3: 0.64965
	loss_value_3: 0.08539
	loss_reward_3: 0.01481
	loss_policy_4: 0.06095
	accuracy_policy_4: 0.65152
	loss_value_4: 0.08726
	loss_reward_4: 0.01727
	loss_policy_5: 0.06096
	accuracy_policy_5: 0.64746
	loss_value_5: 0.08842
	loss_reward_5: 0.01763
	loss_policy: 0.59962
	loss_value: 0.82467
	loss_reward: 0.06967
[2025-05-07 15:47:32] nn step 21500, lr: 0.1.
	loss_policy_0: 0.28638
	accuracy_policy_0: 0.65895
	loss_value_0: 0.38035
	loss_policy_1: 0.05792
	accuracy_policy_1: 0.64891
	loss_value_1: 0.07797
	loss_reward_1: 0.0078
	loss_policy_2: 0.05834
	accuracy_policy_2: 0.64941
	loss_value_2: 0.07978
	loss_reward_2: 0.01142
	loss_policy_3: 0.05863
	accuracy_policy_3: 0.64492
	loss_value_3: 0.08205
	loss_reward_3: 0.01415
	loss_policy_4: 0.0585
	accuracy_policy_4: 0.6482
	loss_value_4: 0.08336
	loss_reward_4: 0.0167
	loss_policy_5: 0.0583
	accuracy_policy_5: 0.65301
	loss_value_5: 0.0845
	loss_reward_5: 0.01721
	loss_policy: 0.57807
	loss_value: 0.78801
	loss_reward: 0.06728
[2025-05-07 15:47:41] nn step 21550, lr: 0.1.
	loss_policy_0: 0.27547
	accuracy_policy_0: 0.6584
	loss_value_0: 0.36445
	loss_policy_1: 0.05565
	accuracy_policy_1: 0.64781
	loss_value_1: 0.0751
	loss_reward_1: 0.00723
	loss_policy_2: 0.05587
	accuracy_policy_2: 0.64758
	loss_value_2: 0.07719
	loss_reward_2: 0.01084
	loss_policy_3: 0.05592
	accuracy_policy_3: 0.64789
	loss_value_3: 0.07891
	loss_reward_3: 0.01295
	loss_policy_4: 0.05624
	accuracy_policy_4: 0.64523
	loss_value_4: 0.08017
	loss_reward_4: 0.01577
	loss_policy_5: 0.0562
	accuracy_policy_5: 0.65332
	loss_value_5: 0.08153
	loss_reward_5: 0.01622
	loss_policy: 0.55535
	loss_value: 0.75735
	loss_reward: 0.06301
[2025-05-07 15:47:49] nn step 21600, lr: 0.1.
	loss_policy_0: 0.2982
	accuracy_policy_0: 0.66156
	loss_value_0: 0.39391
	loss_policy_1: 0.06027
	accuracy_policy_1: 0.64516
	loss_value_1: 0.08123
	loss_reward_1: 0.00828
	loss_policy_2: 0.06048
	accuracy_policy_2: 0.65574
	loss_value_2: 0.08379
	loss_reward_2: 0.01159
	loss_policy_3: 0.06098
	accuracy_policy_3: 0.65023
	loss_value_3: 0.08582
	loss_reward_3: 0.01481
	loss_policy_4: 0.06106
	accuracy_policy_4: 0.65066
	loss_value_4: 0.08756
	loss_reward_4: 0.01785
	loss_policy_5: 0.06105
	accuracy_policy_5: 0.65664
	loss_value_5: 0.08873
	loss_reward_5: 0.018
	loss_policy: 0.60204
	loss_value: 0.82104
	loss_reward: 0.07053
Optimization_Done 21600
[2025-05-07 15:50:56] [command] train weight_iter_21600.pkl 90 109
[2025-05-07 15:51:05] nn step 21650, lr: 0.1.
	loss_policy_0: 0.3023
	accuracy_policy_0: 0.65543
	loss_value_0: 0.4086
	loss_policy_1: 0.06104
	accuracy_policy_1: 0.63863
	loss_value_1: 0.08349
	loss_reward_1: 0.00853
	loss_policy_2: 0.06084
	accuracy_policy_2: 0.64414
	loss_value_2: 0.08588
	loss_reward_2: 0.01221
	loss_policy_3: 0.06134
	accuracy_policy_3: 0.64664
	loss_value_3: 0.08805
	loss_reward_3: 0.01523
	loss_policy_4: 0.06155
	accuracy_policy_4: 0.65035
	loss_value_4: 0.08964
	loss_reward_4: 0.01796
	loss_policy_5: 0.06167
	accuracy_policy_5: 0.65207
	loss_value_5: 0.09085
	loss_reward_5: 0.01801
	loss_policy: 0.60874
	loss_value: 0.84651
	loss_reward: 0.07194
[2025-05-07 15:51:11] nn step 21700, lr: 0.1.
	loss_policy_0: 0.28778
	accuracy_policy_0: 0.64484
	loss_value_0: 0.37571
	loss_policy_1: 0.05756
	accuracy_policy_1: 0.64113
	loss_value_1: 0.07739
	loss_reward_1: 0.00765
	loss_policy_2: 0.05767
	accuracy_policy_2: 0.64141
	loss_value_2: 0.07934
	loss_reward_2: 0.01133
	loss_policy_3: 0.05749
	accuracy_policy_3: 0.6425
	loss_value_3: 0.0814
	loss_reward_3: 0.01332
	loss_policy_4: 0.05782
	accuracy_policy_4: 0.64715
	loss_value_4: 0.08294
	loss_reward_4: 0.01607
	loss_policy_5: 0.0579
	accuracy_policy_5: 0.64836
	loss_value_5: 0.08403
	loss_reward_5: 0.01654
	loss_policy: 0.57621
	loss_value: 0.78082
	loss_reward: 0.0649
[2025-05-07 15:51:19] nn step 21750, lr: 0.1.
	loss_policy_0: 0.28318
	accuracy_policy_0: 0.65203
	loss_value_0: 0.37337
	loss_policy_1: 0.05719
	accuracy_policy_1: 0.63719
	loss_value_1: 0.0765
	loss_reward_1: 0.00785
	loss_policy_2: 0.05667
	accuracy_policy_2: 0.64164
	loss_value_2: 0.07862
	loss_reward_2: 0.01175
	loss_policy_3: 0.05707
	accuracy_policy_3: 0.64234
	loss_value_3: 0.08011
	loss_reward_3: 0.01402
	loss_policy_4: 0.05705
	accuracy_policy_4: 0.6475
	loss_value_4: 0.08162
	loss_reward_4: 0.01627
	loss_policy_5: 0.05729
	accuracy_policy_5: 0.65281
	loss_value_5: 0.08301
	loss_reward_5: 0.01664
	loss_policy: 0.56845
	loss_value: 0.77323
	loss_reward: 0.06654
[2025-05-07 15:51:27] nn step 21800, lr: 0.1.
	loss_policy_0: 0.28341
	accuracy_policy_0: 0.64605
	loss_value_0: 0.36666
	loss_policy_1: 0.05718
	accuracy_policy_1: 0.64059
	loss_value_1: 0.07551
	loss_reward_1: 0.00765
	loss_policy_2: 0.05708
	accuracy_policy_2: 0.64574
	loss_value_2: 0.0776
	loss_reward_2: 0.01109
	loss_policy_3: 0.05692
	accuracy_policy_3: 0.64758
	loss_value_3: 0.07918
	loss_reward_3: 0.01352
	loss_policy_4: 0.05767
	accuracy_policy_4: 0.6466
	loss_value_4: 0.08096
	loss_reward_4: 0.01589
	loss_policy_5: 0.05745
	accuracy_policy_5: 0.64727
	loss_value_5: 0.08251
	loss_reward_5: 0.01624
	loss_policy: 0.56971
	loss_value: 0.76243
	loss_reward: 0.06438
Optimization_Done 21800
[2025-05-07 15:54:29] [command] train weight_iter_21800.pkl 91 110
[2025-05-07 15:54:36] nn step 21850, lr: 0.1.
	loss_policy_0: 0.28785
	accuracy_policy_0: 0.65031
	loss_value_0: 0.38381
	loss_policy_1: 0.05819
	accuracy_policy_1: 0.63848
	loss_value_1: 0.0784
	loss_reward_1: 0.00764
	loss_policy_2: 0.05817
	accuracy_policy_2: 0.64055
	loss_value_2: 0.08052
	loss_reward_2: 0.01163
	loss_policy_3: 0.05856
	accuracy_policy_3: 0.64188
	loss_value_3: 0.08249
	loss_reward_3: 0.01404
	loss_policy_4: 0.05869
	accuracy_policy_4: 0.64148
	loss_value_4: 0.08399
	loss_reward_4: 0.0164
	loss_policy_5: 0.05825
	accuracy_policy_5: 0.65266
	loss_value_5: 0.08505
	loss_reward_5: 0.01658
	loss_policy: 0.57971
	loss_value: 0.79425
	loss_reward: 0.06629
[2025-05-07 15:54:44] nn step 21900, lr: 0.1.
	loss_policy_0: 0.28975
	accuracy_policy_0: 0.64457
	loss_value_0: 0.37988
	loss_policy_1: 0.05801
	accuracy_policy_1: 0.63996
	loss_value_1: 0.07786
	loss_reward_1: 0.00773
	loss_policy_2: 0.05783
	accuracy_policy_2: 0.64098
	loss_value_2: 0.08009
	loss_reward_2: 0.01169
	loss_policy_3: 0.05775
	accuracy_policy_3: 0.64332
	loss_value_3: 0.08199
	loss_reward_3: 0.01413
	loss_policy_4: 0.05817
	accuracy_policy_4: 0.64535
	loss_value_4: 0.08336
	loss_reward_4: 0.01687
	loss_policy_5: 0.05813
	accuracy_policy_5: 0.65008
	loss_value_5: 0.08452
	loss_reward_5: 0.01699
	loss_policy: 0.57965
	loss_value: 0.7877
	loss_reward: 0.06741
[2025-05-07 15:54:52] nn step 21950, lr: 0.1.
	loss_policy_0: 0.291
	accuracy_policy_0: 0.65332
	loss_value_0: 0.38184
	loss_policy_1: 0.05889
	accuracy_policy_1: 0.63832
	loss_value_1: 0.07836
	loss_reward_1: 0.00807
	loss_policy_2: 0.05887
	accuracy_policy_2: 0.6427
	loss_value_2: 0.08031
	loss_reward_2: 0.01159
	loss_policy_3: 0.05884
	accuracy_policy_3: 0.64609
	loss_value_3: 0.08211
	loss_reward_3: 0.01445
	loss_policy_4: 0.05909
	accuracy_policy_4: 0.64484
	loss_value_4: 0.08396
	loss_reward_4: 0.01681
	loss_policy_5: 0.05851
	accuracy_policy_5: 0.6559
	loss_value_5: 0.08532
	loss_reward_5: 0.01707
	loss_policy: 0.58519
	loss_value: 0.7919
	loss_reward: 0.068
[2025-05-07 15:54:59] nn step 22000, lr: 0.1.
	loss_policy_0: 0.27559
	accuracy_policy_0: 0.64395
	loss_value_0: 0.36087
	loss_policy_1: 0.0555
	accuracy_policy_1: 0.63582
	loss_value_1: 0.07423
	loss_reward_1: 0.00758
	loss_policy_2: 0.05567
	accuracy_policy_2: 0.64074
	loss_value_2: 0.07614
	loss_reward_2: 0.01142
	loss_policy_3: 0.0561
	accuracy_policy_3: 0.63613
	loss_value_3: 0.07784
	loss_reward_3: 0.01378
	loss_policy_4: 0.05598
	accuracy_policy_4: 0.64273
	loss_value_4: 0.07926
	loss_reward_4: 0.01581
	loss_policy_5: 0.05617
	accuracy_policy_5: 0.64324
	loss_value_5: 0.08067
	loss_reward_5: 0.01639
	loss_policy: 0.555
	loss_value: 0.74901
	loss_reward: 0.06498
Optimization_Done 22000
[2025-05-07 15:58:10] [command] train weight_iter_22000.pkl 92 111
[2025-05-07 15:58:17] nn step 22050, lr: 0.1.
	loss_policy_0: 0.2728
	accuracy_policy_0: 0.64621
	loss_value_0: 0.36933
	loss_policy_1: 0.05508
	accuracy_policy_1: 0.63988
	loss_value_1: 0.07556
	loss_reward_1: 0.00725
	loss_policy_2: 0.05535
	accuracy_policy_2: 0.63629
	loss_value_2: 0.0775
	loss_reward_2: 0.01093
	loss_policy_3: 0.05575
	accuracy_policy_3: 0.63941
	loss_value_3: 0.07927
	loss_reward_3: 0.01357
	loss_policy_4: 0.05563
	accuracy_policy_4: 0.64387
	loss_value_4: 0.08064
	loss_reward_4: 0.01609
	loss_policy_5: 0.05583
	accuracy_policy_5: 0.64918
	loss_value_5: 0.08205
	loss_reward_5: 0.01601
	loss_policy: 0.55044
	loss_value: 0.76435
	loss_reward: 0.06384
[2025-05-07 15:58:25] nn step 22100, lr: 0.1.
	loss_policy_0: 0.28504
	accuracy_policy_0: 0.64461
	loss_value_0: 0.3773
	loss_policy_1: 0.05765
	accuracy_policy_1: 0.63387
	loss_value_1: 0.0772
	loss_reward_1: 0.00764
	loss_policy_2: 0.05761
	accuracy_policy_2: 0.63961
	loss_value_2: 0.0793
	loss_reward_2: 0.01132
	loss_policy_3: 0.05762
	accuracy_policy_3: 0.64227
	loss_value_3: 0.08131
	loss_reward_3: 0.01391
	loss_policy_4: 0.05794
	accuracy_policy_4: 0.63785
	loss_value_4: 0.0828
	loss_reward_4: 0.01657
	loss_policy_5: 0.05793
	accuracy_policy_5: 0.64945
	loss_value_5: 0.08433
	loss_reward_5: 0.01641
	loss_policy: 0.5738
	loss_value: 0.78224
	loss_reward: 0.06586
[2025-05-07 15:58:33] nn step 22150, lr: 0.1.
	loss_policy_0: 0.28322
	accuracy_policy_0: 0.64961
	loss_value_0: 0.37343
	loss_policy_1: 0.05689
	accuracy_policy_1: 0.6373
	loss_value_1: 0.07653
	loss_reward_1: 0.00745
	loss_policy_2: 0.05718
	accuracy_policy_2: 0.64383
	loss_value_2: 0.07873
	loss_reward_2: 0.01126
	loss_policy_3: 0.05727
	accuracy_policy_3: 0.6384
	loss_value_3: 0.0803
	loss_reward_3: 0.01363
	loss_policy_4: 0.05771
	accuracy_policy_4: 0.64156
	loss_value_4: 0.08178
	loss_reward_4: 0.01617
	loss_policy_5: 0.05739
	accuracy_policy_5: 0.64789
	loss_value_5: 0.08323
	loss_reward_5: 0.01655
	loss_policy: 0.56964
	loss_value: 0.774
	loss_reward: 0.06506
[2025-05-07 15:58:41] nn step 22200, lr: 0.1.
	loss_policy_0: 0.26932
	accuracy_policy_0: 0.65395
	loss_value_0: 0.35357
	loss_policy_1: 0.05433
	accuracy_policy_1: 0.64215
	loss_value_1: 0.0728
	loss_reward_1: 0.00739
	loss_policy_2: 0.05452
	accuracy_policy_2: 0.64281
	loss_value_2: 0.0747
	loss_reward_2: 0.01083
	loss_policy_3: 0.05415
	accuracy_policy_3: 0.64766
	loss_value_3: 0.0761
	loss_reward_3: 0.01302
	loss_policy_4: 0.05476
	accuracy_policy_4: 0.64621
	loss_value_4: 0.07777
	loss_reward_4: 0.01546
	loss_policy_5: 0.05448
	accuracy_policy_5: 0.65117
	loss_value_5: 0.07924
	loss_reward_5: 0.0162
	loss_policy: 0.54155
	loss_value: 0.73419
	loss_reward: 0.0629
Optimization_Done 22200
[2025-05-07 16:01:53] [command] train weight_iter_22200.pkl 93 112
[2025-05-07 16:02:02] nn step 22250, lr: 0.1.
	loss_policy_0: 0.27197
	accuracy_policy_0: 0.65484
	loss_value_0: 0.3676
	loss_policy_1: 0.05496
	accuracy_policy_1: 0.64113
	loss_value_1: 0.07519
	loss_reward_1: 0.00704
	loss_policy_2: 0.05519
	accuracy_policy_2: 0.64375
	loss_value_2: 0.07704
	loss_reward_2: 0.0108
	loss_policy_3: 0.0552
	accuracy_policy_3: 0.64523
	loss_value_3: 0.07878
	loss_reward_3: 0.01323
	loss_policy_4: 0.05496
	accuracy_policy_4: 0.65383
	loss_value_4: 0.0801
	loss_reward_4: 0.01588
	loss_policy_5: 0.05503
	accuracy_policy_5: 0.65523
	loss_value_5: 0.08136
	loss_reward_5: 0.01574
	loss_policy: 0.54732
	loss_value: 0.76007
	loss_reward: 0.06268
[2025-05-07 16:02:08] nn step 22300, lr: 0.1.
	loss_policy_0: 0.2934
	accuracy_policy_0: 0.64852
	loss_value_0: 0.38503
	loss_policy_1: 0.05882
	accuracy_policy_1: 0.64473
	loss_value_1: 0.07899
	loss_reward_1: 0.00771
	loss_policy_2: 0.05878
	accuracy_policy_2: 0.64398
	loss_value_2: 0.08082
	loss_reward_2: 0.01167
	loss_policy_3: 0.05886
	accuracy_policy_3: 0.64801
	loss_value_3: 0.08268
	loss_reward_3: 0.01379
	loss_policy_4: 0.05919
	accuracy_policy_4: 0.64812
	loss_value_4: 0.08461
	loss_reward_4: 0.01647
	loss_policy_5: 0.05904
	accuracy_policy_5: 0.6559
	loss_value_5: 0.08585
	loss_reward_5: 0.017
	loss_policy: 0.5881
	loss_value: 0.79798
	loss_reward: 0.06663
[2025-05-07 16:02:17] nn step 22350, lr: 0.1.
	loss_policy_0: 0.30694
	accuracy_policy_0: 0.63891
	loss_value_0: 0.3996
	loss_policy_1: 0.06079
	accuracy_policy_1: 0.64309
	loss_value_1: 0.08191
	loss_reward_1: 0.00812
	loss_policy_2: 0.06116
	accuracy_policy_2: 0.64355
	loss_value_2: 0.08414
	loss_reward_2: 0.01225
	loss_policy_3: 0.06111
	accuracy_policy_3: 0.64641
	loss_value_3: 0.08605
	loss_reward_3: 0.01492
	loss_policy_4: 0.06132
	accuracy_policy_4: 0.64582
	loss_value_4: 0.08755
	loss_reward_4: 0.01695
	loss_policy_5: 0.06111
	accuracy_policy_5: 0.64949
	loss_value_5: 0.0893
	loss_reward_5: 0.01731
	loss_policy: 0.61243
	loss_value: 0.82855
	loss_reward: 0.06954
[2025-05-07 16:02:25] nn step 22400, lr: 0.1.
	loss_policy_0: 0.28958
	accuracy_policy_0: 0.64875
	loss_value_0: 0.3812
	loss_policy_1: 0.05837
	accuracy_policy_1: 0.64055
	loss_value_1: 0.07827
	loss_reward_1: 0.00775
	loss_policy_2: 0.05826
	accuracy_policy_2: 0.6398
	loss_value_2: 0.08013
	loss_reward_2: 0.01151
	loss_policy_3: 0.05854
	accuracy_policy_3: 0.6418
	loss_value_3: 0.08204
	loss_reward_3: 0.01377
	loss_policy_4: 0.05859
	accuracy_policy_4: 0.64934
	loss_value_4: 0.0837
	loss_reward_4: 0.01682
	loss_policy_5: 0.05898
	accuracy_policy_5: 0.64523
	loss_value_5: 0.0851
	loss_reward_5: 0.01697
	loss_policy: 0.58232
	loss_value: 0.79044
	loss_reward: 0.06682
Optimization_Done 22400
[2025-05-07 16:05:26] [command] train weight_iter_22400.pkl 94 113
[2025-05-07 16:05:34] nn step 22450, lr: 0.1.
	loss_policy_0: 0.28561
	accuracy_policy_0: 0.64496
	loss_value_0: 0.37736
	loss_policy_1: 0.0573
	accuracy_policy_1: 0.63633
	loss_value_1: 0.07738
	loss_reward_1: 0.00746
	loss_policy_2: 0.05706
	accuracy_policy_2: 0.63879
	loss_value_2: 0.07947
	loss_reward_2: 0.01128
	loss_policy_3: 0.05717
	accuracy_policy_3: 0.63816
	loss_value_3: 0.08131
	loss_reward_3: 0.01404
	loss_policy_4: 0.05764
	accuracy_policy_4: 0.64312
	loss_value_4: 0.08299
	loss_reward_4: 0.0157
	loss_policy_5: 0.05754
	accuracy_policy_5: 0.64746
	loss_value_5: 0.0845
	loss_reward_5: 0.01624
	loss_policy: 0.57232
	loss_value: 0.78302
	loss_reward: 0.06472
[2025-05-07 16:05:43] nn step 22500, lr: 0.1.
	loss_policy_0: 0.28634
	accuracy_policy_0: 0.65438
	loss_value_0: 0.38303
	loss_policy_1: 0.05794
	accuracy_policy_1: 0.63828
	loss_value_1: 0.07905
	loss_reward_1: 0.00755
	loss_policy_2: 0.05803
	accuracy_policy_2: 0.64098
	loss_value_2: 0.08097
	loss_reward_2: 0.01164
	loss_policy_3: 0.0582
	accuracy_policy_3: 0.64246
	loss_value_3: 0.08256
	loss_reward_3: 0.01446
	loss_policy_4: 0.05829
	accuracy_policy_4: 0.64363
	loss_value_4: 0.08414
	loss_reward_4: 0.0165
	loss_policy_5: 0.05818
	accuracy_policy_5: 0.64984
	loss_value_5: 0.08559
	loss_reward_5: 0.01623
	loss_policy: 0.57699
	loss_value: 0.79534
	loss_reward: 0.06638
[2025-05-07 16:05:51] nn step 22550, lr: 0.1.
	loss_policy_0: 0.2705
	accuracy_policy_0: 0.65066
	loss_value_0: 0.35626
	loss_policy_1: 0.05499
	accuracy_policy_1: 0.63969
	loss_value_1: 0.0732
	loss_reward_1: 0.00705
	loss_policy_2: 0.05504
	accuracy_policy_2: 0.63523
	loss_value_2: 0.07497
	loss_reward_2: 0.01106
	loss_policy_3: 0.05526
	accuracy_policy_3: 0.64133
	loss_value_3: 0.07706
	loss_reward_3: 0.01304
	loss_policy_4: 0.05549
	accuracy_policy_4: 0.63781
	loss_value_4: 0.07856
	loss_reward_4: 0.01564
	loss_policy_5: 0.05489
	accuracy_policy_5: 0.64793
	loss_value_5: 0.08017
	loss_reward_5: 0.01595
	loss_policy: 0.54618
	loss_value: 0.74021
	loss_reward: 0.06274
[2025-05-07 16:05:58] nn step 22600, lr: 0.1.
	loss_policy_0: 0.28841
	accuracy_policy_0: 0.6507
	loss_value_0: 0.38019
	loss_policy_1: 0.0584
	accuracy_policy_1: 0.64148
	loss_value_1: 0.07803
	loss_reward_1: 0.00822
	loss_policy_2: 0.05857
	accuracy_policy_2: 0.64156
	loss_value_2: 0.08024
	loss_reward_2: 0.01193
	loss_policy_3: 0.05863
	accuracy_policy_3: 0.64113
	loss_value_3: 0.08203
	loss_reward_3: 0.01492
	loss_policy_4: 0.05879
	accuracy_policy_4: 0.64316
	loss_value_4: 0.08393
	loss_reward_4: 0.01726
	loss_policy_5: 0.05879
	accuracy_policy_5: 0.6493
	loss_value_5: 0.0855
	loss_reward_5: 0.01709
	loss_policy: 0.5816
	loss_value: 0.78992
	loss_reward: 0.06943
Optimization_Done 22600
[2025-05-07 16:09:04] [command] train weight_iter_22600.pkl 95 114
[2025-05-07 16:09:13] nn step 22650, lr: 0.1.
	loss_policy_0: 0.27733
	accuracy_policy_0: 0.64449
	loss_value_0: 0.37038
	loss_policy_1: 0.05581
	accuracy_policy_1: 0.63711
	loss_value_1: 0.07605
	loss_reward_1: 0.00774
	loss_policy_2: 0.05606
	accuracy_policy_2: 0.6366
	loss_value_2: 0.07802
	loss_reward_2: 0.01113
	loss_policy_3: 0.05651
	accuracy_policy_3: 0.63617
	loss_value_3: 0.07971
	loss_reward_3: 0.01393
	loss_policy_4: 0.0565
	accuracy_policy_4: 0.63887
	loss_value_4: 0.08132
	loss_reward_4: 0.01643
	loss_policy_5: 0.05643
	accuracy_policy_5: 0.64699
	loss_value_5: 0.0829
	loss_reward_5: 0.01666
	loss_policy: 0.55864
	loss_value: 0.76838
	loss_reward: 0.06589
[2025-05-07 16:09:19] nn step 22700, lr: 0.1.
	loss_policy_0: 0.27916
	accuracy_policy_0: 0.64645
	loss_value_0: 0.36907
	loss_policy_1: 0.05628
	accuracy_policy_1: 0.63652
	loss_value_1: 0.07575
	loss_reward_1: 0.00758
	loss_policy_2: 0.05606
	accuracy_policy_2: 0.64266
	loss_value_2: 0.07758
	loss_reward_2: 0.01089
	loss_policy_3: 0.05633
	accuracy_policy_3: 0.64438
	loss_value_3: 0.07934
	loss_reward_3: 0.01365
	loss_policy_4: 0.05625
	accuracy_policy_4: 0.6441
	loss_value_4: 0.08116
	loss_reward_4: 0.01641
	loss_policy_5: 0.05637
	accuracy_policy_5: 0.64629
	loss_value_5: 0.08246
	loss_reward_5: 0.01678
	loss_policy: 0.56045
	loss_value: 0.76535
	loss_reward: 0.06531
[2025-05-07 16:09:27] nn step 22750, lr: 0.1.
	loss_policy_0: 0.28399
	accuracy_policy_0: 0.65164
	loss_value_0: 0.375
	loss_policy_1: 0.05729
	accuracy_policy_1: 0.64062
	loss_value_1: 0.07688
	loss_reward_1: 0.0078
	loss_policy_2: 0.05758
	accuracy_policy_2: 0.64246
	loss_value_2: 0.079
	loss_reward_2: 0.01172
	loss_policy_3: 0.05762
	accuracy_policy_3: 0.64512
	loss_value_3: 0.08058
	loss_reward_3: 0.01439
	loss_policy_4: 0.0578
	accuracy_policy_4: 0.64844
	loss_value_4: 0.08227
	loss_reward_4: 0.01692
	loss_policy_5: 0.05738
	accuracy_policy_5: 0.65812
	loss_value_5: 0.08336
	loss_reward_5: 0.01693
	loss_policy: 0.57166
	loss_value: 0.77708
	loss_reward: 0.06777
[2025-05-07 16:09:35] nn step 22800, lr: 0.1.
	loss_policy_0: 0.28596
	accuracy_policy_0: 0.64805
	loss_value_0: 0.3739
	loss_policy_1: 0.05702
	accuracy_policy_1: 0.64344
	loss_value_1: 0.07606
	loss_reward_1: 0.00774
	loss_policy_2: 0.0572
	accuracy_policy_2: 0.64594
	loss_value_2: 0.07844
	loss_reward_2: 0.01137
	loss_policy_3: 0.05754
	accuracy_policy_3: 0.64836
	loss_value_3: 0.08
	loss_reward_3: 0.01397
	loss_policy_4: 0.0575
	accuracy_policy_4: 0.64383
	loss_value_4: 0.08211
	loss_reward_4: 0.01656
	loss_policy_5: 0.05769
	accuracy_policy_5: 0.65223
	loss_value_5: 0.08336
	loss_reward_5: 0.0165
	loss_policy: 0.57291
	loss_value: 0.77387
	loss_reward: 0.06615
Optimization_Done 22800
[2025-05-07 16:12:53] [command] train weight_iter_22800.pkl 96 115
[2025-05-07 16:13:02] nn step 22850, lr: 0.1.
	loss_policy_0: 0.28684
	accuracy_policy_0: 0.63508
	loss_value_0: 0.37516
	loss_policy_1: 0.05781
	accuracy_policy_1: 0.62531
	loss_value_1: 0.07716
	loss_reward_1: 0.00748
	loss_policy_2: 0.05758
	accuracy_policy_2: 0.63512
	loss_value_2: 0.07952
	loss_reward_2: 0.01106
	loss_policy_3: 0.05774
	accuracy_policy_3: 0.62789
	loss_value_3: 0.08115
	loss_reward_3: 0.01372
	loss_policy_4: 0.05779
	accuracy_policy_4: 0.63559
	loss_value_4: 0.08258
	loss_reward_4: 0.01623
	loss_policy_5: 0.05833
	accuracy_policy_5: 0.63793
	loss_value_5: 0.0839
	loss_reward_5: 0.01643
	loss_policy: 0.57609
	loss_value: 0.77946
	loss_reward: 0.06492
[2025-05-07 16:13:09] nn step 22900, lr: 0.1.
	loss_policy_0: 0.3009
	accuracy_policy_0: 0.64016
	loss_value_0: 0.3861
	loss_policy_1: 0.06051
	accuracy_policy_1: 0.63055
	loss_value_1: 0.07942
	loss_reward_1: 0.00807
	loss_policy_2: 0.06067
	accuracy_policy_2: 0.63492
	loss_value_2: 0.08171
	loss_reward_2: 0.0121
	loss_policy_3: 0.06058
	accuracy_policy_3: 0.6325
	loss_value_3: 0.08345
	loss_reward_3: 0.01515
	loss_policy_4: 0.06035
	accuracy_policy_4: 0.64355
	loss_value_4: 0.08554
	loss_reward_4: 0.01752
	loss_policy_5: 0.06015
	accuracy_policy_5: 0.64629
	loss_value_5: 0.08677
	loss_reward_5: 0.01731
	loss_policy: 0.60316
	loss_value: 0.80298
	loss_reward: 0.07016
[2025-05-07 16:13:17] nn step 22950, lr: 0.1.
	loss_policy_0: 0.29798
	accuracy_policy_0: 0.64086
	loss_value_0: 0.38234
	loss_policy_1: 0.05996
	accuracy_policy_1: 0.6307
	loss_value_1: 0.07881
	loss_reward_1: 0.00777
	loss_policy_2: 0.06002
	accuracy_policy_2: 0.6352
	loss_value_2: 0.08128
	loss_reward_2: 0.01186
	loss_policy_3: 0.06016
	accuracy_policy_3: 0.63887
	loss_value_3: 0.08322
	loss_reward_3: 0.01456
	loss_policy_4: 0.05993
	accuracy_policy_4: 0.6407
	loss_value_4: 0.08463
	loss_reward_4: 0.01669
	loss_policy_5: 0.06023
	accuracy_policy_5: 0.64625
	loss_value_5: 0.0862
	loss_reward_5: 0.01728
	loss_policy: 0.59829
	loss_value: 0.79648
	loss_reward: 0.06816
[2025-05-07 16:13:23] nn step 23000, lr: 0.1.
	loss_policy_0: 0.28461
	accuracy_policy_0: 0.64449
	loss_value_0: 0.36365
	loss_policy_1: 0.05726
	accuracy_policy_1: 0.63406
	loss_value_1: 0.07501
	loss_reward_1: 0.00775
	loss_policy_2: 0.057
	accuracy_policy_2: 0.63992
	loss_value_2: 0.07713
	loss_reward_2: 0.01113
	loss_policy_3: 0.05727
	accuracy_policy_3: 0.64258
	loss_value_3: 0.07899
	loss_reward_3: 0.01371
	loss_policy_4: 0.05729
	accuracy_policy_4: 0.64012
	loss_value_4: 0.0809
	loss_reward_4: 0.01667
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.64812
	loss_value_5: 0.08231
	loss_reward_5: 0.01636
	loss_policy: 0.57064
	loss_value: 0.75799
	loss_reward: 0.06562
Optimization_Done 23000
[2025-05-07 16:16:54] [command] train weight_iter_23000.pkl 97 116
[2025-05-07 16:17:03] nn step 23050, lr: 0.1.
	loss_policy_0: 0.30151
	accuracy_policy_0: 0.63605
	loss_value_0: 0.39169
	loss_policy_1: 0.06093
	accuracy_policy_1: 0.62484
	loss_value_1: 0.0804
	loss_reward_1: 0.00786
	loss_policy_2: 0.0611
	accuracy_policy_2: 0.62762
	loss_value_2: 0.08278
	loss_reward_2: 0.01188
	loss_policy_3: 0.06124
	accuracy_policy_3: 0.62715
	loss_value_3: 0.08483
	loss_reward_3: 0.01428
	loss_policy_4: 0.0613
	accuracy_policy_4: 0.63098
	loss_value_4: 0.08669
	loss_reward_4: 0.01698
	loss_policy_5: 0.06125
	accuracy_policy_5: 0.6348
	loss_value_5: 0.08814
	loss_reward_5: 0.0174
	loss_policy: 0.60734
	loss_value: 0.81453
	loss_reward: 0.06841
[2025-05-07 16:17:11] nn step 23100, lr: 0.1.
	loss_policy_0: 0.28285
	accuracy_policy_0: 0.63992
	loss_value_0: 0.36712
	loss_policy_1: 0.05705
	accuracy_policy_1: 0.62195
	loss_value_1: 0.07543
	loss_reward_1: 0.00757
	loss_policy_2: 0.05716
	accuracy_policy_2: 0.62992
	loss_value_2: 0.07757
	loss_reward_2: 0.01099
	loss_policy_3: 0.05735
	accuracy_policy_3: 0.62836
	loss_value_3: 0.07938
	loss_reward_3: 0.01381
	loss_policy_4: 0.05721
	accuracy_policy_4: 0.63352
	loss_value_4: 0.08105
	loss_reward_4: 0.01652
	loss_policy_5: 0.05741
	accuracy_policy_5: 0.63336
	loss_value_5: 0.0826
	loss_reward_5: 0.01657
	loss_policy: 0.56904
	loss_value: 0.76315
	loss_reward: 0.06545
[2025-05-07 16:17:19] nn step 23150, lr: 0.1.
	loss_policy_0: 0.29537
	accuracy_policy_0: 0.63758
	loss_value_0: 0.3795
	loss_policy_1: 0.05963
	accuracy_policy_1: 0.6282
	loss_value_1: 0.07787
	loss_reward_1: 0.00764
	loss_policy_2: 0.05985
	accuracy_policy_2: 0.62828
	loss_value_2: 0.08004
	loss_reward_2: 0.01159
	loss_policy_3: 0.05966
	accuracy_policy_3: 0.63645
	loss_value_3: 0.08202
	loss_reward_3: 0.01414
	loss_policy_4: 0.05999
	accuracy_policy_4: 0.64285
	loss_value_4: 0.08372
	loss_reward_4: 0.01649
	loss_policy_5: 0.05982
	accuracy_policy_5: 0.63988
	loss_value_5: 0.08563
	loss_reward_5: 0.01687
	loss_policy: 0.59431
	loss_value: 0.78878
	loss_reward: 0.06672
[2025-05-07 16:17:26] nn step 23200, lr: 0.1.
	loss_policy_0: 0.29653
	accuracy_policy_0: 0.6402
	loss_value_0: 0.38024
	loss_policy_1: 0.05955
	accuracy_policy_1: 0.6298
	loss_value_1: 0.07815
	loss_reward_1: 0.00762
	loss_policy_2: 0.05976
	accuracy_policy_2: 0.63215
	loss_value_2: 0.08053
	loss_reward_2: 0.01117
	loss_policy_3: 0.0599
	accuracy_policy_3: 0.63477
	loss_value_3: 0.08218
	loss_reward_3: 0.01438
	loss_policy_4: 0.06024
	accuracy_policy_4: 0.63813
	loss_value_4: 0.08411
	loss_reward_4: 0.01699
	loss_policy_5: 0.0601
	accuracy_policy_5: 0.64156
	loss_value_5: 0.08579
	loss_reward_5: 0.0172
	loss_policy: 0.59607
	loss_value: 0.79099
	loss_reward: 0.06736
Optimization_Done 23200
[2025-05-07 16:20:46] [command] train weight_iter_23200.pkl 98 117
[2025-05-07 16:20:56] nn step 23250, lr: 0.1.
	loss_policy_0: 0.28934
	accuracy_policy_0: 0.63328
	loss_value_0: 0.375
	loss_policy_1: 0.05836
	accuracy_policy_1: 0.62621
	loss_value_1: 0.07697
	loss_reward_1: 0.00764
	loss_policy_2: 0.05838
	accuracy_policy_2: 0.62551
	loss_value_2: 0.07905
	loss_reward_2: 0.0112
	loss_policy_3: 0.05838
	accuracy_policy_3: 0.62629
	loss_value_3: 0.08094
	loss_reward_3: 0.01421
	loss_policy_4: 0.05856
	accuracy_policy_4: 0.63332
	loss_value_4: 0.0821
	loss_reward_4: 0.01688
	loss_policy_5: 0.05882
	accuracy_policy_5: 0.63211
	loss_value_5: 0.08402
	loss_reward_5: 0.01702
	loss_policy: 0.58183
	loss_value: 0.77808
	loss_reward: 0.06696
[2025-05-07 16:21:04] nn step 23300, lr: 0.1.
	loss_policy_0: 0.29134
	accuracy_policy_0: 0.63305
	loss_value_0: 0.37654
	loss_policy_1: 0.05848
	accuracy_policy_1: 0.62824
	loss_value_1: 0.07735
	loss_reward_1: 0.00801
	loss_policy_2: 0.05854
	accuracy_policy_2: 0.62965
	loss_value_2: 0.07954
	loss_reward_2: 0.01137
	loss_policy_3: 0.05927
	accuracy_policy_3: 0.62859
	loss_value_3: 0.08147
	loss_reward_3: 0.01407
	loss_policy_4: 0.05906
	accuracy_policy_4: 0.63562
	loss_value_4: 0.08292
	loss_reward_4: 0.0169
	loss_policy_5: 0.05858
	accuracy_policy_5: 0.64152
	loss_value_5: 0.08449
	loss_reward_5: 0.01697
	loss_policy: 0.58526
	loss_value: 0.7823
	loss_reward: 0.06731
[2025-05-07 16:21:10] nn step 23350, lr: 0.1.
	loss_policy_0: 0.29641
	accuracy_policy_0: 0.63598
	loss_value_0: 0.38724
	loss_policy_1: 0.05966
	accuracy_policy_1: 0.62727
	loss_value_1: 0.07941
	loss_reward_1: 0.00811
	loss_policy_2: 0.05985
	accuracy_policy_2: 0.63379
	loss_value_2: 0.08166
	loss_reward_2: 0.01213
	loss_policy_3: 0.05998
	accuracy_policy_3: 0.63109
	loss_value_3: 0.08373
	loss_reward_3: 0.01472
	loss_policy_4: 0.06014
	accuracy_policy_4: 0.63813
	loss_value_4: 0.08565
	loss_reward_4: 0.01709
	loss_policy_5: 0.06038
	accuracy_policy_5: 0.63762
	loss_value_5: 0.08721
	loss_reward_5: 0.0177
	loss_policy: 0.59642
	loss_value: 0.80489
	loss_reward: 0.06976
[2025-05-07 16:21:18] nn step 23400, lr: 0.1.
	loss_policy_0: 0.29271
	accuracy_policy_0: 0.6357
	loss_value_0: 0.37253
	loss_policy_1: 0.059
	accuracy_policy_1: 0.62969
	loss_value_1: 0.07667
	loss_reward_1: 0.00778
	loss_policy_2: 0.05929
	accuracy_policy_2: 0.63023
	loss_value_2: 0.07868
	loss_reward_2: 0.01192
	loss_policy_3: 0.05946
	accuracy_policy_3: 0.63121
	loss_value_3: 0.08042
	loss_reward_3: 0.01439
	loss_policy_4: 0.05956
	accuracy_policy_4: 0.63285
	loss_value_4: 0.08231
	loss_reward_4: 0.01737
	loss_policy_5: 0.05932
	accuracy_policy_5: 0.63926
	loss_value_5: 0.08397
	loss_reward_5: 0.01777
	loss_policy: 0.58935
	loss_value: 0.77459
	loss_reward: 0.06924
Optimization_Done 23400
[2025-05-07 16:24:46] [command] train weight_iter_23400.pkl 99 118
[2025-05-07 16:24:55] nn step 23450, lr: 0.1.
	loss_policy_0: 0.29552
	accuracy_policy_0: 0.62695
	loss_value_0: 0.38324
	loss_policy_1: 0.05915
	accuracy_policy_1: 0.62602
	loss_value_1: 0.07831
	loss_reward_1: 0.00774
	loss_policy_2: 0.05903
	accuracy_policy_2: 0.62848
	loss_value_2: 0.08036
	loss_reward_2: 0.0112
	loss_policy_3: 0.05926
	accuracy_policy_3: 0.6298
	loss_value_3: 0.08204
	loss_reward_3: 0.014
	loss_policy_4: 0.05883
	accuracy_policy_4: 0.63504
	loss_value_4: 0.08368
	loss_reward_4: 0.01687
	loss_policy_5: 0.05937
	accuracy_policy_5: 0.6352
	loss_value_5: 0.08524
	loss_reward_5: 0.01693
	loss_policy: 0.59117
	loss_value: 0.79288
	loss_reward: 0.06675
[2025-05-07 16:25:02] nn step 23500, lr: 0.1.
	loss_policy_0: 0.27686
	accuracy_policy_0: 0.62871
	loss_value_0: 0.35178
	loss_policy_1: 0.05515
	accuracy_policy_1: 0.62438
	loss_value_1: 0.07236
	loss_reward_1: 0.00719
	loss_policy_2: 0.05541
	accuracy_policy_2: 0.62125
	loss_value_2: 0.07416
	loss_reward_2: 0.01054
	loss_policy_3: 0.05574
	accuracy_policy_3: 0.62293
	loss_value_3: 0.07607
	loss_reward_3: 0.01349
	loss_policy_4: 0.05567
	accuracy_policy_4: 0.63168
	loss_value_4: 0.07767
	loss_reward_4: 0.01588
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.63426
	loss_value_5: 0.07896
	loss_reward_5: 0.01616
	loss_policy: 0.55464
	loss_value: 0.731
	loss_reward: 0.06326
[2025-05-07 16:25:10] nn step 23550, lr: 0.1.
	loss_policy_0: 0.30607
	accuracy_policy_0: 0.63727
	loss_value_0: 0.39137
	loss_policy_1: 0.06171
	accuracy_policy_1: 0.63129
	loss_value_1: 0.08059
	loss_reward_1: 0.00815
	loss_policy_2: 0.06158
	accuracy_policy_2: 0.63
	loss_value_2: 0.08294
	loss_reward_2: 0.01168
	loss_policy_3: 0.06146
	accuracy_policy_3: 0.63219
	loss_value_3: 0.08503
	loss_reward_3: 0.01513
	loss_policy_4: 0.06132
	accuracy_policy_4: 0.63691
	loss_value_4: 0.08673
	loss_reward_4: 0.01771
	loss_policy_5: 0.06146
	accuracy_policy_5: 0.64336
	loss_value_5: 0.08819
	loss_reward_5: 0.01724
	loss_policy: 0.61359
	loss_value: 0.81484
	loss_reward: 0.0699
[2025-05-07 16:25:18] nn step 23600, lr: 0.1.
	loss_policy_0: 0.30145
	accuracy_policy_0: 0.63426
	loss_value_0: 0.38382
	loss_policy_1: 0.06081
	accuracy_policy_1: 0.62746
	loss_value_1: 0.07873
	loss_reward_1: 0.00805
	loss_policy_2: 0.06103
	accuracy_policy_2: 0.62586
	loss_value_2: 0.08099
	loss_reward_2: 0.0119
	loss_policy_3: 0.06108
	accuracy_policy_3: 0.63219
	loss_value_3: 0.08293
	loss_reward_3: 0.01438
	loss_policy_4: 0.06126
	accuracy_policy_4: 0.63605
	loss_value_4: 0.08466
	loss_reward_4: 0.01727
	loss_policy_5: 0.06083
	accuracy_policy_5: 0.63742
	loss_value_5: 0.0863
	loss_reward_5: 0.01766
	loss_policy: 0.60646
	loss_value: 0.79743
	loss_reward: 0.06926
Optimization_Done 23600
[2025-05-07 16:28:44] [command] train weight_iter_23600.pkl 100 119
[2025-05-07 16:28:52] nn step 23650, lr: 0.1.
	loss_policy_0: 0.30139
	accuracy_policy_0: 0.62867
	loss_value_0: 0.38997
	loss_policy_1: 0.06037
	accuracy_policy_1: 0.6241
	loss_value_1: 0.07985
	loss_reward_1: 0.008
	loss_policy_2: 0.06113
	accuracy_policy_2: 0.61797
	loss_value_2: 0.08197
	loss_reward_2: 0.01148
	loss_policy_3: 0.06063
	accuracy_policy_3: 0.62637
	loss_value_3: 0.08351
	loss_reward_3: 0.01452
	loss_policy_4: 0.0605
	accuracy_policy_4: 0.63379
	loss_value_4: 0.08543
	loss_reward_4: 0.01661
	loss_policy_5: 0.06049
	accuracy_policy_5: 0.63363
	loss_value_5: 0.08699
	loss_reward_5: 0.01758
	loss_policy: 0.60451
	loss_value: 0.80773
	loss_reward: 0.0682
[2025-05-07 16:29:00] nn step 23700, lr: 0.1.
	loss_policy_0: 0.29898
	accuracy_policy_0: 0.6325
	loss_value_0: 0.38499
	loss_policy_1: 0.0605
	accuracy_policy_1: 0.62648
	loss_value_1: 0.07924
	loss_reward_1: 0.00792
	loss_policy_2: 0.06058
	accuracy_policy_2: 0.62039
	loss_value_2: 0.08179
	loss_reward_2: 0.0121
	loss_policy_3: 0.06051
	accuracy_policy_3: 0.63113
	loss_value_3: 0.08375
	loss_reward_3: 0.01467
	loss_policy_4: 0.0609
	accuracy_policy_4: 0.62961
	loss_value_4: 0.08541
	loss_reward_4: 0.01732
	loss_policy_5: 0.06063
	accuracy_policy_5: 0.63672
	loss_value_5: 0.08697
	loss_reward_5: 0.01789
	loss_policy: 0.6021
	loss_value: 0.80215
	loss_reward: 0.0699
[2025-05-07 16:29:08] nn step 23750, lr: 0.1.
	loss_policy_0: 0.30124
	accuracy_policy_0: 0.63242
	loss_value_0: 0.38686
	loss_policy_1: 0.06099
	accuracy_policy_1: 0.61934
	loss_value_1: 0.07935
	loss_reward_1: 0.00826
	loss_policy_2: 0.06093
	accuracy_policy_2: 0.62426
	loss_value_2: 0.08177
	loss_reward_2: 0.01203
	loss_policy_3: 0.06084
	accuracy_policy_3: 0.62953
	loss_value_3: 0.08421
	loss_reward_3: 0.01459
	loss_policy_4: 0.06079
	accuracy_policy_4: 0.63258
	loss_value_4: 0.08583
	loss_reward_4: 0.01775
	loss_policy_5: 0.0611
	accuracy_policy_5: 0.63414
	loss_value_5: 0.08724
	loss_reward_5: 0.0178
	loss_policy: 0.60589
	loss_value: 0.80526
	loss_reward: 0.07044
[2025-05-07 16:29:15] nn step 23800, lr: 0.1.
	loss_policy_0: 0.29532
	accuracy_policy_0: 0.6293
	loss_value_0: 0.37458
	loss_policy_1: 0.05961
	accuracy_policy_1: 0.62375
	loss_value_1: 0.07704
	loss_reward_1: 0.00771
	loss_policy_2: 0.05951
	accuracy_policy_2: 0.62547
	loss_value_2: 0.07919
	loss_reward_2: 0.01152
	loss_policy_3: 0.05981
	accuracy_policy_3: 0.62906
	loss_value_3: 0.08115
	loss_reward_3: 0.01436
	loss_policy_4: 0.06015
	accuracy_policy_4: 0.63156
	loss_value_4: 0.08322
	loss_reward_4: 0.01663
	loss_policy_5: 0.06007
	accuracy_policy_5: 0.63316
	loss_value_5: 0.08465
	loss_reward_5: 0.01779
	loss_policy: 0.59447
	loss_value: 0.77983
	loss_reward: 0.06801
Optimization_Done 23800
[2025-05-07 16:32:53] [command] train weight_iter_23800.pkl 101 120
[2025-05-07 16:33:01] nn step 23850, lr: 0.1.
	loss_policy_0: 0.28379
	accuracy_policy_0: 0.62492
	loss_value_0: 0.36643
	loss_policy_1: 0.05735
	accuracy_policy_1: 0.61762
	loss_value_1: 0.07512
	loss_reward_1: 0.00784
	loss_policy_2: 0.05754
	accuracy_policy_2: 0.61984
	loss_value_2: 0.07724
	loss_reward_2: 0.01136
	loss_policy_3: 0.05758
	accuracy_policy_3: 0.62125
	loss_value_3: 0.07893
	loss_reward_3: 0.01362
	loss_policy_4: 0.05738
	accuracy_policy_4: 0.62434
	loss_value_4: 0.08086
	loss_reward_4: 0.0161
	loss_policy_5: 0.05746
	accuracy_policy_5: 0.62949
	loss_value_5: 0.08227
	loss_reward_5: 0.0165
	loss_policy: 0.5711
	loss_value: 0.76085
	loss_reward: 0.06541
[2025-05-07 16:33:09] nn step 23900, lr: 0.1.
	loss_policy_0: 0.26228
	accuracy_policy_0: 0.63078
	loss_value_0: 0.33374
	loss_policy_1: 0.05294
	accuracy_policy_1: 0.62055
	loss_value_1: 0.06852
	loss_reward_1: 0.00703
	loss_policy_2: 0.053
	accuracy_policy_2: 0.62719
	loss_value_2: 0.0708
	loss_reward_2: 0.01018
	loss_policy_3: 0.05316
	accuracy_policy_3: 0.62445
	loss_value_3: 0.07245
	loss_reward_3: 0.01221
	loss_policy_4: 0.0532
	accuracy_policy_4: 0.62949
	loss_value_4: 0.07399
	loss_reward_4: 0.01474
	loss_policy_5: 0.05334
	accuracy_policy_5: 0.63125
	loss_value_5: 0.07547
	loss_reward_5: 0.01537
	loss_policy: 0.52791
	loss_value: 0.69496
	loss_reward: 0.05954
[2025-05-07 16:33:17] nn step 23950, lr: 0.1.
	loss_policy_0: 0.27057
	accuracy_policy_0: 0.63113
	loss_value_0: 0.34117
	loss_policy_1: 0.05466
	accuracy_policy_1: 0.61938
	loss_value_1: 0.06989
	loss_reward_1: 0.00712
	loss_policy_2: 0.05517
	accuracy_policy_2: 0.61863
	loss_value_2: 0.072
	loss_reward_2: 0.01015
	loss_policy_3: 0.05482
	accuracy_policy_3: 0.62562
	loss_value_3: 0.07411
	loss_reward_3: 0.01284
	loss_policy_4: 0.05476
	accuracy_policy_4: 0.6266
	loss_value_4: 0.07543
	loss_reward_4: 0.0149
	loss_policy_5: 0.05504
	accuracy_policy_5: 0.63102
	loss_value_5: 0.07707
	loss_reward_5: 0.01554
	loss_policy: 0.54502
	loss_value: 0.70968
	loss_reward: 0.06055
[2025-05-07 16:33:24] nn step 24000, lr: 0.1.
	loss_policy_0: 0.28457
	accuracy_policy_0: 0.6368
	loss_value_0: 0.36099
	loss_policy_1: 0.05741
	accuracy_policy_1: 0.62293
	loss_value_1: 0.07414
	loss_reward_1: 0.0074
	loss_policy_2: 0.05736
	accuracy_policy_2: 0.62539
	loss_value_2: 0.07657
	loss_reward_2: 0.01094
	loss_policy_3: 0.05774
	accuracy_policy_3: 0.62367
	loss_value_3: 0.07841
	loss_reward_3: 0.01343
	loss_policy_4: 0.05768
	accuracy_policy_4: 0.62891
	loss_value_4: 0.08023
	loss_reward_4: 0.01642
	loss_policy_5: 0.0577
	accuracy_policy_5: 0.62941
	loss_value_5: 0.08172
	loss_reward_5: 0.0166
	loss_policy: 0.57246
	loss_value: 0.75206
	loss_reward: 0.06479
Optimization_Done 24000
[2025-05-07 16:36:51] [command] train weight_iter_24000.pkl 102 121
[2025-05-07 16:36:59] nn step 24050, lr: 0.1.
	loss_policy_0: 0.30697
	accuracy_policy_0: 0.61824
	loss_value_0: 0.39692
	loss_policy_1: 0.06123
	accuracy_policy_1: 0.61605
	loss_value_1: 0.08091
	loss_reward_1: 0.00796
	loss_policy_2: 0.06133
	accuracy_policy_2: 0.6202
	loss_value_2: 0.08313
	loss_reward_2: 0.01179
	loss_policy_3: 0.0616
	accuracy_policy_3: 0.62109
	loss_value_3: 0.08545
	loss_reward_3: 0.01474
	loss_policy_4: 0.06137
	accuracy_policy_4: 0.6298
	loss_value_4: 0.08709
	loss_reward_4: 0.01744
	loss_policy_5: 0.06177
	accuracy_policy_5: 0.63121
	loss_value_5: 0.08857
	loss_reward_5: 0.0176
	loss_policy: 0.61428
	loss_value: 0.82208
	loss_reward: 0.06953
[2025-05-07 16:37:08] nn step 24100, lr: 0.1.
	loss_policy_0: 0.30893
	accuracy_policy_0: 0.62293
	loss_value_0: 0.39358
	loss_policy_1: 0.06171
	accuracy_policy_1: 0.62316
	loss_value_1: 0.08071
	loss_reward_1: 0.00854
	loss_policy_2: 0.06224
	accuracy_policy_2: 0.62371
	loss_value_2: 0.08295
	loss_reward_2: 0.01228
	loss_policy_3: 0.0621
	accuracy_policy_3: 0.62398
	loss_value_3: 0.08467
	loss_reward_3: 0.01459
	loss_policy_4: 0.06186
	accuracy_policy_4: 0.62676
	loss_value_4: 0.08626
	loss_reward_4: 0.01756
	loss_policy_5: 0.06243
	accuracy_policy_5: 0.63051
	loss_value_5: 0.0882
	loss_reward_5: 0.01795
	loss_policy: 0.61927
	loss_value: 0.81637
	loss_reward: 0.07091
[2025-05-07 16:37:16] nn step 24150, lr: 0.1.
	loss_policy_0: 0.29448
	accuracy_policy_0: 0.63031
	loss_value_0: 0.37556
	loss_policy_1: 0.05983
	accuracy_policy_1: 0.61836
	loss_value_1: 0.07688
	loss_reward_1: 0.008
	loss_policy_2: 0.05965
	accuracy_policy_2: 0.61965
	loss_value_2: 0.07937
	loss_reward_2: 0.01161
	loss_policy_3: 0.05997
	accuracy_policy_3: 0.61953
	loss_value_3: 0.08135
	loss_reward_3: 0.01415
	loss_policy_4: 0.06017
	accuracy_policy_4: 0.62258
	loss_value_4: 0.08337
	loss_reward_4: 0.01673
	loss_policy_5: 0.05978
	accuracy_policy_5: 0.6291
	loss_value_5: 0.08475
	loss_reward_5: 0.01703
	loss_policy: 0.59388
	loss_value: 0.78129
	loss_reward: 0.06752
[2025-05-07 16:37:22] nn step 24200, lr: 0.1.
	loss_policy_0: 0.28055
	accuracy_policy_0: 0.62664
	loss_value_0: 0.35612
	loss_policy_1: 0.05664
	accuracy_policy_1: 0.61863
	loss_value_1: 0.07342
	loss_reward_1: 0.00752
	loss_policy_2: 0.05701
	accuracy_policy_2: 0.61926
	loss_value_2: 0.07546
	loss_reward_2: 0.01098
	loss_policy_3: 0.05719
	accuracy_policy_3: 0.62125
	loss_value_3: 0.07678
	loss_reward_3: 0.01334
	loss_policy_4: 0.05699
	accuracy_policy_4: 0.62559
	loss_value_4: 0.07863
	loss_reward_4: 0.01585
	loss_policy_5: 0.05705
	accuracy_policy_5: 0.62457
	loss_value_5: 0.08028
	loss_reward_5: 0.01589
	loss_policy: 0.56542
	loss_value: 0.74069
	loss_reward: 0.06358
Optimization_Done 24200
[2025-05-07 16:40:53] [command] train weight_iter_24200.pkl 103 122
[2025-05-07 16:41:02] nn step 24250, lr: 0.1.
	loss_policy_0: 0.30016
	accuracy_policy_0: 0.62703
	loss_value_0: 0.38654
	loss_policy_1: 0.06104
	accuracy_policy_1: 0.61789
	loss_value_1: 0.07912
	loss_reward_1: 0.00789
	loss_policy_2: 0.0607
	accuracy_policy_2: 0.62219
	loss_value_2: 0.08133
	loss_reward_2: 0.0115
	loss_policy_3: 0.06085
	accuracy_policy_3: 0.62262
	loss_value_3: 0.08282
	loss_reward_3: 0.0141
	loss_policy_4: 0.06013
	accuracy_policy_4: 0.63516
	loss_value_4: 0.08452
	loss_reward_4: 0.01676
	loss_policy_5: 0.06043
	accuracy_policy_5: 0.63309
	loss_value_5: 0.0859
	loss_reward_5: 0.01682
	loss_policy: 0.60332
	loss_value: 0.80024
	loss_reward: 0.06707
[2025-05-07 16:41:09] nn step 24300, lr: 0.1.
	loss_policy_0: 0.30049
	accuracy_policy_0: 0.61699
	loss_value_0: 0.37537
	loss_policy_1: 0.05937
	accuracy_policy_1: 0.62492
	loss_value_1: 0.07732
	loss_reward_1: 0.00791
	loss_policy_2: 0.05947
	accuracy_policy_2: 0.62406
	loss_value_2: 0.07931
	loss_reward_2: 0.01137
	loss_policy_3: 0.06001
	accuracy_policy_3: 0.62352
	loss_value_3: 0.08129
	loss_reward_3: 0.01406
	loss_policy_4: 0.05982
	accuracy_policy_4: 0.6316
	loss_value_4: 0.08346
	loss_reward_4: 0.01703
	loss_policy_5: 0.05987
	accuracy_policy_5: 0.63656
	loss_value_5: 0.08498
	loss_reward_5: 0.01676
	loss_policy: 0.59903
	loss_value: 0.78173
	loss_reward: 0.06713
[2025-05-07 16:41:17] nn step 24350, lr: 0.1.
	loss_policy_0: 0.28048
	accuracy_policy_0: 0.62938
	loss_value_0: 0.35348
	loss_policy_1: 0.05663
	accuracy_policy_1: 0.62031
	loss_value_1: 0.07266
	loss_reward_1: 0.00765
	loss_policy_2: 0.05652
	accuracy_policy_2: 0.62352
	loss_value_2: 0.07488
	loss_reward_2: 0.01053
	loss_policy_3: 0.0565
	accuracy_policy_3: 0.62664
	loss_value_3: 0.07683
	loss_reward_3: 0.01326
	loss_policy_4: 0.05649
	accuracy_policy_4: 0.62758
	loss_value_4: 0.07839
	loss_reward_4: 0.0163
	loss_policy_5: 0.05611
	accuracy_policy_5: 0.63262
	loss_value_5: 0.07985
	loss_reward_5: 0.01645
	loss_policy: 0.56273
	loss_value: 0.7361
	loss_reward: 0.0642
[2025-05-07 16:41:25] nn step 24400, lr: 0.1.
	loss_policy_0: 0.29588
	accuracy_policy_0: 0.6316
	loss_value_0: 0.37332
	loss_policy_1: 0.06019
	accuracy_policy_1: 0.62062
	loss_value_1: 0.07643
	loss_reward_1: 0.00775
	loss_policy_2: 0.06016
	accuracy_policy_2: 0.6202
	loss_value_2: 0.07864
	loss_reward_2: 0.01114
	loss_policy_3: 0.06008
	accuracy_policy_3: 0.62477
	loss_value_3: 0.08065
	loss_reward_3: 0.01412
	loss_policy_4: 0.05964
	accuracy_policy_4: 0.63016
	loss_value_4: 0.08228
	loss_reward_4: 0.01677
	loss_policy_5: 0.05994
	accuracy_policy_5: 0.63133
	loss_value_5: 0.08381
	loss_reward_5: 0.01725
	loss_policy: 0.59589
	loss_value: 0.77513
	loss_reward: 0.06702
Optimization_Done 24400
[2025-05-07 16:45:00] [command] train weight_iter_24400.pkl 104 123
[2025-05-07 16:45:09] nn step 24450, lr: 0.1.
	loss_policy_0: 0.27458
	accuracy_policy_0: 0.62773
	loss_value_0: 0.35382
	loss_policy_1: 0.05528
	accuracy_policy_1: 0.61957
	loss_value_1: 0.07265
	loss_reward_1: 0.00716
	loss_policy_2: 0.05568
	accuracy_policy_2: 0.6191
	loss_value_2: 0.0746
	loss_reward_2: 0.01048
	loss_policy_3: 0.05546
	accuracy_policy_3: 0.6218
	loss_value_3: 0.07629
	loss_reward_3: 0.01276
	loss_policy_4: 0.05553
	accuracy_policy_4: 0.62484
	loss_value_4: 0.07746
	loss_reward_4: 0.01561
	loss_policy_5: 0.05576
	accuracy_policy_5: 0.62551
	loss_value_5: 0.07866
	loss_reward_5: 0.01588
	loss_policy: 0.55228
	loss_value: 0.73349
	loss_reward: 0.0619
[2025-05-07 16:45:17] nn step 24500, lr: 0.1.
	loss_policy_0: 0.29302
	accuracy_policy_0: 0.62211
	loss_value_0: 0.37126
	loss_policy_1: 0.0588
	accuracy_policy_1: 0.62148
	loss_value_1: 0.0764
	loss_reward_1: 0.00807
	loss_policy_2: 0.05921
	accuracy_policy_2: 0.61957
	loss_value_2: 0.07843
	loss_reward_2: 0.01168
	loss_policy_3: 0.05878
	accuracy_policy_3: 0.62309
	loss_value_3: 0.08034
	loss_reward_3: 0.01385
	loss_policy_4: 0.05896
	accuracy_policy_4: 0.62875
	loss_value_4: 0.0822
	loss_reward_4: 0.01666
	loss_policy_5: 0.05916
	accuracy_policy_5: 0.62871
	loss_value_5: 0.08388
	loss_reward_5: 0.01693
	loss_policy: 0.58792
	loss_value: 0.77251
	loss_reward: 0.06719
[2025-05-07 16:45:23] nn step 24550, lr: 0.1.
	loss_policy_0: 0.31132
	accuracy_policy_0: 0.61762
	loss_value_0: 0.38614
	loss_policy_1: 0.0619
	accuracy_policy_1: 0.61762
	loss_value_1: 0.07934
	loss_reward_1: 0.00829
	loss_policy_2: 0.06159
	accuracy_policy_2: 0.62234
	loss_value_2: 0.08155
	loss_reward_2: 0.01192
	loss_policy_3: 0.06174
	accuracy_policy_3: 0.62121
	loss_value_3: 0.0837
	loss_reward_3: 0.01472
	loss_policy_4: 0.06203
	accuracy_policy_4: 0.62016
	loss_value_4: 0.08535
	loss_reward_4: 0.01758
	loss_policy_5: 0.06178
	accuracy_policy_5: 0.63023
	loss_value_5: 0.08731
	loss_reward_5: 0.01766
	loss_policy: 0.62037
	loss_value: 0.80339
	loss_reward: 0.07016
[2025-05-07 16:45:31] nn step 24600, lr: 0.1.
	loss_policy_0: 0.29826
	accuracy_policy_0: 0.62254
	loss_value_0: 0.37576
	loss_policy_1: 0.06029
	accuracy_policy_1: 0.61492
	loss_value_1: 0.07757
	loss_reward_1: 0.00776
	loss_policy_2: 0.06002
	accuracy_policy_2: 0.61711
	loss_value_2: 0.07986
	loss_reward_2: 0.01126
	loss_policy_3: 0.05976
	accuracy_policy_3: 0.62035
	loss_value_3: 0.08195
	loss_reward_3: 0.01465
	loss_policy_4: 0.06013
	accuracy_policy_4: 0.62988
	loss_value_4: 0.08385
	loss_reward_4: 0.01692
	loss_policy_5: 0.06001
	accuracy_policy_5: 0.62953
	loss_value_5: 0.08546
	loss_reward_5: 0.01742
	loss_policy: 0.59847
	loss_value: 0.78445
	loss_reward: 0.06801
Optimization_Done 24600
[2025-05-07 16:48:58] [command] train weight_iter_24600.pkl 105 124
[2025-05-07 16:49:07] nn step 24650, lr: 0.1.
	loss_policy_0: 0.28265
	accuracy_policy_0: 0.62156
	loss_value_0: 0.36189
	loss_policy_1: 0.05713
	accuracy_policy_1: 0.61648
	loss_value_1: 0.07449
	loss_reward_1: 0.00755
	loss_policy_2: 0.05725
	accuracy_policy_2: 0.62199
	loss_value_2: 0.07639
	loss_reward_2: 0.0109
	loss_policy_3: 0.05716
	accuracy_policy_3: 0.62133
	loss_value_3: 0.07845
	loss_reward_3: 0.01341
	loss_policy_4: 0.05721
	accuracy_policy_4: 0.62727
	loss_value_4: 0.08032
	loss_reward_4: 0.01552
	loss_policy_5: 0.05742
	accuracy_policy_5: 0.6257
	loss_value_5: 0.08186
	loss_reward_5: 0.01599
	loss_policy: 0.56882
	loss_value: 0.75341
	loss_reward: 0.06337
[2025-05-07 16:49:13] nn step 24700, lr: 0.1.
	loss_policy_0: 0.29092
	accuracy_policy_0: 0.61727
	loss_value_0: 0.36279
	loss_policy_1: 0.05812
	accuracy_policy_1: 0.615
	loss_value_1: 0.07454
	loss_reward_1: 0.00793
	loss_policy_2: 0.05787
	accuracy_policy_2: 0.61848
	loss_value_2: 0.07642
	loss_reward_2: 0.01164
	loss_policy_3: 0.05804
	accuracy_policy_3: 0.61891
	loss_value_3: 0.07849
	loss_reward_3: 0.01363
	loss_policy_4: 0.05827
	accuracy_policy_4: 0.62516
	loss_value_4: 0.08014
	loss_reward_4: 0.0164
	loss_policy_5: 0.05804
	accuracy_policy_5: 0.63191
	loss_value_5: 0.08178
	loss_reward_5: 0.01675
	loss_policy: 0.58126
	loss_value: 0.75415
	loss_reward: 0.06635
[2025-05-07 16:49:21] nn step 24750, lr: 0.1.
	loss_policy_0: 0.29983
	accuracy_policy_0: 0.61438
	loss_value_0: 0.37387
	loss_policy_1: 0.05971
	accuracy_policy_1: 0.61434
	loss_value_1: 0.07695
	loss_reward_1: 0.00814
	loss_policy_2: 0.05971
	accuracy_policy_2: 0.61684
	loss_value_2: 0.07908
	loss_reward_2: 0.01198
	loss_policy_3: 0.05994
	accuracy_policy_3: 0.61395
	loss_value_3: 0.08117
	loss_reward_3: 0.01451
	loss_policy_4: 0.05974
	accuracy_policy_4: 0.62082
	loss_value_4: 0.08309
	loss_reward_4: 0.01692
	loss_policy_5: 0.06003
	accuracy_policy_5: 0.62687
	loss_value_5: 0.08489
	loss_reward_5: 0.0175
	loss_policy: 0.59895
	loss_value: 0.77905
	loss_reward: 0.06905
[2025-05-07 16:49:29] nn step 24800, lr: 0.1.
	loss_policy_0: 0.30862
	accuracy_policy_0: 0.62094
	loss_value_0: 0.38596
	loss_policy_1: 0.06216
	accuracy_policy_1: 0.61234
	loss_value_1: 0.07945
	loss_reward_1: 0.00815
	loss_policy_2: 0.06204
	accuracy_policy_2: 0.61676
	loss_value_2: 0.08138
	loss_reward_2: 0.01185
	loss_policy_3: 0.06189
	accuracy_policy_3: 0.62168
	loss_value_3: 0.08354
	loss_reward_3: 0.01442
	loss_policy_4: 0.06185
	accuracy_policy_4: 0.62539
	loss_value_4: 0.08529
	loss_reward_4: 0.0176
	loss_policy_5: 0.06179
	accuracy_policy_5: 0.63016
	loss_value_5: 0.08707
	loss_reward_5: 0.01763
	loss_policy: 0.61834
	loss_value: 0.8027
	loss_reward: 0.06965
Optimization_Done 24800
[2025-05-07 16:53:06] [command] train weight_iter_24800.pkl 106 125
[2025-05-07 16:53:14] nn step 24850, lr: 0.1.
	loss_policy_0: 0.29573
	accuracy_policy_0: 0.62781
	loss_value_0: 0.38555
	loss_policy_1: 0.05954
	accuracy_policy_1: 0.62195
	loss_value_1: 0.07861
	loss_reward_1: 0.00808
	loss_policy_2: 0.0596
	accuracy_policy_2: 0.62512
	loss_value_2: 0.08108
	loss_reward_2: 0.01135
	loss_policy_3: 0.0597
	accuracy_policy_3: 0.62453
	loss_value_3: 0.08303
	loss_reward_3: 0.01426
	loss_policy_4: 0.05977
	accuracy_policy_4: 0.62258
	loss_value_4: 0.08479
	loss_reward_4: 0.01696
	loss_policy_5: 0.05976
	accuracy_policy_5: 0.62891
	loss_value_5: 0.08673
	loss_reward_5: 0.01722
	loss_policy: 0.5941
	loss_value: 0.79979
	loss_reward: 0.06786
[2025-05-07 16:53:22] nn step 24900, lr: 0.1.
	loss_policy_0: 0.28323
	accuracy_policy_0: 0.63125
	loss_value_0: 0.35955
	loss_policy_1: 0.05719
	accuracy_policy_1: 0.62656
	loss_value_1: 0.07373
	loss_reward_1: 0.00751
	loss_policy_2: 0.05704
	accuracy_policy_2: 0.62852
	loss_value_2: 0.07591
	loss_reward_2: 0.0107
	loss_policy_3: 0.05729
	accuracy_policy_3: 0.62711
	loss_value_3: 0.07741
	loss_reward_3: 0.01356
	loss_policy_4: 0.05724
	accuracy_policy_4: 0.63062
	loss_value_4: 0.07918
	loss_reward_4: 0.01599
	loss_policy_5: 0.05728
	accuracy_policy_5: 0.63574
	loss_value_5: 0.08058
	loss_reward_5: 0.01598
	loss_policy: 0.56928
	loss_value: 0.74636
	loss_reward: 0.06374
[2025-05-07 16:53:29] nn step 24950, lr: 0.1.
	loss_policy_0: 0.30908
	accuracy_policy_0: 0.62668
	loss_value_0: 0.39046
	loss_policy_1: 0.06192
	accuracy_policy_1: 0.62117
	loss_value_1: 0.08021
	loss_reward_1: 0.00816
	loss_policy_2: 0.06222
	accuracy_policy_2: 0.61844
	loss_value_2: 0.08239
	loss_reward_2: 0.01212
	loss_policy_3: 0.06204
	accuracy_policy_3: 0.62445
	loss_value_3: 0.08447
	loss_reward_3: 0.01435
	loss_policy_4: 0.06225
	accuracy_policy_4: 0.62738
	loss_value_4: 0.08615
	loss_reward_4: 0.01812
	loss_policy_5: 0.06199
	accuracy_policy_5: 0.63043
	loss_value_5: 0.08758
	loss_reward_5: 0.01764
	loss_policy: 0.61949
	loss_value: 0.81127
	loss_reward: 0.07039
[2025-05-07 16:53:37] nn step 25000, lr: 0.1.
	loss_policy_0: 0.28921
	accuracy_policy_0: 0.62891
	loss_value_0: 0.36709
	loss_policy_1: 0.05834
	accuracy_policy_1: 0.62426
	loss_value_1: 0.07545
	loss_reward_1: 0.00767
	loss_policy_2: 0.05873
	accuracy_policy_2: 0.61996
	loss_value_2: 0.07757
	loss_reward_2: 0.01113
	loss_policy_3: 0.05854
	accuracy_policy_3: 0.62926
	loss_value_3: 0.07975
	loss_reward_3: 0.01378
	loss_policy_4: 0.05848
	accuracy_policy_4: 0.63445
	loss_value_4: 0.0816
	loss_reward_4: 0.01603
	loss_policy_5: 0.0586
	accuracy_policy_5: 0.63043
	loss_value_5: 0.0832
	loss_reward_5: 0.01655
	loss_policy: 0.5819
	loss_value: 0.76466
	loss_reward: 0.06516
Optimization_Done 25000
[2025-05-07 16:57:09] [command] train weight_iter_25000.pkl 107 126
[2025-05-07 16:57:16] nn step 25050, lr: 0.1.
	loss_policy_0: 0.27596
	accuracy_policy_0: 0.6398
	loss_value_0: 0.36222
	loss_policy_1: 0.05511
	accuracy_policy_1: 0.63375
	loss_value_1: 0.07416
	loss_reward_1: 0.00748
	loss_policy_2: 0.05576
	accuracy_policy_2: 0.63395
	loss_value_2: 0.07609
	loss_reward_2: 0.0105
	loss_policy_3: 0.05647
	accuracy_policy_3: 0.62594
	loss_value_3: 0.07771
	loss_reward_3: 0.01336
	loss_policy_4: 0.05564
	accuracy_policy_4: 0.64078
	loss_value_4: 0.07904
	loss_reward_4: 0.01577
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.6427
	loss_value_5: 0.08082
	loss_reward_5: 0.01606
	loss_policy: 0.55484
	loss_value: 0.75004
	loss_reward: 0.06317
[2025-05-07 16:57:24] nn step 25100, lr: 0.1.
	loss_policy_0: 0.28473
	accuracy_policy_0: 0.63117
	loss_value_0: 0.36844
	loss_policy_1: 0.0575
	accuracy_policy_1: 0.63301
	loss_value_1: 0.07554
	loss_reward_1: 0.00769
	loss_policy_2: 0.05753
	accuracy_policy_2: 0.62531
	loss_value_2: 0.07741
	loss_reward_2: 0.01108
	loss_policy_3: 0.0572
	accuracy_policy_3: 0.63137
	loss_value_3: 0.07947
	loss_reward_3: 0.01349
	loss_policy_4: 0.05757
	accuracy_policy_4: 0.63629
	loss_value_4: 0.08141
	loss_reward_4: 0.01618
	loss_policy_5: 0.05767
	accuracy_policy_5: 0.63523
	loss_value_5: 0.08294
	loss_reward_5: 0.01699
	loss_policy: 0.5722
	loss_value: 0.76522
	loss_reward: 0.06544
[2025-05-07 16:57:32] nn step 25150, lr: 0.1.
	loss_policy_0: 0.29404
	accuracy_policy_0: 0.63652
	loss_value_0: 0.37613
	loss_policy_1: 0.05917
	accuracy_policy_1: 0.63035
	loss_value_1: 0.0773
	loss_reward_1: 0.00785
	loss_policy_2: 0.05943
	accuracy_policy_2: 0.63305
	loss_value_2: 0.0795
	loss_reward_2: 0.01145
	loss_policy_3: 0.05929
	accuracy_policy_3: 0.62914
	loss_value_3: 0.08166
	loss_reward_3: 0.0142
	loss_policy_4: 0.05914
	accuracy_policy_4: 0.63168
	loss_value_4: 0.08328
	loss_reward_4: 0.01723
	loss_policy_5: 0.0595
	accuracy_policy_5: 0.63344
	loss_value_5: 0.08518
	loss_reward_5: 0.01741
	loss_policy: 0.59057
	loss_value: 0.78305
	loss_reward: 0.06813
[2025-05-07 16:57:40] nn step 25200, lr: 0.1.
	loss_policy_0: 0.30585
	accuracy_policy_0: 0.63609
	loss_value_0: 0.39178
	loss_policy_1: 0.06156
	accuracy_policy_1: 0.63309
	loss_value_1: 0.08061
	loss_reward_1: 0.00826
	loss_policy_2: 0.06168
	accuracy_policy_2: 0.62945
	loss_value_2: 0.08266
	loss_reward_2: 0.0117
	loss_policy_3: 0.06159
	accuracy_policy_3: 0.63152
	loss_value_3: 0.08432
	loss_reward_3: 0.01459
	loss_policy_4: 0.06178
	accuracy_policy_4: 0.63617
	loss_value_4: 0.0863
	loss_reward_4: 0.01774
	loss_policy_5: 0.06168
	accuracy_policy_5: 0.64113
	loss_value_5: 0.08795
	loss_reward_5: 0.01811
	loss_policy: 0.61415
	loss_value: 0.81362
	loss_reward: 0.07039
Optimization_Done 25200
[2025-05-07 17:01:02] [command] train weight_iter_25200.pkl 108 127
[2025-05-07 17:01:10] nn step 25250, lr: 0.1.
	loss_policy_0: 0.28601
	accuracy_policy_0: 0.63523
	loss_value_0: 0.37472
	loss_policy_1: 0.05696
	accuracy_policy_1: 0.63215
	loss_value_1: 0.07648
	loss_reward_1: 0.00784
	loss_policy_2: 0.05745
	accuracy_policy_2: 0.63574
	loss_value_2: 0.07858
	loss_reward_2: 0.0114
	loss_policy_3: 0.0574
	accuracy_policy_3: 0.63547
	loss_value_3: 0.08054
	loss_reward_3: 0.01365
	loss_policy_4: 0.05758
	accuracy_policy_4: 0.63352
	loss_value_4: 0.08221
	loss_reward_4: 0.01615
	loss_policy_5: 0.05778
	accuracy_policy_5: 0.63391
	loss_value_5: 0.08389
	loss_reward_5: 0.01631
	loss_policy: 0.57318
	loss_value: 0.77642
	loss_reward: 0.06535
[2025-05-07 17:01:18] nn step 25300, lr: 0.1.
	loss_policy_0: 0.28972
	accuracy_policy_0: 0.64281
	loss_value_0: 0.37774
	loss_policy_1: 0.05857
	accuracy_policy_1: 0.63215
	loss_value_1: 0.0775
	loss_reward_1: 0.00803
	loss_policy_2: 0.05908
	accuracy_policy_2: 0.62852
	loss_value_2: 0.07939
	loss_reward_2: 0.01176
	loss_policy_3: 0.05865
	accuracy_policy_3: 0.63555
	loss_value_3: 0.08141
	loss_reward_3: 0.01394
	loss_policy_4: 0.05896
	accuracy_policy_4: 0.63512
	loss_value_4: 0.08297
	loss_reward_4: 0.0169
	loss_policy_5: 0.05914
	accuracy_policy_5: 0.63176
	loss_value_5: 0.0848
	loss_reward_5: 0.0172
	loss_policy: 0.58411
	loss_value: 0.78382
	loss_reward: 0.06784
[2025-05-07 17:01:25] nn step 25350, lr: 0.1.
	loss_policy_0: 0.29415
	accuracy_policy_0: 0.63445
	loss_value_0: 0.37601
	loss_policy_1: 0.0595
	accuracy_policy_1: 0.62723
	loss_value_1: 0.07741
	loss_reward_1: 0.0078
	loss_policy_2: 0.05968
	accuracy_policy_2: 0.62223
	loss_value_2: 0.07994
	loss_reward_2: 0.01137
	loss_policy_3: 0.05931
	accuracy_policy_3: 0.63297
	loss_value_3: 0.08157
	loss_reward_3: 0.01391
	loss_policy_4: 0.05947
	accuracy_policy_4: 0.63539
	loss_value_4: 0.08338
	loss_reward_4: 0.01669
	loss_policy_5: 0.05993
	accuracy_policy_5: 0.63336
	loss_value_5: 0.08514
	loss_reward_5: 0.01683
	loss_policy: 0.59205
	loss_value: 0.78344
	loss_reward: 0.0666
[2025-05-07 17:01:33] nn step 25400, lr: 0.1.
	loss_policy_0: 0.28972
	accuracy_policy_0: 0.63301
	loss_value_0: 0.366
	loss_policy_1: 0.05888
	accuracy_policy_1: 0.62867
	loss_value_1: 0.07501
	loss_reward_1: 0.00786
	loss_policy_2: 0.05852
	accuracy_policy_2: 0.62613
	loss_value_2: 0.07742
	loss_reward_2: 0.01128
	loss_policy_3: 0.05843
	accuracy_policy_3: 0.63035
	loss_value_3: 0.0792
	loss_reward_3: 0.01342
	loss_policy_4: 0.05859
	accuracy_policy_4: 0.63246
	loss_value_4: 0.08063
	loss_reward_4: 0.01641
	loss_policy_5: 0.05837
	accuracy_policy_5: 0.63676
	loss_value_5: 0.08241
	loss_reward_5: 0.01705
	loss_policy: 0.58251
	loss_value: 0.76068
	loss_reward: 0.06602
Optimization_Done 25400
[2025-05-07 17:05:06] [command] train weight_iter_25400.pkl 109 128
[2025-05-07 17:05:15] nn step 25450, lr: 0.1.
	loss_policy_0: 0.2798
	accuracy_policy_0: 0.63422
	loss_value_0: 0.36504
	loss_policy_1: 0.05659
	accuracy_policy_1: 0.62895
	loss_value_1: 0.07509
	loss_reward_1: 0.00778
	loss_policy_2: 0.05686
	accuracy_policy_2: 0.62945
	loss_value_2: 0.07727
	loss_reward_2: 0.01096
	loss_policy_3: 0.05686
	accuracy_policy_3: 0.63215
	loss_value_3: 0.0792
	loss_reward_3: 0.01348
	loss_policy_4: 0.05707
	accuracy_policy_4: 0.63219
	loss_value_4: 0.08054
	loss_reward_4: 0.01584
	loss_policy_5: 0.05693
	accuracy_policy_5: 0.63602
	loss_value_5: 0.08217
	loss_reward_5: 0.01617
	loss_policy: 0.56411
	loss_value: 0.75931
	loss_reward: 0.06422
[2025-05-07 17:05:22] nn step 25500, lr: 0.1.
	loss_policy_0: 0.28266
	accuracy_policy_0: 0.63785
	loss_value_0: 0.36674
	loss_policy_1: 0.05663
	accuracy_policy_1: 0.63156
	loss_value_1: 0.07504
	loss_reward_1: 0.00748
	loss_policy_2: 0.0571
	accuracy_policy_2: 0.63141
	loss_value_2: 0.07748
	loss_reward_2: 0.01067
	loss_policy_3: 0.05716
	accuracy_policy_3: 0.62957
	loss_value_3: 0.07938
	loss_reward_3: 0.01333
	loss_policy_4: 0.0573
	accuracy_policy_4: 0.63441
	loss_value_4: 0.08089
	loss_reward_4: 0.01659
	loss_policy_5: 0.05708
	accuracy_policy_5: 0.63801
	loss_value_5: 0.08257
	loss_reward_5: 0.01644
	loss_policy: 0.56793
	loss_value: 0.76211
	loss_reward: 0.06451
[2025-05-07 17:05:30] nn step 25550, lr: 0.1.
	loss_policy_0: 0.29456
	accuracy_policy_0: 0.62066
	loss_value_0: 0.3764
	loss_policy_1: 0.05836
	accuracy_policy_1: 0.62738
	loss_value_1: 0.07733
	loss_reward_1: 0.00771
	loss_policy_2: 0.05853
	accuracy_policy_2: 0.62633
	loss_value_2: 0.07912
	loss_reward_2: 0.01134
	loss_policy_3: 0.05845
	accuracy_policy_3: 0.62965
	loss_value_3: 0.08119
	loss_reward_3: 0.01337
	loss_policy_4: 0.05856
	accuracy_policy_4: 0.6366
	loss_value_4: 0.08288
	loss_reward_4: 0.01646
	loss_policy_5: 0.0581
	accuracy_policy_5: 0.6382
	loss_value_5: 0.0845
	loss_reward_5: 0.01714
	loss_policy: 0.58656
	loss_value: 0.78143
	loss_reward: 0.06601
[2025-05-07 17:05:38] nn step 25600, lr: 0.1.
	loss_policy_0: 0.29062
	accuracy_policy_0: 0.63867
	loss_value_0: 0.37059
	loss_policy_1: 0.05864
	accuracy_policy_1: 0.62926
	loss_value_1: 0.07601
	loss_reward_1: 0.00792
	loss_policy_2: 0.05903
	accuracy_policy_2: 0.62449
	loss_value_2: 0.07795
	loss_reward_2: 0.01131
	loss_policy_3: 0.05879
	accuracy_policy_3: 0.62215
	loss_value_3: 0.0803
	loss_reward_3: 0.0134
	loss_policy_4: 0.05865
	accuracy_policy_4: 0.63043
	loss_value_4: 0.08202
	loss_reward_4: 0.01685
	loss_policy_5: 0.05866
	accuracy_policy_5: 0.63242
	loss_value_5: 0.08376
	loss_reward_5: 0.01729
	loss_policy: 0.58438
	loss_value: 0.77062
	loss_reward: 0.06677
Optimization_Done 25600
[2025-05-07 17:09:09] [command] train weight_iter_25600.pkl 110 129
[2025-05-07 17:09:18] nn step 25650, lr: 0.1.
	loss_policy_0: 0.27691
	accuracy_policy_0: 0.63496
	loss_value_0: 0.36233
	loss_policy_1: 0.0556
	accuracy_policy_1: 0.63199
	loss_value_1: 0.07412
	loss_reward_1: 0.00761
	loss_policy_2: 0.05599
	accuracy_policy_2: 0.62852
	loss_value_2: 0.07634
	loss_reward_2: 0.01078
	loss_policy_3: 0.05594
	accuracy_policy_3: 0.63023
	loss_value_3: 0.07797
	loss_reward_3: 0.01333
	loss_policy_4: 0.05616
	accuracy_policy_4: 0.62875
	loss_value_4: 0.07989
	loss_reward_4: 0.01616
	loss_policy_5: 0.05573
	accuracy_policy_5: 0.63645
	loss_value_5: 0.08152
	loss_reward_5: 0.01584
	loss_policy: 0.55632
	loss_value: 0.75217
	loss_reward: 0.06372
[2025-05-07 17:09:27] nn step 25700, lr: 0.1.
	loss_policy_0: 0.28093
	accuracy_policy_0: 0.6352
	loss_value_0: 0.36069
	loss_policy_1: 0.05636
	accuracy_policy_1: 0.63438
	loss_value_1: 0.07395
	loss_reward_1: 0.00768
	loss_policy_2: 0.05669
	accuracy_policy_2: 0.62859
	loss_value_2: 0.0757
	loss_reward_2: 0.01084
	loss_policy_3: 0.05682
	accuracy_policy_3: 0.63148
	loss_value_3: 0.07757
	loss_reward_3: 0.01356
	loss_policy_4: 0.0571
	accuracy_policy_4: 0.63234
	loss_value_4: 0.07931
	loss_reward_4: 0.01604
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.63684
	loss_value_5: 0.08063
	loss_reward_5: 0.01629
	loss_policy: 0.56472
	loss_value: 0.74784
	loss_reward: 0.06441
[2025-05-07 17:09:35] nn step 25750, lr: 0.1.
	loss_policy_0: 0.27609
	accuracy_policy_0: 0.6359
	loss_value_0: 0.35467
	loss_policy_1: 0.05569
	accuracy_policy_1: 0.62672
	loss_value_1: 0.07256
	loss_reward_1: 0.00743
	loss_policy_2: 0.0558
	accuracy_policy_2: 0.62922
	loss_value_2: 0.07473
	loss_reward_2: 0.01079
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.63141
	loss_value_3: 0.07632
	loss_reward_3: 0.01321
	loss_policy_4: 0.05552
	accuracy_policy_4: 0.63805
	loss_value_4: 0.0781
	loss_reward_4: 0.01525
	loss_policy_5: 0.05581
	accuracy_policy_5: 0.63434
	loss_value_5: 0.07973
	loss_reward_5: 0.01586
	loss_policy: 0.55473
	loss_value: 0.73611
	loss_reward: 0.06253
[2025-05-07 17:09:41] nn step 25800, lr: 0.1.
	loss_policy_0: 0.28089
	accuracy_policy_0: 0.62562
	loss_value_0: 0.3532
	loss_policy_1: 0.05605
	accuracy_policy_1: 0.63109
	loss_value_1: 0.07257
	loss_reward_1: 0.00759
	loss_policy_2: 0.0564
	accuracy_policy_2: 0.62969
	loss_value_2: 0.07475
	loss_reward_2: 0.01093
	loss_policy_3: 0.05645
	accuracy_policy_3: 0.62883
	loss_value_3: 0.07648
	loss_reward_3: 0.01314
	loss_policy_4: 0.05625
	accuracy_policy_4: 0.63535
	loss_value_4: 0.07831
	loss_reward_4: 0.01596
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.62578
	loss_value_5: 0.07997
	loss_reward_5: 0.01621
	loss_policy: 0.56287
	loss_value: 0.73527
	loss_reward: 0.06382
Optimization_Done 25800
[2025-05-07 17:13:12] [command] train weight_iter_25800.pkl 111 130
[2025-05-07 17:13:20] nn step 25850, lr: 0.1.
	loss_policy_0: 0.27071
	accuracy_policy_0: 0.63539
	loss_value_0: 0.35063
	loss_policy_1: 0.05435
	accuracy_policy_1: 0.62902
	loss_value_1: 0.07204
	loss_reward_1: 0.00708
	loss_policy_2: 0.05494
	accuracy_policy_2: 0.62684
	loss_value_2: 0.07407
	loss_reward_2: 0.01053
	loss_policy_3: 0.05448
	accuracy_policy_3: 0.63258
	loss_value_3: 0.07584
	loss_reward_3: 0.01249
	loss_policy_4: 0.05455
	accuracy_policy_4: 0.63457
	loss_value_4: 0.07724
	loss_reward_4: 0.01524
	loss_policy_5: 0.05459
	accuracy_policy_5: 0.63223
	loss_value_5: 0.0786
	loss_reward_5: 0.01568
	loss_policy: 0.54363
	loss_value: 0.72842
	loss_reward: 0.06102
[2025-05-07 17:13:27] nn step 25900, lr: 0.1.
	loss_policy_0: 0.28696
	accuracy_policy_0: 0.62957
	loss_value_0: 0.36674
	loss_policy_1: 0.05763
	accuracy_policy_1: 0.63227
	loss_value_1: 0.07541
	loss_reward_1: 0.00782
	loss_policy_2: 0.05791
	accuracy_policy_2: 0.62957
	loss_value_2: 0.07742
	loss_reward_2: 0.01135
	loss_policy_3: 0.05764
	accuracy_policy_3: 0.63023
	loss_value_3: 0.07964
	loss_reward_3: 0.01391
	loss_policy_4: 0.05779
	accuracy_policy_4: 0.63438
	loss_value_4: 0.08128
	loss_reward_4: 0.01659
	loss_policy_5: 0.05798
	accuracy_policy_5: 0.63371
	loss_value_5: 0.08302
	loss_reward_5: 0.01683
	loss_policy: 0.57589
	loss_value: 0.7635
	loss_reward: 0.06651
[2025-05-07 17:13:35] nn step 25950, lr: 0.1.
	loss_policy_0: 0.28357
	accuracy_policy_0: 0.63625
	loss_value_0: 0.36324
	loss_policy_1: 0.0571
	accuracy_policy_1: 0.62762
	loss_value_1: 0.07445
	loss_reward_1: 0.00761
	loss_policy_2: 0.05735
	accuracy_policy_2: 0.62813
	loss_value_2: 0.07684
	loss_reward_2: 0.01099
	loss_policy_3: 0.05713
	accuracy_policy_3: 0.62855
	loss_value_3: 0.07871
	loss_reward_3: 0.01367
	loss_policy_4: 0.0572
	accuracy_policy_4: 0.63246
	loss_value_4: 0.08035
	loss_reward_4: 0.01656
	loss_policy_5: 0.05715
	accuracy_policy_5: 0.63207
	loss_value_5: 0.08185
	loss_reward_5: 0.01647
	loss_policy: 0.5695
	loss_value: 0.75545
	loss_reward: 0.0653
[2025-05-07 17:13:43] nn step 26000, lr: 0.1.
	loss_policy_0: 0.27837
	accuracy_policy_0: 0.63707
	loss_value_0: 0.35676
	loss_policy_1: 0.05587
	accuracy_policy_1: 0.62949
	loss_value_1: 0.0731
	loss_reward_1: 0.00766
	loss_policy_2: 0.05632
	accuracy_policy_2: 0.62844
	loss_value_2: 0.0751
	loss_reward_2: 0.01097
	loss_policy_3: 0.05613
	accuracy_policy_3: 0.62875
	loss_value_3: 0.07683
	loss_reward_3: 0.01332
	loss_policy_4: 0.05624
	accuracy_policy_4: 0.63078
	loss_value_4: 0.07823
	loss_reward_4: 0.01606
	loss_policy_5: 0.05594
	accuracy_policy_5: 0.63848
	loss_value_5: 0.07972
	loss_reward_5: 0.01651
	loss_policy: 0.55888
	loss_value: 0.73974
	loss_reward: 0.06453
Optimization_Done 26000
[2025-05-07 17:17:18] [command] train weight_iter_26000.pkl 112 131
[2025-05-07 17:17:27] nn step 26050, lr: 0.1.
	loss_policy_0: 0.28598
	accuracy_policy_0: 0.62941
	loss_value_0: 0.37934
	loss_policy_1: 0.05714
	accuracy_policy_1: 0.63328
	loss_value_1: 0.07752
	loss_reward_1: 0.00765
	loss_policy_2: 0.05746
	accuracy_policy_2: 0.62727
	loss_value_2: 0.07912
	loss_reward_2: 0.01061
	loss_policy_3: 0.05781
	accuracy_policy_3: 0.62273
	loss_value_3: 0.08102
	loss_reward_3: 0.01374
	loss_policy_4: 0.05772
	accuracy_policy_4: 0.62809
	loss_value_4: 0.08249
	loss_reward_4: 0.01693
	loss_policy_5: 0.0578
	accuracy_policy_5: 0.63547
	loss_value_5: 0.08431
	loss_reward_5: 0.01651
	loss_policy: 0.57391
	loss_value: 0.78381
	loss_reward: 0.06544
[2025-05-07 17:17:35] nn step 26100, lr: 0.1.
	loss_policy_0: 0.28749
	accuracy_policy_0: 0.62516
	loss_value_0: 0.37005
	loss_policy_1: 0.05763
	accuracy_policy_1: 0.62262
	loss_value_1: 0.07562
	loss_reward_1: 0.00784
	loss_policy_2: 0.05783
	accuracy_policy_2: 0.62191
	loss_value_2: 0.07761
	loss_reward_2: 0.01049
	loss_policy_3: 0.05755
	accuracy_policy_3: 0.62434
	loss_value_3: 0.07931
	loss_reward_3: 0.01336
	loss_policy_4: 0.05775
	accuracy_policy_4: 0.6275
	loss_value_4: 0.0809
	loss_reward_4: 0.01627
	loss_policy_5: 0.0576
	accuracy_policy_5: 0.63211
	loss_value_5: 0.08253
	loss_reward_5: 0.01686
	loss_policy: 0.57584
	loss_value: 0.76602
	loss_reward: 0.06482
[2025-05-07 17:17:42] nn step 26150, lr: 0.1.
	loss_policy_0: 0.28139
	accuracy_policy_0: 0.63328
	loss_value_0: 0.35984
	loss_policy_1: 0.05676
	accuracy_policy_1: 0.62477
	loss_value_1: 0.07356
	loss_reward_1: 0.0076
	loss_policy_2: 0.05674
	accuracy_policy_2: 0.62996
	loss_value_2: 0.07576
	loss_reward_2: 0.01099
	loss_policy_3: 0.05664
	accuracy_policy_3: 0.63082
	loss_value_3: 0.07754
	loss_reward_3: 0.01348
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.62699
	loss_value_4: 0.07939
	loss_reward_4: 0.01602
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.62965
	loss_value_5: 0.08063
	loss_reward_5: 0.01621
	loss_policy: 0.56499
	loss_value: 0.74671
	loss_reward: 0.0643
[2025-05-07 17:17:49] nn step 26200, lr: 0.1.
	loss_policy_0: 0.28598
	accuracy_policy_0: 0.62848
	loss_value_0: 0.36185
	loss_policy_1: 0.05766
	accuracy_policy_1: 0.62359
	loss_value_1: 0.07441
	loss_reward_1: 0.00738
	loss_policy_2: 0.05766
	accuracy_policy_2: 0.62359
	loss_value_2: 0.07624
	loss_reward_2: 0.01099
	loss_policy_3: 0.05719
	accuracy_policy_3: 0.62531
	loss_value_3: 0.07816
	loss_reward_3: 0.01364
	loss_policy_4: 0.05738
	accuracy_policy_4: 0.62906
	loss_value_4: 0.08004
	loss_reward_4: 0.01637
	loss_policy_5: 0.05699
	accuracy_policy_5: 0.63367
	loss_value_5: 0.08179
	loss_reward_5: 0.01675
	loss_policy: 0.57285
	loss_value: 0.7525
	loss_reward: 0.06514
Optimization_Done 26200
[2025-05-07 17:21:25] [command] train weight_iter_26200.pkl 113 132
[2025-05-07 17:21:33] nn step 26250, lr: 0.1.
	loss_policy_0: 0.2718
	accuracy_policy_0: 0.62953
	loss_value_0: 0.35291
	loss_policy_1: 0.05495
	accuracy_policy_1: 0.62156
	loss_value_1: 0.07215
	loss_reward_1: 0.00743
	loss_policy_2: 0.0551
	accuracy_policy_2: 0.62352
	loss_value_2: 0.07379
	loss_reward_2: 0.01031
	loss_policy_3: 0.05481
	accuracy_policy_3: 0.62488
	loss_value_3: 0.0751
	loss_reward_3: 0.01299
	loss_policy_4: 0.0549
	accuracy_policy_4: 0.63012
	loss_value_4: 0.07662
	loss_reward_4: 0.0156
	loss_policy_5: 0.05496
	accuracy_policy_5: 0.63238
	loss_value_5: 0.07784
	loss_reward_5: 0.01571
	loss_policy: 0.54653
	loss_value: 0.72841
	loss_reward: 0.06205
[2025-05-07 17:21:41] nn step 26300, lr: 0.1.
	loss_policy_0: 0.30305
	accuracy_policy_0: 0.63016
	loss_value_0: 0.38729
	loss_policy_1: 0.06097
	accuracy_policy_1: 0.62445
	loss_value_1: 0.07942
	loss_reward_1: 0.00794
	loss_policy_2: 0.0616
	accuracy_policy_2: 0.61941
	loss_value_2: 0.08152
	loss_reward_2: 0.01157
	loss_policy_3: 0.06134
	accuracy_policy_3: 0.62387
	loss_value_3: 0.08371
	loss_reward_3: 0.0145
	loss_policy_4: 0.06097
	accuracy_policy_4: 0.62902
	loss_value_4: 0.08543
	loss_reward_4: 0.01678
	loss_policy_5: 0.06111
	accuracy_policy_5: 0.62898
	loss_value_5: 0.0872
	loss_reward_5: 0.01756
	loss_policy: 0.60904
	loss_value: 0.80458
	loss_reward: 0.06836
[2025-05-07 17:21:48] nn step 26350, lr: 0.1.
	loss_policy_0: 0.29672
	accuracy_policy_0: 0.62887
	loss_value_0: 0.37857
	loss_policy_1: 0.05933
	accuracy_policy_1: 0.6284
	loss_value_1: 0.07789
	loss_reward_1: 0.00801
	loss_policy_2: 0.05971
	accuracy_policy_2: 0.62594
	loss_value_2: 0.07981
	loss_reward_2: 0.01141
	loss_policy_3: 0.05962
	accuracy_policy_3: 0.62676
	loss_value_3: 0.08176
	loss_reward_3: 0.01469
	loss_policy_4: 0.05967
	accuracy_policy_4: 0.63211
	loss_value_4: 0.0836
	loss_reward_4: 0.0167
	loss_policy_5: 0.05982
	accuracy_policy_5: 0.6307
	loss_value_5: 0.08529
	loss_reward_5: 0.01721
	loss_policy: 0.59488
	loss_value: 0.78692
	loss_reward: 0.06803
[2025-05-07 17:21:57] nn step 26400, lr: 0.1.
	loss_policy_0: 0.30591
	accuracy_policy_0: 0.62965
	loss_value_0: 0.39186
	loss_policy_1: 0.06191
	accuracy_policy_1: 0.62594
	loss_value_1: 0.08048
	loss_reward_1: 0.00834
	loss_policy_2: 0.06165
	accuracy_policy_2: 0.62477
	loss_value_2: 0.08233
	loss_reward_2: 0.01186
	loss_policy_3: 0.06153
	accuracy_policy_3: 0.62781
	loss_value_3: 0.08422
	loss_reward_3: 0.01472
	loss_policy_4: 0.06129
	accuracy_policy_4: 0.6377
	loss_value_4: 0.08586
	loss_reward_4: 0.01756
	loss_policy_5: 0.06183
	accuracy_policy_5: 0.63762
	loss_value_5: 0.08788
	loss_reward_5: 0.01795
	loss_policy: 0.61412
	loss_value: 0.81263
	loss_reward: 0.07043
Optimization_Done 26400
[2025-05-07 17:25:34] [command] train weight_iter_26400.pkl 114 133
[2025-05-07 17:25:43] nn step 26450, lr: 0.1.
	loss_policy_0: 0.28518
	accuracy_policy_0: 0.63574
	loss_value_0: 0.37825
	loss_policy_1: 0.05774
	accuracy_policy_1: 0.62449
	loss_value_1: 0.07745
	loss_reward_1: 0.008
	loss_policy_2: 0.05807
	accuracy_policy_2: 0.62629
	loss_value_2: 0.07939
	loss_reward_2: 0.01111
	loss_policy_3: 0.05798
	accuracy_policy_3: 0.62953
	loss_value_3: 0.08093
	loss_reward_3: 0.0135
	loss_policy_4: 0.05818
	accuracy_policy_4: 0.62926
	loss_value_4: 0.08266
	loss_reward_4: 0.01612
	loss_policy_5: 0.05785
	accuracy_policy_5: 0.6284
	loss_value_5: 0.08432
	loss_reward_5: 0.0168
	loss_policy: 0.57499
	loss_value: 0.78299
	loss_reward: 0.06553
[2025-05-07 17:25:50] nn step 26500, lr: 0.1.
	loss_policy_0: 0.27964
	accuracy_policy_0: 0.62906
	loss_value_0: 0.36238
	loss_policy_1: 0.05648
	accuracy_policy_1: 0.62234
	loss_value_1: 0.0741
	loss_reward_1: 0.00755
	loss_policy_2: 0.05633
	accuracy_policy_2: 0.62461
	loss_value_2: 0.07607
	loss_reward_2: 0.0106
	loss_policy_3: 0.05649
	accuracy_policy_3: 0.6302
	loss_value_3: 0.07792
	loss_reward_3: 0.01313
	loss_policy_4: 0.05672
	accuracy_policy_4: 0.62566
	loss_value_4: 0.07941
	loss_reward_4: 0.0161
	loss_policy_5: 0.05642
	accuracy_policy_5: 0.63148
	loss_value_5: 0.08091
	loss_reward_5: 0.01648
	loss_policy: 0.56208
	loss_value: 0.75078
	loss_reward: 0.06386
[2025-05-07 17:25:58] nn step 26550, lr: 0.1.
	loss_policy_0: 0.30063
	accuracy_policy_0: 0.62805
	loss_value_0: 0.38464
	loss_policy_1: 0.06038
	accuracy_policy_1: 0.62258
	loss_value_1: 0.07897
	loss_reward_1: 0.00805
	loss_policy_2: 0.06042
	accuracy_policy_2: 0.6252
	loss_value_2: 0.08117
	loss_reward_2: 0.01139
	loss_policy_3: 0.05996
	accuracy_policy_3: 0.63164
	loss_value_3: 0.08319
	loss_reward_3: 0.0146
	loss_policy_4: 0.06002
	accuracy_policy_4: 0.63086
	loss_value_4: 0.08528
	loss_reward_4: 0.01695
	loss_policy_5: 0.06011
	accuracy_policy_5: 0.63332
	loss_value_5: 0.08709
	loss_reward_5: 0.01762
	loss_policy: 0.60151
	loss_value: 0.80036
	loss_reward: 0.0686
[2025-05-07 17:26:06] nn step 26600, lr: 0.1.
	loss_policy_0: 0.28132
	accuracy_policy_0: 0.63188
	loss_value_0: 0.35904
	loss_policy_1: 0.05663
	accuracy_policy_1: 0.62996
	loss_value_1: 0.07352
	loss_reward_1: 0.00784
	loss_policy_2: 0.05669
	accuracy_policy_2: 0.62602
	loss_value_2: 0.07559
	loss_reward_2: 0.0109
	loss_policy_3: 0.057
	accuracy_policy_3: 0.62344
	loss_value_3: 0.0777
	loss_reward_3: 0.01353
	loss_policy_4: 0.05671
	accuracy_policy_4: 0.6298
	loss_value_4: 0.07933
	loss_reward_4: 0.01655
	loss_policy_5: 0.05688
	accuracy_policy_5: 0.63281
	loss_value_5: 0.08074
	loss_reward_5: 0.01602
	loss_policy: 0.56523
	loss_value: 0.74591
	loss_reward: 0.06483
Optimization_Done 26600
[2025-05-07 17:29:38] [command] train weight_iter_26600.pkl 115 134
[2025-05-07 17:29:47] nn step 26650, lr: 0.1.
	loss_policy_0: 0.28473
	accuracy_policy_0: 0.62047
	loss_value_0: 0.36675
	loss_policy_1: 0.05672
	accuracy_policy_1: 0.62184
	loss_value_1: 0.07519
	loss_reward_1: 0.00783
	loss_policy_2: 0.05684
	accuracy_policy_2: 0.62621
	loss_value_2: 0.07675
	loss_reward_2: 0.01101
	loss_policy_3: 0.05681
	accuracy_policy_3: 0.62305
	loss_value_3: 0.07876
	loss_reward_3: 0.01345
	loss_policy_4: 0.0568
	accuracy_policy_4: 0.62457
	loss_value_4: 0.08037
	loss_reward_4: 0.01657
	loss_policy_5: 0.05645
	accuracy_policy_5: 0.63863
	loss_value_5: 0.08174
	loss_reward_5: 0.01676
	loss_policy: 0.56836
	loss_value: 0.75956
	loss_reward: 0.06562
[2025-05-07 17:29:55] nn step 26700, lr: 0.1.
	loss_policy_0: 0.29633
	accuracy_policy_0: 0.6325
	loss_value_0: 0.38368
	loss_policy_1: 0.05959
	accuracy_policy_1: 0.62664
	loss_value_1: 0.07868
	loss_reward_1: 0.00806
	loss_policy_2: 0.05937
	accuracy_policy_2: 0.625
	loss_value_2: 0.08029
	loss_reward_2: 0.01143
	loss_policy_3: 0.0593
	accuracy_policy_3: 0.6302
	loss_value_3: 0.08241
	loss_reward_3: 0.01418
	loss_policy_4: 0.05947
	accuracy_policy_4: 0.62762
	loss_value_4: 0.08432
	loss_reward_4: 0.01723
	loss_policy_5: 0.05947
	accuracy_policy_5: 0.63367
	loss_value_5: 0.08598
	loss_reward_5: 0.01729
	loss_policy: 0.59351
	loss_value: 0.79535
	loss_reward: 0.06819
[2025-05-07 17:30:03] nn step 26750, lr: 0.1.
	loss_policy_0: 0.27559
	accuracy_policy_0: 0.62469
	loss_value_0: 0.34963
	loss_policy_1: 0.05486
	accuracy_policy_1: 0.62621
	loss_value_1: 0.0716
	loss_reward_1: 0.00744
	loss_policy_2: 0.05503
	accuracy_policy_2: 0.62098
	loss_value_2: 0.07367
	loss_reward_2: 0.01054
	loss_policy_3: 0.05488
	accuracy_policy_3: 0.6293
	loss_value_3: 0.07556
	loss_reward_3: 0.01345
	loss_policy_4: 0.05487
	accuracy_policy_4: 0.6323
	loss_value_4: 0.07726
	loss_reward_4: 0.01581
	loss_policy_5: 0.05476
	accuracy_policy_5: 0.63492
	loss_value_5: 0.07892
	loss_reward_5: 0.01542
	loss_policy: 0.54999
	loss_value: 0.72664
	loss_reward: 0.06266
[2025-05-07 17:30:09] nn step 26800, lr: 0.1.
	loss_policy_0: 0.30959
	accuracy_policy_0: 0.62102
	loss_value_0: 0.39404
	loss_policy_1: 0.06182
	accuracy_policy_1: 0.62445
	loss_value_1: 0.08086
	loss_reward_1: 0.00827
	loss_policy_2: 0.06204
	accuracy_policy_2: 0.62434
	loss_value_2: 0.08328
	loss_reward_2: 0.01198
	loss_policy_3: 0.06168
	accuracy_policy_3: 0.62945
	loss_value_3: 0.08542
	loss_reward_3: 0.01441
	loss_policy_4: 0.06209
	accuracy_policy_4: 0.62824
	loss_value_4: 0.08707
	loss_reward_4: 0.01772
	loss_policy_5: 0.06196
	accuracy_policy_5: 0.62898
	loss_value_5: 0.08874
	loss_reward_5: 0.01802
	loss_policy: 0.61918
	loss_value: 0.8194
	loss_reward: 0.0704
Optimization_Done 26800
[2025-05-07 17:33:45] [command] train weight_iter_26800.pkl 116 135
[2025-05-07 17:33:52] nn step 26850, lr: 0.1.
	loss_policy_0: 0.29837
	accuracy_policy_0: 0.62855
	loss_value_0: 0.39106
	loss_policy_1: 0.05982
	accuracy_policy_1: 0.6323
	loss_value_1: 0.08008
	loss_reward_1: 0.00816
	loss_policy_2: 0.06
	accuracy_policy_2: 0.62703
	loss_value_2: 0.0819
	loss_reward_2: 0.01153
	loss_policy_3: 0.05986
	accuracy_policy_3: 0.62867
	loss_value_3: 0.08384
	loss_reward_3: 0.01451
	loss_policy_4: 0.06019
	accuracy_policy_4: 0.62824
	loss_value_4: 0.08573
	loss_reward_4: 0.01696
	loss_policy_5: 0.06006
	accuracy_policy_5: 0.63168
	loss_value_5: 0.0873
	loss_reward_5: 0.01764
	loss_policy: 0.59829
	loss_value: 0.80991
	loss_reward: 0.0688
[2025-05-07 17:34:00] nn step 26900, lr: 0.1.
	loss_policy_0: 0.28551
	accuracy_policy_0: 0.63188
	loss_value_0: 0.3737
	loss_policy_1: 0.05742
	accuracy_policy_1: 0.62375
	loss_value_1: 0.07676
	loss_reward_1: 0.00786
	loss_policy_2: 0.05736
	accuracy_policy_2: 0.62707
	loss_value_2: 0.07865
	loss_reward_2: 0.01096
	loss_policy_3: 0.05717
	accuracy_policy_3: 0.63035
	loss_value_3: 0.08049
	loss_reward_3: 0.01387
	loss_policy_4: 0.05754
	accuracy_policy_4: 0.62984
	loss_value_4: 0.08231
	loss_reward_4: 0.01626
	loss_policy_5: 0.05774
	accuracy_policy_5: 0.63395
	loss_value_5: 0.08367
	loss_reward_5: 0.01682
	loss_policy: 0.57274
	loss_value: 0.77559
	loss_reward: 0.06578
[2025-05-07 17:34:08] nn step 26950, lr: 0.1.
	loss_policy_0: 0.29015
	accuracy_policy_0: 0.63594
	loss_value_0: 0.37309
	loss_policy_1: 0.0586
	accuracy_policy_1: 0.62805
	loss_value_1: 0.07615
	loss_reward_1: 0.00789
	loss_policy_2: 0.05889
	accuracy_policy_2: 0.62238
	loss_value_2: 0.07803
	loss_reward_2: 0.01086
	loss_policy_3: 0.05831
	accuracy_policy_3: 0.63141
	loss_value_3: 0.08007
	loss_reward_3: 0.01414
	loss_policy_4: 0.05869
	accuracy_policy_4: 0.63211
	loss_value_4: 0.0817
	loss_reward_4: 0.0165
	loss_policy_5: 0.05869
	accuracy_policy_5: 0.63598
	loss_value_5: 0.08356
	loss_reward_5: 0.01732
	loss_policy: 0.58332
	loss_value: 0.7726
	loss_reward: 0.06671
[2025-05-07 17:34:16] nn step 27000, lr: 0.1.
	loss_policy_0: 0.26482
	accuracy_policy_0: 0.63387
	loss_value_0: 0.33795
	loss_policy_1: 0.05338
	accuracy_policy_1: 0.62602
	loss_value_1: 0.06936
	loss_reward_1: 0.00717
	loss_policy_2: 0.05308
	accuracy_policy_2: 0.62801
	loss_value_2: 0.07138
	loss_reward_2: 0.01001
	loss_policy_3: 0.05335
	accuracy_policy_3: 0.62516
	loss_value_3: 0.07359
	loss_reward_3: 0.01291
	loss_policy_4: 0.05322
	accuracy_policy_4: 0.62879
	loss_value_4: 0.07549
	loss_reward_4: 0.01479
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.63637
	loss_value_5: 0.07688
	loss_reward_5: 0.01549
	loss_policy: 0.53095
	loss_value: 0.70465
	loss_reward: 0.06037
Optimization_Done 27000
[2025-05-07 17:37:47] [command] train weight_iter_27000.pkl 117 136
[2025-05-07 17:37:56] nn step 27050, lr: 0.1.
	loss_policy_0: 0.2846
	accuracy_policy_0: 0.63699
	loss_value_0: 0.37442
	loss_policy_1: 0.05744
	accuracy_policy_1: 0.63879
	loss_value_1: 0.07689
	loss_reward_1: 0.00786
	loss_policy_2: 0.05746
	accuracy_policy_2: 0.63078
	loss_value_2: 0.07855
	loss_reward_2: 0.01072
	loss_policy_3: 0.05751
	accuracy_policy_3: 0.63789
	loss_value_3: 0.08054
	loss_reward_3: 0.01339
	loss_policy_4: 0.05752
	accuracy_policy_4: 0.63605
	loss_value_4: 0.08198
	loss_reward_4: 0.01594
	loss_policy_5: 0.05748
	accuracy_policy_5: 0.63754
	loss_value_5: 0.08361
	loss_reward_5: 0.01696
	loss_policy: 0.57201
	loss_value: 0.77598
	loss_reward: 0.06487
[2025-05-07 17:38:04] nn step 27100, lr: 0.1.
	loss_policy_0: 0.29751
	accuracy_policy_0: 0.63074
	loss_value_0: 0.38014
	loss_policy_1: 0.05911
	accuracy_policy_1: 0.63406
	loss_value_1: 0.07781
	loss_reward_1: 0.00815
	loss_policy_2: 0.05898
	accuracy_policy_2: 0.63258
	loss_value_2: 0.07998
	loss_reward_2: 0.01186
	loss_policy_3: 0.0594
	accuracy_policy_3: 0.63434
	loss_value_3: 0.08192
	loss_reward_3: 0.01428
	loss_policy_4: 0.05923
	accuracy_policy_4: 0.64113
	loss_value_4: 0.08362
	loss_reward_4: 0.0172
	loss_policy_5: 0.05924
	accuracy_policy_5: 0.63527
	loss_value_5: 0.08585
	loss_reward_5: 0.01779
	loss_policy: 0.59348
	loss_value: 0.78933
	loss_reward: 0.06928
[2025-05-07 17:38:10] nn step 27150, lr: 0.1.
	loss_policy_0: 0.28467
	accuracy_policy_0: 0.6375
	loss_value_0: 0.36869
	loss_policy_1: 0.05726
	accuracy_policy_1: 0.63223
	loss_value_1: 0.07542
	loss_reward_1: 0.0079
	loss_policy_2: 0.05749
	accuracy_policy_2: 0.63109
	loss_value_2: 0.07742
	loss_reward_2: 0.01133
	loss_policy_3: 0.05702
	accuracy_policy_3: 0.63949
	loss_value_3: 0.07942
	loss_reward_3: 0.0139
	loss_policy_4: 0.05761
	accuracy_policy_4: 0.6323
	loss_value_4: 0.08113
	loss_reward_4: 0.01662
	loss_policy_5: 0.05737
	accuracy_policy_5: 0.64141
	loss_value_5: 0.08248
	loss_reward_5: 0.01693
	loss_policy: 0.57142
	loss_value: 0.76456
	loss_reward: 0.06668
[2025-05-07 17:38:18] nn step 27200, lr: 0.1.
	loss_policy_0: 0.27068
	accuracy_policy_0: 0.63773
	loss_value_0: 0.34733
	loss_policy_1: 0.05442
	accuracy_policy_1: 0.63598
	loss_value_1: 0.07138
	loss_reward_1: 0.00738
	loss_policy_2: 0.05428
	accuracy_policy_2: 0.63438
	loss_value_2: 0.07312
	loss_reward_2: 0.0103
	loss_policy_3: 0.05422
	accuracy_policy_3: 0.63195
	loss_value_3: 0.07477
	loss_reward_3: 0.01308
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.63273
	loss_value_4: 0.07649
	loss_reward_4: 0.01503
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.64055
	loss_value_5: 0.07825
	loss_reward_5: 0.0158
	loss_policy: 0.54228
	loss_value: 0.72133
	loss_reward: 0.0616
Optimization_Done 27200
[2025-05-07 17:41:43] [command] train weight_iter_27200.pkl 118 137
[2025-05-07 17:41:52] nn step 27250, lr: 0.1.
	loss_policy_0: 0.27395
	accuracy_policy_0: 0.63727
	loss_value_0: 0.35699
	loss_policy_1: 0.05509
	accuracy_policy_1: 0.63664
	loss_value_1: 0.07309
	loss_reward_1: 0.0076
	loss_policy_2: 0.0554
	accuracy_policy_2: 0.63613
	loss_value_2: 0.07519
	loss_reward_2: 0.01067
	loss_policy_3: 0.05543
	accuracy_policy_3: 0.63562
	loss_value_3: 0.07693
	loss_reward_3: 0.0131
	loss_policy_4: 0.05532
	accuracy_policy_4: 0.64078
	loss_value_4: 0.07839
	loss_reward_4: 0.01553
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.63648
	loss_value_5: 0.0798
	loss_reward_5: 0.0161
	loss_policy: 0.55062
	loss_value: 0.7404
	loss_reward: 0.063
[2025-05-07 17:41:59] nn step 27300, lr: 0.1.
	loss_policy_0: 0.28455
	accuracy_policy_0: 0.6398
	loss_value_0: 0.37038
	loss_policy_1: 0.05747
	accuracy_policy_1: 0.63566
	loss_value_1: 0.07588
	loss_reward_1: 0.0078
	loss_policy_2: 0.05723
	accuracy_policy_2: 0.63418
	loss_value_2: 0.07767
	loss_reward_2: 0.0115
	loss_policy_3: 0.05778
	accuracy_policy_3: 0.6366
	loss_value_3: 0.07967
	loss_reward_3: 0.01408
	loss_policy_4: 0.05751
	accuracy_policy_4: 0.6377
	loss_value_4: 0.08168
	loss_reward_4: 0.01622
	loss_policy_5: 0.05784
	accuracy_policy_5: 0.63617
	loss_value_5: 0.08331
	loss_reward_5: 0.01718
	loss_policy: 0.57238
	loss_value: 0.76858
	loss_reward: 0.06679
[2025-05-07 17:42:07] nn step 27350, lr: 0.1.
	loss_policy_0: 0.28569
	accuracy_policy_0: 0.63395
	loss_value_0: 0.36551
	loss_policy_1: 0.05706
	accuracy_policy_1: 0.63473
	loss_value_1: 0.07495
	loss_reward_1: 0.00752
	loss_policy_2: 0.05684
	accuracy_policy_2: 0.63438
	loss_value_2: 0.07704
	loss_reward_2: 0.01087
	loss_policy_3: 0.05705
	accuracy_policy_3: 0.63605
	loss_value_3: 0.07886
	loss_reward_3: 0.01375
	loss_policy_4: 0.05716
	accuracy_policy_4: 0.63316
	loss_value_4: 0.0808
	loss_reward_4: 0.01598
	loss_policy_5: 0.05719
	accuracy_policy_5: 0.63977
	loss_value_5: 0.08246
	loss_reward_5: 0.01663
	loss_policy: 0.571
	loss_value: 0.75962
	loss_reward: 0.06474
[2025-05-07 17:42:15] nn step 27400, lr: 0.1.
	loss_policy_0: 0.27996
	accuracy_policy_0: 0.63691
	loss_value_0: 0.35828
	loss_policy_1: 0.05621
	accuracy_policy_1: 0.63801
	loss_value_1: 0.07365
	loss_reward_1: 0.00778
	loss_policy_2: 0.05652
	accuracy_policy_2: 0.63395
	loss_value_2: 0.0759
	loss_reward_2: 0.01091
	loss_policy_3: 0.05655
	accuracy_policy_3: 0.63449
	loss_value_3: 0.07779
	loss_reward_3: 0.01323
	loss_policy_4: 0.05653
	accuracy_policy_4: 0.63941
	loss_value_4: 0.07932
	loss_reward_4: 0.01602
	loss_policy_5: 0.05626
	accuracy_policy_5: 0.64156
	loss_value_5: 0.08103
	loss_reward_5: 0.01606
	loss_policy: 0.56203
	loss_value: 0.74596
	loss_reward: 0.064
Optimization_Done 27400
[2025-05-07 17:45:41] [command] train weight_iter_27400.pkl 119 138
[2025-05-07 17:45:48] nn step 27450, lr: 0.1.
	loss_policy_0: 0.25904
	accuracy_policy_0: 0.64332
	loss_value_0: 0.34724
	loss_policy_1: 0.05224
	accuracy_policy_1: 0.63801
	loss_value_1: 0.07104
	loss_reward_1: 0.00691
	loss_policy_2: 0.05216
	accuracy_policy_2: 0.63695
	loss_value_2: 0.07272
	loss_reward_2: 0.00973
	loss_policy_3: 0.05225
	accuracy_policy_3: 0.63691
	loss_value_3: 0.0743
	loss_reward_3: 0.01209
	loss_policy_4: 0.05218
	accuracy_policy_4: 0.64219
	loss_value_4: 0.07567
	loss_reward_4: 0.01473
	loss_policy_5: 0.05224
	accuracy_policy_5: 0.64668
	loss_value_5: 0.0771
	loss_reward_5: 0.01534
	loss_policy: 0.52011
	loss_value: 0.71807
	loss_reward: 0.0588
[2025-05-07 17:45:56] nn step 27500, lr: 0.1.
	loss_policy_0: 0.26025
	accuracy_policy_0: 0.64125
	loss_value_0: 0.33871
	loss_policy_1: 0.0523
	accuracy_policy_1: 0.64082
	loss_value_1: 0.06921
	loss_reward_1: 0.00725
	loss_policy_2: 0.05236
	accuracy_policy_2: 0.63742
	loss_value_2: 0.07108
	loss_reward_2: 0.00971
	loss_policy_3: 0.05258
	accuracy_policy_3: 0.6398
	loss_value_3: 0.07266
	loss_reward_3: 0.01237
	loss_policy_4: 0.05218
	accuracy_policy_4: 0.64176
	loss_value_4: 0.07426
	loss_reward_4: 0.01439
	loss_policy_5: 0.05224
	accuracy_policy_5: 0.64699
	loss_value_5: 0.07595
	loss_reward_5: 0.01524
	loss_policy: 0.52191
	loss_value: 0.70187
	loss_reward: 0.05897
[2025-05-07 17:46:04] nn step 27550, lr: 0.1.
	loss_policy_0: 0.30148
	accuracy_policy_0: 0.63684
	loss_value_0: 0.39043
	loss_policy_1: 0.06026
	accuracy_policy_1: 0.63598
	loss_value_1: 0.07991
	loss_reward_1: 0.00848
	loss_policy_2: 0.06053
	accuracy_policy_2: 0.63543
	loss_value_2: 0.08239
	loss_reward_2: 0.01163
	loss_policy_3: 0.06068
	accuracy_policy_3: 0.63863
	loss_value_3: 0.08436
	loss_reward_3: 0.01464
	loss_policy_4: 0.06082
	accuracy_policy_4: 0.63656
	loss_value_4: 0.08585
	loss_reward_4: 0.01747
	loss_policy_5: 0.06053
	accuracy_policy_5: 0.63875
	loss_value_5: 0.08765
	loss_reward_5: 0.01802
	loss_policy: 0.6043
	loss_value: 0.81059
	loss_reward: 0.07025
[2025-05-07 17:46:12] nn step 27600, lr: 0.1.
	loss_policy_0: 0.27673
	accuracy_policy_0: 0.64398
	loss_value_0: 0.36278
	loss_policy_1: 0.05587
	accuracy_policy_1: 0.64223
	loss_value_1: 0.07449
	loss_reward_1: 0.00777
	loss_policy_2: 0.05621
	accuracy_policy_2: 0.63793
	loss_value_2: 0.07663
	loss_reward_2: 0.01077
	loss_policy_3: 0.05661
	accuracy_policy_3: 0.63543
	loss_value_3: 0.07859
	loss_reward_3: 0.0133
	loss_policy_4: 0.05602
	accuracy_policy_4: 0.64285
	loss_value_4: 0.08057
	loss_reward_4: 0.01638
	loss_policy_5: 0.05617
	accuracy_policy_5: 0.65066
	loss_value_5: 0.08202
	loss_reward_5: 0.01613
	loss_policy: 0.55761
	loss_value: 0.75509
	loss_reward: 0.06436
Optimization_Done 27600
[2025-05-07 17:49:37] [command] train weight_iter_27600.pkl 120 139
[2025-05-07 17:49:46] nn step 27650, lr: 0.1.
	loss_policy_0: 0.30799
	accuracy_policy_0: 0.61535
	loss_value_0: 0.39259
	loss_policy_1: 0.05966
	accuracy_policy_1: 0.63715
	loss_value_1: 0.08031
	loss_reward_1: 0.00853
	loss_policy_2: 0.05921
	accuracy_policy_2: 0.63562
	loss_value_2: 0.08256
	loss_reward_2: 0.01171
	loss_policy_3: 0.05927
	accuracy_policy_3: 0.63836
	loss_value_3: 0.08427
	loss_reward_3: 0.01435
	loss_policy_4: 0.05935
	accuracy_policy_4: 0.64207
	loss_value_4: 0.08584
	loss_reward_4: 0.01764
	loss_policy_5: 0.05894
	accuracy_policy_5: 0.64625
	loss_value_5: 0.08753
	loss_reward_5: 0.01724
	loss_policy: 0.60441
	loss_value: 0.81309
	loss_reward: 0.06947
[2025-05-07 17:49:54] nn step 27700, lr: 0.1.
	loss_policy_0: 0.2723
	accuracy_policy_0: 0.64246
	loss_value_0: 0.35498
	loss_policy_1: 0.05456
	accuracy_policy_1: 0.64547
	loss_value_1: 0.07252
	loss_reward_1: 0.00731
	loss_policy_2: 0.05418
	accuracy_policy_2: 0.64117
	loss_value_2: 0.07445
	loss_reward_2: 0.01095
	loss_policy_3: 0.05426
	accuracy_policy_3: 0.64375
	loss_value_3: 0.07618
	loss_reward_3: 0.01307
	loss_policy_4: 0.05424
	accuracy_policy_4: 0.64613
	loss_value_4: 0.07803
	loss_reward_4: 0.01526
	loss_policy_5: 0.05425
	accuracy_policy_5: 0.64539
	loss_value_5: 0.07953
	loss_reward_5: 0.01601
	loss_policy: 0.54379
	loss_value: 0.73569
	loss_reward: 0.0626
[2025-05-07 17:50:02] nn step 27750, lr: 0.1.
	loss_policy_0: 0.29527
	accuracy_policy_0: 0.64652
	loss_value_0: 0.38377
	loss_policy_1: 0.05876
	accuracy_policy_1: 0.64676
	loss_value_1: 0.07865
	loss_reward_1: 0.00815
	loss_policy_2: 0.05945
	accuracy_policy_2: 0.63973
	loss_value_2: 0.08061
	loss_reward_2: 0.01111
	loss_policy_3: 0.05901
	accuracy_policy_3: 0.64078
	loss_value_3: 0.08289
	loss_reward_3: 0.01431
	loss_policy_4: 0.05925
	accuracy_policy_4: 0.64164
	loss_value_4: 0.08442
	loss_reward_4: 0.01705
	loss_policy_5: 0.05915
	accuracy_policy_5: 0.64535
	loss_value_5: 0.08625
	loss_reward_5: 0.01763
	loss_policy: 0.59088
	loss_value: 0.79658
	loss_reward: 0.06826
[2025-05-07 17:50:09] nn step 27800, lr: 0.1.
	loss_policy_0: 0.27205
	accuracy_policy_0: 0.64004
	loss_value_0: 0.3537
	loss_policy_1: 0.05465
	accuracy_policy_1: 0.63859
	loss_value_1: 0.0724
	loss_reward_1: 0.00749
	loss_policy_2: 0.05412
	accuracy_policy_2: 0.64555
	loss_value_2: 0.0746
	loss_reward_2: 0.01083
	loss_policy_3: 0.0545
	accuracy_policy_3: 0.64309
	loss_value_3: 0.07653
	loss_reward_3: 0.01371
	loss_policy_4: 0.05439
	accuracy_policy_4: 0.6459
	loss_value_4: 0.0785
	loss_reward_4: 0.01552
	loss_policy_5: 0.05441
	accuracy_policy_5: 0.64648
	loss_value_5: 0.08017
	loss_reward_5: 0.01633
	loss_policy: 0.54412
	loss_value: 0.73589
	loss_reward: 0.06387
Optimization_Done 27800
[2025-05-07 17:53:40] [command] train weight_iter_27800.pkl 121 140
[2025-05-07 17:53:49] nn step 27850, lr: 0.1.
	loss_policy_0: 0.29119
	accuracy_policy_0: 0.64633
	loss_value_0: 0.38847
	loss_policy_1: 0.05852
	accuracy_policy_1: 0.64141
	loss_value_1: 0.07933
	loss_reward_1: 0.00796
	loss_policy_2: 0.05851
	accuracy_policy_2: 0.63867
	loss_value_2: 0.0813
	loss_reward_2: 0.01119
	loss_policy_3: 0.05853
	accuracy_policy_3: 0.64238
	loss_value_3: 0.08291
	loss_reward_3: 0.01465
	loss_policy_4: 0.05851
	accuracy_policy_4: 0.64324
	loss_value_4: 0.08441
	loss_reward_4: 0.01729
	loss_policy_5: 0.05817
	accuracy_policy_5: 0.64855
	loss_value_5: 0.08596
	loss_reward_5: 0.0166
	loss_policy: 0.58343
	loss_value: 0.80239
	loss_reward: 0.06769
[2025-05-07 17:53:55] nn step 27900, lr: 0.1.
	loss_policy_0: 0.28141
	accuracy_policy_0: 0.64176
	loss_value_0: 0.36638
	loss_policy_1: 0.05647
	accuracy_policy_1: 0.64145
	loss_value_1: 0.07488
	loss_reward_1: 0.00806
	loss_policy_2: 0.05653
	accuracy_policy_2: 0.63543
	loss_value_2: 0.07701
	loss_reward_2: 0.01098
	loss_policy_3: 0.05623
	accuracy_policy_3: 0.64707
	loss_value_3: 0.07894
	loss_reward_3: 0.01349
	loss_policy_4: 0.05693
	accuracy_policy_4: 0.63918
	loss_value_4: 0.08093
	loss_reward_4: 0.01655
	loss_policy_5: 0.05667
	accuracy_policy_5: 0.64238
	loss_value_5: 0.08261
	loss_reward_5: 0.01679
	loss_policy: 0.56424
	loss_value: 0.76075
	loss_reward: 0.06588
[2025-05-07 17:54:03] nn step 27950, lr: 0.1.
	loss_policy_0: 0.28026
	accuracy_policy_0: 0.64477
	loss_value_0: 0.36387
	loss_policy_1: 0.05622
	accuracy_policy_1: 0.63883
	loss_value_1: 0.07466
	loss_reward_1: 0.00771
	loss_policy_2: 0.05632
	accuracy_policy_2: 0.63492
	loss_value_2: 0.07668
	loss_reward_2: 0.01025
	loss_policy_3: 0.05588
	accuracy_policy_3: 0.65043
	loss_value_3: 0.07882
	loss_reward_3: 0.01325
	loss_policy_4: 0.05615
	accuracy_policy_4: 0.64938
	loss_value_4: 0.08053
	loss_reward_4: 0.01593
	loss_policy_5: 0.0565
	accuracy_policy_5: 0.64164
	loss_value_5: 0.08242
	loss_reward_5: 0.01647
	loss_policy: 0.56134
	loss_value: 0.75697
	loss_reward: 0.0636
[2025-05-07 17:54:11] nn step 28000, lr: 0.1.
	loss_policy_0: 0.28674
	accuracy_policy_0: 0.64035
	loss_value_0: 0.37485
	loss_policy_1: 0.05711
	accuracy_policy_1: 0.645
	loss_value_1: 0.07678
	loss_reward_1: 0.0082
	loss_policy_2: 0.05765
	accuracy_policy_2: 0.64066
	loss_value_2: 0.07846
	loss_reward_2: 0.01098
	loss_policy_3: 0.0577
	accuracy_policy_3: 0.64262
	loss_value_3: 0.08047
	loss_reward_3: 0.01379
	loss_policy_4: 0.05729
	accuracy_policy_4: 0.64852
	loss_value_4: 0.08254
	loss_reward_4: 0.01657
	loss_policy_5: 0.05746
	accuracy_policy_5: 0.64652
	loss_value_5: 0.08437
	loss_reward_5: 0.01729
	loss_policy: 0.57394
	loss_value: 0.77747
	loss_reward: 0.06683
Optimization_Done 28000
[2025-05-07 17:57:29] [command] train weight_iter_28000.pkl 122 141
[2025-05-07 17:57:38] nn step 28050, lr: 0.1.
	loss_policy_0: 0.29576
	accuracy_policy_0: 0.63531
	loss_value_0: 0.39008
	loss_policy_1: 0.05918
	accuracy_policy_1: 0.63723
	loss_value_1: 0.07946
	loss_reward_1: 0.00837
	loss_policy_2: 0.05934
	accuracy_policy_2: 0.63363
	loss_value_2: 0.08149
	loss_reward_2: 0.01179
	loss_policy_3: 0.05943
	accuracy_policy_3: 0.63668
	loss_value_3: 0.0832
	loss_reward_3: 0.01474
	loss_policy_4: 0.05907
	accuracy_policy_4: 0.64434
	loss_value_4: 0.08451
	loss_reward_4: 0.0168
	loss_policy_5: 0.05942
	accuracy_policy_5: 0.63941
	loss_value_5: 0.08594
	loss_reward_5: 0.01741
	loss_policy: 0.59221
	loss_value: 0.80467
	loss_reward: 0.0691
[2025-05-07 17:57:44] nn step 28100, lr: 0.1.
	loss_policy_0: 0.27066
	accuracy_policy_0: 0.6377
	loss_value_0: 0.35074
	loss_policy_1: 0.05403
	accuracy_policy_1: 0.64105
	loss_value_1: 0.07204
	loss_reward_1: 0.0073
	loss_policy_2: 0.05411
	accuracy_policy_2: 0.63516
	loss_value_2: 0.07355
	loss_reward_2: 0.00997
	loss_policy_3: 0.05435
	accuracy_policy_3: 0.63113
	loss_value_3: 0.07548
	loss_reward_3: 0.01276
	loss_policy_4: 0.05435
	accuracy_policy_4: 0.63938
	loss_value_4: 0.07724
	loss_reward_4: 0.01525
	loss_policy_5: 0.05466
	accuracy_policy_5: 0.63906
	loss_value_5: 0.0787
	loss_reward_5: 0.01567
	loss_policy: 0.54216
	loss_value: 0.72775
	loss_reward: 0.06095
[2025-05-07 17:57:52] nn step 28150, lr: 0.1.
	loss_policy_0: 0.29274
	accuracy_policy_0: 0.63469
	loss_value_0: 0.37543
	loss_policy_1: 0.05806
	accuracy_policy_1: 0.63766
	loss_value_1: 0.07718
	loss_reward_1: 0.00803
	loss_policy_2: 0.05817
	accuracy_policy_2: 0.63836
	loss_value_2: 0.0791
	loss_reward_2: 0.01088
	loss_policy_3: 0.05857
	accuracy_policy_3: 0.63543
	loss_value_3: 0.08084
	loss_reward_3: 0.01425
	loss_policy_4: 0.05838
	accuracy_policy_4: 0.63668
	loss_value_4: 0.08215
	loss_reward_4: 0.0165
	loss_policy_5: 0.05871
	accuracy_policy_5: 0.6368
	loss_value_5: 0.084
	loss_reward_5: 0.01692
	loss_policy: 0.58463
	loss_value: 0.7787
	loss_reward: 0.06658
[2025-05-07 17:58:00] nn step 28200, lr: 0.1.
	loss_policy_0: 0.29436
	accuracy_policy_0: 0.63926
	loss_value_0: 0.37794
	loss_policy_1: 0.05891
	accuracy_policy_1: 0.63238
	loss_value_1: 0.07759
	loss_reward_1: 0.00787
	loss_policy_2: 0.05872
	accuracy_policy_2: 0.63668
	loss_value_2: 0.07985
	loss_reward_2: 0.01114
	loss_policy_3: 0.05876
	accuracy_policy_3: 0.63473
	loss_value_3: 0.08211
	loss_reward_3: 0.01399
	loss_policy_4: 0.05859
	accuracy_policy_4: 0.64109
	loss_value_4: 0.08367
	loss_reward_4: 0.0167
	loss_policy_5: 0.05916
	accuracy_policy_5: 0.63785
	loss_value_5: 0.08572
	loss_reward_5: 0.01677
	loss_policy: 0.58849
	loss_value: 0.78689
	loss_reward: 0.06647
Optimization_Done 28200
[2025-05-07 18:01:31] [command] train weight_iter_28200.pkl 123 142
[2025-05-07 18:01:41] nn step 28250, lr: 0.1.
	loss_policy_0: 0.30036
	accuracy_policy_0: 0.63996
	loss_value_0: 0.39775
	loss_policy_1: 0.06108
	accuracy_policy_1: 0.63055
	loss_value_1: 0.08131
	loss_reward_1: 0.00836
	loss_policy_2: 0.06137
	accuracy_policy_2: 0.62586
	loss_value_2: 0.08349
	loss_reward_2: 0.01146
	loss_policy_3: 0.0611
	accuracy_policy_3: 0.62879
	loss_value_3: 0.08555
	loss_reward_3: 0.01488
	loss_policy_4: 0.06126
	accuracy_policy_4: 0.63113
	loss_value_4: 0.08726
	loss_reward_4: 0.01784
	loss_policy_5: 0.06121
	accuracy_policy_5: 0.63246
	loss_value_5: 0.0892
	loss_reward_5: 0.01751
	loss_policy: 0.60638
	loss_value: 0.82456
	loss_reward: 0.07004
[2025-05-07 18:01:49] nn step 28300, lr: 0.1.
	loss_policy_0: 0.29074
	accuracy_policy_0: 0.63309
	loss_value_0: 0.377
	loss_policy_1: 0.05832
	accuracy_policy_1: 0.62723
	loss_value_1: 0.07738
	loss_reward_1: 0.00792
	loss_policy_2: 0.0584
	accuracy_policy_2: 0.63355
	loss_value_2: 0.07986
	loss_reward_2: 0.01112
	loss_policy_3: 0.0583
	accuracy_policy_3: 0.63816
	loss_value_3: 0.08163
	loss_reward_3: 0.01376
	loss_policy_4: 0.05833
	accuracy_policy_4: 0.63855
	loss_value_4: 0.08324
	loss_reward_4: 0.01643
	loss_policy_5: 0.05847
	accuracy_policy_5: 0.6359
	loss_value_5: 0.08467
	loss_reward_5: 0.01688
	loss_policy: 0.58257
	loss_value: 0.78378
	loss_reward: 0.06612
[2025-05-07 18:01:55] nn step 28350, lr: 0.1.
	loss_policy_0: 0.27802
	accuracy_policy_0: 0.63719
	loss_value_0: 0.3609
	loss_policy_1: 0.05581
	accuracy_policy_1: 0.63125
	loss_value_1: 0.07379
	loss_reward_1: 0.00761
	loss_policy_2: 0.05626
	accuracy_policy_2: 0.63062
	loss_value_2: 0.07593
	loss_reward_2: 0.0108
	loss_policy_3: 0.05615
	accuracy_policy_3: 0.62957
	loss_value_3: 0.0776
	loss_reward_3: 0.01296
	loss_policy_4: 0.05603
	accuracy_policy_4: 0.63176
	loss_value_4: 0.07953
	loss_reward_4: 0.01548
	loss_policy_5: 0.05586
	accuracy_policy_5: 0.63938
	loss_value_5: 0.08115
	loss_reward_5: 0.01665
	loss_policy: 0.55813
	loss_value: 0.74891
	loss_reward: 0.0635
[2025-05-07 18:02:03] nn step 28400, lr: 0.1.
	loss_policy_0: 0.28358
	accuracy_policy_0: 0.6318
	loss_value_0: 0.36268
	loss_policy_1: 0.05683
	accuracy_policy_1: 0.63461
	loss_value_1: 0.07417
	loss_reward_1: 0.00776
	loss_policy_2: 0.05706
	accuracy_policy_2: 0.63535
	loss_value_2: 0.07616
	loss_reward_2: 0.01055
	loss_policy_3: 0.05676
	accuracy_policy_3: 0.63516
	loss_value_3: 0.07834
	loss_reward_3: 0.01379
	loss_policy_4: 0.05662
	accuracy_policy_4: 0.64289
	loss_value_4: 0.0798
	loss_reward_4: 0.01592
	loss_policy_5: 0.05698
	accuracy_policy_5: 0.6402
	loss_value_5: 0.08141
	loss_reward_5: 0.01635
	loss_policy: 0.56782
	loss_value: 0.75255
	loss_reward: 0.06436
Optimization_Done 28400
[2025-05-07 18:05:32] [command] train weight_iter_28400.pkl 124 143
[2025-05-07 18:05:41] nn step 28450, lr: 0.1.
	loss_policy_0: 0.28866
	accuracy_policy_0: 0.63965
	loss_value_0: 0.38345
	loss_policy_1: 0.05831
	accuracy_policy_1: 0.63227
	loss_value_1: 0.07849
	loss_reward_1: 0.00807
	loss_policy_2: 0.05845
	accuracy_policy_2: 0.62703
	loss_value_2: 0.08034
	loss_reward_2: 0.01093
	loss_policy_3: 0.05824
	accuracy_policy_3: 0.63047
	loss_value_3: 0.08198
	loss_reward_3: 0.01408
	loss_policy_4: 0.05817
	accuracy_policy_4: 0.63613
	loss_value_4: 0.08383
	loss_reward_4: 0.01663
	loss_policy_5: 0.0582
	accuracy_policy_5: 0.63836
	loss_value_5: 0.08572
	loss_reward_5: 0.0165
	loss_policy: 0.58003
	loss_value: 0.79381
	loss_reward: 0.06621
[2025-05-07 18:05:48] nn step 28500, lr: 0.1.
	loss_policy_0: 0.3004
	accuracy_policy_0: 0.62926
	loss_value_0: 0.38478
	loss_policy_1: 0.05948
	accuracy_policy_1: 0.63402
	loss_value_1: 0.07869
	loss_reward_1: 0.0078
	loss_policy_2: 0.05936
	accuracy_policy_2: 0.63488
	loss_value_2: 0.08053
	loss_reward_2: 0.01146
	loss_policy_3: 0.05937
	accuracy_policy_3: 0.635
	loss_value_3: 0.08259
	loss_reward_3: 0.01446
	loss_policy_4: 0.05927
	accuracy_policy_4: 0.63875
	loss_value_4: 0.08448
	loss_reward_4: 0.01683
	loss_policy_5: 0.05919
	accuracy_policy_5: 0.63859
	loss_value_5: 0.08617
	loss_reward_5: 0.01765
	loss_policy: 0.59707
	loss_value: 0.79724
	loss_reward: 0.06821
[2025-05-07 18:05:56] nn step 28550, lr: 0.1.
	loss_policy_0: 0.29222
	accuracy_policy_0: 0.63234
	loss_value_0: 0.3776
	loss_policy_1: 0.05845
	accuracy_policy_1: 0.63164
	loss_value_1: 0.0772
	loss_reward_1: 0.00793
	loss_policy_2: 0.05792
	accuracy_policy_2: 0.63809
	loss_value_2: 0.07955
	loss_reward_2: 0.01105
	loss_policy_3: 0.05821
	accuracy_policy_3: 0.63371
	loss_value_3: 0.08133
	loss_reward_3: 0.0136
	loss_policy_4: 0.05794
	accuracy_policy_4: 0.63645
	loss_value_4: 0.08303
	loss_reward_4: 0.01601
	loss_policy_5: 0.05828
	accuracy_policy_5: 0.63551
	loss_value_5: 0.08468
	loss_reward_5: 0.01705
	loss_policy: 0.58303
	loss_value: 0.7834
	loss_reward: 0.06564
[2025-05-07 18:06:04] nn step 28600, lr: 0.1.
	loss_policy_0: 0.24772
	accuracy_policy_0: 0.6377
	loss_value_0: 0.319
	loss_policy_1: 0.0496
	accuracy_policy_1: 0.63836
	loss_value_1: 0.06535
	loss_reward_1: 0.00639
	loss_policy_2: 0.04974
	accuracy_policy_2: 0.63746
	loss_value_2: 0.06728
	loss_reward_2: 0.00909
	loss_policy_3: 0.05008
	accuracy_policy_3: 0.63414
	loss_value_3: 0.06877
	loss_reward_3: 0.01176
	loss_policy_4: 0.04962
	accuracy_policy_4: 0.64004
	loss_value_4: 0.07021
	loss_reward_4: 0.01368
	loss_policy_5: 0.05002
	accuracy_policy_5: 0.64016
	loss_value_5: 0.07162
	loss_reward_5: 0.01399
	loss_policy: 0.49678
	loss_value: 0.66223
	loss_reward: 0.0549
Optimization_Done 28600
[2025-05-07 18:09:31] [command] train weight_iter_28600.pkl 125 144
[2025-05-07 18:09:41] nn step 28650, lr: 0.1.
	loss_policy_0: 0.26112
	accuracy_policy_0: 0.64137
	loss_value_0: 0.34584
	loss_policy_1: 0.05307
	accuracy_policy_1: 0.63035
	loss_value_1: 0.07083
	loss_reward_1: 0.00742
	loss_policy_2: 0.05281
	accuracy_policy_2: 0.62801
	loss_value_2: 0.07215
	loss_reward_2: 0.00989
	loss_policy_3: 0.05279
	accuracy_policy_3: 0.63414
	loss_value_3: 0.07373
	loss_reward_3: 0.01207
	loss_policy_4: 0.05278
	accuracy_policy_4: 0.63777
	loss_value_4: 0.07496
	loss_reward_4: 0.01518
	loss_policy_5: 0.05289
	accuracy_policy_5: 0.63645
	loss_value_5: 0.07611
	loss_reward_5: 0.01513
	loss_policy: 0.52546
	loss_value: 0.71362
	loss_reward: 0.0597
[2025-05-07 18:09:49] nn step 28700, lr: 0.1.
	loss_policy_0: 0.28585
	accuracy_policy_0: 0.63801
	loss_value_0: 0.37416
	loss_policy_1: 0.05722
	accuracy_policy_1: 0.6316
	loss_value_1: 0.07662
	loss_reward_1: 0.00771
	loss_policy_2: 0.05773
	accuracy_policy_2: 0.6332
	loss_value_2: 0.07854
	loss_reward_2: 0.01051
	loss_policy_3: 0.05792
	accuracy_policy_3: 0.63602
	loss_value_3: 0.08021
	loss_reward_3: 0.01346
	loss_policy_4: 0.05766
	accuracy_policy_4: 0.63871
	loss_value_4: 0.08187
	loss_reward_4: 0.01568
	loss_policy_5: 0.05781
	accuracy_policy_5: 0.64219
	loss_value_5: 0.08386
	loss_reward_5: 0.01611
	loss_policy: 0.5742
	loss_value: 0.77525
	loss_reward: 0.06348
[2025-05-07 18:09:57] nn step 28750, lr: 0.1.
	loss_policy_0: 0.26879
	accuracy_policy_0: 0.6352
	loss_value_0: 0.34813
	loss_policy_1: 0.05411
	accuracy_policy_1: 0.63336
	loss_value_1: 0.07147
	loss_reward_1: 0.00742
	loss_policy_2: 0.05426
	accuracy_policy_2: 0.63559
	loss_value_2: 0.0734
	loss_reward_2: 0.01
	loss_policy_3: 0.05417
	accuracy_policy_3: 0.63375
	loss_value_3: 0.07532
	loss_reward_3: 0.01239
	loss_policy_4: 0.05414
	accuracy_policy_4: 0.63906
	loss_value_4: 0.07699
	loss_reward_4: 0.01516
	loss_policy_5: 0.05403
	accuracy_policy_5: 0.64008
	loss_value_5: 0.07847
	loss_reward_5: 0.01505
	loss_policy: 0.5395
	loss_value: 0.72378
	loss_reward: 0.06003
[2025-05-07 18:10:04] nn step 28800, lr: 0.1.
	loss_policy_0: 0.28435
	accuracy_policy_0: 0.64035
	loss_value_0: 0.36864
	loss_policy_1: 0.057
	accuracy_policy_1: 0.63652
	loss_value_1: 0.07536
	loss_reward_1: 0.00767
	loss_policy_2: 0.05705
	accuracy_policy_2: 0.63914
	loss_value_2: 0.07746
	loss_reward_2: 0.01089
	loss_policy_3: 0.05707
	accuracy_policy_3: 0.63609
	loss_value_3: 0.07891
	loss_reward_3: 0.01377
	loss_policy_4: 0.05713
	accuracy_policy_4: 0.63695
	loss_value_4: 0.08085
	loss_reward_4: 0.01561
	loss_policy_5: 0.05749
	accuracy_policy_5: 0.63859
	loss_value_5: 0.08245
	loss_reward_5: 0.01648
	loss_policy: 0.5701
	loss_value: 0.76369
	loss_reward: 0.06442
Optimization_Done 28800
[2025-05-07 18:13:27] [command] train weight_iter_28800.pkl 126 145
[2025-05-07 18:13:37] nn step 28850, lr: 0.1.
	loss_policy_0: 0.27488
	accuracy_policy_0: 0.65062
	loss_value_0: 0.37349
	loss_policy_1: 0.05551
	accuracy_policy_1: 0.64348
	loss_value_1: 0.07571
	loss_reward_1: 0.00733
	loss_policy_2: 0.05563
	accuracy_policy_2: 0.64191
	loss_value_2: 0.07739
	loss_reward_2: 0.01029
	loss_policy_3: 0.05553
	accuracy_policy_3: 0.64996
	loss_value_3: 0.0791
	loss_reward_3: 0.0126
	loss_policy_4: 0.05588
	accuracy_policy_4: 0.64801
	loss_value_4: 0.08064
	loss_reward_4: 0.01538
	loss_policy_5: 0.05581
	accuracy_policy_5: 0.65137
	loss_value_5: 0.08193
	loss_reward_5: 0.01585
	loss_policy: 0.55323
	loss_value: 0.76825
	loss_reward: 0.06145
[2025-05-07 18:13:45] nn step 28900, lr: 0.1.
	loss_policy_0: 0.28289
	accuracy_policy_0: 0.65242
	loss_value_0: 0.37872
	loss_policy_1: 0.05706
	accuracy_policy_1: 0.6491
	loss_value_1: 0.07683
	loss_reward_1: 0.00801
	loss_policy_2: 0.05737
	accuracy_policy_2: 0.64602
	loss_value_2: 0.07922
	loss_reward_2: 0.01135
	loss_policy_3: 0.05729
	accuracy_policy_3: 0.64484
	loss_value_3: 0.08108
	loss_reward_3: 0.01324
	loss_policy_4: 0.05738
	accuracy_policy_4: 0.64754
	loss_value_4: 0.0829
	loss_reward_4: 0.01611
	loss_policy_5: 0.05736
	accuracy_policy_5: 0.64617
	loss_value_5: 0.08467
	loss_reward_5: 0.01622
	loss_policy: 0.56936
	loss_value: 0.78343
	loss_reward: 0.06494
[2025-05-07 18:13:51] nn step 28950, lr: 0.1.
	loss_policy_0: 0.28959
	accuracy_policy_0: 0.64559
	loss_value_0: 0.37965
	loss_policy_1: 0.05835
	accuracy_policy_1: 0.64141
	loss_value_1: 0.07774
	loss_reward_1: 0.00788
	loss_policy_2: 0.05814
	accuracy_policy_2: 0.64375
	loss_value_2: 0.08009
	loss_reward_2: 0.01114
	loss_policy_3: 0.05792
	accuracy_policy_3: 0.64574
	loss_value_3: 0.08217
	loss_reward_3: 0.01403
	loss_policy_4: 0.05787
	accuracy_policy_4: 0.64859
	loss_value_4: 0.08395
	loss_reward_4: 0.01646
	loss_policy_5: 0.05832
	accuracy_policy_5: 0.64449
	loss_value_5: 0.08522
	loss_reward_5: 0.01718
	loss_policy: 0.58019
	loss_value: 0.78883
	loss_reward: 0.06668
[2025-05-07 18:14:00] nn step 29000, lr: 0.1.
	loss_policy_0: 0.27926
	accuracy_policy_0: 0.63875
	loss_value_0: 0.35932
	loss_policy_1: 0.05575
	accuracy_policy_1: 0.64156
	loss_value_1: 0.07342
	loss_reward_1: 0.00727
	loss_policy_2: 0.05537
	accuracy_policy_2: 0.64414
	loss_value_2: 0.0759
	loss_reward_2: 0.01039
	loss_policy_3: 0.05549
	accuracy_policy_3: 0.64113
	loss_value_3: 0.07774
	loss_reward_3: 0.01366
	loss_policy_4: 0.05539
	accuracy_policy_4: 0.6459
	loss_value_4: 0.07919
	loss_reward_4: 0.01565
	loss_policy_5: 0.05559
	accuracy_policy_5: 0.64523
	loss_value_5: 0.08073
	loss_reward_5: 0.01606
	loss_policy: 0.55685
	loss_value: 0.74629
	loss_reward: 0.06302
Optimization_Done 29000
[2025-05-07 18:17:29] [command] train weight_iter_29000.pkl 127 146
[2025-05-07 18:17:38] nn step 29050, lr: 0.1.
	loss_policy_0: 0.27581
	accuracy_policy_0: 0.64379
	loss_value_0: 0.36695
	loss_policy_1: 0.05565
	accuracy_policy_1: 0.64434
	loss_value_1: 0.07501
	loss_reward_1: 0.00748
	loss_policy_2: 0.05577
	accuracy_policy_2: 0.64398
	loss_value_2: 0.07681
	loss_reward_2: 0.01071
	loss_policy_3: 0.05583
	accuracy_policy_3: 0.64691
	loss_value_3: 0.07883
	loss_reward_3: 0.0133
	loss_policy_4: 0.05588
	accuracy_policy_4: 0.64645
	loss_value_4: 0.0806
	loss_reward_4: 0.0156
	loss_policy_5: 0.05627
	accuracy_policy_5: 0.64379
	loss_value_5: 0.08209
	loss_reward_5: 0.01635
	loss_policy: 0.55521
	loss_value: 0.7603
	loss_reward: 0.06344
[2025-05-07 18:17:46] nn step 29100, lr: 0.1.
	loss_policy_0: 0.27321
	accuracy_policy_0: 0.64633
	loss_value_0: 0.35932
	loss_policy_1: 0.0552
	accuracy_policy_1: 0.6418
	loss_value_1: 0.07354
	loss_reward_1: 0.00729
	loss_policy_2: 0.05546
	accuracy_policy_2: 0.63711
	loss_value_2: 0.07569
	loss_reward_2: 0.01004
	loss_policy_3: 0.05522
	accuracy_policy_3: 0.64
	loss_value_3: 0.07727
	loss_reward_3: 0.01264
	loss_policy_4: 0.05535
	accuracy_policy_4: 0.64188
	loss_value_4: 0.07919
	loss_reward_4: 0.01564
	loss_policy_5: 0.05515
	accuracy_policy_5: 0.64594
	loss_value_5: 0.08046
	loss_reward_5: 0.01566
	loss_policy: 0.5496
	loss_value: 0.74547
	loss_reward: 0.06127
[2025-05-07 18:17:54] nn step 29150, lr: 0.1.
	loss_policy_0: 0.27598
	accuracy_policy_0: 0.63949
	loss_value_0: 0.36173
	loss_policy_1: 0.05572
	accuracy_policy_1: 0.63512
	loss_value_1: 0.0743
	loss_reward_1: 0.00755
	loss_policy_2: 0.05557
	accuracy_policy_2: 0.63848
	loss_value_2: 0.07631
	loss_reward_2: 0.01036
	loss_policy_3: 0.0556
	accuracy_policy_3: 0.6432
	loss_value_3: 0.0782
	loss_reward_3: 0.01286
	loss_policy_4: 0.05565
	accuracy_policy_4: 0.64789
	loss_value_4: 0.0796
	loss_reward_4: 0.01559
	loss_policy_5: 0.05622
	accuracy_policy_5: 0.64219
	loss_value_5: 0.08115
	loss_reward_5: 0.01642
	loss_policy: 0.55473
	loss_value: 0.75129
	loss_reward: 0.06278
[2025-05-07 18:18:02] nn step 29200, lr: 0.1.
	loss_policy_0: 0.26004
	accuracy_policy_0: 0.64945
	loss_value_0: 0.33631
	loss_policy_1: 0.05226
	accuracy_policy_1: 0.63703
	loss_value_1: 0.06894
	loss_reward_1: 0.007
	loss_policy_2: 0.05227
	accuracy_policy_2: 0.63652
	loss_value_2: 0.07106
	loss_reward_2: 0.0099
	loss_policy_3: 0.05242
	accuracy_policy_3: 0.64492
	loss_value_3: 0.07268
	loss_reward_3: 0.01208
	loss_policy_4: 0.05256
	accuracy_policy_4: 0.64719
	loss_value_4: 0.07418
	loss_reward_4: 0.01456
	loss_policy_5: 0.05256
	accuracy_policy_5: 0.64457
	loss_value_5: 0.07567
	loss_reward_5: 0.01495
	loss_policy: 0.52212
	loss_value: 0.69884
	loss_reward: 0.05849
Optimization_Done 29200
[2025-05-07 18:21:29] [command] train weight_iter_29200.pkl 128 147
[2025-05-07 18:21:39] nn step 29250, lr: 0.1.
	loss_policy_0: 0.28103
	accuracy_policy_0: 0.63809
	loss_value_0: 0.37196
	loss_policy_1: 0.05619
	accuracy_policy_1: 0.6332
	loss_value_1: 0.07581
	loss_reward_1: 0.00747
	loss_policy_2: 0.05638
	accuracy_policy_2: 0.6357
	loss_value_2: 0.07804
	loss_reward_2: 0.01037
	loss_policy_3: 0.05637
	accuracy_policy_3: 0.64043
	loss_value_3: 0.07993
	loss_reward_3: 0.01267
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.63934
	loss_value_4: 0.08158
	loss_reward_4: 0.01589
	loss_policy_5: 0.05645
	accuracy_policy_5: 0.64457
	loss_value_5: 0.08301
	loss_reward_5: 0.01592
	loss_policy: 0.56285
	loss_value: 0.77033
	loss_reward: 0.06233
[2025-05-07 18:21:47] nn step 29300, lr: 0.1.
	loss_policy_0: 0.26861
	accuracy_policy_0: 0.64215
	loss_value_0: 0.35061
	loss_policy_1: 0.05353
	accuracy_policy_1: 0.6377
	loss_value_1: 0.07198
	loss_reward_1: 0.0071
	loss_policy_2: 0.05391
	accuracy_policy_2: 0.63391
	loss_value_2: 0.07379
	loss_reward_2: 0.01027
	loss_policy_3: 0.05399
	accuracy_policy_3: 0.63516
	loss_value_3: 0.07548
	loss_reward_3: 0.01293
	loss_policy_4: 0.05397
	accuracy_policy_4: 0.63898
	loss_value_4: 0.07704
	loss_reward_4: 0.01491
	loss_policy_5: 0.05381
	accuracy_policy_5: 0.63988
	loss_value_5: 0.07859
	loss_reward_5: 0.01599
	loss_policy: 0.53781
	loss_value: 0.72748
	loss_reward: 0.0612
[2025-05-07 18:21:53] nn step 29350, lr: 0.1.
	loss_policy_0: 0.29596
	accuracy_policy_0: 0.64457
	loss_value_0: 0.38365
	loss_policy_1: 0.05917
	accuracy_policy_1: 0.63895
	loss_value_1: 0.0788
	loss_reward_1: 0.00796
	loss_policy_2: 0.05963
	accuracy_policy_2: 0.63863
	loss_value_2: 0.08125
	loss_reward_2: 0.01119
	loss_policy_3: 0.05948
	accuracy_policy_3: 0.64164
	loss_value_3: 0.08308
	loss_reward_3: 0.0138
	loss_policy_4: 0.05897
	accuracy_policy_4: 0.64875
	loss_value_4: 0.08478
	loss_reward_4: 0.01666
	loss_policy_5: 0.05913
	accuracy_policy_5: 0.64852
	loss_value_5: 0.08659
	loss_reward_5: 0.01732
	loss_policy: 0.59234
	loss_value: 0.79815
	loss_reward: 0.06694
[2025-05-07 18:22:01] nn step 29400, lr: 0.1.
	loss_policy_0: 0.28066
	accuracy_policy_0: 0.64191
	loss_value_0: 0.36469
	loss_policy_1: 0.05617
	accuracy_policy_1: 0.64148
	loss_value_1: 0.07439
	loss_reward_1: 0.00779
	loss_policy_2: 0.05662
	accuracy_policy_2: 0.63809
	loss_value_2: 0.07666
	loss_reward_2: 0.01083
	loss_policy_3: 0.05634
	accuracy_policy_3: 0.63953
	loss_value_3: 0.07852
	loss_reward_3: 0.01337
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.63871
	loss_value_4: 0.08019
	loss_reward_4: 0.01613
	loss_policy_5: 0.05643
	accuracy_policy_5: 0.6375
	loss_value_5: 0.08184
	loss_reward_5: 0.01698
	loss_policy: 0.56285
	loss_value: 0.7563
	loss_reward: 0.06511
Optimization_Done 29400
[2025-05-07 18:25:33] [command] train weight_iter_29400.pkl 129 148
[2025-05-07 18:25:42] nn step 29450, lr: 0.1.
	loss_policy_0: 0.27382
	accuracy_policy_0: 0.64758
	loss_value_0: 0.37206
	loss_policy_1: 0.0554
	accuracy_policy_1: 0.63789
	loss_value_1: 0.07603
	loss_reward_1: 0.00748
	loss_policy_2: 0.05568
	accuracy_policy_2: 0.63625
	loss_value_2: 0.0777
	loss_reward_2: 0.01062
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.63547
	loss_value_3: 0.07937
	loss_reward_3: 0.01332
	loss_policy_4: 0.05565
	accuracy_policy_4: 0.64121
	loss_value_4: 0.08102
	loss_reward_4: 0.01592
	loss_policy_5: 0.05552
	accuracy_policy_5: 0.6484
	loss_value_5: 0.08224
	loss_reward_5: 0.01669
	loss_policy: 0.55189
	loss_value: 0.76843
	loss_reward: 0.06403
[2025-05-07 18:25:50] nn step 29500, lr: 0.1.
	loss_policy_0: 0.27264
	accuracy_policy_0: 0.64902
	loss_value_0: 0.35844
	loss_policy_1: 0.05505
	accuracy_policy_1: 0.64227
	loss_value_1: 0.0732
	loss_reward_1: 0.00738
	loss_policy_2: 0.05496
	accuracy_policy_2: 0.6382
	loss_value_2: 0.07515
	loss_reward_2: 0.01043
	loss_policy_3: 0.0548
	accuracy_policy_3: 0.63664
	loss_value_3: 0.07669
	loss_reward_3: 0.01324
	loss_policy_4: 0.05474
	accuracy_policy_4: 0.64258
	loss_value_4: 0.07858
	loss_reward_4: 0.01506
	loss_policy_5: 0.05507
	accuracy_policy_5: 0.64133
	loss_value_5: 0.07988
	loss_reward_5: 0.0161
	loss_policy: 0.54728
	loss_value: 0.74194
	loss_reward: 0.06221
[2025-05-07 18:25:58] nn step 29550, lr: 0.1.
	loss_policy_0: 0.28238
	accuracy_policy_0: 0.64293
	loss_value_0: 0.37039
	loss_policy_1: 0.05677
	accuracy_policy_1: 0.63762
	loss_value_1: 0.07604
	loss_reward_1: 0.00744
	loss_policy_2: 0.05663
	accuracy_policy_2: 0.63688
	loss_value_2: 0.0779
	loss_reward_2: 0.01054
	loss_policy_3: 0.05632
	accuracy_policy_3: 0.64441
	loss_value_3: 0.07954
	loss_reward_3: 0.0134
	loss_policy_4: 0.05676
	accuracy_policy_4: 0.64645
	loss_value_4: 0.08109
	loss_reward_4: 0.01608
	loss_policy_5: 0.05629
	accuracy_policy_5: 0.64566
	loss_value_5: 0.08273
	loss_reward_5: 0.0166
	loss_policy: 0.56515
	loss_value: 0.7677
	loss_reward: 0.06406
[2025-05-07 18:26:06] nn step 29600, lr: 0.1.
	loss_policy_0: 0.26411
	accuracy_policy_0: 0.63832
	loss_value_0: 0.34312
	loss_policy_1: 0.05274
	accuracy_policy_1: 0.63758
	loss_value_1: 0.07026
	loss_reward_1: 0.00722
	loss_policy_2: 0.05326
	accuracy_policy_2: 0.63406
	loss_value_2: 0.0726
	loss_reward_2: 0.01013
	loss_policy_3: 0.05329
	accuracy_policy_3: 0.64133
	loss_value_3: 0.07433
	loss_reward_3: 0.01234
	loss_policy_4: 0.05326
	accuracy_policy_4: 0.64422
	loss_value_4: 0.07609
	loss_reward_4: 0.01487
	loss_policy_5: 0.05336
	accuracy_policy_5: 0.63973
	loss_value_5: 0.07741
	loss_reward_5: 0.01523
	loss_policy: 0.53002
	loss_value: 0.7138
	loss_reward: 0.0598
Optimization_Done 29600
[2025-05-07 18:29:30] [command] train weight_iter_29600.pkl 130 149
[2025-05-07 18:29:40] nn step 29650, lr: 0.1.
	loss_policy_0: 0.27034
	accuracy_policy_0: 0.65094
	loss_value_0: 0.36632
	loss_policy_1: 0.05463
	accuracy_policy_1: 0.64125
	loss_value_1: 0.07481
	loss_reward_1: 0.00714
	loss_policy_2: 0.05474
	accuracy_policy_2: 0.64223
	loss_value_2: 0.07687
	loss_reward_2: 0.01044
	loss_policy_3: 0.05496
	accuracy_policy_3: 0.64359
	loss_value_3: 0.07849
	loss_reward_3: 0.0127
	loss_policy_4: 0.0548
	accuracy_policy_4: 0.64551
	loss_value_4: 0.08007
	loss_reward_4: 0.01521
	loss_policy_5: 0.05486
	accuracy_policy_5: 0.64742
	loss_value_5: 0.0817
	loss_reward_5: 0.01623
	loss_policy: 0.54434
	loss_value: 0.75826
	loss_reward: 0.06171
[2025-05-07 18:29:48] nn step 29700, lr: 0.1.
	loss_policy_0: 0.27672
	accuracy_policy_0: 0.65133
	loss_value_0: 0.36877
	loss_policy_1: 0.05584
	accuracy_policy_1: 0.64453
	loss_value_1: 0.07533
	loss_reward_1: 0.00741
	loss_policy_2: 0.05615
	accuracy_policy_2: 0.64344
	loss_value_2: 0.0774
	loss_reward_2: 0.01016
	loss_policy_3: 0.05592
	accuracy_policy_3: 0.64379
	loss_value_3: 0.07904
	loss_reward_3: 0.01266
	loss_policy_4: 0.05628
	accuracy_policy_4: 0.64703
	loss_value_4: 0.08048
	loss_reward_4: 0.01567
	loss_policy_5: 0.05633
	accuracy_policy_5: 0.64711
	loss_value_5: 0.08196
	loss_reward_5: 0.01598
	loss_policy: 0.55725
	loss_value: 0.76297
	loss_reward: 0.06189
[2025-05-07 18:29:54] nn step 29750, lr: 0.1.
	loss_policy_0: 0.27825
	accuracy_policy_0: 0.64492
	loss_value_0: 0.36303
	loss_policy_1: 0.05582
	accuracy_policy_1: 0.64707
	loss_value_1: 0.07424
	loss_reward_1: 0.00742
	loss_policy_2: 0.05601
	accuracy_policy_2: 0.64605
	loss_value_2: 0.07656
	loss_reward_2: 0.01051
	loss_policy_3: 0.05589
	accuracy_policy_3: 0.64848
	loss_value_3: 0.07834
	loss_reward_3: 0.01318
	loss_policy_4: 0.05588
	accuracy_policy_4: 0.64922
	loss_value_4: 0.07992
	loss_reward_4: 0.01616
	loss_policy_5: 0.05658
	accuracy_policy_5: 0.64352
	loss_value_5: 0.08151
	loss_reward_5: 0.01607
	loss_policy: 0.55843
	loss_value: 0.7536
	loss_reward: 0.06334
[2025-05-07 18:30:02] nn step 29800, lr: 0.1.
	loss_policy_0: 0.2804
	accuracy_policy_0: 0.64211
	loss_value_0: 0.36799
	loss_policy_1: 0.0562
	accuracy_policy_1: 0.6452
	loss_value_1: 0.07545
	loss_reward_1: 0.00759
	loss_policy_2: 0.05692
	accuracy_policy_2: 0.63789
	loss_value_2: 0.07703
	loss_reward_2: 0.01056
	loss_policy_3: 0.05642
	accuracy_policy_3: 0.6452
	loss_value_3: 0.07903
	loss_reward_3: 0.01306
	loss_policy_4: 0.05637
	accuracy_policy_4: 0.64496
	loss_value_4: 0.08031
	loss_reward_4: 0.01585
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.64531
	loss_value_5: 0.08212
	loss_reward_5: 0.01618
	loss_policy: 0.56314
	loss_value: 0.76193
	loss_reward: 0.06325
Optimization_Done 29800
[2025-05-07 18:33:26] [command] train weight_iter_29800.pkl 131 150
[2025-05-07 18:33:36] nn step 29850, lr: 0.1.
	loss_policy_0: 0.27181
	accuracy_policy_0: 0.64996
	loss_value_0: 0.36398
	loss_policy_1: 0.05468
	accuracy_policy_1: 0.6457
	loss_value_1: 0.07414
	loss_reward_1: 0.00728
	loss_policy_2: 0.05512
	accuracy_policy_2: 0.64336
	loss_value_2: 0.07582
	loss_reward_2: 0.01053
	loss_policy_3: 0.05494
	accuracy_policy_3: 0.64723
	loss_value_3: 0.07757
	loss_reward_3: 0.01316
	loss_policy_4: 0.05441
	accuracy_policy_4: 0.65453
	loss_value_4: 0.07894
	loss_reward_4: 0.01559
	loss_policy_5: 0.05455
	accuracy_policy_5: 0.65227
	loss_value_5: 0.08039
	loss_reward_5: 0.01645
	loss_policy: 0.5455
	loss_value: 0.75083
	loss_reward: 0.06302
[2025-05-07 18:33:42] nn step 29900, lr: 0.1.
	loss_policy_0: 0.25501
	accuracy_policy_0: 0.65562
	loss_value_0: 0.34073
	loss_policy_1: 0.05132
	accuracy_policy_1: 0.64801
	loss_value_1: 0.06943
	loss_reward_1: 0.00697
	loss_policy_2: 0.0517
	accuracy_policy_2: 0.64453
	loss_value_2: 0.07138
	loss_reward_2: 0.00977
	loss_policy_3: 0.0519
	accuracy_policy_3: 0.64562
	loss_value_3: 0.07336
	loss_reward_3: 0.01192
	loss_policy_4: 0.05168
	accuracy_policy_4: 0.64941
	loss_value_4: 0.07487
	loss_reward_4: 0.01474
	loss_policy_5: 0.0519
	accuracy_policy_5: 0.64664
	loss_value_5: 0.07607
	loss_reward_5: 0.01492
	loss_policy: 0.5135
	loss_value: 0.70585
	loss_reward: 0.05832
[2025-05-07 18:33:50] nn step 29950, lr: 0.1.
	loss_policy_0: 0.27679
	accuracy_policy_0: 0.65176
	loss_value_0: 0.36435
	loss_policy_1: 0.05578
	accuracy_policy_1: 0.64766
	loss_value_1: 0.07484
	loss_reward_1: 0.00755
	loss_policy_2: 0.05621
	accuracy_policy_2: 0.64254
	loss_value_2: 0.07679
	loss_reward_2: 0.01039
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.65164
	loss_value_3: 0.07868
	loss_reward_3: 0.0129
	loss_policy_4: 0.05573
	accuracy_policy_4: 0.6582
	loss_value_4: 0.08051
	loss_reward_4: 0.01559
	loss_policy_5: 0.05623
	accuracy_policy_5: 0.64875
	loss_value_5: 0.08211
	loss_reward_5: 0.01645
	loss_policy: 0.55656
	loss_value: 0.75728
	loss_reward: 0.06287
[2025-05-07 18:33:58] nn step 30000, lr: 0.1.
	loss_policy_0: 0.26634
	accuracy_policy_0: 0.65566
	loss_value_0: 0.34963
	loss_policy_1: 0.05365
	accuracy_policy_1: 0.64977
	loss_value_1: 0.07178
	loss_reward_1: 0.0071
	loss_policy_2: 0.05364
	accuracy_policy_2: 0.64887
	loss_value_2: 0.07379
	loss_reward_2: 0.0102
	loss_policy_3: 0.05332
	accuracy_policy_3: 0.65156
	loss_value_3: 0.07533
	loss_reward_3: 0.01249
	loss_policy_4: 0.05359
	accuracy_policy_4: 0.6523
	loss_value_4: 0.0772
	loss_reward_4: 0.01534
	loss_policy_5: 0.05389
	accuracy_policy_5: 0.65402
	loss_value_5: 0.07901
	loss_reward_5: 0.01613
	loss_policy: 0.53444
	loss_value: 0.72674
	loss_reward: 0.06126
Optimization_Done 30000
[2025-05-07 18:37:29] [command] train weight_iter_30000.pkl 132 151
[2025-05-07 18:37:36] nn step 30050, lr: 0.1.
	loss_policy_0: 0.26403
	accuracy_policy_0: 0.64793
	loss_value_0: 0.36023
	loss_policy_1: 0.05246
	accuracy_policy_1: 0.65688
	loss_value_1: 0.07345
	loss_reward_1: 0.0074
	loss_policy_2: 0.05281
	accuracy_policy_2: 0.65047
	loss_value_2: 0.07534
	loss_reward_2: 0.01044
	loss_policy_3: 0.05306
	accuracy_policy_3: 0.64633
	loss_value_3: 0.07685
	loss_reward_3: 0.01298
	loss_policy_4: 0.05287
	accuracy_policy_4: 0.65363
	loss_value_4: 0.07855
	loss_reward_4: 0.01522
	loss_policy_5: 0.05313
	accuracy_policy_5: 0.6534
	loss_value_5: 0.07989
	loss_reward_5: 0.01599
	loss_policy: 0.52836
	loss_value: 0.74432
	loss_reward: 0.06202
[2025-05-07 18:37:44] nn step 30100, lr: 0.1.
	loss_policy_0: 0.2821
	accuracy_policy_0: 0.64926
	loss_value_0: 0.37208
	loss_policy_1: 0.05567
	accuracy_policy_1: 0.64777
	loss_value_1: 0.07571
	loss_reward_1: 0.00752
	loss_policy_2: 0.05541
	accuracy_policy_2: 0.64926
	loss_value_2: 0.07773
	loss_reward_2: 0.01035
	loss_policy_3: 0.05515
	accuracy_policy_3: 0.65113
	loss_value_3: 0.07943
	loss_reward_3: 0.01327
	loss_policy_4: 0.05558
	accuracy_policy_4: 0.66133
	loss_value_4: 0.08139
	loss_reward_4: 0.01616
	loss_policy_5: 0.05541
	accuracy_policy_5: 0.6566
	loss_value_5: 0.08295
	loss_reward_5: 0.01602
	loss_policy: 0.55932
	loss_value: 0.76929
	loss_reward: 0.06333
[2025-05-07 18:37:52] nn step 30150, lr: 0.1.
	loss_policy_0: 0.27213
	accuracy_policy_0: 0.65457
	loss_value_0: 0.36438
	loss_policy_1: 0.05467
	accuracy_policy_1: 0.6523
	loss_value_1: 0.07465
	loss_reward_1: 0.00728
	loss_policy_2: 0.05474
	accuracy_policy_2: 0.65348
	loss_value_2: 0.07644
	loss_reward_2: 0.01051
	loss_policy_3: 0.05487
	accuracy_policy_3: 0.65191
	loss_value_3: 0.0781
	loss_reward_3: 0.01309
	loss_policy_4: 0.05526
	accuracy_policy_4: 0.65066
	loss_value_4: 0.07995
	loss_reward_4: 0.01539
	loss_policy_5: 0.05509
	accuracy_policy_5: 0.65754
	loss_value_5: 0.08173
	loss_reward_5: 0.0158
	loss_policy: 0.54676
	loss_value: 0.75525
	loss_reward: 0.06207
[2025-05-07 18:37:59] nn step 30200, lr: 0.1.
	loss_policy_0: 0.27549
	accuracy_policy_0: 0.65543
	loss_value_0: 0.36908
	loss_policy_1: 0.05544
	accuracy_policy_1: 0.6468
	loss_value_1: 0.07526
	loss_reward_1: 0.00764
	loss_policy_2: 0.05544
	accuracy_policy_2: 0.64836
	loss_value_2: 0.07733
	loss_reward_2: 0.01051
	loss_policy_3: 0.05528
	accuracy_policy_3: 0.64832
	loss_value_3: 0.07927
	loss_reward_3: 0.01328
	loss_policy_4: 0.05566
	accuracy_policy_4: 0.65512
	loss_value_4: 0.08085
	loss_reward_4: 0.01586
	loss_policy_5: 0.05604
	accuracy_policy_5: 0.64879
	loss_value_5: 0.08243
	loss_reward_5: 0.01624
	loss_policy: 0.55336
	loss_value: 0.76421
	loss_reward: 0.06353
Optimization_Done 30200
[2025-05-07 18:41:26] [command] train weight_iter_30200.pkl 133 152
[2025-05-07 18:41:35] nn step 30250, lr: 0.1.
	loss_policy_0: 0.2677
	accuracy_policy_0: 0.65969
	loss_value_0: 0.37213
	loss_policy_1: 0.05421
	accuracy_policy_1: 0.65156
	loss_value_1: 0.07571
	loss_reward_1: 0.00732
	loss_policy_2: 0.05454
	accuracy_policy_2: 0.6518
	loss_value_2: 0.07732
	loss_reward_2: 0.0105
	loss_policy_3: 0.05447
	accuracy_policy_3: 0.6518
	loss_value_3: 0.07899
	loss_reward_3: 0.01286
	loss_policy_4: 0.0544
	accuracy_policy_4: 0.66113
	loss_value_4: 0.0806
	loss_reward_4: 0.01569
	loss_policy_5: 0.05497
	accuracy_policy_5: 0.65676
	loss_value_5: 0.08203
	loss_reward_5: 0.01585
	loss_policy: 0.54028
	loss_value: 0.76677
	loss_reward: 0.06222
[2025-05-07 18:41:43] nn step 30300, lr: 0.1.
	loss_policy_0: 0.26037
	accuracy_policy_0: 0.65781
	loss_value_0: 0.34898
	loss_policy_1: 0.05262
	accuracy_policy_1: 0.64758
	loss_value_1: 0.07099
	loss_reward_1: 0.00677
	loss_policy_2: 0.05258
	accuracy_policy_2: 0.65055
	loss_value_2: 0.07264
	loss_reward_2: 0.00995
	loss_policy_3: 0.05253
	accuracy_policy_3: 0.65004
	loss_value_3: 0.07434
	loss_reward_3: 0.01233
	loss_policy_4: 0.05276
	accuracy_policy_4: 0.65109
	loss_value_4: 0.07597
	loss_reward_4: 0.01483
	loss_policy_5: 0.0528
	accuracy_policy_5: 0.65301
	loss_value_5: 0.07725
	loss_reward_5: 0.01562
	loss_policy: 0.52365
	loss_value: 0.72017
	loss_reward: 0.05951
[2025-05-07 18:41:50] nn step 30350, lr: 0.1.
	loss_policy_0: 0.25591
	accuracy_policy_0: 0.66062
	loss_value_0: 0.3516
	loss_policy_1: 0.05172
	accuracy_policy_1: 0.65156
	loss_value_1: 0.07201
	loss_reward_1: 0.00705
	loss_policy_2: 0.05179
	accuracy_policy_2: 0.65402
	loss_value_2: 0.0736
	loss_reward_2: 0.00978
	loss_policy_3: 0.05212
	accuracy_policy_3: 0.6577
	loss_value_3: 0.07542
	loss_reward_3: 0.01308
	loss_policy_4: 0.05209
	accuracy_policy_4: 0.65879
	loss_value_4: 0.07719
	loss_reward_4: 0.01508
	loss_policy_5: 0.05223
	accuracy_policy_5: 0.65867
	loss_value_5: 0.07846
	loss_reward_5: 0.01547
	loss_policy: 0.51586
	loss_value: 0.72829
	loss_reward: 0.06045
[2025-05-07 18:41:58] nn step 30400, lr: 0.1.
	loss_policy_0: 0.26498
	accuracy_policy_0: 0.65684
	loss_value_0: 0.35525
	loss_policy_1: 0.05349
	accuracy_policy_1: 0.64863
	loss_value_1: 0.07259
	loss_reward_1: 0.00709
	loss_policy_2: 0.05333
	accuracy_policy_2: 0.64914
	loss_value_2: 0.07434
	loss_reward_2: 0.01022
	loss_policy_3: 0.05332
	accuracy_policy_3: 0.65332
	loss_value_3: 0.07637
	loss_reward_3: 0.01276
	loss_policy_4: 0.0534
	accuracy_policy_4: 0.65488
	loss_value_4: 0.07803
	loss_reward_4: 0.01561
	loss_policy_5: 0.05322
	accuracy_policy_5: 0.6559
	loss_value_5: 0.07935
	loss_reward_5: 0.01573
	loss_policy: 0.53173
	loss_value: 0.73593
	loss_reward: 0.06141
Optimization_Done 30400
[2025-05-07 18:45:18] [command] train weight_iter_30400.pkl 134 153
[2025-05-07 18:45:27] nn step 30450, lr: 0.1.
	loss_policy_0: 0.26954
	accuracy_policy_0: 0.66
	loss_value_0: 0.37381
	loss_policy_1: 0.0549
	accuracy_policy_1: 0.65102
	loss_value_1: 0.07608
	loss_reward_1: 0.00747
	loss_policy_2: 0.05488
	accuracy_policy_2: 0.65129
	loss_value_2: 0.07787
	loss_reward_2: 0.00995
	loss_policy_3: 0.05485
	accuracy_policy_3: 0.65414
	loss_value_3: 0.07974
	loss_reward_3: 0.0127
	loss_policy_4: 0.05469
	accuracy_policy_4: 0.66109
	loss_value_4: 0.0816
	loss_reward_4: 0.01514
	loss_policy_5: 0.05499
	accuracy_policy_5: 0.65633
	loss_value_5: 0.08296
	loss_reward_5: 0.01617
	loss_policy: 0.54384
	loss_value: 0.77205
	loss_reward: 0.06143
[2025-05-07 18:45:35] nn step 30500, lr: 0.1.
	loss_policy_0: 0.26772
	accuracy_policy_0: 0.65707
	loss_value_0: 0.35665
	loss_policy_1: 0.05352
	accuracy_policy_1: 0.64926
	loss_value_1: 0.07255
	loss_reward_1: 0.00732
	loss_policy_2: 0.05318
	accuracy_policy_2: 0.65504
	loss_value_2: 0.07448
	loss_reward_2: 0.01024
	loss_policy_3: 0.05365
	accuracy_policy_3: 0.65242
	loss_value_3: 0.07606
	loss_reward_3: 0.0126
	loss_policy_4: 0.05346
	accuracy_policy_4: 0.65496
	loss_value_4: 0.07747
	loss_reward_4: 0.01476
	loss_policy_5: 0.05364
	accuracy_policy_5: 0.65539
	loss_value_5: 0.07901
	loss_reward_5: 0.01537
	loss_policy: 0.53517
	loss_value: 0.73621
	loss_reward: 0.06029
[2025-05-07 18:45:42] nn step 30550, lr: 0.1.
	loss_policy_0: 0.27512
	accuracy_policy_0: 0.66094
	loss_value_0: 0.36961
	loss_policy_1: 0.05581
	accuracy_policy_1: 0.64984
	loss_value_1: 0.07545
	loss_reward_1: 0.00768
	loss_policy_2: 0.05574
	accuracy_policy_2: 0.65016
	loss_value_2: 0.07736
	loss_reward_2: 0.01056
	loss_policy_3: 0.0558
	accuracy_policy_3: 0.65023
	loss_value_3: 0.07912
	loss_reward_3: 0.01325
	loss_policy_4: 0.0554
	accuracy_policy_4: 0.66184
	loss_value_4: 0.08072
	loss_reward_4: 0.01636
	loss_policy_5: 0.05586
	accuracy_policy_5: 0.65695
	loss_value_5: 0.08223
	loss_reward_5: 0.01605
	loss_policy: 0.55373
	loss_value: 0.7645
	loss_reward: 0.0639
[2025-05-07 18:45:50] nn step 30600, lr: 0.1.
	loss_policy_0: 0.27623
	accuracy_policy_0: 0.65465
	loss_value_0: 0.3667
	loss_policy_1: 0.05495
	accuracy_policy_1: 0.65801
	loss_value_1: 0.07493
	loss_reward_1: 0.0075
	loss_policy_2: 0.0552
	accuracy_policy_2: 0.65527
	loss_value_2: 0.07672
	loss_reward_2: 0.01029
	loss_policy_3: 0.05529
	accuracy_policy_3: 0.6573
	loss_value_3: 0.07854
	loss_reward_3: 0.01269
	loss_policy_4: 0.0552
	accuracy_policy_4: 0.66117
	loss_value_4: 0.08034
	loss_reward_4: 0.01541
	loss_policy_5: 0.05525
	accuracy_policy_5: 0.65652
	loss_value_5: 0.08183
	loss_reward_5: 0.01629
	loss_policy: 0.55211
	loss_value: 0.75906
	loss_reward: 0.06218
Optimization_Done 30600
[2025-05-07 18:49:20] [command] train weight_iter_30600.pkl 135 154
[2025-05-07 18:49:28] nn step 30650, lr: 0.1.
	loss_policy_0: 0.2552
	accuracy_policy_0: 0.66027
	loss_value_0: 0.35244
	loss_policy_1: 0.05147
	accuracy_policy_1: 0.65629
	loss_value_1: 0.07165
	loss_reward_1: 0.00694
	loss_policy_2: 0.05135
	accuracy_policy_2: 0.65559
	loss_value_2: 0.07348
	loss_reward_2: 0.00976
	loss_policy_3: 0.05163
	accuracy_policy_3: 0.65504
	loss_value_3: 0.07499
	loss_reward_3: 0.01259
	loss_policy_4: 0.05164
	accuracy_policy_4: 0.65328
	loss_value_4: 0.07624
	loss_reward_4: 0.01468
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.66109
	loss_value_5: 0.07766
	loss_reward_5: 0.01487
	loss_policy: 0.51268
	loss_value: 0.72646
	loss_reward: 0.05885
[2025-05-07 18:49:36] nn step 30700, lr: 0.1.
	loss_policy_0: 0.26994
	accuracy_policy_0: 0.65871
	loss_value_0: 0.36792
	loss_policy_1: 0.05418
	accuracy_policy_1: 0.65457
	loss_value_1: 0.07504
	loss_reward_1: 0.00747
	loss_policy_2: 0.05444
	accuracy_policy_2: 0.65465
	loss_value_2: 0.07706
	loss_reward_2: 0.01015
	loss_policy_3: 0.05473
	accuracy_policy_3: 0.65582
	loss_value_3: 0.07866
	loss_reward_3: 0.0125
	loss_policy_4: 0.05465
	accuracy_policy_4: 0.65699
	loss_value_4: 0.08014
	loss_reward_4: 0.01529
	loss_policy_5: 0.05459
	accuracy_policy_5: 0.66055
	loss_value_5: 0.08178
	loss_reward_5: 0.01589
	loss_policy: 0.54253
	loss_value: 0.7606
	loss_reward: 0.0613
[2025-05-07 18:49:44] nn step 30750, lr: 0.1.
	loss_policy_0: 0.30068
	accuracy_policy_0: 0.64355
	loss_value_0: 0.39607
	loss_policy_1: 0.05972
	accuracy_policy_1: 0.65164
	loss_value_1: 0.08078
	loss_reward_1: 0.00792
	loss_policy_2: 0.05945
	accuracy_policy_2: 0.65316
	loss_value_2: 0.08278
	loss_reward_2: 0.01119
	loss_policy_3: 0.05935
	accuracy_policy_3: 0.65902
	loss_value_3: 0.08474
	loss_reward_3: 0.01374
	loss_policy_4: 0.05944
	accuracy_policy_4: 0.65574
	loss_value_4: 0.08664
	loss_reward_4: 0.01649
	loss_policy_5: 0.05937
	accuracy_policy_5: 0.65848
	loss_value_5: 0.08853
	loss_reward_5: 0.0175
	loss_policy: 0.59802
	loss_value: 0.81954
	loss_reward: 0.06685
[2025-05-07 18:49:52] nn step 30800, lr: 0.1.
	loss_policy_0: 0.2824
	accuracy_policy_0: 0.65004
	loss_value_0: 0.37573
	loss_policy_1: 0.05678
	accuracy_policy_1: 0.64676
	loss_value_1: 0.07676
	loss_reward_1: 0.00743
	loss_policy_2: 0.05639
	accuracy_policy_2: 0.64906
	loss_value_2: 0.07873
	loss_reward_2: 0.01048
	loss_policy_3: 0.05685
	accuracy_policy_3: 0.65168
	loss_value_3: 0.0806
	loss_reward_3: 0.0133
	loss_policy_4: 0.0566
	accuracy_policy_4: 0.65594
	loss_value_4: 0.08204
	loss_reward_4: 0.01599
	loss_policy_5: 0.05673
	accuracy_policy_5: 0.6541
	loss_value_5: 0.08391
	loss_reward_5: 0.01656
	loss_policy: 0.56575
	loss_value: 0.77776
	loss_reward: 0.06377
Optimization_Done 30800
[2025-05-07 18:53:08] [command] train weight_iter_30800.pkl 136 155
[2025-05-07 18:53:15] nn step 30850, lr: 0.1.
	loss_policy_0: 0.27317
	accuracy_policy_0: 0.66078
	loss_value_0: 0.37085
	loss_policy_1: 0.0552
	accuracy_policy_1: 0.6525
	loss_value_1: 0.07549
	loss_reward_1: 0.00739
	loss_policy_2: 0.05542
	accuracy_policy_2: 0.65641
	loss_value_2: 0.07698
	loss_reward_2: 0.01015
	loss_policy_3: 0.05565
	accuracy_policy_3: 0.65461
	loss_value_3: 0.07897
	loss_reward_3: 0.0131
	loss_policy_4: 0.05543
	accuracy_policy_4: 0.65496
	loss_value_4: 0.0805
	loss_reward_4: 0.01558
	loss_policy_5: 0.0556
	accuracy_policy_5: 0.65441
	loss_value_5: 0.08201
	loss_reward_5: 0.0162
	loss_policy: 0.55046
	loss_value: 0.76481
	loss_reward: 0.06242
[2025-05-07 18:53:23] nn step 30900, lr: 0.1.
	loss_policy_0: 0.27474
	accuracy_policy_0: 0.66398
	loss_value_0: 0.37372
	loss_policy_1: 0.05563
	accuracy_policy_1: 0.65539
	loss_value_1: 0.07651
	loss_reward_1: 0.00752
	loss_policy_2: 0.05562
	accuracy_policy_2: 0.65258
	loss_value_2: 0.07834
	loss_reward_2: 0.01038
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.6541
	loss_value_3: 0.08036
	loss_reward_3: 0.01285
	loss_policy_4: 0.05579
	accuracy_policy_4: 0.65656
	loss_value_4: 0.08198
	loss_reward_4: 0.01565
	loss_policy_5: 0.05581
	accuracy_policy_5: 0.66023
	loss_value_5: 0.08352
	loss_reward_5: 0.0166
	loss_policy: 0.55339
	loss_value: 0.77442
	loss_reward: 0.063
[2025-05-07 18:53:31] nn step 30950, lr: 0.1.
	loss_policy_0: 0.2781
	accuracy_policy_0: 0.65305
	loss_value_0: 0.37212
	loss_policy_1: 0.05576
	accuracy_policy_1: 0.65223
	loss_value_1: 0.07576
	loss_reward_1: 0.00764
	loss_policy_2: 0.05638
	accuracy_policy_2: 0.65219
	loss_value_2: 0.07739
	loss_reward_2: 0.01045
	loss_policy_3: 0.0562
	accuracy_policy_3: 0.65547
	loss_value_3: 0.07945
	loss_reward_3: 0.01325
	loss_policy_4: 0.05621
	accuracy_policy_4: 0.65453
	loss_value_4: 0.08152
	loss_reward_4: 0.0161
	loss_policy_5: 0.05615
	accuracy_policy_5: 0.65613
	loss_value_5: 0.08304
	loss_reward_5: 0.0166
	loss_policy: 0.55881
	loss_value: 0.76928
	loss_reward: 0.06404
[2025-05-07 18:53:39] nn step 31000, lr: 0.1.
	loss_policy_0: 0.27937
	accuracy_policy_0: 0.65977
	loss_value_0: 0.37077
	loss_policy_1: 0.05638
	accuracy_policy_1: 0.65359
	loss_value_1: 0.07597
	loss_reward_1: 0.00781
	loss_policy_2: 0.05647
	accuracy_policy_2: 0.65398
	loss_value_2: 0.07788
	loss_reward_2: 0.01065
	loss_policy_3: 0.05637
	accuracy_policy_3: 0.65844
	loss_value_3: 0.07983
	loss_reward_3: 0.01337
	loss_policy_4: 0.0565
	accuracy_policy_4: 0.65547
	loss_value_4: 0.08169
	loss_reward_4: 0.01579
	loss_policy_5: 0.0568
	accuracy_policy_5: 0.65504
	loss_value_5: 0.08354
	loss_reward_5: 0.01729
	loss_policy: 0.56189
	loss_value: 0.76968
	loss_reward: 0.06491
Optimization_Done 31000
[2025-05-07 18:57:04] [command] train weight_iter_31000.pkl 137 156
[2025-05-07 18:57:13] nn step 31050, lr: 0.1.
	loss_policy_0: 0.28381
	accuracy_policy_0: 0.65094
	loss_value_0: 0.37831
	loss_policy_1: 0.05659
	accuracy_policy_1: 0.64773
	loss_value_1: 0.07711
	loss_reward_1: 0.00744
	loss_policy_2: 0.05605
	accuracy_policy_2: 0.65395
	loss_value_2: 0.07891
	loss_reward_2: 0.01048
	loss_policy_3: 0.05627
	accuracy_policy_3: 0.65422
	loss_value_3: 0.08047
	loss_reward_3: 0.01324
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.64926
	loss_value_4: 0.08219
	loss_reward_4: 0.01588
	loss_policy_5: 0.05659
	accuracy_policy_5: 0.65301
	loss_value_5: 0.08368
	loss_reward_5: 0.01648
	loss_policy: 0.56595
	loss_value: 0.78066
	loss_reward: 0.06352
[2025-05-07 18:57:21] nn step 31100, lr: 0.1.
	loss_policy_0: 0.27844
	accuracy_policy_0: 0.65164
	loss_value_0: 0.36723
	loss_policy_1: 0.05548
	accuracy_policy_1: 0.64973
	loss_value_1: 0.07494
	loss_reward_1: 0.00743
	loss_policy_2: 0.05561
	accuracy_policy_2: 0.64797
	loss_value_2: 0.07687
	loss_reward_2: 0.01056
	loss_policy_3: 0.0552
	accuracy_policy_3: 0.65648
	loss_value_3: 0.07848
	loss_reward_3: 0.01355
	loss_policy_4: 0.05513
	accuracy_policy_4: 0.65934
	loss_value_4: 0.0801
	loss_reward_4: 0.01557
	loss_policy_5: 0.05535
	accuracy_policy_5: 0.65691
	loss_value_5: 0.08167
	loss_reward_5: 0.01613
	loss_policy: 0.55521
	loss_value: 0.75931
	loss_reward: 0.06324
[2025-05-07 18:57:28] nn step 31150, lr: 0.1.
	loss_policy_0: 0.27615
	accuracy_policy_0: 0.65465
	loss_value_0: 0.37009
	loss_policy_1: 0.05555
	accuracy_policy_1: 0.65301
	loss_value_1: 0.07543
	loss_reward_1: 0.00755
	loss_policy_2: 0.05604
	accuracy_policy_2: 0.65512
	loss_value_2: 0.07704
	loss_reward_2: 0.01075
	loss_policy_3: 0.0557
	accuracy_policy_3: 0.65344
	loss_value_3: 0.07878
	loss_reward_3: 0.01279
	loss_policy_4: 0.05569
	accuracy_policy_4: 0.65832
	loss_value_4: 0.08045
	loss_reward_4: 0.01584
	loss_policy_5: 0.05571
	accuracy_policy_5: 0.66426
	loss_value_5: 0.08241
	loss_reward_5: 0.01662
	loss_policy: 0.55483
	loss_value: 0.76419
	loss_reward: 0.06355
[2025-05-07 18:57:36] nn step 31200, lr: 0.1.
	loss_policy_0: 0.27037
	accuracy_policy_0: 0.65574
	loss_value_0: 0.35605
	loss_policy_1: 0.05431
	accuracy_policy_1: 0.6527
	loss_value_1: 0.07279
	loss_reward_1: 0.00723
	loss_policy_2: 0.05441
	accuracy_policy_2: 0.65598
	loss_value_2: 0.07501
	loss_reward_2: 0.01008
	loss_policy_3: 0.05472
	accuracy_policy_3: 0.65387
	loss_value_3: 0.0767
	loss_reward_3: 0.01255
	loss_policy_4: 0.0547
	accuracy_policy_4: 0.64992
	loss_value_4: 0.07823
	loss_reward_4: 0.0156
	loss_policy_5: 0.05483
	accuracy_policy_5: 0.65578
	loss_value_5: 0.08037
	loss_reward_5: 0.01586
	loss_policy: 0.54334
	loss_value: 0.73915
	loss_reward: 0.06131
Optimization_Done 31200
[2025-05-07 19:00:55] [command] train weight_iter_31200.pkl 138 157
[2025-05-07 19:01:05] nn step 31250, lr: 0.1.
	loss_policy_0: 0.26349
	accuracy_policy_0: 0.66547
	loss_value_0: 0.36166
	loss_policy_1: 0.05313
	accuracy_policy_1: 0.66062
	loss_value_1: 0.07368
	loss_reward_1: 0.00715
	loss_policy_2: 0.0537
	accuracy_policy_2: 0.65477
	loss_value_2: 0.07526
	loss_reward_2: 0.00968
	loss_policy_3: 0.05351
	accuracy_policy_3: 0.65074
	loss_value_3: 0.07676
	loss_reward_3: 0.01305
	loss_policy_4: 0.05369
	accuracy_policy_4: 0.65586
	loss_value_4: 0.07858
	loss_reward_4: 0.01499
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.65504
	loss_value_5: 0.08006
	loss_reward_5: 0.01562
	loss_policy: 0.53126
	loss_value: 0.746
	loss_reward: 0.06049
[2025-05-07 19:01:13] nn step 31300, lr: 0.1.
	loss_policy_0: 0.266
	accuracy_policy_0: 0.66012
	loss_value_0: 0.35759
	loss_policy_1: 0.05353
	accuracy_policy_1: 0.65727
	loss_value_1: 0.07277
	loss_reward_1: 0.007
	loss_policy_2: 0.05342
	accuracy_policy_2: 0.65613
	loss_value_2: 0.07507
	loss_reward_2: 0.01051
	loss_policy_3: 0.05363
	accuracy_policy_3: 0.65543
	loss_value_3: 0.07736
	loss_reward_3: 0.01264
	loss_policy_4: 0.05341
	accuracy_policy_4: 0.65836
	loss_value_4: 0.07894
	loss_reward_4: 0.01528
	loss_policy_5: 0.05342
	accuracy_policy_5: 0.66074
	loss_value_5: 0.08069
	loss_reward_5: 0.01627
	loss_policy: 0.53341
	loss_value: 0.74243
	loss_reward: 0.06169
[2025-05-07 19:01:19] nn step 31350, lr: 0.1.
	loss_policy_0: 0.28521
	accuracy_policy_0: 0.65586
	loss_value_0: 0.37706
	loss_policy_1: 0.05736
	accuracy_policy_1: 0.65352
	loss_value_1: 0.07726
	loss_reward_1: 0.00742
	loss_policy_2: 0.05716
	accuracy_policy_2: 0.65512
	loss_value_2: 0.07928
	loss_reward_2: 0.01041
	loss_policy_3: 0.05703
	accuracy_policy_3: 0.65594
	loss_value_3: 0.08082
	loss_reward_3: 0.01277
	loss_policy_4: 0.05713
	accuracy_policy_4: 0.66281
	loss_value_4: 0.08267
	loss_reward_4: 0.01625
	loss_policy_5: 0.05764
	accuracy_policy_5: 0.65586
	loss_value_5: 0.08461
	loss_reward_5: 0.01655
	loss_policy: 0.57154
	loss_value: 0.78171
	loss_reward: 0.06341
[2025-05-07 19:01:27] nn step 31400, lr: 0.1.
	loss_policy_0: 0.28102
	accuracy_policy_0: 0.66004
	loss_value_0: 0.37582
	loss_policy_1: 0.05632
	accuracy_policy_1: 0.66238
	loss_value_1: 0.07675
	loss_reward_1: 0.00757
	loss_policy_2: 0.05678
	accuracy_policy_2: 0.65402
	loss_value_2: 0.07871
	loss_reward_2: 0.01057
	loss_policy_3: 0.05666
	accuracy_policy_3: 0.65625
	loss_value_3: 0.08069
	loss_reward_3: 0.01335
	loss_policy_4: 0.05701
	accuracy_policy_4: 0.65234
	loss_value_4: 0.08248
	loss_reward_4: 0.01569
	loss_policy_5: 0.05742
	accuracy_policy_5: 0.65043
	loss_value_5: 0.08392
	loss_reward_5: 0.01697
	loss_policy: 0.5652
	loss_value: 0.77837
	loss_reward: 0.06414
Optimization_Done 31400
[2025-05-07 19:04:55] [command] train weight_iter_31400.pkl 139 158
[2025-05-07 19:05:03] nn step 31450, lr: 0.1.
	loss_policy_0: 0.25924
	accuracy_policy_0: 0.65188
	loss_value_0: 0.34802
	loss_policy_1: 0.05209
	accuracy_policy_1: 0.65
	loss_value_1: 0.07051
	loss_reward_1: 0.00686
	loss_policy_2: 0.05227
	accuracy_policy_2: 0.6534
	loss_value_2: 0.07228
	loss_reward_2: 0.00932
	loss_policy_3: 0.05219
	accuracy_policy_3: 0.65395
	loss_value_3: 0.07382
	loss_reward_3: 0.01188
	loss_policy_4: 0.05213
	accuracy_policy_4: 0.65664
	loss_value_4: 0.07535
	loss_reward_4: 0.01437
	loss_policy_5: 0.05238
	accuracy_policy_5: 0.65352
	loss_value_5: 0.07678
	loss_reward_5: 0.01537
	loss_policy: 0.52029
	loss_value: 0.71676
	loss_reward: 0.0578
[2025-05-07 19:05:11] nn step 31500, lr: 0.1.
	loss_policy_0: 0.27115
	accuracy_policy_0: 0.65383
	loss_value_0: 0.36111
	loss_policy_1: 0.05437
	accuracy_policy_1: 0.64969
	loss_value_1: 0.07378
	loss_reward_1: 0.00722
	loss_policy_2: 0.05462
	accuracy_policy_2: 0.64977
	loss_value_2: 0.07586
	loss_reward_2: 0.0102
	loss_policy_3: 0.05449
	accuracy_policy_3: 0.65164
	loss_value_3: 0.07747
	loss_reward_3: 0.01286
	loss_policy_4: 0.05491
	accuracy_policy_4: 0.65176
	loss_value_4: 0.07923
	loss_reward_4: 0.01557
	loss_policy_5: 0.05505
	accuracy_policy_5: 0.6527
	loss_value_5: 0.08071
	loss_reward_5: 0.01577
	loss_policy: 0.54459
	loss_value: 0.74817
	loss_reward: 0.06163
[2025-05-07 19:05:19] nn step 31550, lr: 0.1.
	loss_policy_0: 0.292
	accuracy_policy_0: 0.6548
	loss_value_0: 0.38649
	loss_policy_1: 0.05872
	accuracy_policy_1: 0.65164
	loss_value_1: 0.07894
	loss_reward_1: 0.00777
	loss_policy_2: 0.05884
	accuracy_policy_2: 0.6502
	loss_value_2: 0.08094
	loss_reward_2: 0.01096
	loss_policy_3: 0.05864
	accuracy_policy_3: 0.64875
	loss_value_3: 0.08308
	loss_reward_3: 0.01426
	loss_policy_4: 0.05904
	accuracy_policy_4: 0.6516
	loss_value_4: 0.08497
	loss_reward_4: 0.01616
	loss_policy_5: 0.05885
	accuracy_policy_5: 0.65824
	loss_value_5: 0.08653
	loss_reward_5: 0.0174
	loss_policy: 0.58609
	loss_value: 0.80095
	loss_reward: 0.06655
[2025-05-07 19:05:27] nn step 31600, lr: 0.1.
	loss_policy_0: 0.2805
	accuracy_policy_0: 0.65324
	loss_value_0: 0.36964
	loss_policy_1: 0.05617
	accuracy_policy_1: 0.6507
	loss_value_1: 0.07534
	loss_reward_1: 0.0076
	loss_policy_2: 0.05638
	accuracy_policy_2: 0.65184
	loss_value_2: 0.07741
	loss_reward_2: 0.01071
	loss_policy_3: 0.05683
	accuracy_policy_3: 0.64637
	loss_value_3: 0.0794
	loss_reward_3: 0.01341
	loss_policy_4: 0.05646
	accuracy_policy_4: 0.65234
	loss_value_4: 0.08128
	loss_reward_4: 0.01583
	loss_policy_5: 0.05624
	accuracy_policy_5: 0.65227
	loss_value_5: 0.08274
	loss_reward_5: 0.0166
	loss_policy: 0.56258
	loss_value: 0.76581
	loss_reward: 0.06415
Optimization_Done 31600
[2025-05-07 19:08:41] [command] train weight_iter_31600.pkl 140 159
[2025-05-07 19:08:50] nn step 31650, lr: 0.1.
	loss_policy_0: 0.26839
	accuracy_policy_0: 0.64324
	loss_value_0: 0.35811
	loss_policy_1: 0.05398
	accuracy_policy_1: 0.63953
	loss_value_1: 0.07315
	loss_reward_1: 0.00735
	loss_policy_2: 0.05403
	accuracy_policy_2: 0.63895
	loss_value_2: 0.07503
	loss_reward_2: 0.00983
	loss_policy_3: 0.05398
	accuracy_policy_3: 0.64082
	loss_value_3: 0.07664
	loss_reward_3: 0.0124
	loss_policy_4: 0.05412
	accuracy_policy_4: 0.64535
	loss_value_4: 0.07813
	loss_reward_4: 0.01537
	loss_policy_5: 0.05389
	accuracy_policy_5: 0.64625
	loss_value_5: 0.07977
	loss_reward_5: 0.01554
	loss_policy: 0.53839
	loss_value: 0.74085
	loss_reward: 0.06049
[2025-05-07 19:08:56] nn step 31700, lr: 0.1.
	loss_policy_0: 0.28404
	accuracy_policy_0: 0.63473
	loss_value_0: 0.37012
	loss_policy_1: 0.05608
	accuracy_policy_1: 0.65004
	loss_value_1: 0.07603
	loss_reward_1: 0.00758
	loss_policy_2: 0.05619
	accuracy_policy_2: 0.64426
	loss_value_2: 0.07804
	loss_reward_2: 0.01095
	loss_policy_3: 0.05655
	accuracy_policy_3: 0.64309
	loss_value_3: 0.07935
	loss_reward_3: 0.01408
	loss_policy_4: 0.05637
	accuracy_policy_4: 0.6475
	loss_value_4: 0.08116
	loss_reward_4: 0.01575
	loss_policy_5: 0.05637
	accuracy_policy_5: 0.64754
	loss_value_5: 0.08284
	loss_reward_5: 0.01654
	loss_policy: 0.56561
	loss_value: 0.76755
	loss_reward: 0.0649
[2025-05-07 19:09:04] nn step 31750, lr: 0.1.
	loss_policy_0: 0.26674
	accuracy_policy_0: 0.64809
	loss_value_0: 0.34646
	loss_policy_1: 0.05345
	accuracy_policy_1: 0.6434
	loss_value_1: 0.0707
	loss_reward_1: 0.00716
	loss_policy_2: 0.05379
	accuracy_policy_2: 0.63727
	loss_value_2: 0.07246
	loss_reward_2: 0.01023
	loss_policy_3: 0.05387
	accuracy_policy_3: 0.63875
	loss_value_3: 0.07456
	loss_reward_3: 0.01281
	loss_policy_4: 0.05367
	accuracy_policy_4: 0.64312
	loss_value_4: 0.07635
	loss_reward_4: 0.01506
	loss_policy_5: 0.0533
	accuracy_policy_5: 0.64875
	loss_value_5: 0.07765
	loss_reward_5: 0.01562
	loss_policy: 0.53482
	loss_value: 0.71818
	loss_reward: 0.06089
[2025-05-07 19:09:12] nn step 31800, lr: 0.1.
	loss_policy_0: 0.28101
	accuracy_policy_0: 0.65363
	loss_value_0: 0.36885
	loss_policy_1: 0.05634
	accuracy_policy_1: 0.64695
	loss_value_1: 0.0756
	loss_reward_1: 0.00775
	loss_policy_2: 0.05651
	accuracy_policy_2: 0.6468
	loss_value_2: 0.07771
	loss_reward_2: 0.01049
	loss_policy_3: 0.05674
	accuracy_policy_3: 0.6484
	loss_value_3: 0.07957
	loss_reward_3: 0.01329
	loss_policy_4: 0.05683
	accuracy_policy_4: 0.64742
	loss_value_4: 0.08111
	loss_reward_4: 0.01571
	loss_policy_5: 0.05684
	accuracy_policy_5: 0.65383
	loss_value_5: 0.08273
	loss_reward_5: 0.01669
	loss_policy: 0.56427
	loss_value: 0.76556
	loss_reward: 0.06393
Optimization_Done 31800
[2025-05-07 19:12:30] [command] train weight_iter_31800.pkl 141 160
[2025-05-07 19:12:38] nn step 31850, lr: 0.1.
	loss_policy_0: 0.27256
	accuracy_policy_0: 0.63223
	loss_value_0: 0.35686
	loss_policy_1: 0.05484
	accuracy_policy_1: 0.6302
	loss_value_1: 0.07237
	loss_reward_1: 0.00718
	loss_policy_2: 0.05482
	accuracy_policy_2: 0.6373
	loss_value_2: 0.07411
	loss_reward_2: 0.00954
	loss_policy_3: 0.05479
	accuracy_policy_3: 0.63215
	loss_value_3: 0.0759
	loss_reward_3: 0.01218
	loss_policy_4: 0.05501
	accuracy_policy_4: 0.63891
	loss_value_4: 0.07762
	loss_reward_4: 0.01495
	loss_policy_5: 0.05519
	accuracy_policy_5: 0.62988
	loss_value_5: 0.079
	loss_reward_5: 0.01559
	loss_policy: 0.54721
	loss_value: 0.73585
	loss_reward: 0.05944
[2025-05-07 19:12:46] nn step 31900, lr: 0.1.
	loss_policy_0: 0.27263
	accuracy_policy_0: 0.6427
	loss_value_0: 0.35652
	loss_policy_1: 0.0553
	accuracy_policy_1: 0.63805
	loss_value_1: 0.07298
	loss_reward_1: 0.00737
	loss_policy_2: 0.05569
	accuracy_policy_2: 0.63164
	loss_value_2: 0.07485
	loss_reward_2: 0.0098
	loss_policy_3: 0.05571
	accuracy_policy_3: 0.63363
	loss_value_3: 0.07659
	loss_reward_3: 0.01236
	loss_policy_4: 0.05571
	accuracy_policy_4: 0.63594
	loss_value_4: 0.07841
	loss_reward_4: 0.01543
	loss_policy_5: 0.05589
	accuracy_policy_5: 0.63797
	loss_value_5: 0.07995
	loss_reward_5: 0.01568
	loss_policy: 0.55094
	loss_value: 0.7393
	loss_reward: 0.06063
[2025-05-07 19:12:54] nn step 31950, lr: 0.1.
	loss_policy_0: 0.27357
	accuracy_policy_0: 0.64035
	loss_value_0: 0.35333
	loss_policy_1: 0.05515
	accuracy_policy_1: 0.63766
	loss_value_1: 0.07237
	loss_reward_1: 0.00727
	loss_policy_2: 0.05497
	accuracy_policy_2: 0.63762
	loss_value_2: 0.07428
	loss_reward_2: 0.00992
	loss_policy_3: 0.055
	accuracy_policy_3: 0.64125
	loss_value_3: 0.076
	loss_reward_3: 0.01277
	loss_policy_4: 0.05486
	accuracy_policy_4: 0.6459
	loss_value_4: 0.07758
	loss_reward_4: 0.01538
	loss_policy_5: 0.05516
	accuracy_policy_5: 0.64164
	loss_value_5: 0.07907
	loss_reward_5: 0.01614
	loss_policy: 0.5487
	loss_value: 0.73263
	loss_reward: 0.0615
[2025-05-07 19:13:02] nn step 32000, lr: 0.1.
	loss_policy_0: 0.28589
	accuracy_policy_0: 0.64227
	loss_value_0: 0.36936
	loss_policy_1: 0.05744
	accuracy_policy_1: 0.63406
	loss_value_1: 0.07527
	loss_reward_1: 0.00783
	loss_policy_2: 0.0575
	accuracy_policy_2: 0.63719
	loss_value_2: 0.07755
	loss_reward_2: 0.01073
	loss_policy_3: 0.05752
	accuracy_policy_3: 0.63844
	loss_value_3: 0.07965
	loss_reward_3: 0.01389
	loss_policy_4: 0.0574
	accuracy_policy_4: 0.6423
	loss_value_4: 0.08139
	loss_reward_4: 0.01628
	loss_policy_5: 0.05789
	accuracy_policy_5: 0.64086
	loss_value_5: 0.0831
	loss_reward_5: 0.01708
	loss_policy: 0.57365
	loss_value: 0.76632
	loss_reward: 0.06581
Optimization_Done 32000
[2025-05-07 19:16:25] [command] train weight_iter_32000.pkl 142 161
[2025-05-07 19:16:32] nn step 32050, lr: 0.1.
	loss_policy_0: 0.2755
	accuracy_policy_0: 0.63676
	loss_value_0: 0.36365
	loss_policy_1: 0.05519
	accuracy_policy_1: 0.63699
	loss_value_1: 0.07411
	loss_reward_1: 0.00722
	loss_policy_2: 0.05516
	accuracy_policy_2: 0.63445
	loss_value_2: 0.07657
	loss_reward_2: 0.01018
	loss_policy_3: 0.05536
	accuracy_policy_3: 0.63711
	loss_value_3: 0.0779
	loss_reward_3: 0.01291
	loss_policy_4: 0.05567
	accuracy_policy_4: 0.63656
	loss_value_4: 0.07952
	loss_reward_4: 0.01498
	loss_policy_5: 0.05566
	accuracy_policy_5: 0.63996
	loss_value_5: 0.08092
	loss_reward_5: 0.01622
	loss_policy: 0.55255
	loss_value: 0.75268
	loss_reward: 0.06151
[2025-05-07 19:16:40] nn step 32100, lr: 0.1.
	loss_policy_0: 0.26941
	accuracy_policy_0: 0.63758
	loss_value_0: 0.3495
	loss_policy_1: 0.05395
	accuracy_policy_1: 0.6348
	loss_value_1: 0.07134
	loss_reward_1: 0.00677
	loss_policy_2: 0.05447
	accuracy_policy_2: 0.6352
	loss_value_2: 0.07346
	loss_reward_2: 0.0098
	loss_policy_3: 0.05436
	accuracy_policy_3: 0.63914
	loss_value_3: 0.07519
	loss_reward_3: 0.01264
	loss_policy_4: 0.05429
	accuracy_policy_4: 0.63852
	loss_value_4: 0.0766
	loss_reward_4: 0.01505
	loss_policy_5: 0.0544
	accuracy_policy_5: 0.63676
	loss_value_5: 0.07831
	loss_reward_5: 0.01555
	loss_policy: 0.54087
	loss_value: 0.7244
	loss_reward: 0.0598
[2025-05-07 19:16:48] nn step 32150, lr: 0.1.
	loss_policy_0: 0.25766
	accuracy_policy_0: 0.64984
	loss_value_0: 0.33716
	loss_policy_1: 0.05207
	accuracy_policy_1: 0.6359
	loss_value_1: 0.06901
	loss_reward_1: 0.00691
	loss_policy_2: 0.05246
	accuracy_policy_2: 0.63383
	loss_value_2: 0.07064
	loss_reward_2: 0.00959
	loss_policy_3: 0.05213
	accuracy_policy_3: 0.64004
	loss_value_3: 0.07223
	loss_reward_3: 0.01216
	loss_policy_4: 0.0522
	accuracy_policy_4: 0.64484
	loss_value_4: 0.07377
	loss_reward_4: 0.01468
	loss_policy_5: 0.05213
	accuracy_policy_5: 0.6407
	loss_value_5: 0.07535
	loss_reward_5: 0.0146
	loss_policy: 0.51864
	loss_value: 0.69816
	loss_reward: 0.05794
[2025-05-07 19:16:55] nn step 32200, lr: 0.1.
	loss_policy_0: 0.28609
	accuracy_policy_0: 0.6459
	loss_value_0: 0.3718
	loss_policy_1: 0.05739
	accuracy_policy_1: 0.64727
	loss_value_1: 0.07615
	loss_reward_1: 0.0076
	loss_policy_2: 0.05767
	accuracy_policy_2: 0.64426
	loss_value_2: 0.07791
	loss_reward_2: 0.01018
	loss_policy_3: 0.05757
	accuracy_policy_3: 0.6493
	loss_value_3: 0.08008
	loss_reward_3: 0.01337
	loss_policy_4: 0.05767
	accuracy_policy_4: 0.64875
	loss_value_4: 0.08159
	loss_reward_4: 0.01619
	loss_policy_5: 0.05788
	accuracy_policy_5: 0.64426
	loss_value_5: 0.08303
	loss_reward_5: 0.01697
	loss_policy: 0.57427
	loss_value: 0.77055
	loss_reward: 0.06432
Optimization_Done 32200
[2025-05-07 19:20:17] [command] train weight_iter_32200.pkl 143 162
[2025-05-07 19:20:26] nn step 32250, lr: 0.1.
	loss_policy_0: 0.27545
	accuracy_policy_0: 0.63914
	loss_value_0: 0.36275
	loss_policy_1: 0.05506
	accuracy_policy_1: 0.64043
	loss_value_1: 0.07402
	loss_reward_1: 0.00741
	loss_policy_2: 0.05539
	accuracy_policy_2: 0.64223
	loss_value_2: 0.07622
	loss_reward_2: 0.00988
	loss_policy_3: 0.05546
	accuracy_policy_3: 0.64062
	loss_value_3: 0.07763
	loss_reward_3: 0.0129
	loss_policy_4: 0.05544
	accuracy_policy_4: 0.64055
	loss_value_4: 0.07901
	loss_reward_4: 0.01542
	loss_policy_5: 0.05551
	accuracy_policy_5: 0.63965
	loss_value_5: 0.08063
	loss_reward_5: 0.01583
	loss_policy: 0.55231
	loss_value: 0.75026
	loss_reward: 0.06144
[2025-05-07 19:20:34] nn step 32300, lr: 0.1.
	loss_policy_0: 0.26503
	accuracy_policy_0: 0.64504
	loss_value_0: 0.348
	loss_policy_1: 0.05378
	accuracy_policy_1: 0.63379
	loss_value_1: 0.07112
	loss_reward_1: 0.0073
	loss_policy_2: 0.05366
	accuracy_policy_2: 0.63664
	loss_value_2: 0.07284
	loss_reward_2: 0.00974
	loss_policy_3: 0.05377
	accuracy_policy_3: 0.64016
	loss_value_3: 0.07473
	loss_reward_3: 0.01257
	loss_policy_4: 0.05371
	accuracy_policy_4: 0.63922
	loss_value_4: 0.07601
	loss_reward_4: 0.01511
	loss_policy_5: 0.05351
	accuracy_policy_5: 0.64199
	loss_value_5: 0.07771
	loss_reward_5: 0.01564
	loss_policy: 0.53345
	loss_value: 0.72042
	loss_reward: 0.06036
[2025-05-07 19:20:42] nn step 32350, lr: 0.1.
	loss_policy_0: 0.28478
	accuracy_policy_0: 0.6368
	loss_value_0: 0.36642
	loss_policy_1: 0.05692
	accuracy_policy_1: 0.6343
	loss_value_1: 0.07516
	loss_reward_1: 0.00735
	loss_policy_2: 0.05685
	accuracy_policy_2: 0.64047
	loss_value_2: 0.077
	loss_reward_2: 0.0104
	loss_policy_3: 0.05726
	accuracy_policy_3: 0.64102
	loss_value_3: 0.07883
	loss_reward_3: 0.01326
	loss_policy_4: 0.05736
	accuracy_policy_4: 0.63902
	loss_value_4: 0.08092
	loss_reward_4: 0.01603
	loss_policy_5: 0.05708
	accuracy_policy_5: 0.64012
	loss_value_5: 0.08281
	loss_reward_5: 0.01662
	loss_policy: 0.57026
	loss_value: 0.76115
	loss_reward: 0.06366
[2025-05-07 19:20:48] nn step 32400, lr: 0.1.
	loss_policy_0: 0.26517
	accuracy_policy_0: 0.64953
	loss_value_0: 0.34676
	loss_policy_1: 0.05358
	accuracy_policy_1: 0.63973
	loss_value_1: 0.07106
	loss_reward_1: 0.00705
	loss_policy_2: 0.05374
	accuracy_policy_2: 0.64121
	loss_value_2: 0.07303
	loss_reward_2: 0.00981
	loss_policy_3: 0.05375
	accuracy_policy_3: 0.64301
	loss_value_3: 0.07456
	loss_reward_3: 0.01226
	loss_policy_4: 0.05389
	accuracy_policy_4: 0.64188
	loss_value_4: 0.07603
	loss_reward_4: 0.0153
	loss_policy_5: 0.05353
	accuracy_policy_5: 0.64336
	loss_value_5: 0.07775
	loss_reward_5: 0.01626
	loss_policy: 0.53367
	loss_value: 0.71918
	loss_reward: 0.06068
Optimization_Done 32400
[2025-05-07 19:24:10] [command] train weight_iter_32400.pkl 144 163
[2025-05-07 19:24:19] nn step 32450, lr: 0.1.
	loss_policy_0: 0.26428
	accuracy_policy_0: 0.64352
	loss_value_0: 0.35106
	loss_policy_1: 0.05308
	accuracy_policy_1: 0.64137
	loss_value_1: 0.07132
	loss_reward_1: 0.00694
	loss_policy_2: 0.05293
	accuracy_policy_2: 0.64473
	loss_value_2: 0.07312
	loss_reward_2: 0.00946
	loss_policy_3: 0.05306
	accuracy_policy_3: 0.64176
	loss_value_3: 0.07474
	loss_reward_3: 0.01224
	loss_policy_4: 0.05271
	accuracy_policy_4: 0.6448
	loss_value_4: 0.07652
	loss_reward_4: 0.01492
	loss_policy_5: 0.05266
	accuracy_policy_5: 0.65117
	loss_value_5: 0.07815
	loss_reward_5: 0.01528
	loss_policy: 0.52873
	loss_value: 0.7249
	loss_reward: 0.05884
[2025-05-07 19:24:27] nn step 32500, lr: 0.1.
	loss_policy_0: 0.30276
	accuracy_policy_0: 0.63156
	loss_value_0: 0.39097
	loss_policy_1: 0.05985
	accuracy_policy_1: 0.63617
	loss_value_1: 0.07995
	loss_reward_1: 0.00787
	loss_policy_2: 0.05973
	accuracy_policy_2: 0.63957
	loss_value_2: 0.08189
	loss_reward_2: 0.01119
	loss_policy_3: 0.06001
	accuracy_policy_3: 0.64027
	loss_value_3: 0.08373
	loss_reward_3: 0.01403
	loss_policy_4: 0.06019
	accuracy_policy_4: 0.63918
	loss_value_4: 0.08534
	loss_reward_4: 0.01659
	loss_policy_5: 0.06014
	accuracy_policy_5: 0.63879
	loss_value_5: 0.08708
	loss_reward_5: 0.01781
	loss_policy: 0.60269
	loss_value: 0.80896
	loss_reward: 0.0675
[2025-05-07 19:24:33] nn step 32550, lr: 0.1.
	loss_policy_0: 0.27544
	accuracy_policy_0: 0.65055
	loss_value_0: 0.36105
	loss_policy_1: 0.05569
	accuracy_policy_1: 0.63813
	loss_value_1: 0.07392
	loss_reward_1: 0.00712
	loss_policy_2: 0.05575
	accuracy_policy_2: 0.64238
	loss_value_2: 0.07549
	loss_reward_2: 0.00993
	loss_policy_3: 0.05568
	accuracy_policy_3: 0.63793
	loss_value_3: 0.0771
	loss_reward_3: 0.01306
	loss_policy_4: 0.05556
	accuracy_policy_4: 0.64402
	loss_value_4: 0.07894
	loss_reward_4: 0.01549
	loss_policy_5: 0.05605
	accuracy_policy_5: 0.63863
	loss_value_5: 0.08057
	loss_reward_5: 0.01595
	loss_policy: 0.55417
	loss_value: 0.74706
	loss_reward: 0.06155
[2025-05-07 19:24:41] nn step 32600, lr: 0.1.
	loss_policy_0: 0.29486
	accuracy_policy_0: 0.64516
	loss_value_0: 0.38244
	loss_policy_1: 0.05913
	accuracy_policy_1: 0.63863
	loss_value_1: 0.07819
	loss_reward_1: 0.00776
	loss_policy_2: 0.05913
	accuracy_policy_2: 0.64426
	loss_value_2: 0.08027
	loss_reward_2: 0.01081
	loss_policy_3: 0.05959
	accuracy_policy_3: 0.64395
	loss_value_3: 0.08232
	loss_reward_3: 0.01378
	loss_policy_4: 0.05932
	accuracy_policy_4: 0.64266
	loss_value_4: 0.0844
	loss_reward_4: 0.01626
	loss_policy_5: 0.05956
	accuracy_policy_5: 0.64613
	loss_value_5: 0.08608
	loss_reward_5: 0.01697
	loss_policy: 0.59158
	loss_value: 0.7937
	loss_reward: 0.06558
Optimization_Done 32600
[2025-05-07 19:28:02] [command] train weight_iter_32600.pkl 145 164
[2025-05-07 19:28:11] nn step 32650, lr: 0.1.
	loss_policy_0: 0.30007
	accuracy_policy_0: 0.6432
	loss_value_0: 0.4004
	loss_policy_1: 0.06054
	accuracy_policy_1: 0.63809
	loss_value_1: 0.08153
	loss_reward_1: 0.00805
	loss_policy_2: 0.06055
	accuracy_policy_2: 0.63891
	loss_value_2: 0.08385
	loss_reward_2: 0.01117
	loss_policy_3: 0.06037
	accuracy_policy_3: 0.64188
	loss_value_3: 0.08555
	loss_reward_3: 0.01367
	loss_policy_4: 0.06065
	accuracy_policy_4: 0.64117
	loss_value_4: 0.08688
	loss_reward_4: 0.0177
	loss_policy_5: 0.06092
	accuracy_policy_5: 0.64008
	loss_value_5: 0.08863
	loss_reward_5: 0.01851
	loss_policy: 0.6031
	loss_value: 0.82685
	loss_reward: 0.0691
[2025-05-07 19:28:18] nn step 32700, lr: 0.1.
	loss_policy_0: 0.26438
	accuracy_policy_0: 0.63945
	loss_value_0: 0.34787
	loss_policy_1: 0.05282
	accuracy_policy_1: 0.63715
	loss_value_1: 0.07118
	loss_reward_1: 0.00711
	loss_policy_2: 0.05307
	accuracy_policy_2: 0.63648
	loss_value_2: 0.07273
	loss_reward_2: 0.00996
	loss_policy_3: 0.05292
	accuracy_policy_3: 0.64359
	loss_value_3: 0.07439
	loss_reward_3: 0.01272
	loss_policy_4: 0.05335
	accuracy_policy_4: 0.64168
	loss_value_4: 0.07614
	loss_reward_4: 0.01462
	loss_policy_5: 0.05324
	accuracy_policy_5: 0.6432
	loss_value_5: 0.07772
	loss_reward_5: 0.01564
	loss_policy: 0.52978
	loss_value: 0.72003
	loss_reward: 0.06005
[2025-05-07 19:28:26] nn step 32750, lr: 0.1.
	loss_policy_0: 0.26932
	accuracy_policy_0: 0.64703
	loss_value_0: 0.35266
	loss_policy_1: 0.05442
	accuracy_policy_1: 0.63902
	loss_value_1: 0.07212
	loss_reward_1: 0.00718
	loss_policy_2: 0.05484
	accuracy_policy_2: 0.63531
	loss_value_2: 0.07372
	loss_reward_2: 0.01001
	loss_policy_3: 0.05443
	accuracy_policy_3: 0.64121
	loss_value_3: 0.0756
	loss_reward_3: 0.01257
	loss_policy_4: 0.05391
	accuracy_policy_4: 0.64918
	loss_value_4: 0.0772
	loss_reward_4: 0.01514
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.64195
	loss_value_5: 0.07842
	loss_reward_5: 0.01613
	loss_policy: 0.54153
	loss_value: 0.72972
	loss_reward: 0.06103
[2025-05-07 19:28:34] nn step 32800, lr: 0.1.
	loss_policy_0: 0.29489
	accuracy_policy_0: 0.64254
	loss_value_0: 0.38569
	loss_policy_1: 0.05938
	accuracy_policy_1: 0.63887
	loss_value_1: 0.07879
	loss_reward_1: 0.00795
	loss_policy_2: 0.05912
	accuracy_policy_2: 0.64215
	loss_value_2: 0.08075
	loss_reward_2: 0.01122
	loss_policy_3: 0.05908
	accuracy_policy_3: 0.6448
	loss_value_3: 0.08287
	loss_reward_3: 0.014
	loss_policy_4: 0.05942
	accuracy_policy_4: 0.64141
	loss_value_4: 0.0847
	loss_reward_4: 0.01691
	loss_policy_5: 0.05923
	accuracy_policy_5: 0.64461
	loss_value_5: 0.08646
	loss_reward_5: 0.01754
	loss_policy: 0.59111
	loss_value: 0.79926
	loss_reward: 0.06763
Optimization_Done 32800
[2025-05-07 19:31:57] [command] train weight_iter_32800.pkl 146 165
[2025-05-07 19:32:05] nn step 32850, lr: 0.1.
	loss_policy_0: 0.28512
	accuracy_policy_0: 0.62191
	loss_value_0: 0.36237
	loss_policy_1: 0.05647
	accuracy_policy_1: 0.6232
	loss_value_1: 0.07386
	loss_reward_1: 0.00738
	loss_policy_2: 0.05643
	accuracy_policy_2: 0.62121
	loss_value_2: 0.0757
	loss_reward_2: 0.00981
	loss_policy_3: 0.05628
	accuracy_policy_3: 0.63137
	loss_value_3: 0.07727
	loss_reward_3: 0.0126
	loss_policy_4: 0.0565
	accuracy_policy_4: 0.62711
	loss_value_4: 0.07897
	loss_reward_4: 0.01502
	loss_policy_5: 0.05655
	accuracy_policy_5: 0.62594
	loss_value_5: 0.08049
	loss_reward_5: 0.016
	loss_policy: 0.56736
	loss_value: 0.74867
	loss_reward: 0.06081
[2025-05-07 19:32:12] nn step 32900, lr: 0.1.
	loss_policy_0: 0.27884
	accuracy_policy_0: 0.63629
	loss_value_0: 0.35715
	loss_policy_1: 0.05599
	accuracy_policy_1: 0.6302
	loss_value_1: 0.07317
	loss_reward_1: 0.00702
	loss_policy_2: 0.05607
	accuracy_policy_2: 0.62973
	loss_value_2: 0.0751
	loss_reward_2: 0.00986
	loss_policy_3: 0.05605
	accuracy_policy_3: 0.6309
	loss_value_3: 0.07675
	loss_reward_3: 0.01276
	loss_policy_4: 0.05577
	accuracy_policy_4: 0.63527
	loss_value_4: 0.07826
	loss_reward_4: 0.01549
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.63598
	loss_value_5: 0.07965
	loss_reward_5: 0.01631
	loss_policy: 0.55869
	loss_value: 0.74008
	loss_reward: 0.06144
[2025-05-07 19:32:20] nn step 32950, lr: 0.1.
	loss_policy_0: 0.28138
	accuracy_policy_0: 0.63918
	loss_value_0: 0.36034
	loss_policy_1: 0.05661
	accuracy_policy_1: 0.63086
	loss_value_1: 0.07373
	loss_reward_1: 0.00744
	loss_policy_2: 0.0565
	accuracy_policy_2: 0.62953
	loss_value_2: 0.07577
	loss_reward_2: 0.01001
	loss_policy_3: 0.05674
	accuracy_policy_3: 0.63496
	loss_value_3: 0.07784
	loss_reward_3: 0.01266
	loss_policy_4: 0.05661
	accuracy_policy_4: 0.63535
	loss_value_4: 0.07937
	loss_reward_4: 0.01555
	loss_policy_5: 0.05678
	accuracy_policy_5: 0.63469
	loss_value_5: 0.08105
	loss_reward_5: 0.01589
	loss_policy: 0.56461
	loss_value: 0.74811
	loss_reward: 0.06155
[2025-05-07 19:32:28] nn step 33000, lr: 0.1.
	loss_policy_0: 0.28662
	accuracy_policy_0: 0.63742
	loss_value_0: 0.36711
	loss_policy_1: 0.05778
	accuracy_policy_1: 0.63242
	loss_value_1: 0.07517
	loss_reward_1: 0.00735
	loss_policy_2: 0.05787
	accuracy_policy_2: 0.63062
	loss_value_2: 0.0771
	loss_reward_2: 0.01033
	loss_policy_3: 0.05786
	accuracy_policy_3: 0.63367
	loss_value_3: 0.07909
	loss_reward_3: 0.01318
	loss_policy_4: 0.05804
	accuracy_policy_4: 0.63168
	loss_value_4: 0.08083
	loss_reward_4: 0.01585
	loss_policy_5: 0.05825
	accuracy_policy_5: 0.63164
	loss_value_5: 0.08263
	loss_reward_5: 0.01647
	loss_policy: 0.57642
	loss_value: 0.76194
	loss_reward: 0.06317
Optimization_Done 33000
[2025-05-07 19:36:07] [command] train weight_iter_33000.pkl 147 166
[2025-05-07 19:36:16] nn step 33050, lr: 0.1.
	loss_policy_0: 0.27011
	accuracy_policy_0: 0.64242
	loss_value_0: 0.36111
	loss_policy_1: 0.05459
	accuracy_policy_1: 0.64051
	loss_value_1: 0.07323
	loss_reward_1: 0.00734
	loss_policy_2: 0.05508
	accuracy_policy_2: 0.6343
	loss_value_2: 0.07542
	loss_reward_2: 0.00995
	loss_policy_3: 0.05486
	accuracy_policy_3: 0.63914
	loss_value_3: 0.07725
	loss_reward_3: 0.01262
	loss_policy_4: 0.05519
	accuracy_policy_4: 0.6352
	loss_value_4: 0.07878
	loss_reward_4: 0.01518
	loss_policy_5: 0.05487
	accuracy_policy_5: 0.63973
	loss_value_5: 0.08028
	loss_reward_5: 0.0161
	loss_policy: 0.54471
	loss_value: 0.74607
	loss_reward: 0.06119
[2025-05-07 19:36:23] nn step 33100, lr: 0.1.
	loss_policy_0: 0.27272
	accuracy_policy_0: 0.64008
	loss_value_0: 0.35225
	loss_policy_1: 0.05462
	accuracy_policy_1: 0.63594
	loss_value_1: 0.07232
	loss_reward_1: 0.00723
	loss_policy_2: 0.05463
	accuracy_policy_2: 0.63559
	loss_value_2: 0.07413
	loss_reward_2: 0.00965
	loss_policy_3: 0.05483
	accuracy_policy_3: 0.63477
	loss_value_3: 0.07565
	loss_reward_3: 0.01286
	loss_policy_4: 0.05411
	accuracy_policy_4: 0.64082
	loss_value_4: 0.07703
	loss_reward_4: 0.01508
	loss_policy_5: 0.05468
	accuracy_policy_5: 0.63852
	loss_value_5: 0.07842
	loss_reward_5: 0.01567
	loss_policy: 0.5456
	loss_value: 0.7298
	loss_reward: 0.0605
[2025-05-07 19:36:31] nn step 33150, lr: 0.1.
	loss_policy_0: 0.27002
	accuracy_policy_0: 0.64004
	loss_value_0: 0.34941
	loss_policy_1: 0.05453
	accuracy_policy_1: 0.63512
	loss_value_1: 0.07129
	loss_reward_1: 0.00715
	loss_policy_2: 0.05479
	accuracy_policy_2: 0.63316
	loss_value_2: 0.07318
	loss_reward_2: 0.00993
	loss_policy_3: 0.0544
	accuracy_policy_3: 0.63801
	loss_value_3: 0.07493
	loss_reward_3: 0.01251
	loss_policy_4: 0.05443
	accuracy_policy_4: 0.63949
	loss_value_4: 0.07649
	loss_reward_4: 0.01485
	loss_policy_5: 0.05443
	accuracy_policy_5: 0.63918
	loss_value_5: 0.07799
	loss_reward_5: 0.01602
	loss_policy: 0.54259
	loss_value: 0.72329
	loss_reward: 0.06045
[2025-05-07 19:36:39] nn step 33200, lr: 0.1.
	loss_policy_0: 0.27953
	accuracy_policy_0: 0.63816
	loss_value_0: 0.35737
	loss_policy_1: 0.05597
	accuracy_policy_1: 0.63758
	loss_value_1: 0.07324
	loss_reward_1: 0.00734
	loss_policy_2: 0.05623
	accuracy_policy_2: 0.63438
	loss_value_2: 0.07539
	loss_reward_2: 0.01013
	loss_policy_3: 0.05616
	accuracy_policy_3: 0.63652
	loss_value_3: 0.07678
	loss_reward_3: 0.0127
	loss_policy_4: 0.05617
	accuracy_policy_4: 0.63813
	loss_value_4: 0.07851
	loss_reward_4: 0.01586
	loss_policy_5: 0.05609
	accuracy_policy_5: 0.64254
	loss_value_5: 0.08025
	loss_reward_5: 0.01659
	loss_policy: 0.56015
	loss_value: 0.74153
	loss_reward: 0.06262
Optimization_Done 33200
[2025-05-07 19:39:59] [command] train weight_iter_33200.pkl 148 167
[2025-05-07 19:40:08] nn step 33250, lr: 0.1.
	loss_policy_0: 0.28361
	accuracy_policy_0: 0.64039
	loss_value_0: 0.36976
	loss_policy_1: 0.05741
	accuracy_policy_1: 0.63215
	loss_value_1: 0.07555
	loss_reward_1: 0.00773
	loss_policy_2: 0.05695
	accuracy_policy_2: 0.63492
	loss_value_2: 0.0776
	loss_reward_2: 0.01048
	loss_policy_3: 0.05725
	accuracy_policy_3: 0.64027
	loss_value_3: 0.07929
	loss_reward_3: 0.01298
	loss_policy_4: 0.05728
	accuracy_policy_4: 0.6373
	loss_value_4: 0.08077
	loss_reward_4: 0.0157
	loss_policy_5: 0.05739
	accuracy_policy_5: 0.63359
	loss_value_5: 0.08215
	loss_reward_5: 0.01631
	loss_policy: 0.5699
	loss_value: 0.76511
	loss_reward: 0.0632
[2025-05-07 19:40:14] nn step 33300, lr: 0.1.
	loss_policy_0: 0.28688
	accuracy_policy_0: 0.64684
	loss_value_0: 0.37224
	loss_policy_1: 0.05722
	accuracy_policy_1: 0.63887
	loss_value_1: 0.07578
	loss_reward_1: 0.00726
	loss_policy_2: 0.05784
	accuracy_policy_2: 0.63457
	loss_value_2: 0.07771
	loss_reward_2: 0.01017
	loss_policy_3: 0.05752
	accuracy_policy_3: 0.63719
	loss_value_3: 0.07944
	loss_reward_3: 0.01325
	loss_policy_4: 0.05771
	accuracy_policy_4: 0.64141
	loss_value_4: 0.08089
	loss_reward_4: 0.01569
	loss_policy_5: 0.05785
	accuracy_policy_5: 0.63625
	loss_value_5: 0.08282
	loss_reward_5: 0.01684
	loss_policy: 0.57502
	loss_value: 0.76888
	loss_reward: 0.06322
[2025-05-07 19:40:23] nn step 33350, lr: 0.1.
	loss_policy_0: 0.27915
	accuracy_policy_0: 0.63711
	loss_value_0: 0.35718
	loss_policy_1: 0.05591
	accuracy_policy_1: 0.63781
	loss_value_1: 0.07291
	loss_reward_1: 0.00733
	loss_policy_2: 0.05616
	accuracy_policy_2: 0.63395
	loss_value_2: 0.07492
	loss_reward_2: 0.00968
	loss_policy_3: 0.05658
	accuracy_policy_3: 0.63457
	loss_value_3: 0.07682
	loss_reward_3: 0.01282
	loss_policy_4: 0.05633
	accuracy_policy_4: 0.63512
	loss_value_4: 0.07843
	loss_reward_4: 0.01548
	loss_policy_5: 0.05633
	accuracy_policy_5: 0.6384
	loss_value_5: 0.07997
	loss_reward_5: 0.01605
	loss_policy: 0.56045
	loss_value: 0.74024
	loss_reward: 0.06137
[2025-05-07 19:40:31] nn step 33400, lr: 0.1.
	loss_policy_0: 0.28698
	accuracy_policy_0: 0.6384
	loss_value_0: 0.36979
	loss_policy_1: 0.05749
	accuracy_policy_1: 0.64023
	loss_value_1: 0.07555
	loss_reward_1: 0.00748
	loss_policy_2: 0.05783
	accuracy_policy_2: 0.63578
	loss_value_2: 0.07759
	loss_reward_2: 0.01091
	loss_policy_3: 0.05804
	accuracy_policy_3: 0.63613
	loss_value_3: 0.07936
	loss_reward_3: 0.01338
	loss_policy_4: 0.05782
	accuracy_policy_4: 0.64449
	loss_value_4: 0.08084
	loss_reward_4: 0.01603
	loss_policy_5: 0.05814
	accuracy_policy_5: 0.63926
	loss_value_5: 0.08245
	loss_reward_5: 0.01689
	loss_policy: 0.5763
	loss_value: 0.76557
	loss_reward: 0.06469
Optimization_Done 33400
[2025-05-07 19:44:05] [command] train weight_iter_33400.pkl 149 168
[2025-05-07 19:44:13] nn step 33450, lr: 0.1.
	loss_policy_0: 0.28065
	accuracy_policy_0: 0.63727
	loss_value_0: 0.36615
	loss_policy_1: 0.05645
	accuracy_policy_1: 0.63434
	loss_value_1: 0.07465
	loss_reward_1: 0.00742
	loss_policy_2: 0.05656
	accuracy_policy_2: 0.6341
	loss_value_2: 0.07638
	loss_reward_2: 0.01022
	loss_policy_3: 0.0569
	accuracy_policy_3: 0.63293
	loss_value_3: 0.07802
	loss_reward_3: 0.01307
	loss_policy_4: 0.05686
	accuracy_policy_4: 0.63148
	loss_value_4: 0.07962
	loss_reward_4: 0.01589
	loss_policy_5: 0.05645
	accuracy_policy_5: 0.63559
	loss_value_5: 0.08132
	loss_reward_5: 0.01668
	loss_policy: 0.56387
	loss_value: 0.75614
	loss_reward: 0.06328
[2025-05-07 19:44:22] nn step 33500, lr: 0.1.
	loss_policy_0: 0.27839
	accuracy_policy_0: 0.6427
	loss_value_0: 0.36223
	loss_policy_1: 0.05674
	accuracy_policy_1: 0.63328
	loss_value_1: 0.07416
	loss_reward_1: 0.00738
	loss_policy_2: 0.05681
	accuracy_policy_2: 0.63746
	loss_value_2: 0.07595
	loss_reward_2: 0.00998
	loss_policy_3: 0.05655
	accuracy_policy_3: 0.63395
	loss_value_3: 0.07773
	loss_reward_3: 0.01259
	loss_policy_4: 0.05676
	accuracy_policy_4: 0.63273
	loss_value_4: 0.07939
	loss_reward_4: 0.01562
	loss_policy_5: 0.05666
	accuracy_policy_5: 0.63402
	loss_value_5: 0.08078
	loss_reward_5: 0.01616
	loss_policy: 0.5619
	loss_value: 0.75024
	loss_reward: 0.06173
[2025-05-07 19:44:28] nn step 33550, lr: 0.1.
	loss_policy_0: 0.29326
	accuracy_policy_0: 0.63863
	loss_value_0: 0.37581
	loss_policy_1: 0.0594
	accuracy_policy_1: 0.63551
	loss_value_1: 0.07666
	loss_reward_1: 0.00774
	loss_policy_2: 0.05932
	accuracy_policy_2: 0.63629
	loss_value_2: 0.07869
	loss_reward_2: 0.01048
	loss_policy_3: 0.05915
	accuracy_policy_3: 0.63523
	loss_value_3: 0.0804
	loss_reward_3: 0.01338
	loss_policy_4: 0.0589
	accuracy_policy_4: 0.6427
	loss_value_4: 0.08206
	loss_reward_4: 0.01646
	loss_policy_5: 0.05912
	accuracy_policy_5: 0.64312
	loss_value_5: 0.08441
	loss_reward_5: 0.01681
	loss_policy: 0.58915
	loss_value: 0.77803
	loss_reward: 0.06486
[2025-05-07 19:44:36] nn step 33600, lr: 0.1.
	loss_policy_0: 0.29186
	accuracy_policy_0: 0.63266
	loss_value_0: 0.36777
	loss_policy_1: 0.05813
	accuracy_policy_1: 0.63336
	loss_value_1: 0.07512
	loss_reward_1: 0.00741
	loss_policy_2: 0.0576
	accuracy_policy_2: 0.64094
	loss_value_2: 0.07716
	loss_reward_2: 0.01021
	loss_policy_3: 0.058
	accuracy_policy_3: 0.63734
	loss_value_3: 0.07934
	loss_reward_3: 0.01337
	loss_policy_4: 0.05827
	accuracy_policy_4: 0.63207
	loss_value_4: 0.08118
	loss_reward_4: 0.01549
	loss_policy_5: 0.05814
	accuracy_policy_5: 0.63313
	loss_value_5: 0.08257
	loss_reward_5: 0.01681
	loss_policy: 0.58199
	loss_value: 0.76315
	loss_reward: 0.06327
Optimization_Done 33600
[2025-05-07 19:48:00] [command] train weight_iter_33600.pkl 150 169
[2025-05-07 19:48:09] nn step 33650, lr: 0.1.
	loss_policy_0: 0.27953
	accuracy_policy_0: 0.61578
	loss_value_0: 0.35593
	loss_policy_1: 0.05528
	accuracy_policy_1: 0.6166
	loss_value_1: 0.07252
	loss_reward_1: 0.0073
	loss_policy_2: 0.05513
	accuracy_policy_2: 0.61988
	loss_value_2: 0.07441
	loss_reward_2: 0.00951
	loss_policy_3: 0.05532
	accuracy_policy_3: 0.61547
	loss_value_3: 0.07585
	loss_reward_3: 0.01259
	loss_policy_4: 0.05551
	accuracy_policy_4: 0.61891
	loss_value_4: 0.07733
	loss_reward_4: 0.01445
	loss_policy_5: 0.05557
	accuracy_policy_5: 0.61648
	loss_value_5: 0.0786
	loss_reward_5: 0.01543
	loss_policy: 0.55636
	loss_value: 0.73462
	loss_reward: 0.05927
[2025-05-07 19:48:16] nn step 33700, lr: 0.1.
	loss_policy_0: 0.29668
	accuracy_policy_0: 0.62875
	loss_value_0: 0.38202
	loss_policy_1: 0.05998
	accuracy_policy_1: 0.61973
	loss_value_1: 0.078
	loss_reward_1: 0.00769
	loss_policy_2: 0.05991
	accuracy_policy_2: 0.62133
	loss_value_2: 0.08003
	loss_reward_2: 0.01021
	loss_policy_3: 0.05948
	accuracy_policy_3: 0.62438
	loss_value_3: 0.08182
	loss_reward_3: 0.01327
	loss_policy_4: 0.05966
	accuracy_policy_4: 0.62598
	loss_value_4: 0.08367
	loss_reward_4: 0.01587
	loss_policy_5: 0.0598
	accuracy_policy_5: 0.6234
	loss_value_5: 0.08548
	loss_reward_5: 0.01698
	loss_policy: 0.5955
	loss_value: 0.79103
	loss_reward: 0.06402
[2025-05-07 19:48:24] nn step 33750, lr: 0.1.
	loss_policy_0: 0.29075
	accuracy_policy_0: 0.63086
	loss_value_0: 0.36802
	loss_policy_1: 0.05853
	accuracy_policy_1: 0.61988
	loss_value_1: 0.0754
	loss_reward_1: 0.00746
	loss_policy_2: 0.05868
	accuracy_policy_2: 0.6216
	loss_value_2: 0.07738
	loss_reward_2: 0.00985
	loss_policy_3: 0.05852
	accuracy_policy_3: 0.62496
	loss_value_3: 0.07947
	loss_reward_3: 0.01315
	loss_policy_4: 0.05832
	accuracy_policy_4: 0.62766
	loss_value_4: 0.08109
	loss_reward_4: 0.01562
	loss_policy_5: 0.05865
	accuracy_policy_5: 0.62336
	loss_value_5: 0.08278
	loss_reward_5: 0.01612
	loss_policy: 0.58346
	loss_value: 0.76414
	loss_reward: 0.0622
[2025-05-07 19:48:32] nn step 33800, lr: 0.1.
	loss_policy_0: 0.28814
	accuracy_policy_0: 0.63395
	loss_value_0: 0.36695
	loss_policy_1: 0.05792
	accuracy_policy_1: 0.62687
	loss_value_1: 0.07519
	loss_reward_1: 0.00693
	loss_policy_2: 0.05806
	accuracy_policy_2: 0.63105
	loss_value_2: 0.07714
	loss_reward_2: 0.00975
	loss_policy_3: 0.05848
	accuracy_policy_3: 0.6259
	loss_value_3: 0.07903
	loss_reward_3: 0.01291
	loss_policy_4: 0.05841
	accuracy_policy_4: 0.62852
	loss_value_4: 0.08095
	loss_reward_4: 0.01538
	loss_policy_5: 0.05829
	accuracy_policy_5: 0.63176
	loss_value_5: 0.08229
	loss_reward_5: 0.01631
	loss_policy: 0.57929
	loss_value: 0.76155
	loss_reward: 0.06129
Optimization_Done 33800
[2025-05-07 19:52:02] [command] train weight_iter_33800.pkl 151 170
[2025-05-07 19:52:11] nn step 33850, lr: 0.1.
	loss_policy_0: 0.27977
	accuracy_policy_0: 0.62027
	loss_value_0: 0.36201
	loss_policy_1: 0.05625
	accuracy_policy_1: 0.61266
	loss_value_1: 0.07378
	loss_reward_1: 0.0073
	loss_policy_2: 0.05645
	accuracy_policy_2: 0.61246
	loss_value_2: 0.0756
	loss_reward_2: 0.01002
	loss_policy_3: 0.0564
	accuracy_policy_3: 0.62016
	loss_value_3: 0.07754
	loss_reward_3: 0.01246
	loss_policy_4: 0.05666
	accuracy_policy_4: 0.61863
	loss_value_4: 0.07922
	loss_reward_4: 0.01504
	loss_policy_5: 0.05665
	accuracy_policy_5: 0.61824
	loss_value_5: 0.08068
	loss_reward_5: 0.01585
	loss_policy: 0.56218
	loss_value: 0.74882
	loss_reward: 0.06067
[2025-05-07 19:52:19] nn step 33900, lr: 0.1.
	loss_policy_0: 0.30866
	accuracy_policy_0: 0.62578
	loss_value_0: 0.39485
	loss_policy_1: 0.06203
	accuracy_policy_1: 0.61809
	loss_value_1: 0.08082
	loss_reward_1: 0.00805
	loss_policy_2: 0.06232
	accuracy_policy_2: 0.62023
	loss_value_2: 0.08291
	loss_reward_2: 0.0111
	loss_policy_3: 0.06193
	accuracy_policy_3: 0.61816
	loss_value_3: 0.08489
	loss_reward_3: 0.01371
	loss_policy_4: 0.06193
	accuracy_policy_4: 0.61996
	loss_value_4: 0.08681
	loss_reward_4: 0.01639
	loss_policy_5: 0.06188
	accuracy_policy_5: 0.6202
	loss_value_5: 0.08855
	loss_reward_5: 0.01747
	loss_policy: 0.61875
	loss_value: 0.81883
	loss_reward: 0.06672
[2025-05-07 19:52:27] nn step 33950, lr: 0.1.
	loss_policy_0: 0.29396
	accuracy_policy_0: 0.61438
	loss_value_0: 0.36895
	loss_policy_1: 0.0581
	accuracy_policy_1: 0.62277
	loss_value_1: 0.07534
	loss_reward_1: 0.00737
	loss_policy_2: 0.0582
	accuracy_policy_2: 0.6202
	loss_value_2: 0.0773
	loss_reward_2: 0.01032
	loss_policy_3: 0.05799
	accuracy_policy_3: 0.62105
	loss_value_3: 0.07909
	loss_reward_3: 0.01297
	loss_policy_4: 0.05792
	accuracy_policy_4: 0.62078
	loss_value_4: 0.08093
	loss_reward_4: 0.01612
	loss_policy_5: 0.05817
	accuracy_policy_5: 0.62738
	loss_value_5: 0.08254
	loss_reward_5: 0.0167
	loss_policy: 0.58434
	loss_value: 0.76417
	loss_reward: 0.0635
[2025-05-07 19:52:34] nn step 34000, lr: 0.1.
	loss_policy_0: 0.29894
	accuracy_policy_0: 0.62715
	loss_value_0: 0.38077
	loss_policy_1: 0.06003
	accuracy_policy_1: 0.62187
	loss_value_1: 0.07741
	loss_reward_1: 0.00767
	loss_policy_2: 0.05984
	accuracy_policy_2: 0.62641
	loss_value_2: 0.0796
	loss_reward_2: 0.01073
	loss_policy_3: 0.06021
	accuracy_policy_3: 0.6232
	loss_value_3: 0.08178
	loss_reward_3: 0.01366
	loss_policy_4: 0.05964
	accuracy_policy_4: 0.62832
	loss_value_4: 0.08368
	loss_reward_4: 0.01646
	loss_policy_5: 0.05987
	accuracy_policy_5: 0.62602
	loss_value_5: 0.08563
	loss_reward_5: 0.0174
	loss_policy: 0.59853
	loss_value: 0.78887
	loss_reward: 0.06593
Optimization_Done 34000
[2025-05-07 19:55:53] [command] train weight_iter_34000.pkl 152 171
[2025-05-07 19:56:01] nn step 34050, lr: 0.1.
	loss_policy_0: 0.27727
	accuracy_policy_0: 0.62895
	loss_value_0: 0.35895
	loss_policy_1: 0.05606
	accuracy_policy_1: 0.61742
	loss_value_1: 0.07318
	loss_reward_1: 0.00705
	loss_policy_2: 0.05585
	accuracy_policy_2: 0.6243
	loss_value_2: 0.07479
	loss_reward_2: 0.00959
	loss_policy_3: 0.05577
	accuracy_policy_3: 0.6227
	loss_value_3: 0.07652
	loss_reward_3: 0.01278
	loss_policy_4: 0.05609
	accuracy_policy_4: 0.62762
	loss_value_4: 0.07801
	loss_reward_4: 0.01495
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.62777
	loss_value_5: 0.07968
	loss_reward_5: 0.0156
	loss_policy: 0.55692
	loss_value: 0.74113
	loss_reward: 0.05997
[2025-05-07 19:56:09] nn step 34100, lr: 0.1.
	loss_policy_0: 0.26644
	accuracy_policy_0: 0.63191
	loss_value_0: 0.34088
	loss_policy_1: 0.05354
	accuracy_policy_1: 0.62742
	loss_value_1: 0.06959
	loss_reward_1: 0.0067
	loss_policy_2: 0.05354
	accuracy_policy_2: 0.62973
	loss_value_2: 0.07168
	loss_reward_2: 0.00906
	loss_policy_3: 0.05367
	accuracy_policy_3: 0.63043
	loss_value_3: 0.07315
	loss_reward_3: 0.01171
	loss_policy_4: 0.05353
	accuracy_policy_4: 0.63094
	loss_value_4: 0.07436
	loss_reward_4: 0.01459
	loss_policy_5: 0.05326
	accuracy_policy_5: 0.62902
	loss_value_5: 0.07576
	loss_reward_5: 0.01535
	loss_policy: 0.53398
	loss_value: 0.70542
	loss_reward: 0.05742
[2025-05-07 19:56:17] nn step 34150, lr: 0.1.
	loss_policy_0: 0.2897
	accuracy_policy_0: 0.63961
	loss_value_0: 0.37028
	loss_policy_1: 0.05874
	accuracy_policy_1: 0.62363
	loss_value_1: 0.07553
	loss_reward_1: 0.00736
	loss_policy_2: 0.05844
	accuracy_policy_2: 0.63
	loss_value_2: 0.07749
	loss_reward_2: 0.01023
	loss_policy_3: 0.05847
	accuracy_policy_3: 0.62965
	loss_value_3: 0.07939
	loss_reward_3: 0.01286
	loss_policy_4: 0.05863
	accuracy_policy_4: 0.62656
	loss_value_4: 0.08115
	loss_reward_4: 0.01601
	loss_policy_5: 0.05903
	accuracy_policy_5: 0.62824
	loss_value_5: 0.08269
	loss_reward_5: 0.0166
	loss_policy: 0.583
	loss_value: 0.76653
	loss_reward: 0.06306
[2025-05-07 19:56:24] nn step 34200, lr: 0.1.
	loss_policy_0: 0.27469
	accuracy_policy_0: 0.63254
	loss_value_0: 0.34724
	loss_policy_1: 0.05559
	accuracy_policy_1: 0.62801
	loss_value_1: 0.07106
	loss_reward_1: 0.0071
	loss_policy_2: 0.05571
	accuracy_policy_2: 0.62543
	loss_value_2: 0.07289
	loss_reward_2: 0.00951
	loss_policy_3: 0.05525
	accuracy_policy_3: 0.6309
	loss_value_3: 0.0751
	loss_reward_3: 0.0122
	loss_policy_4: 0.05508
	accuracy_policy_4: 0.63395
	loss_value_4: 0.07683
	loss_reward_4: 0.01494
	loss_policy_5: 0.05536
	accuracy_policy_5: 0.62641
	loss_value_5: 0.07855
	loss_reward_5: 0.01545
	loss_policy: 0.55168
	loss_value: 0.72166
	loss_reward: 0.05919
Optimization_Done 34200
[2025-05-07 19:59:52] [command] train weight_iter_34200.pkl 153 172
[2025-05-07 20:00:01] nn step 34250, lr: 0.1.
	loss_policy_0: 0.28608
	accuracy_policy_0: 0.62969
	loss_value_0: 0.3706
	loss_policy_1: 0.05807
	accuracy_policy_1: 0.62129
	loss_value_1: 0.07558
	loss_reward_1: 0.00736
	loss_policy_2: 0.05824
	accuracy_policy_2: 0.61906
	loss_value_2: 0.07731
	loss_reward_2: 0.00956
	loss_policy_3: 0.05779
	accuracy_policy_3: 0.62719
	loss_value_3: 0.07923
	loss_reward_3: 0.01291
	loss_policy_4: 0.0581
	accuracy_policy_4: 0.62656
	loss_value_4: 0.08093
	loss_reward_4: 0.01574
	loss_policy_5: 0.0582
	accuracy_policy_5: 0.62641
	loss_value_5: 0.08231
	loss_reward_5: 0.01627
	loss_policy: 0.57647
	loss_value: 0.76597
	loss_reward: 0.06183
[2025-05-07 20:00:08] nn step 34300, lr: 0.1.
	loss_policy_0: 0.28814
	accuracy_policy_0: 0.61555
	loss_value_0: 0.35589
	loss_policy_1: 0.05718
	accuracy_policy_1: 0.62012
	loss_value_1: 0.0729
	loss_reward_1: 0.00714
	loss_policy_2: 0.05723
	accuracy_policy_2: 0.61637
	loss_value_2: 0.07475
	loss_reward_2: 0.0098
	loss_policy_3: 0.05705
	accuracy_policy_3: 0.62047
	loss_value_3: 0.07663
	loss_reward_3: 0.01287
	loss_policy_4: 0.05716
	accuracy_policy_4: 0.62465
	loss_value_4: 0.07828
	loss_reward_4: 0.01512
	loss_policy_5: 0.05728
	accuracy_policy_5: 0.62449
	loss_value_5: 0.07995
	loss_reward_5: 0.01609
	loss_policy: 0.57406
	loss_value: 0.7384
	loss_reward: 0.06103
[2025-05-07 20:00:16] nn step 34350, lr: 0.1.
	loss_policy_0: 0.28234
	accuracy_policy_0: 0.6275
	loss_value_0: 0.35707
	loss_policy_1: 0.05665
	accuracy_policy_1: 0.62793
	loss_value_1: 0.07323
	loss_reward_1: 0.0072
	loss_policy_2: 0.05655
	accuracy_policy_2: 0.62797
	loss_value_2: 0.07492
	loss_reward_2: 0.00975
	loss_policy_3: 0.05672
	accuracy_policy_3: 0.63062
	loss_value_3: 0.07672
	loss_reward_3: 0.01249
	loss_policy_4: 0.05733
	accuracy_policy_4: 0.62125
	loss_value_4: 0.0779
	loss_reward_4: 0.01528
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.62238
	loss_value_5: 0.07948
	loss_reward_5: 0.01596
	loss_policy: 0.56681
	loss_value: 0.73931
	loss_reward: 0.06068
[2025-05-07 20:00:24] nn step 34400, lr: 0.1.
	loss_policy_0: 0.27672
	accuracy_policy_0: 0.63512
	loss_value_0: 0.35047
	loss_policy_1: 0.05538
	accuracy_policy_1: 0.62797
	loss_value_1: 0.07159
	loss_reward_1: 0.0074
	loss_policy_2: 0.05556
	accuracy_policy_2: 0.62793
	loss_value_2: 0.07352
	loss_reward_2: 0.01002
	loss_policy_3: 0.05581
	accuracy_policy_3: 0.62566
	loss_value_3: 0.07558
	loss_reward_3: 0.01254
	loss_policy_4: 0.05558
	accuracy_policy_4: 0.62488
	loss_value_4: 0.07703
	loss_reward_4: 0.01514
	loss_policy_5: 0.05567
	accuracy_policy_5: 0.62707
	loss_value_5: 0.0786
	loss_reward_5: 0.01606
	loss_policy: 0.55472
	loss_value: 0.72681
	loss_reward: 0.06115
Optimization_Done 34400
[2025-05-07 20:03:44] [command] train weight_iter_34400.pkl 154 173
[2025-05-07 20:03:53] nn step 34450, lr: 0.1.
	loss_policy_0: 0.29133
	accuracy_policy_0: 0.63945
	loss_value_0: 0.38515
	loss_policy_1: 0.05884
	accuracy_policy_1: 0.62984
	loss_value_1: 0.07833
	loss_reward_1: 0.00724
	loss_policy_2: 0.05909
	accuracy_policy_2: 0.63223
	loss_value_2: 0.08013
	loss_reward_2: 0.01002
	loss_policy_3: 0.05906
	accuracy_policy_3: 0.62832
	loss_value_3: 0.08166
	loss_reward_3: 0.01313
	loss_policy_4: 0.05883
	accuracy_policy_4: 0.63316
	loss_value_4: 0.08334
	loss_reward_4: 0.01577
	loss_policy_5: 0.05942
	accuracy_policy_5: 0.62977
	loss_value_5: 0.08474
	loss_reward_5: 0.01637
	loss_policy: 0.58658
	loss_value: 0.79334
	loss_reward: 0.06254
[2025-05-07 20:03:59] nn step 34500, lr: 0.1.
	loss_policy_0: 0.28231
	accuracy_policy_0: 0.63926
	loss_value_0: 0.36438
	loss_policy_1: 0.05657
	accuracy_policy_1: 0.63496
	loss_value_1: 0.07385
	loss_reward_1: 0.00718
	loss_policy_2: 0.05681
	accuracy_policy_2: 0.62617
	loss_value_2: 0.07545
	loss_reward_2: 0.00958
	loss_policy_3: 0.05646
	accuracy_policy_3: 0.63512
	loss_value_3: 0.07746
	loss_reward_3: 0.01243
	loss_policy_4: 0.05699
	accuracy_policy_4: 0.63426
	loss_value_4: 0.07919
	loss_reward_4: 0.01524
	loss_policy_5: 0.05763
	accuracy_policy_5: 0.62766
	loss_value_5: 0.08099
	loss_reward_5: 0.01572
	loss_policy: 0.56678
	loss_value: 0.75131
	loss_reward: 0.06015
[2025-05-07 20:04:07] nn step 34550, lr: 0.1.
	loss_policy_0: 0.28256
	accuracy_policy_0: 0.63523
	loss_value_0: 0.36051
	loss_policy_1: 0.05723
	accuracy_policy_1: 0.62203
	loss_value_1: 0.07378
	loss_reward_1: 0.00725
	loss_policy_2: 0.05689
	accuracy_policy_2: 0.62969
	loss_value_2: 0.07551
	loss_reward_2: 0.00959
	loss_policy_3: 0.05707
	accuracy_policy_3: 0.6284
	loss_value_3: 0.07698
	loss_reward_3: 0.0129
	loss_policy_4: 0.05741
	accuracy_policy_4: 0.62828
	loss_value_4: 0.07893
	loss_reward_4: 0.01554
	loss_policy_5: 0.05743
	accuracy_policy_5: 0.63316
	loss_value_5: 0.08059
	loss_reward_5: 0.01558
	loss_policy: 0.5686
	loss_value: 0.7463
	loss_reward: 0.06087
[2025-05-07 20:04:15] nn step 34600, lr: 0.1.
	loss_policy_0: 0.28926
	accuracy_policy_0: 0.63281
	loss_value_0: 0.36504
	loss_policy_1: 0.05787
	accuracy_policy_1: 0.63375
	loss_value_1: 0.07465
	loss_reward_1: 0.00709
	loss_policy_2: 0.05818
	accuracy_policy_2: 0.63121
	loss_value_2: 0.07634
	loss_reward_2: 0.00945
	loss_policy_3: 0.05824
	accuracy_policy_3: 0.63367
	loss_value_3: 0.07842
	loss_reward_3: 0.01273
	loss_policy_4: 0.0586
	accuracy_policy_4: 0.62574
	loss_value_4: 0.08013
	loss_reward_4: 0.01558
	loss_policy_5: 0.05832
	accuracy_policy_5: 0.63266
	loss_value_5: 0.08179
	loss_reward_5: 0.01593
	loss_policy: 0.58048
	loss_value: 0.75637
	loss_reward: 0.06078
Optimization_Done 34600
[2025-05-07 20:07:42] [command] train weight_iter_34600.pkl 155 174
[2025-05-07 20:07:51] nn step 34650, lr: 0.1.
	loss_policy_0: 0.27941
	accuracy_policy_0: 0.63273
	loss_value_0: 0.36426
	loss_policy_1: 0.05621
	accuracy_policy_1: 0.6234
	loss_value_1: 0.07434
	loss_reward_1: 0.00711
	loss_policy_2: 0.05627
	accuracy_policy_2: 0.62695
	loss_value_2: 0.07598
	loss_reward_2: 0.00973
	loss_policy_3: 0.05634
	accuracy_policy_3: 0.62813
	loss_value_3: 0.07751
	loss_reward_3: 0.01241
	loss_policy_4: 0.05629
	accuracy_policy_4: 0.62957
	loss_value_4: 0.07886
	loss_reward_4: 0.01434
	loss_policy_5: 0.05637
	accuracy_policy_5: 0.63125
	loss_value_5: 0.08037
	loss_reward_5: 0.01548
	loss_policy: 0.56091
	loss_value: 0.75132
	loss_reward: 0.05906
[2025-05-07 20:07:59] nn step 34700, lr: 0.1.
	loss_policy_0: 0.29706
	accuracy_policy_0: 0.63254
	loss_value_0: 0.38188
	loss_policy_1: 0.05985
	accuracy_policy_1: 0.62523
	loss_value_1: 0.07785
	loss_reward_1: 0.00753
	loss_policy_2: 0.06021
	accuracy_policy_2: 0.63074
	loss_value_2: 0.07941
	loss_reward_2: 0.01004
	loss_policy_3: 0.06025
	accuracy_policy_3: 0.62961
	loss_value_3: 0.081
	loss_reward_3: 0.01286
	loss_policy_4: 0.05996
	accuracy_policy_4: 0.63281
	loss_value_4: 0.08297
	loss_reward_4: 0.01538
	loss_policy_5: 0.06016
	accuracy_policy_5: 0.6323
	loss_value_5: 0.08453
	loss_reward_5: 0.01657
	loss_policy: 0.59748
	loss_value: 0.78763
	loss_reward: 0.06239
[2025-05-07 20:08:07] nn step 34750, lr: 0.1.
	loss_policy_0: 0.26738
	accuracy_policy_0: 0.63195
	loss_value_0: 0.33688
	loss_policy_1: 0.05394
	accuracy_policy_1: 0.62789
	loss_value_1: 0.06868
	loss_reward_1: 0.00658
	loss_policy_2: 0.05387
	accuracy_policy_2: 0.62586
	loss_value_2: 0.0706
	loss_reward_2: 0.00906
	loss_policy_3: 0.05337
	accuracy_policy_3: 0.63449
	loss_value_3: 0.07223
	loss_reward_3: 0.01128
	loss_policy_4: 0.05358
	accuracy_policy_4: 0.63379
	loss_value_4: 0.07358
	loss_reward_4: 0.01407
	loss_policy_5: 0.05355
	accuracy_policy_5: 0.63453
	loss_value_5: 0.0751
	loss_reward_5: 0.01457
	loss_policy: 0.53569
	loss_value: 0.69707
	loss_reward: 0.05556
[2025-05-07 20:08:13] nn step 34800, lr: 0.1.
	loss_policy_0: 0.27947
	accuracy_policy_0: 0.63441
	loss_value_0: 0.34949
	loss_policy_1: 0.05618
	accuracy_policy_1: 0.63078
	loss_value_1: 0.07164
	loss_reward_1: 0.00691
	loss_policy_2: 0.05595
	accuracy_policy_2: 0.63633
	loss_value_2: 0.07346
	loss_reward_2: 0.0093
	loss_policy_3: 0.05639
	accuracy_policy_3: 0.62594
	loss_value_3: 0.07534
	loss_reward_3: 0.0125
	loss_policy_4: 0.05653
	accuracy_policy_4: 0.62957
	loss_value_4: 0.07693
	loss_reward_4: 0.015
	loss_policy_5: 0.05669
	accuracy_policy_5: 0.62953
	loss_value_5: 0.07867
	loss_reward_5: 0.01534
	loss_policy: 0.56121
	loss_value: 0.72552
	loss_reward: 0.05905
Optimization_Done 34800
[2025-05-07 20:11:35] [command] train weight_iter_34800.pkl 156 175
[2025-05-07 20:11:43] nn step 34850, lr: 0.1.
	loss_policy_0: 0.27594
	accuracy_policy_0: 0.63555
	loss_value_0: 0.35626
	loss_policy_1: 0.05569
	accuracy_policy_1: 0.6291
	loss_value_1: 0.0722
	loss_reward_1: 0.00699
	loss_policy_2: 0.05535
	accuracy_policy_2: 0.63176
	loss_value_2: 0.07393
	loss_reward_2: 0.00924
	loss_policy_3: 0.05515
	accuracy_policy_3: 0.6309
	loss_value_3: 0.0755
	loss_reward_3: 0.01212
	loss_policy_4: 0.05528
	accuracy_policy_4: 0.6377
	loss_value_4: 0.07752
	loss_reward_4: 0.01457
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.6327
	loss_value_5: 0.07907
	loss_reward_5: 0.01546
	loss_policy: 0.55331
	loss_value: 0.73449
	loss_reward: 0.05838
[2025-05-07 20:11:52] nn step 34900, lr: 0.1.
	loss_policy_0: 0.28329
	accuracy_policy_0: 0.63672
	loss_value_0: 0.36561
	loss_policy_1: 0.05737
	accuracy_policy_1: 0.62719
	loss_value_1: 0.07444
	loss_reward_1: 0.00746
	loss_policy_2: 0.05746
	accuracy_policy_2: 0.6277
	loss_value_2: 0.07617
	loss_reward_2: 0.0098
	loss_policy_3: 0.05763
	accuracy_policy_3: 0.63082
	loss_value_3: 0.07818
	loss_reward_3: 0.01283
	loss_policy_4: 0.05753
	accuracy_policy_4: 0.6291
	loss_value_4: 0.07967
	loss_reward_4: 0.01528
	loss_policy_5: 0.0579
	accuracy_policy_5: 0.6298
	loss_value_5: 0.08127
	loss_reward_5: 0.01596
	loss_policy: 0.57118
	loss_value: 0.75535
	loss_reward: 0.06134
[2025-05-07 20:11:58] nn step 34950, lr: 0.1.
	loss_policy_0: 0.27525
	accuracy_policy_0: 0.64199
	loss_value_0: 0.35327
	loss_policy_1: 0.05534
	accuracy_policy_1: 0.63855
	loss_value_1: 0.07223
	loss_reward_1: 0.00695
	loss_policy_2: 0.05575
	accuracy_policy_2: 0.63098
	loss_value_2: 0.07424
	loss_reward_2: 0.00952
	loss_policy_3: 0.056
	accuracy_policy_3: 0.63086
	loss_value_3: 0.07601
	loss_reward_3: 0.01234
	loss_policy_4: 0.05578
	accuracy_policy_4: 0.62996
	loss_value_4: 0.07751
	loss_reward_4: 0.01488
	loss_policy_5: 0.05587
	accuracy_policy_5: 0.63258
	loss_value_5: 0.07904
	loss_reward_5: 0.01524
	loss_policy: 0.55399
	loss_value: 0.7323
	loss_reward: 0.05892
[2025-05-07 20:12:06] nn step 35000, lr: 0.1.
	loss_policy_0: 0.28679
	accuracy_policy_0: 0.6402
	loss_value_0: 0.36459
	loss_policy_1: 0.0577
	accuracy_policy_1: 0.63602
	loss_value_1: 0.07441
	loss_reward_1: 0.00769
	loss_policy_2: 0.05803
	accuracy_policy_2: 0.6334
	loss_value_2: 0.07655
	loss_reward_2: 0.00946
	loss_policy_3: 0.05811
	accuracy_policy_3: 0.63121
	loss_value_3: 0.07843
	loss_reward_3: 0.01238
	loss_policy_4: 0.05825
	accuracy_policy_4: 0.63094
	loss_value_4: 0.08005
	loss_reward_4: 0.0151
	loss_policy_5: 0.05823
	accuracy_policy_5: 0.63461
	loss_value_5: 0.08175
	loss_reward_5: 0.01631
	loss_policy: 0.57711
	loss_value: 0.75578
	loss_reward: 0.06093
Optimization_Done 35000
[2025-05-07 20:15:30] [command] train weight_iter_35000.pkl 157 176
[2025-05-07 20:15:39] nn step 35050, lr: 0.1.
	loss_policy_0: 0.28464
	accuracy_policy_0: 0.63152
	loss_value_0: 0.36924
	loss_policy_1: 0.05708
	accuracy_policy_1: 0.63434
	loss_value_1: 0.07533
	loss_reward_1: 0.00706
	loss_policy_2: 0.05723
	accuracy_policy_2: 0.63523
	loss_value_2: 0.07698
	loss_reward_2: 0.00977
	loss_policy_3: 0.05729
	accuracy_policy_3: 0.63164
	loss_value_3: 0.07862
	loss_reward_3: 0.01282
	loss_policy_4: 0.05729
	accuracy_policy_4: 0.63062
	loss_value_4: 0.07988
	loss_reward_4: 0.01552
	loss_policy_5: 0.05755
	accuracy_policy_5: 0.63035
	loss_value_5: 0.08173
	loss_reward_5: 0.0159
	loss_policy: 0.57108
	loss_value: 0.76177
	loss_reward: 0.06106
[2025-05-07 20:15:45] nn step 35100, lr: 0.1.
	loss_policy_0: 0.28456
	accuracy_policy_0: 0.64047
	loss_value_0: 0.37092
	loss_policy_1: 0.05735
	accuracy_policy_1: 0.63613
	loss_value_1: 0.07549
	loss_reward_1: 0.00706
	loss_policy_2: 0.0576
	accuracy_policy_2: 0.6375
	loss_value_2: 0.07716
	loss_reward_2: 0.00959
	loss_policy_3: 0.05786
	accuracy_policy_3: 0.63539
	loss_value_3: 0.07875
	loss_reward_3: 0.01274
	loss_policy_4: 0.05726
	accuracy_policy_4: 0.63535
	loss_value_4: 0.08043
	loss_reward_4: 0.01492
	loss_policy_5: 0.05765
	accuracy_policy_5: 0.63398
	loss_value_5: 0.08156
	loss_reward_5: 0.01658
	loss_policy: 0.57229
	loss_value: 0.76433
	loss_reward: 0.06089
[2025-05-07 20:15:53] nn step 35150, lr: 0.1.
	loss_policy_0: 0.28861
	accuracy_policy_0: 0.64398
	loss_value_0: 0.36915
	loss_policy_1: 0.0585
	accuracy_policy_1: 0.63594
	loss_value_1: 0.07536
	loss_reward_1: 0.00697
	loss_policy_2: 0.05875
	accuracy_policy_2: 0.6343
	loss_value_2: 0.07741
	loss_reward_2: 0.00953
	loss_policy_3: 0.05814
	accuracy_policy_3: 0.63809
	loss_value_3: 0.07919
	loss_reward_3: 0.01244
	loss_policy_4: 0.05875
	accuracy_policy_4: 0.63203
	loss_value_4: 0.08101
	loss_reward_4: 0.01534
	loss_policy_5: 0.0593
	accuracy_policy_5: 0.62824
	loss_value_5: 0.08243
	loss_reward_5: 0.01617
	loss_policy: 0.58205
	loss_value: 0.76455
	loss_reward: 0.06045
[2025-05-07 20:16:01] nn step 35200, lr: 0.1.
	loss_policy_0: 0.25555
	accuracy_policy_0: 0.63969
	loss_value_0: 0.3244
	loss_policy_1: 0.05141
	accuracy_policy_1: 0.63535
	loss_value_1: 0.06624
	loss_reward_1: 0.00631
	loss_policy_2: 0.05188
	accuracy_policy_2: 0.63188
	loss_value_2: 0.06795
	loss_reward_2: 0.00876
	loss_policy_3: 0.05188
	accuracy_policy_3: 0.6343
	loss_value_3: 0.06946
	loss_reward_3: 0.01121
	loss_policy_4: 0.0519
	accuracy_policy_4: 0.63211
	loss_value_4: 0.07096
	loss_reward_4: 0.01327
	loss_policy_5: 0.0519
	accuracy_policy_5: 0.63277
	loss_value_5: 0.07265
	loss_reward_5: 0.014
	loss_policy: 0.51452
	loss_value: 0.67165
	loss_reward: 0.05355
Optimization_Done 35200
[2025-05-07 20:19:26] [command] train weight_iter_35200.pkl 158 177
[2025-05-07 20:19:34] nn step 35250, lr: 0.1.
	loss_policy_0: 0.26859
	accuracy_policy_0: 0.63945
	loss_value_0: 0.34925
	loss_policy_1: 0.05454
	accuracy_policy_1: 0.63266
	loss_value_1: 0.0712
	loss_reward_1: 0.0068
	loss_policy_2: 0.05442
	accuracy_policy_2: 0.63516
	loss_value_2: 0.07299
	loss_reward_2: 0.00891
	loss_policy_3: 0.05421
	accuracy_policy_3: 0.63863
	loss_value_3: 0.0746
	loss_reward_3: 0.012
	loss_policy_4: 0.054
	accuracy_policy_4: 0.63887
	loss_value_4: 0.07617
	loss_reward_4: 0.01425
	loss_policy_5: 0.05444
	accuracy_policy_5: 0.6427
	loss_value_5: 0.07757
	loss_reward_5: 0.01505
	loss_policy: 0.5402
	loss_value: 0.7218
	loss_reward: 0.05701
[2025-05-07 20:19:42] nn step 35300, lr: 0.1.
	loss_policy_0: 0.28232
	accuracy_policy_0: 0.63902
	loss_value_0: 0.36284
	loss_policy_1: 0.05641
	accuracy_policy_1: 0.63758
	loss_value_1: 0.07374
	loss_reward_1: 0.00721
	loss_policy_2: 0.0566
	accuracy_policy_2: 0.63727
	loss_value_2: 0.07501
	loss_reward_2: 0.00951
	loss_policy_3: 0.05676
	accuracy_policy_3: 0.63375
	loss_value_3: 0.07695
	loss_reward_3: 0.01224
	loss_policy_4: 0.05685
	accuracy_policy_4: 0.63445
	loss_value_4: 0.0787
	loss_reward_4: 0.01513
	loss_policy_5: 0.05688
	accuracy_policy_5: 0.63801
	loss_value_5: 0.08045
	loss_reward_5: 0.01592
	loss_policy: 0.56582
	loss_value: 0.7477
	loss_reward: 0.06001
[2025-05-07 20:19:50] nn step 35350, lr: 0.1.
	loss_policy_0: 0.27402
	accuracy_policy_0: 0.6352
	loss_value_0: 0.34594
	loss_policy_1: 0.05444
	accuracy_policy_1: 0.63668
	loss_value_1: 0.07031
	loss_reward_1: 0.00696
	loss_policy_2: 0.05437
	accuracy_policy_2: 0.63512
	loss_value_2: 0.07195
	loss_reward_2: 0.0096
	loss_policy_3: 0.05428
	accuracy_policy_3: 0.63613
	loss_value_3: 0.07377
	loss_reward_3: 0.01211
	loss_policy_4: 0.05432
	accuracy_policy_4: 0.64414
	loss_value_4: 0.07535
	loss_reward_4: 0.01484
	loss_policy_5: 0.05457
	accuracy_policy_5: 0.63492
	loss_value_5: 0.07689
	loss_reward_5: 0.01566
	loss_policy: 0.54599
	loss_value: 0.71421
	loss_reward: 0.05918
[2025-05-07 20:19:58] nn step 35400, lr: 0.1.
	loss_policy_0: 0.27738
	accuracy_policy_0: 0.64398
	loss_value_0: 0.35642
	loss_policy_1: 0.05578
	accuracy_policy_1: 0.64223
	loss_value_1: 0.07278
	loss_reward_1: 0.00701
	loss_policy_2: 0.0559
	accuracy_policy_2: 0.64395
	loss_value_2: 0.07477
	loss_reward_2: 0.00973
	loss_policy_3: 0.056
	accuracy_policy_3: 0.63492
	loss_value_3: 0.07669
	loss_reward_3: 0.01205
	loss_policy_4: 0.05601
	accuracy_policy_4: 0.64172
	loss_value_4: 0.07842
	loss_reward_4: 0.0149
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.63785
	loss_value_5: 0.07972
	loss_reward_5: 0.01548
	loss_policy: 0.55703
	loss_value: 0.7388
	loss_reward: 0.05918
Optimization_Done 35400
[2025-05-07 20:23:31] [command] train weight_iter_35400.pkl 159 178
[2025-05-07 20:23:40] nn step 35450, lr: 0.1.
	loss_policy_0: 0.28626
	accuracy_policy_0: 0.63746
	loss_value_0: 0.37194
	loss_policy_1: 0.0576
	accuracy_policy_1: 0.63277
	loss_value_1: 0.07594
	loss_reward_1: 0.00733
	loss_policy_2: 0.05792
	accuracy_policy_2: 0.63121
	loss_value_2: 0.07771
	loss_reward_2: 0.0097
	loss_policy_3: 0.05789
	accuracy_policy_3: 0.62969
	loss_value_3: 0.07944
	loss_reward_3: 0.0128
	loss_policy_4: 0.05764
	accuracy_policy_4: 0.63309
	loss_value_4: 0.0809
	loss_reward_4: 0.01504
	loss_policy_5: 0.05763
	accuracy_policy_5: 0.63402
	loss_value_5: 0.08246
	loss_reward_5: 0.01558
	loss_policy: 0.57492
	loss_value: 0.76839
	loss_reward: 0.06046
[2025-05-07 20:23:47] nn step 35500, lr: 0.1.
	loss_policy_0: 0.27205
	accuracy_policy_0: 0.64031
	loss_value_0: 0.34791
	loss_policy_1: 0.0548
	accuracy_policy_1: 0.64062
	loss_value_1: 0.07107
	loss_reward_1: 0.00695
	loss_policy_2: 0.05475
	accuracy_policy_2: 0.63586
	loss_value_2: 0.07268
	loss_reward_2: 0.00898
	loss_policy_3: 0.05493
	accuracy_policy_3: 0.63141
	loss_value_3: 0.0742
	loss_reward_3: 0.01135
	loss_policy_4: 0.05494
	accuracy_policy_4: 0.63508
	loss_value_4: 0.07601
	loss_reward_4: 0.01417
	loss_policy_5: 0.05481
	accuracy_policy_5: 0.63582
	loss_value_5: 0.07746
	loss_reward_5: 0.01473
	loss_policy: 0.5463
	loss_value: 0.71932
	loss_reward: 0.05617
[2025-05-07 20:23:55] nn step 35550, lr: 0.1.
	loss_policy_0: 0.2789
	accuracy_policy_0: 0.64094
	loss_value_0: 0.35427
	loss_policy_1: 0.05582
	accuracy_policy_1: 0.64023
	loss_value_1: 0.07216
	loss_reward_1: 0.00689
	loss_policy_2: 0.05653
	accuracy_policy_2: 0.63094
	loss_value_2: 0.07376
	loss_reward_2: 0.00915
	loss_policy_3: 0.05615
	accuracy_policy_3: 0.6359
	loss_value_3: 0.07548
	loss_reward_3: 0.01178
	loss_policy_4: 0.05638
	accuracy_policy_4: 0.63094
	loss_value_4: 0.07713
	loss_reward_4: 0.01483
	loss_policy_5: 0.05648
	accuracy_policy_5: 0.63383
	loss_value_5: 0.07843
	loss_reward_5: 0.01563
	loss_policy: 0.56026
	loss_value: 0.73123
	loss_reward: 0.05828
[2025-05-07 20:24:03] nn step 35600, lr: 0.1.
	loss_policy_0: 0.26362
	accuracy_policy_0: 0.64324
	loss_value_0: 0.33624
	loss_policy_1: 0.05274
	accuracy_policy_1: 0.63855
	loss_value_1: 0.06867
	loss_reward_1: 0.00657
	loss_policy_2: 0.05313
	accuracy_policy_2: 0.63441
	loss_value_2: 0.07025
	loss_reward_2: 0.00895
	loss_policy_3: 0.05319
	accuracy_policy_3: 0.63434
	loss_value_3: 0.07174
	loss_reward_3: 0.01192
	loss_policy_4: 0.0532
	accuracy_policy_4: 0.63379
	loss_value_4: 0.07342
	loss_reward_4: 0.01375
	loss_policy_5: 0.05322
	accuracy_policy_5: 0.64301
	loss_value_5: 0.07501
	loss_reward_5: 0.01474
	loss_policy: 0.5291
	loss_value: 0.69533
	loss_reward: 0.05594
Optimization_Done 35600
[2025-05-07 20:27:20] [command] train weight_iter_35600.pkl 160 179
[2025-05-07 20:27:29] nn step 35650, lr: 0.1.
	loss_policy_0: 0.26733
	accuracy_policy_0: 0.63895
	loss_value_0: 0.34682
	loss_policy_1: 0.05357
	accuracy_policy_1: 0.63691
	loss_value_1: 0.07062
	loss_reward_1: 0.00672
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.63105
	loss_value_2: 0.07222
	loss_reward_2: 0.00902
	loss_policy_3: 0.05419
	accuracy_policy_3: 0.63207
	loss_value_3: 0.07366
	loss_reward_3: 0.01173
	loss_policy_4: 0.054
	accuracy_policy_4: 0.63789
	loss_value_4: 0.0751
	loss_reward_4: 0.01399
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.63684
	loss_value_5: 0.07676
	loss_reward_5: 0.01465
	loss_policy: 0.53737
	loss_value: 0.71517
	loss_reward: 0.05611
[2025-05-07 20:27:35] nn step 35700, lr: 0.1.
	loss_policy_0: 0.27979
	accuracy_policy_0: 0.64332
	loss_value_0: 0.35601
	loss_policy_1: 0.05649
	accuracy_policy_1: 0.63105
	loss_value_1: 0.07267
	loss_reward_1: 0.00696
	loss_policy_2: 0.05659
	accuracy_policy_2: 0.63594
	loss_value_2: 0.07444
	loss_reward_2: 0.0092
	loss_policy_3: 0.05664
	accuracy_policy_3: 0.63562
	loss_value_3: 0.07639
	loss_reward_3: 0.01256
	loss_policy_4: 0.05676
	accuracy_policy_4: 0.63879
	loss_value_4: 0.07803
	loss_reward_4: 0.01483
	loss_policy_5: 0.05689
	accuracy_policy_5: 0.63629
	loss_value_5: 0.07975
	loss_reward_5: 0.01529
	loss_policy: 0.56317
	loss_value: 0.73729
	loss_reward: 0.05884
[2025-05-07 20:27:43] nn step 35750, lr: 0.1.
	loss_policy_0: 0.28354
	accuracy_policy_0: 0.64145
	loss_value_0: 0.35615
	loss_policy_1: 0.05651
	accuracy_policy_1: 0.63559
	loss_value_1: 0.07237
	loss_reward_1: 0.00718
	loss_policy_2: 0.05703
	accuracy_policy_2: 0.63703
	loss_value_2: 0.07441
	loss_reward_2: 0.00966
	loss_policy_3: 0.05703
	accuracy_policy_3: 0.63324
	loss_value_3: 0.0761
	loss_reward_3: 0.01226
	loss_policy_4: 0.05675
	accuracy_policy_4: 0.64082
	loss_value_4: 0.07766
	loss_reward_4: 0.01489
	loss_policy_5: 0.05681
	accuracy_policy_5: 0.64465
	loss_value_5: 0.07943
	loss_reward_5: 0.0155
	loss_policy: 0.56768
	loss_value: 0.73612
	loss_reward: 0.05949
[2025-05-07 20:27:51] nn step 35800, lr: 0.1.
	loss_policy_0: 0.29112
	accuracy_policy_0: 0.64375
	loss_value_0: 0.37
	loss_policy_1: 0.05832
	accuracy_policy_1: 0.63711
	loss_value_1: 0.07534
	loss_reward_1: 0.00738
	loss_policy_2: 0.05851
	accuracy_policy_2: 0.63645
	loss_value_2: 0.07744
	loss_reward_2: 0.01001
	loss_policy_3: 0.0584
	accuracy_policy_3: 0.63727
	loss_value_3: 0.079
	loss_reward_3: 0.01271
	loss_policy_4: 0.05882
	accuracy_policy_4: 0.63703
	loss_value_4: 0.08063
	loss_reward_4: 0.01528
	loss_policy_5: 0.05919
	accuracy_policy_5: 0.63926
	loss_value_5: 0.08247
	loss_reward_5: 0.01582
	loss_policy: 0.58436
	loss_value: 0.76488
	loss_reward: 0.0612
Optimization_Done 35800
[2025-05-07 20:31:11] [command] train weight_iter_35800.pkl 161 180
[2025-05-07 20:31:20] nn step 35850, lr: 0.1.
	loss_policy_0: 0.26895
	accuracy_policy_0: 0.64457
	loss_value_0: 0.3519
	loss_policy_1: 0.05431
	accuracy_policy_1: 0.64547
	loss_value_1: 0.0716
	loss_reward_1: 0.00693
	loss_policy_2: 0.05435
	accuracy_policy_2: 0.63586
	loss_value_2: 0.07306
	loss_reward_2: 0.00932
	loss_policy_3: 0.05436
	accuracy_policy_3: 0.63992
	loss_value_3: 0.07459
	loss_reward_3: 0.01167
	loss_policy_4: 0.05428
	accuracy_policy_4: 0.64383
	loss_value_4: 0.07586
	loss_reward_4: 0.01456
	loss_policy_5: 0.05432
	accuracy_policy_5: 0.64945
	loss_value_5: 0.07722
	loss_reward_5: 0.01532
	loss_policy: 0.54058
	loss_value: 0.72423
	loss_reward: 0.0578
[2025-05-07 20:31:26] nn step 35900, lr: 0.1.
	loss_policy_0: 0.30036
	accuracy_policy_0: 0.62352
	loss_value_0: 0.3731
	loss_policy_1: 0.05884
	accuracy_policy_1: 0.63535
	loss_value_1: 0.07595
	loss_reward_1: 0.00751
	loss_policy_2: 0.05916
	accuracy_policy_2: 0.63273
	loss_value_2: 0.07774
	loss_reward_2: 0.01036
	loss_policy_3: 0.05914
	accuracy_policy_3: 0.63539
	loss_value_3: 0.07963
	loss_reward_3: 0.01305
	loss_policy_4: 0.05908
	accuracy_policy_4: 0.6341
	loss_value_4: 0.0811
	loss_reward_4: 0.01573
	loss_policy_5: 0.05878
	accuracy_policy_5: 0.64113
	loss_value_5: 0.08246
	loss_reward_5: 0.01628
	loss_policy: 0.59535
	loss_value: 0.76999
	loss_reward: 0.06293
[2025-05-07 20:31:34] nn step 35950, lr: 0.1.
	loss_policy_0: 0.27634
	accuracy_policy_0: 0.63762
	loss_value_0: 0.35409
	loss_policy_1: 0.05529
	accuracy_policy_1: 0.64062
	loss_value_1: 0.07197
	loss_reward_1: 0.00704
	loss_policy_2: 0.05531
	accuracy_policy_2: 0.63703
	loss_value_2: 0.07391
	loss_reward_2: 0.00939
	loss_policy_3: 0.05536
	accuracy_policy_3: 0.63941
	loss_value_3: 0.07568
	loss_reward_3: 0.01208
	loss_policy_4: 0.05541
	accuracy_policy_4: 0.63492
	loss_value_4: 0.07718
	loss_reward_4: 0.01448
	loss_policy_5: 0.05525
	accuracy_policy_5: 0.64223
	loss_value_5: 0.0786
	loss_reward_5: 0.01528
	loss_policy: 0.55296
	loss_value: 0.73143
	loss_reward: 0.05827
[2025-05-07 20:31:42] nn step 36000, lr: 0.1.
	loss_policy_0: 0.27951
	accuracy_policy_0: 0.6423
	loss_value_0: 0.35442
	loss_policy_1: 0.05597
	accuracy_policy_1: 0.64094
	loss_value_1: 0.07226
	loss_reward_1: 0.007
	loss_policy_2: 0.05609
	accuracy_policy_2: 0.63703
	loss_value_2: 0.07446
	loss_reward_2: 0.00974
	loss_policy_3: 0.05652
	accuracy_policy_3: 0.63746
	loss_value_3: 0.07603
	loss_reward_3: 0.01299
	loss_policy_4: 0.05668
	accuracy_policy_4: 0.64219
	loss_value_4: 0.07764
	loss_reward_4: 0.01533
	loss_policy_5: 0.05657
	accuracy_policy_5: 0.64164
	loss_value_5: 0.07918
	loss_reward_5: 0.01597
	loss_policy: 0.56133
	loss_value: 0.73399
	loss_reward: 0.06104
Optimization_Done 36000
[2025-05-07 20:35:11] [command] train weight_iter_36000.pkl 162 181
[2025-05-07 20:35:20] nn step 36050, lr: 0.1.
	loss_policy_0: 0.26856
	accuracy_policy_0: 0.65254
	loss_value_0: 0.36104
	loss_policy_1: 0.05348
	accuracy_policy_1: 0.65254
	loss_value_1: 0.07305
	loss_reward_1: 0.00641
	loss_policy_2: 0.05376
	accuracy_policy_2: 0.64762
	loss_value_2: 0.07474
	loss_reward_2: 0.00887
	loss_policy_3: 0.05402
	accuracy_policy_3: 0.64703
	loss_value_3: 0.07574
	loss_reward_3: 0.01148
	loss_policy_4: 0.05393
	accuracy_policy_4: 0.65074
	loss_value_4: 0.07695
	loss_reward_4: 0.01372
	loss_policy_5: 0.05423
	accuracy_policy_5: 0.64938
	loss_value_5: 0.07848
	loss_reward_5: 0.01475
	loss_policy: 0.53797
	loss_value: 0.74
	loss_reward: 0.05523
[2025-05-07 20:35:28] nn step 36100, lr: 0.1.
	loss_policy_0: 0.27802
	accuracy_policy_0: 0.65254
	loss_value_0: 0.3646
	loss_policy_1: 0.05582
	accuracy_policy_1: 0.65141
	loss_value_1: 0.07413
	loss_reward_1: 0.00717
	loss_policy_2: 0.05621
	accuracy_policy_2: 0.64715
	loss_value_2: 0.07615
	loss_reward_2: 0.00962
	loss_policy_3: 0.05589
	accuracy_policy_3: 0.64805
	loss_value_3: 0.07735
	loss_reward_3: 0.01263
	loss_policy_4: 0.05596
	accuracy_policy_4: 0.65312
	loss_value_4: 0.07926
	loss_reward_4: 0.01469
	loss_policy_5: 0.0564
	accuracy_policy_5: 0.6541
	loss_value_5: 0.08096
	loss_reward_5: 0.01553
	loss_policy: 0.55829
	loss_value: 0.75245
	loss_reward: 0.05965
[2025-05-07 20:35:36] nn step 36150, lr: 0.1.
	loss_policy_0: 0.27566
	accuracy_policy_0: 0.65363
	loss_value_0: 0.35873
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.64832
	loss_value_1: 0.07314
	loss_reward_1: 0.00708
	loss_policy_2: 0.05549
	accuracy_policy_2: 0.64922
	loss_value_2: 0.07466
	loss_reward_2: 0.00965
	loss_policy_3: 0.05581
	accuracy_policy_3: 0.64871
	loss_value_3: 0.0764
	loss_reward_3: 0.01218
	loss_policy_4: 0.05588
	accuracy_policy_4: 0.64996
	loss_value_4: 0.07815
	loss_reward_4: 0.01488
	loss_policy_5: 0.05634
	accuracy_policy_5: 0.6482
	loss_value_5: 0.0798
	loss_reward_5: 0.0155
	loss_policy: 0.55454
	loss_value: 0.74089
	loss_reward: 0.0593
[2025-05-07 20:35:43] nn step 36200, lr: 0.1.
	loss_policy_0: 0.27244
	accuracy_policy_0: 0.65102
	loss_value_0: 0.35325
	loss_policy_1: 0.05533
	accuracy_policy_1: 0.64188
	loss_value_1: 0.07165
	loss_reward_1: 0.00691
	loss_policy_2: 0.05507
	accuracy_policy_2: 0.64219
	loss_value_2: 0.07314
	loss_reward_2: 0.00939
	loss_policy_3: 0.05517
	accuracy_policy_3: 0.64902
	loss_value_3: 0.07488
	loss_reward_3: 0.01188
	loss_policy_4: 0.0553
	accuracy_policy_4: 0.64785
	loss_value_4: 0.07664
	loss_reward_4: 0.01462
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.64953
	loss_value_5: 0.07805
	loss_reward_5: 0.01524
	loss_policy: 0.54875
	loss_value: 0.72761
	loss_reward: 0.05803
Optimization_Done 36200
[2025-05-07 20:39:11] [command] train weight_iter_36200.pkl 163 182
[2025-05-07 20:39:20] nn step 36250, lr: 0.1.
	loss_policy_0: 0.28169
	accuracy_policy_0: 0.64715
	loss_value_0: 0.366
	loss_policy_1: 0.05588
	accuracy_policy_1: 0.65012
	loss_value_1: 0.0741
	loss_reward_1: 0.00697
	loss_policy_2: 0.05613
	accuracy_policy_2: 0.64641
	loss_value_2: 0.07549
	loss_reward_2: 0.00927
	loss_policy_3: 0.05622
	accuracy_policy_3: 0.63754
	loss_value_3: 0.07728
	loss_reward_3: 0.01239
	loss_policy_4: 0.05569
	accuracy_policy_4: 0.65223
	loss_value_4: 0.07866
	loss_reward_4: 0.0143
	loss_policy_5: 0.05596
	accuracy_policy_5: 0.65504
	loss_value_5: 0.08014
	loss_reward_5: 0.01524
	loss_policy: 0.56157
	loss_value: 0.75168
	loss_reward: 0.05817
[2025-05-07 20:39:27] nn step 36300, lr: 0.1.
	loss_policy_0: 0.27255
	accuracy_policy_0: 0.66027
	loss_value_0: 0.35434
	loss_policy_1: 0.05493
	accuracy_policy_1: 0.6573
	loss_value_1: 0.07207
	loss_reward_1: 0.00662
	loss_policy_2: 0.05538
	accuracy_policy_2: 0.65211
	loss_value_2: 0.07378
	loss_reward_2: 0.00967
	loss_policy_3: 0.05529
	accuracy_policy_3: 0.64379
	loss_value_3: 0.07562
	loss_reward_3: 0.01214
	loss_policy_4: 0.05545
	accuracy_policy_4: 0.64852
	loss_value_4: 0.07704
	loss_reward_4: 0.01474
	loss_policy_5: 0.05537
	accuracy_policy_5: 0.65168
	loss_value_5: 0.07825
	loss_reward_5: 0.01522
	loss_policy: 0.54897
	loss_value: 0.73111
	loss_reward: 0.0584
[2025-05-07 20:39:35] nn step 36350, lr: 0.1.
	loss_policy_0: 0.25757
	accuracy_policy_0: 0.66168
	loss_value_0: 0.3345
	loss_policy_1: 0.05205
	accuracy_policy_1: 0.65234
	loss_value_1: 0.06831
	loss_reward_1: 0.00642
	loss_policy_2: 0.05196
	accuracy_policy_2: 0.65344
	loss_value_2: 0.06996
	loss_reward_2: 0.00881
	loss_policy_3: 0.0524
	accuracy_policy_3: 0.64648
	loss_value_3: 0.07187
	loss_reward_3: 0.01152
	loss_policy_4: 0.05208
	accuracy_policy_4: 0.6559
	loss_value_4: 0.07339
	loss_reward_4: 0.01389
	loss_policy_5: 0.0525
	accuracy_policy_5: 0.65262
	loss_value_5: 0.07468
	loss_reward_5: 0.01466
	loss_policy: 0.51856
	loss_value: 0.69271
	loss_reward: 0.05531
[2025-05-07 20:39:43] nn step 36400, lr: 0.1.
	loss_policy_0: 0.28582
	accuracy_policy_0: 0.65789
	loss_value_0: 0.36351
	loss_policy_1: 0.05723
	accuracy_policy_1: 0.64961
	loss_value_1: 0.07409
	loss_reward_1: 0.00722
	loss_policy_2: 0.05701
	accuracy_policy_2: 0.64867
	loss_value_2: 0.07629
	loss_reward_2: 0.00944
	loss_policy_3: 0.05717
	accuracy_policy_3: 0.65156
	loss_value_3: 0.0779
	loss_reward_3: 0.01252
	loss_policy_4: 0.05715
	accuracy_policy_4: 0.65566
	loss_value_4: 0.07964
	loss_reward_4: 0.0152
	loss_policy_5: 0.05705
	accuracy_policy_5: 0.65566
	loss_value_5: 0.0812
	loss_reward_5: 0.01575
	loss_policy: 0.57143
	loss_value: 0.75263
	loss_reward: 0.06014
Optimization_Done 36400
[2025-05-07 20:43:00] [command] train weight_iter_36400.pkl 164 183
[2025-05-07 20:43:09] nn step 36450, lr: 0.1.
	loss_policy_0: 0.27253
	accuracy_policy_0: 0.6441
	loss_value_0: 0.34966
	loss_policy_1: 0.05439
	accuracy_policy_1: 0.645
	loss_value_1: 0.07104
	loss_reward_1: 0.00656
	loss_policy_2: 0.05457
	accuracy_policy_2: 0.64578
	loss_value_2: 0.07253
	loss_reward_2: 0.00915
	loss_policy_3: 0.0549
	accuracy_policy_3: 0.64207
	loss_value_3: 0.07419
	loss_reward_3: 0.01179
	loss_policy_4: 0.05442
	accuracy_policy_4: 0.64648
	loss_value_4: 0.07587
	loss_reward_4: 0.01419
	loss_policy_5: 0.0545
	accuracy_policy_5: 0.64883
	loss_value_5: 0.07745
	loss_reward_5: 0.01454
	loss_policy: 0.54532
	loss_value: 0.72073
	loss_reward: 0.05624
[2025-05-07 20:43:16] nn step 36500, lr: 0.1.
	loss_policy_0: 0.28173
	accuracy_policy_0: 0.65508
	loss_value_0: 0.36153
	loss_policy_1: 0.05655
	accuracy_policy_1: 0.64863
	loss_value_1: 0.07398
	loss_reward_1: 0.00693
	loss_policy_2: 0.0567
	accuracy_policy_2: 0.65004
	loss_value_2: 0.07571
	loss_reward_2: 0.00926
	loss_policy_3: 0.05679
	accuracy_policy_3: 0.64453
	loss_value_3: 0.07726
	loss_reward_3: 0.01237
	loss_policy_4: 0.05671
	accuracy_policy_4: 0.65078
	loss_value_4: 0.07882
	loss_reward_4: 0.01446
	loss_policy_5: 0.05707
	accuracy_policy_5: 0.64906
	loss_value_5: 0.08013
	loss_reward_5: 0.01523
	loss_policy: 0.56555
	loss_value: 0.74743
	loss_reward: 0.05824
[2025-05-07 20:43:24] nn step 36550, lr: 0.1.
	loss_policy_0: 0.28276
	accuracy_policy_0: 0.65289
	loss_value_0: 0.36472
	loss_policy_1: 0.05743
	accuracy_policy_1: 0.64668
	loss_value_1: 0.07447
	loss_reward_1: 0.0069
	loss_policy_2: 0.05792
	accuracy_policy_2: 0.64438
	loss_value_2: 0.07627
	loss_reward_2: 0.00938
	loss_policy_3: 0.05787
	accuracy_policy_3: 0.64457
	loss_value_3: 0.07805
	loss_reward_3: 0.01222
	loss_policy_4: 0.05799
	accuracy_policy_4: 0.64469
	loss_value_4: 0.07958
	loss_reward_4: 0.01469
	loss_policy_5: 0.05739
	accuracy_policy_5: 0.65434
	loss_value_5: 0.08134
	loss_reward_5: 0.01566
	loss_policy: 0.57136
	loss_value: 0.75443
	loss_reward: 0.05885
[2025-05-07 20:43:32] nn step 36600, lr: 0.1.
	loss_policy_0: 0.29061
	accuracy_policy_0: 0.64375
	loss_value_0: 0.36301
	loss_policy_1: 0.05758
	accuracy_policy_1: 0.64496
	loss_value_1: 0.07399
	loss_reward_1: 0.00743
	loss_policy_2: 0.05725
	accuracy_policy_2: 0.64449
	loss_value_2: 0.07576
	loss_reward_2: 0.00963
	loss_policy_3: 0.05775
	accuracy_policy_3: 0.65129
	loss_value_3: 0.07748
	loss_reward_3: 0.01237
	loss_policy_4: 0.05756
	accuracy_policy_4: 0.65121
	loss_value_4: 0.07958
	loss_reward_4: 0.01531
	loss_policy_5: 0.05748
	accuracy_policy_5: 0.65469
	loss_value_5: 0.08132
	loss_reward_5: 0.01583
	loss_policy: 0.57824
	loss_value: 0.75114
	loss_reward: 0.06057
Optimization_Done 36600
[2025-05-07 20:47:00] [command] train weight_iter_36600.pkl 165 184
[2025-05-07 20:47:07] nn step 36650, lr: 0.1.
	loss_policy_0: 0.26498
	accuracy_policy_0: 0.65156
	loss_value_0: 0.3499
	loss_policy_1: 0.05291
	accuracy_policy_1: 0.64902
	loss_value_1: 0.07062
	loss_reward_1: 0.00653
	loss_policy_2: 0.05296
	accuracy_policy_2: 0.6498
	loss_value_2: 0.07217
	loss_reward_2: 0.00879
	loss_policy_3: 0.05328
	accuracy_policy_3: 0.64117
	loss_value_3: 0.07365
	loss_reward_3: 0.01128
	loss_policy_4: 0.05363
	accuracy_policy_4: 0.63754
	loss_value_4: 0.07512
	loss_reward_4: 0.01368
	loss_policy_5: 0.05345
	accuracy_policy_5: 0.64387
	loss_value_5: 0.0767
	loss_reward_5: 0.01423
	loss_policy: 0.53121
	loss_value: 0.71817
	loss_reward: 0.0545
[2025-05-07 20:47:15] nn step 36700, lr: 0.1.
	loss_policy_0: 0.2709
	accuracy_policy_0: 0.65352
	loss_value_0: 0.35561
	loss_policy_1: 0.05485
	accuracy_policy_1: 0.64496
	loss_value_1: 0.07256
	loss_reward_1: 0.00676
	loss_policy_2: 0.05494
	accuracy_policy_2: 0.64543
	loss_value_2: 0.07392
	loss_reward_2: 0.00931
	loss_policy_3: 0.05514
	accuracy_policy_3: 0.64141
	loss_value_3: 0.07548
	loss_reward_3: 0.0115
	loss_policy_4: 0.05473
	accuracy_policy_4: 0.64375
	loss_value_4: 0.07727
	loss_reward_4: 0.01428
	loss_policy_5: 0.05503
	accuracy_policy_5: 0.64824
	loss_value_5: 0.07872
	loss_reward_5: 0.01535
	loss_policy: 0.54559
	loss_value: 0.73355
	loss_reward: 0.0572
[2025-05-07 20:47:23] nn step 36750, lr: 0.1.
	loss_policy_0: 0.27824
	accuracy_policy_0: 0.65234
	loss_value_0: 0.35663
	loss_policy_1: 0.05622
	accuracy_policy_1: 0.64594
	loss_value_1: 0.07292
	loss_reward_1: 0.0073
	loss_policy_2: 0.05662
	accuracy_policy_2: 0.64215
	loss_value_2: 0.07476
	loss_reward_2: 0.00897
	loss_policy_3: 0.05685
	accuracy_policy_3: 0.64363
	loss_value_3: 0.07661
	loss_reward_3: 0.01209
	loss_policy_4: 0.05671
	accuracy_policy_4: 0.64633
	loss_value_4: 0.0782
	loss_reward_4: 0.01528
	loss_policy_5: 0.05671
	accuracy_policy_5: 0.64773
	loss_value_5: 0.0798
	loss_reward_5: 0.01533
	loss_policy: 0.56135
	loss_value: 0.73892
	loss_reward: 0.05896
[2025-05-07 20:47:30] nn step 36800, lr: 0.1.
	loss_policy_0: 0.27493
	accuracy_policy_0: 0.65859
	loss_value_0: 0.35024
	loss_policy_1: 0.05532
	accuracy_policy_1: 0.65004
	loss_value_1: 0.0715
	loss_reward_1: 0.00708
	loss_policy_2: 0.0557
	accuracy_policy_2: 0.6432
	loss_value_2: 0.07335
	loss_reward_2: 0.00941
	loss_policy_3: 0.05583
	accuracy_policy_3: 0.64492
	loss_value_3: 0.07512
	loss_reward_3: 0.01185
	loss_policy_4: 0.05568
	accuracy_policy_4: 0.64473
	loss_value_4: 0.07712
	loss_reward_4: 0.01457
	loss_policy_5: 0.05578
	accuracy_policy_5: 0.64848
	loss_value_5: 0.07881
	loss_reward_5: 0.01539
	loss_policy: 0.55324
	loss_value: 0.72613
	loss_reward: 0.05829
Optimization_Done 36800
[2025-05-07 20:50:58] [command] train weight_iter_36800.pkl 166 185
[2025-05-07 20:51:07] nn step 36850, lr: 0.1.
	loss_policy_0: 0.26578
	accuracy_policy_0: 0.64941
	loss_value_0: 0.35173
	loss_policy_1: 0.0538
	accuracy_policy_1: 0.64141
	loss_value_1: 0.07132
	loss_reward_1: 0.00654
	loss_policy_2: 0.0539
	accuracy_policy_2: 0.64508
	loss_value_2: 0.07317
	loss_reward_2: 0.00875
	loss_policy_3: 0.05401
	accuracy_policy_3: 0.64375
	loss_value_3: 0.07477
	loss_reward_3: 0.01165
	loss_policy_4: 0.05433
	accuracy_policy_4: 0.64434
	loss_value_4: 0.07642
	loss_reward_4: 0.01354
	loss_policy_5: 0.05461
	accuracy_policy_5: 0.64621
	loss_value_5: 0.07786
	loss_reward_5: 0.01481
	loss_policy: 0.53643
	loss_value: 0.72527
	loss_reward: 0.05528
[2025-05-07 20:51:15] nn step 36900, lr: 0.1.
	loss_policy_0: 0.25039
	accuracy_policy_0: 0.65289
	loss_value_0: 0.32621
	loss_policy_1: 0.05096
	accuracy_policy_1: 0.64359
	loss_value_1: 0.06655
	loss_reward_1: 0.00615
	loss_policy_2: 0.05127
	accuracy_policy_2: 0.64207
	loss_value_2: 0.06843
	loss_reward_2: 0.00863
	loss_policy_3: 0.05136
	accuracy_policy_3: 0.64148
	loss_value_3: 0.07018
	loss_reward_3: 0.01118
	loss_policy_4: 0.0515
	accuracy_policy_4: 0.64438
	loss_value_4: 0.07154
	loss_reward_4: 0.01344
	loss_policy_5: 0.05135
	accuracy_policy_5: 0.64723
	loss_value_5: 0.07311
	loss_reward_5: 0.01364
	loss_policy: 0.50683
	loss_value: 0.67604
	loss_reward: 0.05303
[2025-05-07 20:51:21] nn step 36950, lr: 0.1.
	loss_policy_0: 0.27559
	accuracy_policy_0: 0.65027
	loss_value_0: 0.35432
	loss_policy_1: 0.05576
	accuracy_policy_1: 0.64543
	loss_value_1: 0.07224
	loss_reward_1: 0.00686
	loss_policy_2: 0.05614
	accuracy_policy_2: 0.64461
	loss_value_2: 0.07428
	loss_reward_2: 0.00933
	loss_policy_3: 0.05612
	accuracy_policy_3: 0.6427
	loss_value_3: 0.07603
	loss_reward_3: 0.01182
	loss_policy_4: 0.05614
	accuracy_policy_4: 0.64715
	loss_value_4: 0.07741
	loss_reward_4: 0.0144
	loss_policy_5: 0.05633
	accuracy_policy_5: 0.64453
	loss_value_5: 0.07901
	loss_reward_5: 0.01492
	loss_policy: 0.55608
	loss_value: 0.73329
	loss_reward: 0.05733
[2025-05-07 20:51:29] nn step 37000, lr: 0.1.
	loss_policy_0: 0.27103
	accuracy_policy_0: 0.65387
	loss_value_0: 0.34708
	loss_policy_1: 0.05519
	accuracy_policy_1: 0.64086
	loss_value_1: 0.07107
	loss_reward_1: 0.00675
	loss_policy_2: 0.0551
	accuracy_policy_2: 0.64199
	loss_value_2: 0.07287
	loss_reward_2: 0.00918
	loss_policy_3: 0.05526
	accuracy_policy_3: 0.63875
	loss_value_3: 0.07463
	loss_reward_3: 0.01185
	loss_policy_4: 0.05531
	accuracy_policy_4: 0.64387
	loss_value_4: 0.0763
	loss_reward_4: 0.01444
	loss_policy_5: 0.05531
	accuracy_policy_5: 0.64797
	loss_value_5: 0.07792
	loss_reward_5: 0.01519
	loss_policy: 0.5472
	loss_value: 0.71987
	loss_reward: 0.05741
Optimization_Done 37000
[2025-05-07 20:54:51] [command] train weight_iter_37000.pkl 167 186
[2025-05-07 20:55:00] nn step 37050, lr: 0.1.
	loss_policy_0: 0.27345
	accuracy_policy_0: 0.66203
	loss_value_0: 0.36214
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.65102
	loss_value_1: 0.07351
	loss_reward_1: 0.00703
	loss_policy_2: 0.05574
	accuracy_policy_2: 0.64781
	loss_value_2: 0.07521
	loss_reward_2: 0.00909
	loss_policy_3: 0.05584
	accuracy_policy_3: 0.64898
	loss_value_3: 0.07634
	loss_reward_3: 0.01141
	loss_policy_4: 0.05579
	accuracy_policy_4: 0.65305
	loss_value_4: 0.07835
	loss_reward_4: 0.01435
	loss_policy_5: 0.05545
	accuracy_policy_5: 0.65438
	loss_value_5: 0.07952
	loss_reward_5: 0.0146
	loss_policy: 0.55163
	loss_value: 0.74506
	loss_reward: 0.05649
[2025-05-07 20:55:06] nn step 37100, lr: 0.1.
	loss_policy_0: 0.28276
	accuracy_policy_0: 0.65723
	loss_value_0: 0.36502
	loss_policy_1: 0.0571
	accuracy_policy_1: 0.64559
	loss_value_1: 0.07391
	loss_reward_1: 0.00715
	loss_policy_2: 0.05697
	accuracy_policy_2: 0.64562
	loss_value_2: 0.07559
	loss_reward_2: 0.00982
	loss_policy_3: 0.0571
	accuracy_policy_3: 0.64949
	loss_value_3: 0.07757
	loss_reward_3: 0.01272
	loss_policy_4: 0.05685
	accuracy_policy_4: 0.65469
	loss_value_4: 0.07944
	loss_reward_4: 0.01488
	loss_policy_5: 0.05707
	accuracy_policy_5: 0.65605
	loss_value_5: 0.08079
	loss_reward_5: 0.01588
	loss_policy: 0.56785
	loss_value: 0.75231
	loss_reward: 0.06046
[2025-05-07 20:55:15] nn step 37150, lr: 0.1.
	loss_policy_0: 0.28128
	accuracy_policy_0: 0.65605
	loss_value_0: 0.36231
	loss_policy_1: 0.05682
	accuracy_policy_1: 0.65102
	loss_value_1: 0.07417
	loss_reward_1: 0.00687
	loss_policy_2: 0.05697
	accuracy_policy_2: 0.64758
	loss_value_2: 0.07613
	loss_reward_2: 0.00938
	loss_policy_3: 0.0571
	accuracy_policy_3: 0.65074
	loss_value_3: 0.07795
	loss_reward_3: 0.01216
	loss_policy_4: 0.05737
	accuracy_policy_4: 0.6493
	loss_value_4: 0.07958
	loss_reward_4: 0.01525
	loss_policy_5: 0.05712
	accuracy_policy_5: 0.6523
	loss_value_5: 0.08117
	loss_reward_5: 0.01573
	loss_policy: 0.56665
	loss_value: 0.75131
	loss_reward: 0.05939
[2025-05-07 20:55:23] nn step 37200, lr: 0.1.
	loss_policy_0: 0.28329
	accuracy_policy_0: 0.65375
	loss_value_0: 0.35972
	loss_policy_1: 0.05685
	accuracy_policy_1: 0.64684
	loss_value_1: 0.07333
	loss_reward_1: 0.00712
	loss_policy_2: 0.0569
	accuracy_policy_2: 0.64727
	loss_value_2: 0.07556
	loss_reward_2: 0.00906
	loss_policy_3: 0.05728
	accuracy_policy_3: 0.64961
	loss_value_3: 0.07758
	loss_reward_3: 0.01177
	loss_policy_4: 0.05732
	accuracy_policy_4: 0.65023
	loss_value_4: 0.07938
	loss_reward_4: 0.01495
	loss_policy_5: 0.05709
	accuracy_policy_5: 0.65992
	loss_value_5: 0.08094
	loss_reward_5: 0.01534
	loss_policy: 0.56873
	loss_value: 0.7465
	loss_reward: 0.05824
Optimization_Done 37200
[2025-05-07 20:58:45] [command] train weight_iter_37200.pkl 168 187
[2025-05-07 20:58:52] nn step 37250, lr: 0.1.
	loss_policy_0: 0.26523
	accuracy_policy_0: 0.65801
	loss_value_0: 0.35455
	loss_policy_1: 0.0537
	accuracy_policy_1: 0.65043
	loss_value_1: 0.07237
	loss_reward_1: 0.00681
	loss_policy_2: 0.05354
	accuracy_policy_2: 0.65652
	loss_value_2: 0.07372
	loss_reward_2: 0.00909
	loss_policy_3: 0.05408
	accuracy_policy_3: 0.65094
	loss_value_3: 0.0752
	loss_reward_3: 0.01131
	loss_policy_4: 0.05407
	accuracy_policy_4: 0.65648
	loss_value_4: 0.07671
	loss_reward_4: 0.01418
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.65691
	loss_value_5: 0.07793
	loss_reward_5: 0.01456
	loss_policy: 0.53432
	loss_value: 0.73048
	loss_reward: 0.05596
[2025-05-07 20:59:00] nn step 37300, lr: 0.1.
	loss_policy_0: 0.26544
	accuracy_policy_0: 0.6541
	loss_value_0: 0.34259
	loss_policy_1: 0.05352
	accuracy_policy_1: 0.64977
	loss_value_1: 0.06976
	loss_reward_1: 0.00639
	loss_policy_2: 0.0533
	accuracy_policy_2: 0.64875
	loss_value_2: 0.07171
	loss_reward_2: 0.00853
	loss_policy_3: 0.05345
	accuracy_policy_3: 0.64809
	loss_value_3: 0.07309
	loss_reward_3: 0.01127
	loss_policy_4: 0.0533
	accuracy_policy_4: 0.655
	loss_value_4: 0.07446
	loss_reward_4: 0.01353
	loss_policy_5: 0.05367
	accuracy_policy_5: 0.65855
	loss_value_5: 0.07601
	loss_reward_5: 0.01422
	loss_policy: 0.53269
	loss_value: 0.70762
	loss_reward: 0.05393
[2025-05-07 20:59:08] nn step 37350, lr: 0.1.
	loss_policy_0: 0.27918
	accuracy_policy_0: 0.66266
	loss_value_0: 0.36227
	loss_policy_1: 0.05638
	accuracy_policy_1: 0.65504
	loss_value_1: 0.07392
	loss_reward_1: 0.00693
	loss_policy_2: 0.05686
	accuracy_policy_2: 0.65508
	loss_value_2: 0.07577
	loss_reward_2: 0.00944
	loss_policy_3: 0.05692
	accuracy_policy_3: 0.6502
	loss_value_3: 0.07738
	loss_reward_3: 0.01155
	loss_policy_4: 0.0566
	accuracy_policy_4: 0.65387
	loss_value_4: 0.07844
	loss_reward_4: 0.01455
	loss_policy_5: 0.05703
	accuracy_policy_5: 0.65664
	loss_value_5: 0.08022
	loss_reward_5: 0.01557
	loss_policy: 0.56297
	loss_value: 0.74801
	loss_reward: 0.05805
[2025-05-07 20:59:16] nn step 37400, lr: 0.1.
	loss_policy_0: 0.2859
	accuracy_policy_0: 0.65617
	loss_value_0: 0.36545
	loss_policy_1: 0.05768
	accuracy_policy_1: 0.65293
	loss_value_1: 0.07497
	loss_reward_1: 0.0073
	loss_policy_2: 0.05774
	accuracy_policy_2: 0.65219
	loss_value_2: 0.07669
	loss_reward_2: 0.00941
	loss_policy_3: 0.0575
	accuracy_policy_3: 0.65164
	loss_value_3: 0.0784
	loss_reward_3: 0.01218
	loss_policy_4: 0.05783
	accuracy_policy_4: 0.65168
	loss_value_4: 0.07989
	loss_reward_4: 0.01531
	loss_policy_5: 0.05785
	accuracy_policy_5: 0.65578
	loss_value_5: 0.08138
	loss_reward_5: 0.01552
	loss_policy: 0.5745
	loss_value: 0.75678
	loss_reward: 0.05972
Optimization_Done 37400
[2025-05-07 21:02:49] [command] train weight_iter_37400.pkl 169 188
[2025-05-07 21:02:58] nn step 37450, lr: 0.1.
	loss_policy_0: 0.28266
	accuracy_policy_0: 0.65668
	loss_value_0: 0.37153
	loss_policy_1: 0.05702
	accuracy_policy_1: 0.64043
	loss_value_1: 0.07551
	loss_reward_1: 0.00709
	loss_policy_2: 0.0574
	accuracy_policy_2: 0.64051
	loss_value_2: 0.07723
	loss_reward_2: 0.00961
	loss_policy_3: 0.05795
	accuracy_policy_3: 0.63715
	loss_value_3: 0.07864
	loss_reward_3: 0.01216
	loss_policy_4: 0.05809
	accuracy_policy_4: 0.64129
	loss_value_4: 0.08022
	loss_reward_4: 0.01494
	loss_policy_5: 0.05804
	accuracy_policy_5: 0.64461
	loss_value_5: 0.08157
	loss_reward_5: 0.01584
	loss_policy: 0.57116
	loss_value: 0.7647
	loss_reward: 0.05964
[2025-05-07 21:03:06] nn step 37500, lr: 0.1.
	loss_policy_0: 0.26003
	accuracy_policy_0: 0.64539
	loss_value_0: 0.33799
	loss_policy_1: 0.05275
	accuracy_policy_1: 0.64383
	loss_value_1: 0.0688
	loss_reward_1: 0.0066
	loss_policy_2: 0.05276
	accuracy_policy_2: 0.6427
	loss_value_2: 0.07041
	loss_reward_2: 0.00864
	loss_policy_3: 0.05296
	accuracy_policy_3: 0.64316
	loss_value_3: 0.07163
	loss_reward_3: 0.01128
	loss_policy_4: 0.05273
	accuracy_policy_4: 0.64531
	loss_value_4: 0.07312
	loss_reward_4: 0.01382
	loss_policy_5: 0.053
	accuracy_policy_5: 0.65016
	loss_value_5: 0.07421
	loss_reward_5: 0.01421
	loss_policy: 0.52424
	loss_value: 0.69617
	loss_reward: 0.05455
[2025-05-07 21:03:13] nn step 37550, lr: 0.1.
	loss_policy_0: 0.27325
	accuracy_policy_0: 0.65199
	loss_value_0: 0.35198
	loss_policy_1: 0.05532
	accuracy_policy_1: 0.64016
	loss_value_1: 0.07153
	loss_reward_1: 0.00677
	loss_policy_2: 0.05556
	accuracy_policy_2: 0.64074
	loss_value_2: 0.07318
	loss_reward_2: 0.00893
	loss_policy_3: 0.05534
	accuracy_policy_3: 0.64816
	loss_value_3: 0.07481
	loss_reward_3: 0.01148
	loss_policy_4: 0.05555
	accuracy_policy_4: 0.64816
	loss_value_4: 0.07639
	loss_reward_4: 0.0143
	loss_policy_5: 0.05539
	accuracy_policy_5: 0.65086
	loss_value_5: 0.07779
	loss_reward_5: 0.01528
	loss_policy: 0.55042
	loss_value: 0.72567
	loss_reward: 0.05676
[2025-05-07 21:03:21] nn step 37600, lr: 0.1.
	loss_policy_0: 0.28211
	accuracy_policy_0: 0.63598
	loss_value_0: 0.34667
	loss_policy_1: 0.05526
	accuracy_policy_1: 0.64266
	loss_value_1: 0.07073
	loss_reward_1: 0.00687
	loss_policy_2: 0.05582
	accuracy_policy_2: 0.64309
	loss_value_2: 0.0722
	loss_reward_2: 0.00894
	loss_policy_3: 0.05628
	accuracy_policy_3: 0.6393
	loss_value_3: 0.07397
	loss_reward_3: 0.01175
	loss_policy_4: 0.05626
	accuracy_policy_4: 0.64008
	loss_value_4: 0.07562
	loss_reward_4: 0.01443
	loss_policy_5: 0.05609
	accuracy_policy_5: 0.64191
	loss_value_5: 0.07692
	loss_reward_5: 0.015
	loss_policy: 0.56183
	loss_value: 0.71611
	loss_reward: 0.05699
Optimization_Done 37600
[2025-05-07 21:06:37] [command] train weight_iter_37600.pkl 170 189
[2025-05-07 21:06:46] nn step 37650, lr: 0.1.
	loss_policy_0: 0.28952
	accuracy_policy_0: 0.65156
	loss_value_0: 0.37791
	loss_policy_1: 0.05804
	accuracy_policy_1: 0.64578
	loss_value_1: 0.07685
	loss_reward_1: 0.00731
	loss_policy_2: 0.05845
	accuracy_policy_2: 0.6468
	loss_value_2: 0.07836
	loss_reward_2: 0.01005
	loss_policy_3: 0.05861
	accuracy_policy_3: 0.64777
	loss_value_3: 0.07991
	loss_reward_3: 0.01261
	loss_policy_4: 0.0586
	accuracy_policy_4: 0.65219
	loss_value_4: 0.08139
	loss_reward_4: 0.01579
	loss_policy_5: 0.05915
	accuracy_policy_5: 0.6482
	loss_value_5: 0.08279
	loss_reward_5: 0.01632
	loss_policy: 0.58237
	loss_value: 0.77721
	loss_reward: 0.06208
[2025-05-07 21:06:54] nn step 37700, lr: 0.1.
	loss_policy_0: 0.25779
	accuracy_policy_0: 0.65559
	loss_value_0: 0.33081
	loss_policy_1: 0.05231
	accuracy_policy_1: 0.65082
	loss_value_1: 0.06756
	loss_reward_1: 0.00643
	loss_policy_2: 0.05242
	accuracy_policy_2: 0.64711
	loss_value_2: 0.06915
	loss_reward_2: 0.00837
	loss_policy_3: 0.05265
	accuracy_policy_3: 0.63992
	loss_value_3: 0.07077
	loss_reward_3: 0.01119
	loss_policy_4: 0.05297
	accuracy_policy_4: 0.63828
	loss_value_4: 0.07218
	loss_reward_4: 0.01362
	loss_policy_5: 0.05274
	accuracy_policy_5: 0.64898
	loss_value_5: 0.0734
	loss_reward_5: 0.01426
	loss_policy: 0.52089
	loss_value: 0.68386
	loss_reward: 0.05387
[2025-05-07 21:07:01] nn step 37750, lr: 0.1.
	loss_policy_0: 0.27946
	accuracy_policy_0: 0.6559
	loss_value_0: 0.35661
	loss_policy_1: 0.05613
	accuracy_policy_1: 0.64719
	loss_value_1: 0.07297
	loss_reward_1: 0.00703
	loss_policy_2: 0.05614
	accuracy_policy_2: 0.64848
	loss_value_2: 0.0745
	loss_reward_2: 0.00893
	loss_policy_3: 0.05618
	accuracy_policy_3: 0.64797
	loss_value_3: 0.07628
	loss_reward_3: 0.01218
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.65258
	loss_value_4: 0.07794
	loss_reward_4: 0.01467
	loss_policy_5: 0.05687
	accuracy_policy_5: 0.65184
	loss_value_5: 0.07944
	loss_reward_5: 0.01528
	loss_policy: 0.56122
	loss_value: 0.73774
	loss_reward: 0.05809
[2025-05-07 21:07:09] nn step 37800, lr: 0.1.
	loss_policy_0: 0.27949
	accuracy_policy_0: 0.64848
	loss_value_0: 0.35449
	loss_policy_1: 0.05656
	accuracy_policy_1: 0.63883
	loss_value_1: 0.07213
	loss_reward_1: 0.00669
	loss_policy_2: 0.05655
	accuracy_policy_2: 0.64121
	loss_value_2: 0.07398
	loss_reward_2: 0.0092
	loss_policy_3: 0.05688
	accuracy_policy_3: 0.64426
	loss_value_3: 0.07538
	loss_reward_3: 0.01235
	loss_policy_4: 0.05648
	accuracy_policy_4: 0.64734
	loss_value_4: 0.07722
	loss_reward_4: 0.01446
	loss_policy_5: 0.0567
	accuracy_policy_5: 0.65125
	loss_value_5: 0.0791
	loss_reward_5: 0.01512
	loss_policy: 0.56265
	loss_value: 0.73229
	loss_reward: 0.05781
Optimization_Done 37800
[2025-05-07 21:10:35] [command] train weight_iter_37800.pkl 171 190
[2025-05-07 21:10:42] nn step 37850, lr: 0.1.
	loss_policy_0: 0.28497
	accuracy_policy_0: 0.65062
	loss_value_0: 0.36829
	loss_policy_1: 0.05776
	accuracy_policy_1: 0.64641
	loss_value_1: 0.07464
	loss_reward_1: 0.00714
	loss_policy_2: 0.05782
	accuracy_policy_2: 0.64332
	loss_value_2: 0.07641
	loss_reward_2: 0.00941
	loss_policy_3: 0.05805
	accuracy_policy_3: 0.64059
	loss_value_3: 0.07805
	loss_reward_3: 0.01243
	loss_policy_4: 0.05781
	accuracy_policy_4: 0.64918
	loss_value_4: 0.07981
	loss_reward_4: 0.01496
	loss_policy_5: 0.05806
	accuracy_policy_5: 0.65449
	loss_value_5: 0.08105
	loss_reward_5: 0.01561
	loss_policy: 0.57448
	loss_value: 0.75826
	loss_reward: 0.05955
[2025-05-07 21:10:50] nn step 37900, lr: 0.1.
	loss_policy_0: 0.27812
	accuracy_policy_0: 0.655
	loss_value_0: 0.35186
	loss_policy_1: 0.05619
	accuracy_policy_1: 0.64488
	loss_value_1: 0.07176
	loss_reward_1: 0.00683
	loss_policy_2: 0.05615
	accuracy_policy_2: 0.64621
	loss_value_2: 0.07335
	loss_reward_2: 0.00896
	loss_policy_3: 0.05668
	accuracy_policy_3: 0.64105
	loss_value_3: 0.07497
	loss_reward_3: 0.01199
	loss_policy_4: 0.05681
	accuracy_policy_4: 0.64816
	loss_value_4: 0.07654
	loss_reward_4: 0.01413
	loss_policy_5: 0.05666
	accuracy_policy_5: 0.65461
	loss_value_5: 0.07817
	loss_reward_5: 0.01475
	loss_policy: 0.56061
	loss_value: 0.72666
	loss_reward: 0.05666
[2025-05-07 21:10:58] nn step 37950, lr: 0.1.
	loss_policy_0: 0.27819
	accuracy_policy_0: 0.65324
	loss_value_0: 0.34976
	loss_policy_1: 0.05628
	accuracy_policy_1: 0.64297
	loss_value_1: 0.07126
	loss_reward_1: 0.00707
	loss_policy_2: 0.05573
	accuracy_policy_2: 0.64762
	loss_value_2: 0.07296
	loss_reward_2: 0.0093
	loss_policy_3: 0.05645
	accuracy_policy_3: 0.64578
	loss_value_3: 0.07454
	loss_reward_3: 0.01209
	loss_policy_4: 0.05645
	accuracy_policy_4: 0.65016
	loss_value_4: 0.07613
	loss_reward_4: 0.01445
	loss_policy_5: 0.05652
	accuracy_policy_5: 0.65344
	loss_value_5: 0.07752
	loss_reward_5: 0.01506
	loss_policy: 0.55962
	loss_value: 0.72216
	loss_reward: 0.05797
[2025-05-07 21:11:06] nn step 38000, lr: 0.1.
	loss_policy_0: 0.28026
	accuracy_policy_0: 0.65641
	loss_value_0: 0.35329
	loss_policy_1: 0.05648
	accuracy_policy_1: 0.64797
	loss_value_1: 0.07229
	loss_reward_1: 0.00703
	loss_policy_2: 0.05716
	accuracy_policy_2: 0.64742
	loss_value_2: 0.0741
	loss_reward_2: 0.00909
	loss_policy_3: 0.05734
	accuracy_policy_3: 0.64355
	loss_value_3: 0.07558
	loss_reward_3: 0.01199
	loss_policy_4: 0.05765
	accuracy_policy_4: 0.64746
	loss_value_4: 0.07731
	loss_reward_4: 0.01469
	loss_policy_5: 0.05771
	accuracy_policy_5: 0.65355
	loss_value_5: 0.07874
	loss_reward_5: 0.0157
	loss_policy: 0.5666
	loss_value: 0.73131
	loss_reward: 0.05849
Optimization_Done 38000
[2025-05-07 21:14:30] [command] train weight_iter_38000.pkl 172 191
[2025-05-07 21:14:39] nn step 38050, lr: 0.1.
	loss_policy_0: 0.28299
	accuracy_policy_0: 0.65555
	loss_value_0: 0.36477
	loss_policy_1: 0.05696
	accuracy_policy_1: 0.64371
	loss_value_1: 0.07409
	loss_reward_1: 0.0068
	loss_policy_2: 0.05741
	accuracy_policy_2: 0.64223
	loss_value_2: 0.07534
	loss_reward_2: 0.00912
	loss_policy_3: 0.05783
	accuracy_policy_3: 0.63852
	loss_value_3: 0.07669
	loss_reward_3: 0.01209
	loss_policy_4: 0.0578
	accuracy_policy_4: 0.64336
	loss_value_4: 0.07787
	loss_reward_4: 0.01449
	loss_policy_5: 0.05776
	accuracy_policy_5: 0.64785
	loss_value_5: 0.07937
	loss_reward_5: 0.01481
	loss_policy: 0.57075
	loss_value: 0.74812
	loss_reward: 0.05732
[2025-05-07 21:14:47] nn step 38100, lr: 0.1.
	loss_policy_0: 0.26327
	accuracy_policy_0: 0.65184
	loss_value_0: 0.33302
	loss_policy_1: 0.05374
	accuracy_policy_1: 0.64469
	loss_value_1: 0.06789
	loss_reward_1: 0.00653
	loss_policy_2: 0.05375
	accuracy_policy_2: 0.63891
	loss_value_2: 0.06977
	loss_reward_2: 0.00876
	loss_policy_3: 0.0538
	accuracy_policy_3: 0.64023
	loss_value_3: 0.07126
	loss_reward_3: 0.0114
	loss_policy_4: 0.05404
	accuracy_policy_4: 0.64301
	loss_value_4: 0.07287
	loss_reward_4: 0.01351
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.65262
	loss_value_5: 0.07402
	loss_reward_5: 0.01428
	loss_policy: 0.53235
	loss_value: 0.68883
	loss_reward: 0.05449
[2025-05-07 21:14:56] nn step 38150, lr: 0.1.
	loss_policy_0: 0.27745
	accuracy_policy_0: 0.65078
	loss_value_0: 0.34816
	loss_policy_1: 0.0562
	accuracy_policy_1: 0.63582
	loss_value_1: 0.07089
	loss_reward_1: 0.00695
	loss_policy_2: 0.0563
	accuracy_policy_2: 0.64059
	loss_value_2: 0.07278
	loss_reward_2: 0.0092
	loss_policy_3: 0.05675
	accuracy_policy_3: 0.63277
	loss_value_3: 0.0744
	loss_reward_3: 0.01174
	loss_policy_4: 0.05687
	accuracy_policy_4: 0.64008
	loss_value_4: 0.07613
	loss_reward_4: 0.01451
	loss_policy_5: 0.05663
	accuracy_policy_5: 0.64766
	loss_value_5: 0.07737
	loss_reward_5: 0.0154
	loss_policy: 0.5602
	loss_value: 0.71973
	loss_reward: 0.0578
[2025-05-07 21:15:02] nn step 38200, lr: 0.1.
	loss_policy_0: 0.28036
	accuracy_policy_0: 0.64414
	loss_value_0: 0.34569
	loss_policy_1: 0.05642
	accuracy_policy_1: 0.63547
	loss_value_1: 0.0708
	loss_reward_1: 0.00702
	loss_policy_2: 0.05641
	accuracy_policy_2: 0.64023
	loss_value_2: 0.07249
	loss_reward_2: 0.00917
	loss_policy_3: 0.05658
	accuracy_policy_3: 0.64168
	loss_value_3: 0.0745
	loss_reward_3: 0.01163
	loss_policy_4: 0.05678
	accuracy_policy_4: 0.64246
	loss_value_4: 0.0758
	loss_reward_4: 0.01482
	loss_policy_5: 0.0566
	accuracy_policy_5: 0.6493
	loss_value_5: 0.07742
	loss_reward_5: 0.01562
	loss_policy: 0.56315
	loss_value: 0.7167
	loss_reward: 0.05826
Optimization_Done 38200
[2025-05-07 21:18:28] [command] train weight_iter_38200.pkl 173 192
[2025-05-07 21:18:37] nn step 38250, lr: 0.1.
	loss_policy_0: 0.28134
	accuracy_policy_0: 0.64777
	loss_value_0: 0.36075
	loss_policy_1: 0.05671
	accuracy_policy_1: 0.64457
	loss_value_1: 0.07319
	loss_reward_1: 0.00679
	loss_policy_2: 0.05703
	accuracy_policy_2: 0.64262
	loss_value_2: 0.07447
	loss_reward_2: 0.0095
	loss_policy_3: 0.05716
	accuracy_policy_3: 0.64301
	loss_value_3: 0.07612
	loss_reward_3: 0.01184
	loss_policy_4: 0.05737
	accuracy_policy_4: 0.64684
	loss_value_4: 0.07742
	loss_reward_4: 0.01432
	loss_policy_5: 0.05717
	accuracy_policy_5: 0.64895
	loss_value_5: 0.07858
	loss_reward_5: 0.01572
	loss_policy: 0.56678
	loss_value: 0.74052
	loss_reward: 0.05818
[2025-05-07 21:18:45] nn step 38300, lr: 0.1.
	loss_policy_0: 0.28182
	accuracy_policy_0: 0.65105
	loss_value_0: 0.35508
	loss_policy_1: 0.05702
	accuracy_policy_1: 0.64379
	loss_value_1: 0.07247
	loss_reward_1: 0.00689
	loss_policy_2: 0.05762
	accuracy_policy_2: 0.63879
	loss_value_2: 0.07435
	loss_reward_2: 0.00895
	loss_policy_3: 0.05754
	accuracy_policy_3: 0.63891
	loss_value_3: 0.07583
	loss_reward_3: 0.01237
	loss_policy_4: 0.05784
	accuracy_policy_4: 0.64293
	loss_value_4: 0.07759
	loss_reward_4: 0.01474
	loss_policy_5: 0.05742
	accuracy_policy_5: 0.64977
	loss_value_5: 0.07903
	loss_reward_5: 0.01575
	loss_policy: 0.56927
	loss_value: 0.73436
	loss_reward: 0.05871
[2025-05-07 21:18:51] nn step 38350, lr: 0.1.
	loss_policy_0: 0.28294
	accuracy_policy_0: 0.64988
	loss_value_0: 0.35674
	loss_policy_1: 0.05748
	accuracy_policy_1: 0.64086
	loss_value_1: 0.07275
	loss_reward_1: 0.00718
	loss_policy_2: 0.05739
	accuracy_policy_2: 0.64207
	loss_value_2: 0.07464
	loss_reward_2: 0.00919
	loss_policy_3: 0.05752
	accuracy_policy_3: 0.63965
	loss_value_3: 0.07621
	loss_reward_3: 0.01187
	loss_policy_4: 0.05756
	accuracy_policy_4: 0.64996
	loss_value_4: 0.07803
	loss_reward_4: 0.01468
	loss_policy_5: 0.05726
	accuracy_policy_5: 0.6557
	loss_value_5: 0.07919
	loss_reward_5: 0.01584
	loss_policy: 0.57016
	loss_value: 0.73757
	loss_reward: 0.05876
[2025-05-07 21:18:59] nn step 38400, lr: 0.1.
	loss_policy_0: 0.28588
	accuracy_policy_0: 0.65168
	loss_value_0: 0.35747
	loss_policy_1: 0.05819
	accuracy_policy_1: 0.64168
	loss_value_1: 0.07328
	loss_reward_1: 0.00699
	loss_policy_2: 0.05848
	accuracy_policy_2: 0.64285
	loss_value_2: 0.07506
	loss_reward_2: 0.00921
	loss_policy_3: 0.0585
	accuracy_policy_3: 0.64234
	loss_value_3: 0.07631
	loss_reward_3: 0.01229
	loss_policy_4: 0.05848
	accuracy_policy_4: 0.64316
	loss_value_4: 0.07787
	loss_reward_4: 0.01512
	loss_policy_5: 0.05826
	accuracy_policy_5: 0.64812
	loss_value_5: 0.07957
	loss_reward_5: 0.01576
	loss_policy: 0.57778
	loss_value: 0.73957
	loss_reward: 0.05936
Optimization_Done 38400
[2025-05-07 21:22:22] [command] train weight_iter_38400.pkl 174 193
[2025-05-07 21:22:31] nn step 38450, lr: 0.1.
	loss_policy_0: 0.28655
	accuracy_policy_0: 0.6441
	loss_value_0: 0.36851
	loss_policy_1: 0.05816
	accuracy_policy_1: 0.63551
	loss_value_1: 0.0749
	loss_reward_1: 0.00687
	loss_policy_2: 0.05838
	accuracy_policy_2: 0.63734
	loss_value_2: 0.07636
	loss_reward_2: 0.00937
	loss_policy_3: 0.05796
	accuracy_policy_3: 0.63633
	loss_value_3: 0.07775
	loss_reward_3: 0.01198
	loss_policy_4: 0.05808
	accuracy_policy_4: 0.6418
	loss_value_4: 0.07928
	loss_reward_4: 0.01491
	loss_policy_5: 0.058
	accuracy_policy_5: 0.6452
	loss_value_5: 0.08036
	loss_reward_5: 0.01565
	loss_policy: 0.57713
	loss_value: 0.75718
	loss_reward: 0.05878
[2025-05-07 21:22:37] nn step 38500, lr: 0.1.
	loss_policy_0: 0.27224
	accuracy_policy_0: 0.64215
	loss_value_0: 0.34038
	loss_policy_1: 0.05525
	accuracy_policy_1: 0.64062
	loss_value_1: 0.06941
	loss_reward_1: 0.00661
	loss_policy_2: 0.05563
	accuracy_policy_2: 0.63539
	loss_value_2: 0.07115
	loss_reward_2: 0.00913
	loss_policy_3: 0.05519
	accuracy_policy_3: 0.63828
	loss_value_3: 0.07238
	loss_reward_3: 0.01143
	loss_policy_4: 0.05519
	accuracy_policy_4: 0.64277
	loss_value_4: 0.07386
	loss_reward_4: 0.01429
	loss_policy_5: 0.05545
	accuracy_policy_5: 0.64473
	loss_value_5: 0.07511
	loss_reward_5: 0.01527
	loss_policy: 0.54896
	loss_value: 0.70229
	loss_reward: 0.05674
[2025-05-07 21:22:45] nn step 38550, lr: 0.1.
	loss_policy_0: 0.28264
	accuracy_policy_0: 0.63199
	loss_value_0: 0.34092
	loss_policy_1: 0.05625
	accuracy_policy_1: 0.63672
	loss_value_1: 0.06971
	loss_reward_1: 0.00642
	loss_policy_2: 0.05629
	accuracy_policy_2: 0.63848
	loss_value_2: 0.07136
	loss_reward_2: 0.00849
	loss_policy_3: 0.05678
	accuracy_policy_3: 0.63574
	loss_value_3: 0.07283
	loss_reward_3: 0.01151
	loss_policy_4: 0.05641
	accuracy_policy_4: 0.64375
	loss_value_4: 0.07453
	loss_reward_4: 0.014
	loss_policy_5: 0.0565
	accuracy_policy_5: 0.64793
	loss_value_5: 0.07565
	loss_reward_5: 0.01449
	loss_policy: 0.56487
	loss_value: 0.705
	loss_reward: 0.05491
[2025-05-07 21:22:53] nn step 38600, lr: 0.1.
	loss_policy_0: 0.2636
	accuracy_policy_0: 0.64938
	loss_value_0: 0.32384
	loss_policy_1: 0.05339
	accuracy_policy_1: 0.63918
	loss_value_1: 0.06618
	loss_reward_1: 0.00639
	loss_policy_2: 0.05329
	accuracy_policy_2: 0.6409
	loss_value_2: 0.06767
	loss_reward_2: 0.00851
	loss_policy_3: 0.0531
	accuracy_policy_3: 0.64258
	loss_value_3: 0.06925
	loss_reward_3: 0.01112
	loss_policy_4: 0.05314
	accuracy_policy_4: 0.64527
	loss_value_4: 0.07053
	loss_reward_4: 0.01333
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.65113
	loss_value_5: 0.07172
	loss_reward_5: 0.01401
	loss_policy: 0.52963
	loss_value: 0.66919
	loss_reward: 0.05337
Optimization_Done 38600
[2025-05-07 21:26:15] [command] train weight_iter_38600.pkl 175 194
[2025-05-07 21:26:23] nn step 38650, lr: 0.1.
	loss_policy_0: 0.28578
	accuracy_policy_0: 0.64102
	loss_value_0: 0.35779
	loss_policy_1: 0.05744
	accuracy_policy_1: 0.63336
	loss_value_1: 0.07273
	loss_reward_1: 0.0067
	loss_policy_2: 0.05797
	accuracy_policy_2: 0.63379
	loss_value_2: 0.07442
	loss_reward_2: 0.00933
	loss_policy_3: 0.0578
	accuracy_policy_3: 0.63195
	loss_value_3: 0.07559
	loss_reward_3: 0.01202
	loss_policy_4: 0.05801
	accuracy_policy_4: 0.63645
	loss_value_4: 0.07711
	loss_reward_4: 0.01418
	loss_policy_5: 0.05817
	accuracy_policy_5: 0.64051
	loss_value_5: 0.07843
	loss_reward_5: 0.01526
	loss_policy: 0.57517
	loss_value: 0.73609
	loss_reward: 0.05749
[2025-05-07 21:26:31] nn step 38700, lr: 0.1.
	loss_policy_0: 0.2666
	accuracy_policy_0: 0.64488
	loss_value_0: 0.32859
	loss_policy_1: 0.05376
	accuracy_policy_1: 0.63168
	loss_value_1: 0.06709
	loss_reward_1: 0.0064
	loss_policy_2: 0.05394
	accuracy_policy_2: 0.63734
	loss_value_2: 0.06865
	loss_reward_2: 0.00844
	loss_policy_3: 0.05437
	accuracy_policy_3: 0.63273
	loss_value_3: 0.07011
	loss_reward_3: 0.01149
	loss_policy_4: 0.05411
	accuracy_policy_4: 0.64246
	loss_value_4: 0.07161
	loss_reward_4: 0.01357
	loss_policy_5: 0.05416
	accuracy_policy_5: 0.64594
	loss_value_5: 0.07301
	loss_reward_5: 0.01433
	loss_policy: 0.53694
	loss_value: 0.67906
	loss_reward: 0.05424
[2025-05-07 21:26:39] nn step 38750, lr: 0.1.
	loss_policy_0: 0.27873
	accuracy_policy_0: 0.6498
	loss_value_0: 0.34483
	loss_policy_1: 0.0561
	accuracy_policy_1: 0.64117
	loss_value_1: 0.07035
	loss_reward_1: 0.00665
	loss_policy_2: 0.05713
	accuracy_policy_2: 0.6341
	loss_value_2: 0.07228
	loss_reward_2: 0.00835
	loss_policy_3: 0.05711
	accuracy_policy_3: 0.63988
	loss_value_3: 0.07374
	loss_reward_3: 0.01161
	loss_policy_4: 0.05719
	accuracy_policy_4: 0.64039
	loss_value_4: 0.07504
	loss_reward_4: 0.01418
	loss_policy_5: 0.05665
	accuracy_policy_5: 0.64895
	loss_value_5: 0.07668
	loss_reward_5: 0.015
	loss_policy: 0.56291
	loss_value: 0.71293
	loss_reward: 0.05579
[2025-05-07 21:26:47] nn step 38800, lr: 0.1.
	loss_policy_0: 0.29253
	accuracy_policy_0: 0.64961
	loss_value_0: 0.36074
	loss_policy_1: 0.05941
	accuracy_policy_1: 0.63738
	loss_value_1: 0.07366
	loss_reward_1: 0.00723
	loss_policy_2: 0.05955
	accuracy_policy_2: 0.6373
	loss_value_2: 0.07572
	loss_reward_2: 0.00994
	loss_policy_3: 0.05914
	accuracy_policy_3: 0.64043
	loss_value_3: 0.07764
	loss_reward_3: 0.01269
	loss_policy_4: 0.05933
	accuracy_policy_4: 0.64766
	loss_value_4: 0.07931
	loss_reward_4: 0.01566
	loss_policy_5: 0.05965
	accuracy_policy_5: 0.645
	loss_value_5: 0.08055
	loss_reward_5: 0.01624
	loss_policy: 0.58962
	loss_value: 0.74762
	loss_reward: 0.06176
Optimization_Done 38800
[2025-05-07 21:30:07] [command] train weight_iter_38800.pkl 176 195
[2025-05-07 21:30:14] nn step 38850, lr: 0.1.
	loss_policy_0: 0.26068
	accuracy_policy_0: 0.6484
	loss_value_0: 0.32797
	loss_policy_1: 0.05272
	accuracy_policy_1: 0.64098
	loss_value_1: 0.06665
	loss_reward_1: 0.00635
	loss_policy_2: 0.05299
	accuracy_policy_2: 0.63535
	loss_value_2: 0.06823
	loss_reward_2: 0.00826
	loss_policy_3: 0.05294
	accuracy_policy_3: 0.64129
	loss_value_3: 0.06963
	loss_reward_3: 0.01055
	loss_policy_4: 0.05306
	accuracy_policy_4: 0.64555
	loss_value_4: 0.07074
	loss_reward_4: 0.0135
	loss_policy_5: 0.05309
	accuracy_policy_5: 0.64586
	loss_value_5: 0.07219
	loss_reward_5: 0.01419
	loss_policy: 0.52547
	loss_value: 0.6754
	loss_reward: 0.05285
[2025-05-07 21:30:22] nn step 38900, lr: 0.1.
	loss_policy_0: 0.27728
	accuracy_policy_0: 0.64539
	loss_value_0: 0.34015
	loss_policy_1: 0.05564
	accuracy_policy_1: 0.63883
	loss_value_1: 0.06918
	loss_reward_1: 0.00644
	loss_policy_2: 0.05596
	accuracy_policy_2: 0.63676
	loss_value_2: 0.07096
	loss_reward_2: 0.00879
	loss_policy_3: 0.05574
	accuracy_policy_3: 0.6375
	loss_value_3: 0.07237
	loss_reward_3: 0.01141
	loss_policy_4: 0.05574
	accuracy_policy_4: 0.64508
	loss_value_4: 0.07378
	loss_reward_4: 0.01373
	loss_policy_5: 0.0567
	accuracy_policy_5: 0.64375
	loss_value_5: 0.07533
	loss_reward_5: 0.01478
	loss_policy: 0.55706
	loss_value: 0.70177
	loss_reward: 0.05515
[2025-05-07 21:30:30] nn step 38950, lr: 0.1.
	loss_policy_0: 0.28951
	accuracy_policy_0: 0.64094
	loss_value_0: 0.35383
	loss_policy_1: 0.05862
	accuracy_policy_1: 0.63906
	loss_value_1: 0.07199
	loss_reward_1: 0.00678
	loss_policy_2: 0.05868
	accuracy_policy_2: 0.64434
	loss_value_2: 0.07385
	loss_reward_2: 0.0092
	loss_policy_3: 0.05897
	accuracy_policy_3: 0.64172
	loss_value_3: 0.07563
	loss_reward_3: 0.01229
	loss_policy_4: 0.05848
	accuracy_policy_4: 0.64582
	loss_value_4: 0.07729
	loss_reward_4: 0.01449
	loss_policy_5: 0.05876
	accuracy_policy_5: 0.65059
	loss_value_5: 0.07864
	loss_reward_5: 0.0151
	loss_policy: 0.58302
	loss_value: 0.73122
	loss_reward: 0.05786
[2025-05-07 21:30:37] nn step 39000, lr: 0.1.
	loss_policy_0: 0.28067
	accuracy_policy_0: 0.63391
	loss_value_0: 0.3363
	loss_policy_1: 0.05578
	accuracy_policy_1: 0.6375
	loss_value_1: 0.0687
	loss_reward_1: 0.00644
	loss_policy_2: 0.05585
	accuracy_policy_2: 0.63707
	loss_value_2: 0.07048
	loss_reward_2: 0.00882
	loss_policy_3: 0.05582
	accuracy_policy_3: 0.63992
	loss_value_3: 0.07187
	loss_reward_3: 0.01174
	loss_policy_4: 0.05587
	accuracy_policy_4: 0.63957
	loss_value_4: 0.07334
	loss_reward_4: 0.01388
	loss_policy_5: 0.05581
	accuracy_policy_5: 0.64754
	loss_value_5: 0.07526
	loss_reward_5: 0.01484
	loss_policy: 0.55981
	loss_value: 0.69596
	loss_reward: 0.05572
Optimization_Done 39000
[2025-05-07 21:34:01] [command] train weight_iter_39000.pkl 177 196
[2025-05-07 21:34:09] nn step 39050, lr: 0.1.
	loss_policy_0: 0.27477
	accuracy_policy_0: 0.64512
	loss_value_0: 0.33941
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.64027
	loss_value_1: 0.06918
	loss_reward_1: 0.00632
	loss_policy_2: 0.0557
	accuracy_policy_2: 0.63969
	loss_value_2: 0.07041
	loss_reward_2: 0.00917
	loss_policy_3: 0.05543
	accuracy_policy_3: 0.63789
	loss_value_3: 0.07183
	loss_reward_3: 0.01137
	loss_policy_4: 0.0557
	accuracy_policy_4: 0.63941
	loss_value_4: 0.07354
	loss_reward_4: 0.0139
	loss_policy_5: 0.05565
	accuracy_policy_5: 0.64711
	loss_value_5: 0.07453
	loss_reward_5: 0.01466
	loss_policy: 0.55261
	loss_value: 0.6989
	loss_reward: 0.05542
[2025-05-07 21:34:17] nn step 39100, lr: 0.1.
	loss_policy_0: 0.27773
	accuracy_policy_0: 0.64023
	loss_value_0: 0.33824
	loss_policy_1: 0.05598
	accuracy_policy_1: 0.63594
	loss_value_1: 0.06891
	loss_reward_1: 0.00658
	loss_policy_2: 0.05608
	accuracy_policy_2: 0.64004
	loss_value_2: 0.07041
	loss_reward_2: 0.00865
	loss_policy_3: 0.0562
	accuracy_policy_3: 0.63242
	loss_value_3: 0.07235
	loss_reward_3: 0.0114
	loss_policy_4: 0.056
	accuracy_policy_4: 0.64598
	loss_value_4: 0.07389
	loss_reward_4: 0.01417
	loss_policy_5: 0.05615
	accuracy_policy_5: 0.64621
	loss_value_5: 0.07549
	loss_reward_5: 0.01515
	loss_policy: 0.55813
	loss_value: 0.6993
	loss_reward: 0.05595
[2025-05-07 21:34:24] nn step 39150, lr: 0.1.
	loss_policy_0: 0.28712
	accuracy_policy_0: 0.65066
	loss_value_0: 0.34979
	loss_policy_1: 0.0578
	accuracy_policy_1: 0.64211
	loss_value_1: 0.0714
	loss_reward_1: 0.00687
	loss_policy_2: 0.0579
	accuracy_policy_2: 0.645
	loss_value_2: 0.073
	loss_reward_2: 0.00968
	loss_policy_3: 0.05787
	accuracy_policy_3: 0.64664
	loss_value_3: 0.07464
	loss_reward_3: 0.01194
	loss_policy_4: 0.05798
	accuracy_policy_4: 0.64883
	loss_value_4: 0.07609
	loss_reward_4: 0.01433
	loss_policy_5: 0.05784
	accuracy_policy_5: 0.65332
	loss_value_5: 0.07763
	loss_reward_5: 0.01562
	loss_policy: 0.57651
	loss_value: 0.72256
	loss_reward: 0.05844
[2025-05-07 21:34:32] nn step 39200, lr: 0.1.
	loss_policy_0: 0.29154
	accuracy_policy_0: 0.65137
	loss_value_0: 0.35244
	loss_policy_1: 0.05872
	accuracy_policy_1: 0.64246
	loss_value_1: 0.07208
	loss_reward_1: 0.00695
	loss_policy_2: 0.05915
	accuracy_policy_2: 0.64359
	loss_value_2: 0.074
	loss_reward_2: 0.00935
	loss_policy_3: 0.05938
	accuracy_policy_3: 0.63941
	loss_value_3: 0.07573
	loss_reward_3: 0.01199
	loss_policy_4: 0.05949
	accuracy_policy_4: 0.64926
	loss_value_4: 0.07731
	loss_reward_4: 0.01494
	loss_policy_5: 0.05929
	accuracy_policy_5: 0.65051
	loss_value_5: 0.07894
	loss_reward_5: 0.01547
	loss_policy: 0.58757
	loss_value: 0.73049
	loss_reward: 0.0587
Optimization_Done 39200
[2025-05-07 21:37:52] [command] train weight_iter_39200.pkl 178 197
[2025-05-07 21:38:01] nn step 39250, lr: 0.1.
	loss_policy_0: 0.2799
	accuracy_policy_0: 0.64828
	loss_value_0: 0.34435
	loss_policy_1: 0.05637
	accuracy_policy_1: 0.64
	loss_value_1: 0.07
	loss_reward_1: 0.00685
	loss_policy_2: 0.0567
	accuracy_policy_2: 0.64516
	loss_value_2: 0.07176
	loss_reward_2: 0.0087
	loss_policy_3: 0.05686
	accuracy_policy_3: 0.63691
	loss_value_3: 0.07291
	loss_reward_3: 0.01164
	loss_policy_4: 0.05686
	accuracy_policy_4: 0.64621
	loss_value_4: 0.07446
	loss_reward_4: 0.01432
	loss_policy_5: 0.05662
	accuracy_policy_5: 0.65195
	loss_value_5: 0.07577
	loss_reward_5: 0.01501
	loss_policy: 0.5633
	loss_value: 0.70925
	loss_reward: 0.05654
[2025-05-07 21:38:08] nn step 39300, lr: 0.1.
	loss_policy_0: 0.2759
	accuracy_policy_0: 0.64266
	loss_value_0: 0.33334
	loss_policy_1: 0.05538
	accuracy_policy_1: 0.63277
	loss_value_1: 0.06785
	loss_reward_1: 0.00645
	loss_policy_2: 0.05543
	accuracy_policy_2: 0.64309
	loss_value_2: 0.06981
	loss_reward_2: 0.00846
	loss_policy_3: 0.05544
	accuracy_policy_3: 0.63492
	loss_value_3: 0.07122
	loss_reward_3: 0.01134
	loss_policy_4: 0.05583
	accuracy_policy_4: 0.64098
	loss_value_4: 0.07278
	loss_reward_4: 0.01353
	loss_policy_5: 0.0553
	accuracy_policy_5: 0.64598
	loss_value_5: 0.07436
	loss_reward_5: 0.0146
	loss_policy: 0.55328
	loss_value: 0.68935
	loss_reward: 0.05438
[2025-05-07 21:38:15] nn step 39350, lr: 0.1.
	loss_policy_0: 0.2813
	accuracy_policy_0: 0.64008
	loss_value_0: 0.33874
	loss_policy_1: 0.05675
	accuracy_policy_1: 0.63625
	loss_value_1: 0.06936
	loss_reward_1: 0.00658
	loss_policy_2: 0.05696
	accuracy_policy_2: 0.63789
	loss_value_2: 0.0709
	loss_reward_2: 0.00863
	loss_policy_3: 0.05689
	accuracy_policy_3: 0.63871
	loss_value_3: 0.07243
	loss_reward_3: 0.01109
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.64828
	loss_value_4: 0.07391
	loss_reward_4: 0.01449
	loss_policy_5: 0.05709
	accuracy_policy_5: 0.6466
	loss_value_5: 0.07562
	loss_reward_5: 0.01461
	loss_policy: 0.56562
	loss_value: 0.70096
	loss_reward: 0.0554
[2025-05-07 21:38:23] nn step 39400, lr: 0.1.
	loss_policy_0: 0.25472
	accuracy_policy_0: 0.64395
	loss_value_0: 0.30475
	loss_policy_1: 0.05131
	accuracy_policy_1: 0.64004
	loss_value_1: 0.06244
	loss_reward_1: 0.00603
	loss_policy_2: 0.05167
	accuracy_policy_2: 0.64191
	loss_value_2: 0.06378
	loss_reward_2: 0.00826
	loss_policy_3: 0.05103
	accuracy_policy_3: 0.64461
	loss_value_3: 0.06519
	loss_reward_3: 0.01061
	loss_policy_4: 0.05152
	accuracy_policy_4: 0.64672
	loss_value_4: 0.06659
	loss_reward_4: 0.01247
	loss_policy_5: 0.05174
	accuracy_policy_5: 0.64992
	loss_value_5: 0.06781
	loss_reward_5: 0.01362
	loss_policy: 0.51199
	loss_value: 0.63055
	loss_reward: 0.05099
Optimization_Done 39400
[2025-05-07 21:41:53] [command] train weight_iter_39400.pkl 179 198
[2025-05-07 21:42:00] nn step 39450, lr: 0.1.
	loss_policy_0: 0.27934
	accuracy_policy_0: 0.64301
	loss_value_0: 0.34443
	loss_policy_1: 0.05613
	accuracy_policy_1: 0.6377
	loss_value_1: 0.06996
	loss_reward_1: 0.00679
	loss_policy_2: 0.05633
	accuracy_policy_2: 0.64086
	loss_value_2: 0.07146
	loss_reward_2: 0.00884
	loss_policy_3: 0.05637
	accuracy_policy_3: 0.64191
	loss_value_3: 0.07295
	loss_reward_3: 0.01164
	loss_policy_4: 0.05645
	accuracy_policy_4: 0.64293
	loss_value_4: 0.07448
	loss_reward_4: 0.01424
	loss_policy_5: 0.05636
	accuracy_policy_5: 0.65176
	loss_value_5: 0.07582
	loss_reward_5: 0.01475
	loss_policy: 0.56098
	loss_value: 0.70911
	loss_reward: 0.05627
[2025-05-07 21:42:08] nn step 39500, lr: 0.1.
	loss_policy_0: 0.31496
	accuracy_policy_0: 0.6468
	loss_value_0: 0.38062
	loss_policy_1: 0.06365
	accuracy_policy_1: 0.63578
	loss_value_1: 0.07784
	loss_reward_1: 0.00746
	loss_policy_2: 0.06328
	accuracy_policy_2: 0.64113
	loss_value_2: 0.07924
	loss_reward_2: 0.01005
	loss_policy_3: 0.06352
	accuracy_policy_3: 0.64309
	loss_value_3: 0.08069
	loss_reward_3: 0.01301
	loss_policy_4: 0.0634
	accuracy_policy_4: 0.64785
	loss_value_4: 0.08263
	loss_reward_4: 0.01575
	loss_policy_5: 0.06421
	accuracy_policy_5: 0.64316
	loss_value_5: 0.08454
	loss_reward_5: 0.01675
	loss_policy: 0.63302
	loss_value: 0.78557
	loss_reward: 0.06302
[2025-05-07 21:42:16] nn step 39550, lr: 0.1.
	loss_policy_0: 0.27659
	accuracy_policy_0: 0.64492
	loss_value_0: 0.33206
	loss_policy_1: 0.05611
	accuracy_policy_1: 0.64195
	loss_value_1: 0.06744
	loss_reward_1: 0.00629
	loss_policy_2: 0.05632
	accuracy_policy_2: 0.64316
	loss_value_2: 0.069
	loss_reward_2: 0.0083
	loss_policy_3: 0.05617
	accuracy_policy_3: 0.64016
	loss_value_3: 0.07069
	loss_reward_3: 0.01143
	loss_policy_4: 0.0557
	accuracy_policy_4: 0.65039
	loss_value_4: 0.07199
	loss_reward_4: 0.01397
	loss_policy_5: 0.05624
	accuracy_policy_5: 0.6473
	loss_value_5: 0.07342
	loss_reward_5: 0.01477
	loss_policy: 0.55713
	loss_value: 0.6846
	loss_reward: 0.05475
[2025-05-07 21:42:24] nn step 39600, lr: 0.1.
	loss_policy_0: 0.28723
	accuracy_policy_0: 0.64016
	loss_value_0: 0.3423
	loss_policy_1: 0.0578
	accuracy_policy_1: 0.63914
	loss_value_1: 0.06957
	loss_reward_1: 0.00699
	loss_policy_2: 0.05786
	accuracy_policy_2: 0.63801
	loss_value_2: 0.07141
	loss_reward_2: 0.00886
	loss_policy_3: 0.05757
	accuracy_policy_3: 0.63586
	loss_value_3: 0.07313
	loss_reward_3: 0.01172
	loss_policy_4: 0.05735
	accuracy_policy_4: 0.64402
	loss_value_4: 0.07464
	loss_reward_4: 0.01425
	loss_policy_5: 0.05786
	accuracy_policy_5: 0.64504
	loss_value_5: 0.07663
	loss_reward_5: 0.0157
	loss_policy: 0.57567
	loss_value: 0.70769
	loss_reward: 0.05753
Optimization_Done 39600
[2025-05-07 21:45:45] [command] train weight_iter_39600.pkl 180 199
[2025-05-07 21:45:52] nn step 39650, lr: 0.1.
	loss_policy_0: 0.27181
	accuracy_policy_0: 0.64375
	loss_value_0: 0.33336
	loss_policy_1: 0.05448
	accuracy_policy_1: 0.64145
	loss_value_1: 0.06747
	loss_reward_1: 0.0066
	loss_policy_2: 0.05481
	accuracy_policy_2: 0.64258
	loss_value_2: 0.06883
	loss_reward_2: 0.00878
	loss_policy_3: 0.05513
	accuracy_policy_3: 0.64062
	loss_value_3: 0.07056
	loss_reward_3: 0.01125
	loss_policy_4: 0.05496
	accuracy_policy_4: 0.64516
	loss_value_4: 0.07211
	loss_reward_4: 0.01357
	loss_policy_5: 0.05486
	accuracy_policy_5: 0.64914
	loss_value_5: 0.07293
	loss_reward_5: 0.01425
	loss_policy: 0.54604
	loss_value: 0.68527
	loss_reward: 0.05444
[2025-05-07 21:46:00] nn step 39700, lr: 0.1.
	loss_policy_0: 0.29274
	accuracy_policy_0: 0.64137
	loss_value_0: 0.35148
	loss_policy_1: 0.0592
	accuracy_policy_1: 0.63336
	loss_value_1: 0.07176
	loss_reward_1: 0.00683
	loss_policy_2: 0.0594
	accuracy_policy_2: 0.63492
	loss_value_2: 0.07366
	loss_reward_2: 0.00904
	loss_policy_3: 0.05914
	accuracy_policy_3: 0.63867
	loss_value_3: 0.07523
	loss_reward_3: 0.01242
	loss_policy_4: 0.05918
	accuracy_policy_4: 0.64418
	loss_value_4: 0.07679
	loss_reward_4: 0.01525
	loss_policy_5: 0.05942
	accuracy_policy_5: 0.6443
	loss_value_5: 0.07831
	loss_reward_5: 0.01532
	loss_policy: 0.58908
	loss_value: 0.72723
	loss_reward: 0.05886
[2025-05-07 21:46:08] nn step 39750, lr: 0.1.
	loss_policy_0: 0.28674
	accuracy_policy_0: 0.64457
	loss_value_0: 0.34667
	loss_policy_1: 0.0577
	accuracy_policy_1: 0.63836
	loss_value_1: 0.07065
	loss_reward_1: 0.00674
	loss_policy_2: 0.05767
	accuracy_policy_2: 0.64578
	loss_value_2: 0.07263
	loss_reward_2: 0.00947
	loss_policy_3: 0.05794
	accuracy_policy_3: 0.64312
	loss_value_3: 0.07452
	loss_reward_3: 0.01218
	loss_policy_4: 0.05806
	accuracy_policy_4: 0.65055
	loss_value_4: 0.07612
	loss_reward_4: 0.01419
	loss_policy_5: 0.05816
	accuracy_policy_5: 0.65121
	loss_value_5: 0.07748
	loss_reward_5: 0.01526
	loss_policy: 0.57626
	loss_value: 0.71807
	loss_reward: 0.05785
[2025-05-07 21:46:14] nn step 39800, lr: 0.1.
	loss_policy_0: 0.27712
	accuracy_policy_0: 0.64688
	loss_value_0: 0.3303
	loss_policy_1: 0.05589
	accuracy_policy_1: 0.6373
	loss_value_1: 0.06741
	loss_reward_1: 0.00655
	loss_policy_2: 0.0558
	accuracy_policy_2: 0.64238
	loss_value_2: 0.06933
	loss_reward_2: 0.00887
	loss_policy_3: 0.05626
	accuracy_policy_3: 0.63785
	loss_value_3: 0.07104
	loss_reward_3: 0.01158
	loss_policy_4: 0.05642
	accuracy_policy_4: 0.64484
	loss_value_4: 0.07265
	loss_reward_4: 0.014
	loss_policy_5: 0.05613
	accuracy_policy_5: 0.64598
	loss_value_5: 0.07439
	loss_reward_5: 0.01468
	loss_policy: 0.55761
	loss_value: 0.68511
	loss_reward: 0.05569
Optimization_Done 39800
[2025-05-07 21:49:27] [command] train weight_iter_39800.pkl 181 200
[2025-05-07 21:49:34] nn step 39850, lr: 0.1.
	loss_policy_0: 0.26323
	accuracy_policy_0: 0.65223
	loss_value_0: 0.32442
	loss_policy_1: 0.05354
	accuracy_policy_1: 0.6423
	loss_value_1: 0.06585
	loss_reward_1: 0.00612
	loss_policy_2: 0.05293
	accuracy_policy_2: 0.64328
	loss_value_2: 0.06711
	loss_reward_2: 0.0081
	loss_policy_3: 0.05357
	accuracy_policy_3: 0.64414
	loss_value_3: 0.06873
	loss_reward_3: 0.01036
	loss_policy_4: 0.0536
	accuracy_policy_4: 0.64539
	loss_value_4: 0.07021
	loss_reward_4: 0.01298
	loss_policy_5: 0.0537
	accuracy_policy_5: 0.64766
	loss_value_5: 0.07155
	loss_reward_5: 0.014
	loss_policy: 0.53057
	loss_value: 0.66785
	loss_reward: 0.05156
[2025-05-07 21:49:42] nn step 39900, lr: 0.1.
	loss_policy_0: 0.30253
	accuracy_policy_0: 0.64664
	loss_value_0: 0.36304
	loss_policy_1: 0.0607
	accuracy_policy_1: 0.63992
	loss_value_1: 0.0738
	loss_reward_1: 0.00689
	loss_policy_2: 0.0605
	accuracy_policy_2: 0.64898
	loss_value_2: 0.07543
	loss_reward_2: 0.00942
	loss_policy_3: 0.06099
	accuracy_policy_3: 0.64223
	loss_value_3: 0.07709
	loss_reward_3: 0.01235
	loss_policy_4: 0.06122
	accuracy_policy_4: 0.64336
	loss_value_4: 0.07892
	loss_reward_4: 0.01504
	loss_policy_5: 0.0616
	accuracy_policy_5: 0.64766
	loss_value_5: 0.08053
	loss_reward_5: 0.01605
	loss_policy: 0.60755
	loss_value: 0.74881
	loss_reward: 0.05975
[2025-05-07 21:49:50] nn step 39950, lr: 0.1.
	loss_policy_0: 0.28483
	accuracy_policy_0: 0.65016
	loss_value_0: 0.34242
	loss_policy_1: 0.05738
	accuracy_policy_1: 0.64352
	loss_value_1: 0.07015
	loss_reward_1: 0.00676
	loss_policy_2: 0.05755
	accuracy_policy_2: 0.64352
	loss_value_2: 0.07203
	loss_reward_2: 0.0088
	loss_policy_3: 0.05753
	accuracy_policy_3: 0.64773
	loss_value_3: 0.07354
	loss_reward_3: 0.01221
	loss_policy_4: 0.05787
	accuracy_policy_4: 0.6425
	loss_value_4: 0.075
	loss_reward_4: 0.01478
	loss_policy_5: 0.05748
	accuracy_policy_5: 0.65641
	loss_value_5: 0.07631
	loss_reward_5: 0.01537
	loss_policy: 0.57264
	loss_value: 0.70945
	loss_reward: 0.05793
[2025-05-07 21:49:58] nn step 40000, lr: 0.1.
	loss_policy_0: 0.28677
	accuracy_policy_0: 0.65156
	loss_value_0: 0.34454
	loss_policy_1: 0.05748
	accuracy_policy_1: 0.64355
	loss_value_1: 0.07022
	loss_reward_1: 0.00669
	loss_policy_2: 0.05779
	accuracy_policy_2: 0.64715
	loss_value_2: 0.07204
	loss_reward_2: 0.00866
	loss_policy_3: 0.05813
	accuracy_policy_3: 0.64402
	loss_value_3: 0.07353
	loss_reward_3: 0.012
	loss_policy_4: 0.05817
	accuracy_policy_4: 0.64555
	loss_value_4: 0.07517
	loss_reward_4: 0.01475
	loss_policy_5: 0.05775
	accuracy_policy_5: 0.65645
	loss_value_5: 0.07678
	loss_reward_5: 0.0154
	loss_policy: 0.57609
	loss_value: 0.71229
	loss_reward: 0.0575
Optimization_Done 40000
[2025-05-07 21:53:26] [command] train weight_iter_40000.pkl 182 201
[2025-05-07 21:53:35] nn step 40050, lr: 0.1.
	loss_policy_0: 0.27164
	accuracy_policy_0: 0.64648
	loss_value_0: 0.32939
	loss_policy_1: 0.05469
	accuracy_policy_1: 0.63492
	loss_value_1: 0.06708
	loss_reward_1: 0.00644
	loss_policy_2: 0.05447
	accuracy_policy_2: 0.63715
	loss_value_2: 0.06914
	loss_reward_2: 0.00861
	loss_policy_3: 0.05508
	accuracy_policy_3: 0.63965
	loss_value_3: 0.07074
	loss_reward_3: 0.01098
	loss_policy_4: 0.05515
	accuracy_policy_4: 0.64465
	loss_value_4: 0.07234
	loss_reward_4: 0.01371
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.64566
	loss_value_5: 0.0739
	loss_reward_5: 0.01449
	loss_policy: 0.54629
	loss_value: 0.68259
	loss_reward: 0.05424
[2025-05-07 21:53:43] nn step 40100, lr: 0.1.
	loss_policy_0: 0.25709
	accuracy_policy_0: 0.64223
	loss_value_0: 0.30761
	loss_policy_1: 0.05176
	accuracy_policy_1: 0.63566
	loss_value_1: 0.06289
	loss_reward_1: 0.00615
	loss_policy_2: 0.05182
	accuracy_policy_2: 0.6377
	loss_value_2: 0.06435
	loss_reward_2: 0.00799
	loss_policy_3: 0.05156
	accuracy_policy_3: 0.64105
	loss_value_3: 0.06571
	loss_reward_3: 0.0107
	loss_policy_4: 0.05193
	accuracy_policy_4: 0.64445
	loss_value_4: 0.06728
	loss_reward_4: 0.01283
	loss_policy_5: 0.05181
	accuracy_policy_5: 0.64762
	loss_value_5: 0.06867
	loss_reward_5: 0.01381
	loss_policy: 0.51598
	loss_value: 0.63651
	loss_reward: 0.05149
[2025-05-07 21:53:49] nn step 40150, lr: 0.1.
	loss_policy_0: 0.28663
	accuracy_policy_0: 0.64523
	loss_value_0: 0.34342
	loss_policy_1: 0.05833
	accuracy_policy_1: 0.63621
	loss_value_1: 0.07001
	loss_reward_1: 0.00673
	loss_policy_2: 0.05819
	accuracy_policy_2: 0.63957
	loss_value_2: 0.07152
	loss_reward_2: 0.00894
	loss_policy_3: 0.05859
	accuracy_policy_3: 0.63684
	loss_value_3: 0.07339
	loss_reward_3: 0.01174
	loss_policy_4: 0.05856
	accuracy_policy_4: 0.64324
	loss_value_4: 0.07537
	loss_reward_4: 0.01411
	loss_policy_5: 0.05855
	accuracy_policy_5: 0.64781
	loss_value_5: 0.07712
	loss_reward_5: 0.01493
	loss_policy: 0.57885
	loss_value: 0.71083
	loss_reward: 0.05646
[2025-05-07 21:53:57] nn step 40200, lr: 0.1.
	loss_policy_0: 0.28565
	accuracy_policy_0: 0.6409
	loss_value_0: 0.33864
	loss_policy_1: 0.05738
	accuracy_policy_1: 0.63426
	loss_value_1: 0.06946
	loss_reward_1: 0.00686
	loss_policy_2: 0.05732
	accuracy_policy_2: 0.63719
	loss_value_2: 0.07116
	loss_reward_2: 0.00893
	loss_policy_3: 0.05717
	accuracy_policy_3: 0.63938
	loss_value_3: 0.07301
	loss_reward_3: 0.01172
	loss_policy_4: 0.05759
	accuracy_policy_4: 0.64023
	loss_value_4: 0.07456
	loss_reward_4: 0.01404
	loss_policy_5: 0.05778
	accuracy_policy_5: 0.65219
	loss_value_5: 0.07621
	loss_reward_5: 0.01518
	loss_policy: 0.57289
	loss_value: 0.70304
	loss_reward: 0.05673
Optimization_Done 40200
[2025-05-07 21:57:22] [command] train weight_iter_40200.pkl 183 202
[2025-05-07 21:57:31] nn step 40250, lr: 0.1.
	loss_policy_0: 0.28085
	accuracy_policy_0: 0.6368
	loss_value_0: 0.34092
	loss_policy_1: 0.05617
	accuracy_policy_1: 0.63246
	loss_value_1: 0.06934
	loss_reward_1: 0.00678
	loss_policy_2: 0.0565
	accuracy_policy_2: 0.63586
	loss_value_2: 0.07106
	loss_reward_2: 0.00871
	loss_policy_3: 0.05648
	accuracy_policy_3: 0.63664
	loss_value_3: 0.07237
	loss_reward_3: 0.01145
	loss_policy_4: 0.05645
	accuracy_policy_4: 0.63805
	loss_value_4: 0.07412
	loss_reward_4: 0.01413
	loss_policy_5: 0.05692
	accuracy_policy_5: 0.64047
	loss_value_5: 0.07578
	loss_reward_5: 0.01497
	loss_policy: 0.56336
	loss_value: 0.7036
	loss_reward: 0.05605
[2025-05-07 21:57:37] nn step 40300, lr: 0.1.
	loss_policy_0: 0.28759
	accuracy_policy_0: 0.64086
	loss_value_0: 0.34497
	loss_policy_1: 0.05845
	accuracy_policy_1: 0.63125
	loss_value_1: 0.07062
	loss_reward_1: 0.00683
	loss_policy_2: 0.05811
	accuracy_policy_2: 0.63379
	loss_value_2: 0.07257
	loss_reward_2: 0.0087
	loss_policy_3: 0.05854
	accuracy_policy_3: 0.63227
	loss_value_3: 0.074
	loss_reward_3: 0.01181
	loss_policy_4: 0.05882
	accuracy_policy_4: 0.63629
	loss_value_4: 0.07549
	loss_reward_4: 0.0146
	loss_policy_5: 0.05922
	accuracy_policy_5: 0.64188
	loss_value_5: 0.07679
	loss_reward_5: 0.01539
	loss_policy: 0.58073
	loss_value: 0.71444
	loss_reward: 0.05732
[2025-05-07 21:57:45] nn step 40350, lr: 0.1.
	loss_policy_0: 0.30272
	accuracy_policy_0: 0.63996
	loss_value_0: 0.35898
	loss_policy_1: 0.06065
	accuracy_policy_1: 0.63402
	loss_value_1: 0.07341
	loss_reward_1: 0.0071
	loss_policy_2: 0.06101
	accuracy_policy_2: 0.63672
	loss_value_2: 0.07538
	loss_reward_2: 0.00912
	loss_policy_3: 0.06104
	accuracy_policy_3: 0.63484
	loss_value_3: 0.07744
	loss_reward_3: 0.0125
	loss_policy_4: 0.06107
	accuracy_policy_4: 0.64234
	loss_value_4: 0.07893
	loss_reward_4: 0.0151
	loss_policy_5: 0.06157
	accuracy_policy_5: 0.64086
	loss_value_5: 0.08042
	loss_reward_5: 0.01618
	loss_policy: 0.60807
	loss_value: 0.74456
	loss_reward: 0.06
[2025-05-07 21:57:53] nn step 40400, lr: 0.1.
	loss_policy_0: 0.27725
	accuracy_policy_0: 0.64387
	loss_value_0: 0.33162
	loss_policy_1: 0.05632
	accuracy_policy_1: 0.63414
	loss_value_1: 0.06782
	loss_reward_1: 0.0067
	loss_policy_2: 0.05641
	accuracy_policy_2: 0.63703
	loss_value_2: 0.0699
	loss_reward_2: 0.00869
	loss_policy_3: 0.05618
	accuracy_policy_3: 0.63828
	loss_value_3: 0.07115
	loss_reward_3: 0.01166
	loss_policy_4: 0.05637
	accuracy_policy_4: 0.64094
	loss_value_4: 0.07329
	loss_reward_4: 0.01426
	loss_policy_5: 0.05632
	accuracy_policy_5: 0.64609
	loss_value_5: 0.07472
	loss_reward_5: 0.01475
	loss_policy: 0.55885
	loss_value: 0.68851
	loss_reward: 0.05606
Optimization_Done 40400
[2025-05-07 22:01:18] [command] train weight_iter_40400.pkl 184 203
[2025-05-07 22:01:25] nn step 40450, lr: 0.1.
	loss_policy_0: 0.28543
	accuracy_policy_0: 0.64102
	loss_value_0: 0.35003
	loss_policy_1: 0.05747
	accuracy_policy_1: 0.63336
	loss_value_1: 0.07116
	loss_reward_1: 0.00653
	loss_policy_2: 0.05754
	accuracy_policy_2: 0.63512
	loss_value_2: 0.07261
	loss_reward_2: 0.00848
	loss_policy_3: 0.05793
	accuracy_policy_3: 0.63793
	loss_value_3: 0.07443
	loss_reward_3: 0.01122
	loss_policy_4: 0.0578
	accuracy_policy_4: 0.64238
	loss_value_4: 0.07599
	loss_reward_4: 0.01408
	loss_policy_5: 0.05788
	accuracy_policy_5: 0.64445
	loss_value_5: 0.07705
	loss_reward_5: 0.01526
	loss_policy: 0.57405
	loss_value: 0.72126
	loss_reward: 0.05557
[2025-05-07 22:01:33] nn step 40500, lr: 0.1.
	loss_policy_0: 0.28968
	accuracy_policy_0: 0.64402
	loss_value_0: 0.34943
	loss_policy_1: 0.05911
	accuracy_policy_1: 0.63094
	loss_value_1: 0.07135
	loss_reward_1: 0.00676
	loss_policy_2: 0.05886
	accuracy_policy_2: 0.63809
	loss_value_2: 0.07334
	loss_reward_2: 0.00909
	loss_policy_3: 0.0589
	accuracy_policy_3: 0.63891
	loss_value_3: 0.07465
	loss_reward_3: 0.01168
	loss_policy_4: 0.05906
	accuracy_policy_4: 0.64176
	loss_value_4: 0.07628
	loss_reward_4: 0.01453
	loss_policy_5: 0.0591
	accuracy_policy_5: 0.6425
	loss_value_5: 0.078
	loss_reward_5: 0.01553
	loss_policy: 0.5847
	loss_value: 0.72305
	loss_reward: 0.05759
[2025-05-07 22:01:41] nn step 40550, lr: 0.1.
	loss_policy_0: 0.27784
	accuracy_policy_0: 0.6409
	loss_value_0: 0.3319
	loss_policy_1: 0.05592
	accuracy_policy_1: 0.63207
	loss_value_1: 0.06804
	loss_reward_1: 0.00635
	loss_policy_2: 0.05622
	accuracy_policy_2: 0.6348
	loss_value_2: 0.07005
	loss_reward_2: 0.00875
	loss_policy_3: 0.05624
	accuracy_policy_3: 0.6375
	loss_value_3: 0.07153
	loss_reward_3: 0.01129
	loss_policy_4: 0.05639
	accuracy_policy_4: 0.64223
	loss_value_4: 0.07294
	loss_reward_4: 0.01357
	loss_policy_5: 0.05613
	accuracy_policy_5: 0.65082
	loss_value_5: 0.07487
	loss_reward_5: 0.01465
	loss_policy: 0.55874
	loss_value: 0.68934
	loss_reward: 0.05461
[2025-05-07 22:01:49] nn step 40600, lr: 0.1.
	loss_policy_0: 0.26417
	accuracy_policy_0: 0.64957
	loss_value_0: 0.31334
	loss_policy_1: 0.05323
	accuracy_policy_1: 0.63742
	loss_value_1: 0.0642
	loss_reward_1: 0.00637
	loss_policy_2: 0.05343
	accuracy_policy_2: 0.63262
	loss_value_2: 0.06581
	loss_reward_2: 0.00814
	loss_policy_3: 0.05336
	accuracy_policy_3: 0.64184
	loss_value_3: 0.06723
	loss_reward_3: 0.01071
	loss_policy_4: 0.05381
	accuracy_policy_4: 0.63926
	loss_value_4: 0.06883
	loss_reward_4: 0.01342
	loss_policy_5: 0.0538
	accuracy_policy_5: 0.64344
	loss_value_5: 0.07034
	loss_reward_5: 0.01441
	loss_policy: 0.5318
	loss_value: 0.64975
	loss_reward: 0.05305
Optimization_Done 40600
[2025-05-07 22:05:09] [command] train weight_iter_40600.pkl 185 204
[2025-05-07 22:05:17] nn step 40650, lr: 0.1.
	loss_policy_0: 0.3019
	accuracy_policy_0: 0.64383
	loss_value_0: 0.37082
	loss_policy_1: 0.06114
	accuracy_policy_1: 0.62859
	loss_value_1: 0.07536
	loss_reward_1: 0.00711
	loss_policy_2: 0.06115
	accuracy_policy_2: 0.63414
	loss_value_2: 0.07717
	loss_reward_2: 0.00959
	loss_policy_3: 0.06112
	accuracy_policy_3: 0.63492
	loss_value_3: 0.0791
	loss_reward_3: 0.01242
	loss_policy_4: 0.06158
	accuracy_policy_4: 0.63938
	loss_value_4: 0.08023
	loss_reward_4: 0.0154
	loss_policy_5: 0.06118
	accuracy_policy_5: 0.64398
	loss_value_5: 0.08196
	loss_reward_5: 0.01637
	loss_policy: 0.60806
	loss_value: 0.76466
	loss_reward: 0.0609
[2025-05-07 22:05:25] nn step 40700, lr: 0.1.
	loss_policy_0: 0.29238
	accuracy_policy_0: 0.64566
	loss_value_0: 0.35575
	loss_policy_1: 0.0592
	accuracy_policy_1: 0.63484
	loss_value_1: 0.0728
	loss_reward_1: 0.00693
	loss_policy_2: 0.05898
	accuracy_policy_2: 0.63793
	loss_value_2: 0.07456
	loss_reward_2: 0.00912
	loss_policy_3: 0.05907
	accuracy_policy_3: 0.63957
	loss_value_3: 0.07589
	loss_reward_3: 0.01231
	loss_policy_4: 0.05924
	accuracy_policy_4: 0.64137
	loss_value_4: 0.07769
	loss_reward_4: 0.01485
	loss_policy_5: 0.05907
	accuracy_policy_5: 0.64734
	loss_value_5: 0.0793
	loss_reward_5: 0.01524
	loss_policy: 0.58795
	loss_value: 0.73598
	loss_reward: 0.05845
[2025-05-07 22:05:32] nn step 40750, lr: 0.1.
	loss_policy_0: 0.30164
	accuracy_policy_0: 0.63562
	loss_value_0: 0.35389
	loss_policy_1: 0.06025
	accuracy_policy_1: 0.63422
	loss_value_1: 0.07238
	loss_reward_1: 0.00702
	loss_policy_2: 0.06017
	accuracy_policy_2: 0.63516
	loss_value_2: 0.07404
	loss_reward_2: 0.00922
	loss_policy_3: 0.06027
	accuracy_policy_3: 0.63762
	loss_value_3: 0.07591
	loss_reward_3: 0.01258
	loss_policy_4: 0.06051
	accuracy_policy_4: 0.64195
	loss_value_4: 0.07772
	loss_reward_4: 0.01498
	loss_policy_5: 0.06067
	accuracy_policy_5: 0.64004
	loss_value_5: 0.07937
	loss_reward_5: 0.0157
	loss_policy: 0.60352
	loss_value: 0.7333
	loss_reward: 0.05949
[2025-05-07 22:05:40] nn step 40800, lr: 0.1.
	loss_policy_0: 0.29446
	accuracy_policy_0: 0.63555
	loss_value_0: 0.3476
	loss_policy_1: 0.05877
	accuracy_policy_1: 0.63172
	loss_value_1: 0.0709
	loss_reward_1: 0.00682
	loss_policy_2: 0.05913
	accuracy_policy_2: 0.63289
	loss_value_2: 0.07267
	loss_reward_2: 0.00896
	loss_policy_3: 0.05924
	accuracy_policy_3: 0.6327
	loss_value_3: 0.07441
	loss_reward_3: 0.01188
	loss_policy_4: 0.05923
	accuracy_policy_4: 0.64496
	loss_value_4: 0.07596
	loss_reward_4: 0.01452
	loss_policy_5: 0.05911
	accuracy_policy_5: 0.64758
	loss_value_5: 0.07755
	loss_reward_5: 0.01539
	loss_policy: 0.58994
	loss_value: 0.71908
	loss_reward: 0.05757
Optimization_Done 40800
[2025-05-07 22:09:06] [command] train weight_iter_40800.pkl 186 205
[2025-05-07 22:09:15] nn step 40850, lr: 0.1.
	loss_policy_0: 0.2823
	accuracy_policy_0: 0.64285
	loss_value_0: 0.34276
	loss_policy_1: 0.05713
	accuracy_policy_1: 0.63148
	loss_value_1: 0.06941
	loss_reward_1: 0.00683
	loss_policy_2: 0.05731
	accuracy_policy_2: 0.63836
	loss_value_2: 0.07089
	loss_reward_2: 0.00897
	loss_policy_3: 0.05714
	accuracy_policy_3: 0.63996
	loss_value_3: 0.07243
	loss_reward_3: 0.01172
	loss_policy_4: 0.05736
	accuracy_policy_4: 0.63879
	loss_value_4: 0.07375
	loss_reward_4: 0.01386
	loss_policy_5: 0.0576
	accuracy_policy_5: 0.64574
	loss_value_5: 0.07543
	loss_reward_5: 0.0157
	loss_policy: 0.56885
	loss_value: 0.70466
	loss_reward: 0.05708
[2025-05-07 22:09:24] nn step 40900, lr: 0.1.
	loss_policy_0: 0.28612
	accuracy_policy_0: 0.64801
	loss_value_0: 0.34499
	loss_policy_1: 0.05789
	accuracy_policy_1: 0.63371
	loss_value_1: 0.07021
	loss_reward_1: 0.00672
	loss_policy_2: 0.05789
	accuracy_policy_2: 0.63551
	loss_value_2: 0.07179
	loss_reward_2: 0.00903
	loss_policy_3: 0.05788
	accuracy_policy_3: 0.63516
	loss_value_3: 0.07354
	loss_reward_3: 0.01167
	loss_policy_4: 0.05811
	accuracy_policy_4: 0.64223
	loss_value_4: 0.07512
	loss_reward_4: 0.01411
	loss_policy_5: 0.05845
	accuracy_policy_5: 0.64676
	loss_value_5: 0.07653
	loss_reward_5: 0.01522
	loss_policy: 0.57634
	loss_value: 0.71218
	loss_reward: 0.05675
[2025-05-07 22:09:30] nn step 40950, lr: 0.1.
	loss_policy_0: 0.29498
	accuracy_policy_0: 0.64414
	loss_value_0: 0.34969
	loss_policy_1: 0.05969
	accuracy_policy_1: 0.63246
	loss_value_1: 0.07115
	loss_reward_1: 0.00709
	loss_policy_2: 0.05947
	accuracy_policy_2: 0.63758
	loss_value_2: 0.07312
	loss_reward_2: 0.00915
	loss_policy_3: 0.05992
	accuracy_policy_3: 0.63797
	loss_value_3: 0.07509
	loss_reward_3: 0.01215
	loss_policy_4: 0.05992
	accuracy_policy_4: 0.64883
	loss_value_4: 0.07661
	loss_reward_4: 0.01479
	loss_policy_5: 0.0601
	accuracy_policy_5: 0.64121
	loss_value_5: 0.07825
	loss_reward_5: 0.01554
	loss_policy: 0.59407
	loss_value: 0.72392
	loss_reward: 0.05873
[2025-05-07 22:09:38] nn step 41000, lr: 0.1.
	loss_policy_0: 0.28984
	accuracy_policy_0: 0.6477
	loss_value_0: 0.34308
	loss_policy_1: 0.05811
	accuracy_policy_1: 0.63719
	loss_value_1: 0.07005
	loss_reward_1: 0.007
	loss_policy_2: 0.05864
	accuracy_policy_2: 0.63621
	loss_value_2: 0.07166
	loss_reward_2: 0.00929
	loss_policy_3: 0.05875
	accuracy_policy_3: 0.63613
	loss_value_3: 0.07348
	loss_reward_3: 0.0117
	loss_policy_4: 0.05904
	accuracy_policy_4: 0.63555
	loss_value_4: 0.0752
	loss_reward_4: 0.01477
	loss_policy_5: 0.05904
	accuracy_policy_5: 0.64625
	loss_value_5: 0.07707
	loss_reward_5: 0.01554
	loss_policy: 0.58342
	loss_value: 0.71054
	loss_reward: 0.0583
Optimization_Done 41000
[2025-05-07 22:13:02] [command] train weight_iter_41000.pkl 187 206
[2025-05-07 22:13:11] nn step 41050, lr: 0.1.
	loss_policy_0: 0.27962
	accuracy_policy_0: 0.65102
	loss_value_0: 0.34356
	loss_policy_1: 0.05659
	accuracy_policy_1: 0.64449
	loss_value_1: 0.06988
	loss_reward_1: 0.00674
	loss_policy_2: 0.05675
	accuracy_policy_2: 0.64281
	loss_value_2: 0.07139
	loss_reward_2: 0.00892
	loss_policy_3: 0.05664
	accuracy_policy_3: 0.64789
	loss_value_3: 0.07287
	loss_reward_3: 0.01153
	loss_policy_4: 0.057
	accuracy_policy_4: 0.64391
	loss_value_4: 0.07439
	loss_reward_4: 0.01409
	loss_policy_5: 0.05724
	accuracy_policy_5: 0.64656
	loss_value_5: 0.07572
	loss_reward_5: 0.01495
	loss_policy: 0.56383
	loss_value: 0.7078
	loss_reward: 0.05623
[2025-05-07 22:13:18] nn step 41100, lr: 0.1.
	loss_policy_0: 0.2924
	accuracy_policy_0: 0.64605
	loss_value_0: 0.35403
	loss_policy_1: 0.05866
	accuracy_policy_1: 0.63715
	loss_value_1: 0.07193
	loss_reward_1: 0.00702
	loss_policy_2: 0.05936
	accuracy_policy_2: 0.63758
	loss_value_2: 0.07367
	loss_reward_2: 0.00923
	loss_policy_3: 0.05919
	accuracy_policy_3: 0.64164
	loss_value_3: 0.07549
	loss_reward_3: 0.01195
	loss_policy_4: 0.05941
	accuracy_policy_4: 0.64137
	loss_value_4: 0.07662
	loss_reward_4: 0.01491
	loss_policy_5: 0.05944
	accuracy_policy_5: 0.64961
	loss_value_5: 0.07809
	loss_reward_5: 0.01594
	loss_policy: 0.58846
	loss_value: 0.72983
	loss_reward: 0.05906
[2025-05-07 22:13:26] nn step 41150, lr: 0.1.
	loss_policy_0: 0.29463
	accuracy_policy_0: 0.64969
	loss_value_0: 0.35521
	loss_policy_1: 0.05921
	accuracy_policy_1: 0.63973
	loss_value_1: 0.07227
	loss_reward_1: 0.00685
	loss_policy_2: 0.05964
	accuracy_policy_2: 0.64277
	loss_value_2: 0.07421
	loss_reward_2: 0.0092
	loss_policy_3: 0.05994
	accuracy_policy_3: 0.63867
	loss_value_3: 0.07577
	loss_reward_3: 0.01229
	loss_policy_4: 0.06002
	accuracy_policy_4: 0.64434
	loss_value_4: 0.07746
	loss_reward_4: 0.01514
	loss_policy_5: 0.05997
	accuracy_policy_5: 0.6484
	loss_value_5: 0.07929
	loss_reward_5: 0.01629
	loss_policy: 0.59341
	loss_value: 0.73421
	loss_reward: 0.05977
[2025-05-07 22:13:34] nn step 41200, lr: 0.1.
	loss_policy_0: 0.30026
	accuracy_policy_0: 0.64645
	loss_value_0: 0.35798
	loss_policy_1: 0.06065
	accuracy_policy_1: 0.63633
	loss_value_1: 0.07305
	loss_reward_1: 0.00721
	loss_policy_2: 0.06081
	accuracy_policy_2: 0.63668
	loss_value_2: 0.07513
	loss_reward_2: 0.00943
	loss_policy_3: 0.0612
	accuracy_policy_3: 0.63906
	loss_value_3: 0.07675
	loss_reward_3: 0.01207
	loss_policy_4: 0.06144
	accuracy_policy_4: 0.63672
	loss_value_4: 0.07823
	loss_reward_4: 0.01546
	loss_policy_5: 0.06112
	accuracy_policy_5: 0.64949
	loss_value_5: 0.0796
	loss_reward_5: 0.01645
	loss_policy: 0.60548
	loss_value: 0.74073
	loss_reward: 0.06062
Optimization_Done 41200
[2025-05-07 22:17:00] [command] train weight_iter_41200.pkl 188 207
[2025-05-07 22:17:07] nn step 41250, lr: 0.1.
	loss_policy_0: 0.27564
	accuracy_policy_0: 0.65059
	loss_value_0: 0.33646
	loss_policy_1: 0.05533
	accuracy_policy_1: 0.64059
	loss_value_1: 0.06872
	loss_reward_1: 0.00637
	loss_policy_2: 0.0554
	accuracy_policy_2: 0.6434
	loss_value_2: 0.07008
	loss_reward_2: 0.00902
	loss_policy_3: 0.05569
	accuracy_policy_3: 0.64488
	loss_value_3: 0.07138
	loss_reward_3: 0.01154
	loss_policy_4: 0.05591
	accuracy_policy_4: 0.64637
	loss_value_4: 0.07274
	loss_reward_4: 0.01363
	loss_policy_5: 0.05576
	accuracy_policy_5: 0.65188
	loss_value_5: 0.07415
	loss_reward_5: 0.01482
	loss_policy: 0.55373
	loss_value: 0.69351
	loss_reward: 0.05538
[2025-05-07 22:17:15] nn step 41300, lr: 0.1.
	loss_policy_0: 0.27522
	accuracy_policy_0: 0.64871
	loss_value_0: 0.33024
	loss_policy_1: 0.05574
	accuracy_policy_1: 0.63594
	loss_value_1: 0.06757
	loss_reward_1: 0.00654
	loss_policy_2: 0.05533
	accuracy_policy_2: 0.64145
	loss_value_2: 0.06958
	loss_reward_2: 0.00851
	loss_policy_3: 0.05596
	accuracy_policy_3: 0.64078
	loss_value_3: 0.07093
	loss_reward_3: 0.01125
	loss_policy_4: 0.05595
	accuracy_policy_4: 0.64402
	loss_value_4: 0.07232
	loss_reward_4: 0.0141
	loss_policy_5: 0.05616
	accuracy_policy_5: 0.65422
	loss_value_5: 0.07352
	loss_reward_5: 0.01481
	loss_policy: 0.55437
	loss_value: 0.68416
	loss_reward: 0.05521
[2025-05-07 22:17:23] nn step 41350, lr: 0.1.
	loss_policy_0: 0.28316
	accuracy_policy_0: 0.64695
	loss_value_0: 0.33618
	loss_policy_1: 0.05716
	accuracy_policy_1: 0.63734
	loss_value_1: 0.0687
	loss_reward_1: 0.00642
	loss_policy_2: 0.05718
	accuracy_policy_2: 0.64211
	loss_value_2: 0.07038
	loss_reward_2: 0.00883
	loss_policy_3: 0.05743
	accuracy_policy_3: 0.64633
	loss_value_3: 0.07188
	loss_reward_3: 0.01166
	loss_policy_4: 0.05731
	accuracy_policy_4: 0.64625
	loss_value_4: 0.07326
	loss_reward_4: 0.01423
	loss_policy_5: 0.0573
	accuracy_policy_5: 0.65516
	loss_value_5: 0.0747
	loss_reward_5: 0.01508
	loss_policy: 0.56953
	loss_value: 0.69511
	loss_reward: 0.05621
[2025-05-07 22:17:31] nn step 41400, lr: 0.1.
	loss_policy_0: 0.26974
	accuracy_policy_0: 0.65234
	loss_value_0: 0.32101
	loss_policy_1: 0.05442
	accuracy_policy_1: 0.64191
	loss_value_1: 0.06552
	loss_reward_1: 0.00634
	loss_policy_2: 0.05449
	accuracy_policy_2: 0.6452
	loss_value_2: 0.06732
	loss_reward_2: 0.00835
	loss_policy_3: 0.05513
	accuracy_policy_3: 0.64137
	loss_value_3: 0.06874
	loss_reward_3: 0.01085
	loss_policy_4: 0.05526
	accuracy_policy_4: 0.6459
	loss_value_4: 0.07002
	loss_reward_4: 0.01351
	loss_policy_5: 0.05527
	accuracy_policy_5: 0.65305
	loss_value_5: 0.07186
	loss_reward_5: 0.01458
	loss_policy: 0.54431
	loss_value: 0.66448
	loss_reward: 0.05363
Optimization_Done 41400
[2025-05-07 22:20:45] [command] train weight_iter_41400.pkl 189 208
[2025-05-07 22:20:53] nn step 41450, lr: 0.1.
	loss_policy_0: 0.28813
	accuracy_policy_0: 0.65574
	loss_value_0: 0.35095
	loss_policy_1: 0.05777
	accuracy_policy_1: 0.64867
	loss_value_1: 0.07171
	loss_reward_1: 0.00674
	loss_policy_2: 0.05804
	accuracy_policy_2: 0.64988
	loss_value_2: 0.07327
	loss_reward_2: 0.00887
	loss_policy_3: 0.05806
	accuracy_policy_3: 0.65164
	loss_value_3: 0.07459
	loss_reward_3: 0.01146
	loss_policy_4: 0.05796
	accuracy_policy_4: 0.65621
	loss_value_4: 0.07598
	loss_reward_4: 0.01447
	loss_policy_5: 0.05772
	accuracy_policy_5: 0.66492
	loss_value_5: 0.07717
	loss_reward_5: 0.01541
	loss_policy: 0.57767
	loss_value: 0.72368
	loss_reward: 0.05695
[2025-05-07 22:21:01] nn step 41500, lr: 0.1.
	loss_policy_0: 0.27683
	accuracy_policy_0: 0.66301
	loss_value_0: 0.33455
	loss_policy_1: 0.05597
	accuracy_policy_1: 0.64863
	loss_value_1: 0.06832
	loss_reward_1: 0.00657
	loss_policy_2: 0.05631
	accuracy_policy_2: 0.65043
	loss_value_2: 0.07009
	loss_reward_2: 0.00877
	loss_policy_3: 0.05608
	accuracy_policy_3: 0.65523
	loss_value_3: 0.07184
	loss_reward_3: 0.01133
	loss_policy_4: 0.056
	accuracy_policy_4: 0.65777
	loss_value_4: 0.07312
	loss_reward_4: 0.01411
	loss_policy_5: 0.05591
	accuracy_policy_5: 0.67004
	loss_value_5: 0.07479
	loss_reward_5: 0.01491
	loss_policy: 0.55708
	loss_value: 0.69271
	loss_reward: 0.05568
[2025-05-07 22:21:09] nn step 41550, lr: 0.1.
	loss_policy_0: 0.28233
	accuracy_policy_0: 0.65945
	loss_value_0: 0.34129
	loss_policy_1: 0.05702
	accuracy_policy_1: 0.64992
	loss_value_1: 0.06946
	loss_reward_1: 0.00685
	loss_policy_2: 0.05704
	accuracy_policy_2: 0.65258
	loss_value_2: 0.07125
	loss_reward_2: 0.00917
	loss_policy_3: 0.05769
	accuracy_policy_3: 0.65418
	loss_value_3: 0.07309
	loss_reward_3: 0.01178
	loss_policy_4: 0.05736
	accuracy_policy_4: 0.6634
	loss_value_4: 0.0744
	loss_reward_4: 0.01432
	loss_policy_5: 0.05783
	accuracy_policy_5: 0.65684
	loss_value_5: 0.07593
	loss_reward_5: 0.01549
	loss_policy: 0.56927
	loss_value: 0.7054
	loss_reward: 0.05761
[2025-05-07 22:21:17] nn step 41600, lr: 0.1.
	loss_policy_0: 0.28846
	accuracy_policy_0: 0.66605
	loss_value_0: 0.34659
	loss_policy_1: 0.05873
	accuracy_policy_1: 0.64746
	loss_value_1: 0.07067
	loss_reward_1: 0.00705
	loss_policy_2: 0.05836
	accuracy_policy_2: 0.65895
	loss_value_2: 0.07244
	loss_reward_2: 0.00916
	loss_policy_3: 0.05868
	accuracy_policy_3: 0.65957
	loss_value_3: 0.07427
	loss_reward_3: 0.01163
	loss_policy_4: 0.05861
	accuracy_policy_4: 0.66457
	loss_value_4: 0.07595
	loss_reward_4: 0.01468
	loss_policy_5: 0.0586
	accuracy_policy_5: 0.66879
	loss_value_5: 0.07738
	loss_reward_5: 0.01584
	loss_policy: 0.58144
	loss_value: 0.7173
	loss_reward: 0.05836
Optimization_Done 41600
[2025-05-07 22:24:49] [command] train weight_iter_41600.pkl 190 209
[2025-05-07 22:24:58] nn step 41650, lr: 0.1.
	loss_policy_0: 0.26468
	accuracy_policy_0: 0.67059
	loss_value_0: 0.32625
	loss_policy_1: 0.05381
	accuracy_policy_1: 0.65258
	loss_value_1: 0.06617
	loss_reward_1: 0.00603
	loss_policy_2: 0.05395
	accuracy_policy_2: 0.6534
	loss_value_2: 0.06785
	loss_reward_2: 0.00829
	loss_policy_3: 0.05396
	accuracy_policy_3: 0.66445
	loss_value_3: 0.06943
	loss_reward_3: 0.01082
	loss_policy_4: 0.05405
	accuracy_policy_4: 0.66016
	loss_value_4: 0.07068
	loss_reward_4: 0.0131
	loss_policy_5: 0.05432
	accuracy_policy_5: 0.67043
	loss_value_5: 0.0722
	loss_reward_5: 0.01432
	loss_policy: 0.53477
	loss_value: 0.67257
	loss_reward: 0.05255
[2025-05-07 22:25:05] nn step 41700, lr: 0.1.
	loss_policy_0: 0.26132
	accuracy_policy_0: 0.66406
	loss_value_0: 0.31638
	loss_policy_1: 0.05255
	accuracy_policy_1: 0.65293
	loss_value_1: 0.06432
	loss_reward_1: 0.00628
	loss_policy_2: 0.05305
	accuracy_policy_2: 0.65598
	loss_value_2: 0.06591
	loss_reward_2: 0.00844
	loss_policy_3: 0.05301
	accuracy_policy_3: 0.65914
	loss_value_3: 0.06744
	loss_reward_3: 0.01058
	loss_policy_4: 0.05315
	accuracy_policy_4: 0.66238
	loss_value_4: 0.06874
	loss_reward_4: 0.01303
	loss_policy_5: 0.05279
	accuracy_policy_5: 0.66867
	loss_value_5: 0.07008
	loss_reward_5: 0.01404
	loss_policy: 0.52587
	loss_value: 0.65287
	loss_reward: 0.05236
[2025-05-07 22:25:12] nn step 41750, lr: 0.1.
	loss_policy_0: 0.24961
	accuracy_policy_0: 0.67188
	loss_value_0: 0.30339
	loss_policy_1: 0.05098
	accuracy_policy_1: 0.65562
	loss_value_1: 0.06176
	loss_reward_1: 0.00584
	loss_policy_2: 0.05117
	accuracy_policy_2: 0.65625
	loss_value_2: 0.06308
	loss_reward_2: 0.00801
	loss_policy_3: 0.05122
	accuracy_policy_3: 0.65629
	loss_value_3: 0.06462
	loss_reward_3: 0.01032
	loss_policy_4: 0.05142
	accuracy_policy_4: 0.66152
	loss_value_4: 0.06577
	loss_reward_4: 0.0133
	loss_policy_5: 0.0513
	accuracy_policy_5: 0.67031
	loss_value_5: 0.06734
	loss_reward_5: 0.01411
	loss_policy: 0.5057
	loss_value: 0.62595
	loss_reward: 0.05158
[2025-05-07 22:25:20] nn step 41800, lr: 0.1.
	loss_policy_0: 0.27194
	accuracy_policy_0: 0.65508
	loss_value_0: 0.31618
	loss_policy_1: 0.05391
	accuracy_policy_1: 0.65609
	loss_value_1: 0.06503
	loss_reward_1: 0.00637
	loss_policy_2: 0.05421
	accuracy_policy_2: 0.65516
	loss_value_2: 0.06661
	loss_reward_2: 0.00823
	loss_policy_3: 0.05443
	accuracy_policy_3: 0.65918
	loss_value_3: 0.06822
	loss_reward_3: 0.01097
	loss_policy_4: 0.05458
	accuracy_policy_4: 0.66078
	loss_value_4: 0.06953
	loss_reward_4: 0.01358
	loss_policy_5: 0.05425
	accuracy_policy_5: 0.66762
	loss_value_5: 0.07088
	loss_reward_5: 0.01431
	loss_policy: 0.54332
	loss_value: 0.65645
	loss_reward: 0.05346
Optimization_Done 41800
[2025-05-07 22:28:43] [command] train weight_iter_41800.pkl 191 210
[2025-05-07 22:28:52] nn step 41850, lr: 0.1.
	loss_policy_0: 0.29395
	accuracy_policy_0: 0.65688
	loss_value_0: 0.3636
	loss_policy_1: 0.0593
	accuracy_policy_1: 0.65121
	loss_value_1: 0.07378
	loss_reward_1: 0.00768
	loss_policy_2: 0.05914
	accuracy_policy_2: 0.65195
	loss_value_2: 0.0752
	loss_reward_2: 0.01012
	loss_policy_3: 0.05922
	accuracy_policy_3: 0.65734
	loss_value_3: 0.07635
	loss_reward_3: 0.01285
	loss_policy_4: 0.05939
	accuracy_policy_4: 0.65859
	loss_value_4: 0.07751
	loss_reward_4: 0.01535
	loss_policy_5: 0.05954
	accuracy_policy_5: 0.66492
	loss_value_5: 0.07911
	loss_reward_5: 0.01631
	loss_policy: 0.59054
	loss_value: 0.74555
	loss_reward: 0.06231
[2025-05-07 22:28:58] nn step 41900, lr: 0.1.
	loss_policy_0: 0.29745
	accuracy_policy_0: 0.66465
	loss_value_0: 0.36212
	loss_policy_1: 0.05999
	accuracy_policy_1: 0.65613
	loss_value_1: 0.07384
	loss_reward_1: 0.00744
	loss_policy_2: 0.06019
	accuracy_policy_2: 0.65551
	loss_value_2: 0.07527
	loss_reward_2: 0.00992
	loss_policy_3: 0.06045
	accuracy_policy_3: 0.65367
	loss_value_3: 0.07691
	loss_reward_3: 0.01256
	loss_policy_4: 0.0606
	accuracy_policy_4: 0.65836
	loss_value_4: 0.07841
	loss_reward_4: 0.01531
	loss_policy_5: 0.06024
	accuracy_policy_5: 0.66797
	loss_value_5: 0.08033
	loss_reward_5: 0.01624
	loss_policy: 0.59892
	loss_value: 0.74687
	loss_reward: 0.06148
[2025-05-07 22:29:06] nn step 41950, lr: 0.1.
	loss_policy_0: 0.27441
	accuracy_policy_0: 0.6634
	loss_value_0: 0.33214
	loss_policy_1: 0.05547
	accuracy_policy_1: 0.65242
	loss_value_1: 0.06758
	loss_reward_1: 0.00658
	loss_policy_2: 0.05522
	accuracy_policy_2: 0.65891
	loss_value_2: 0.06954
	loss_reward_2: 0.00876
	loss_policy_3: 0.05562
	accuracy_policy_3: 0.66414
	loss_value_3: 0.0712
	loss_reward_3: 0.01147
	loss_policy_4: 0.05582
	accuracy_policy_4: 0.66125
	loss_value_4: 0.07227
	loss_reward_4: 0.01401
	loss_policy_5: 0.05567
	accuracy_policy_5: 0.66949
	loss_value_5: 0.07371
	loss_reward_5: 0.01487
	loss_policy: 0.55222
	loss_value: 0.68643
	loss_reward: 0.05569
[2025-05-07 22:29:14] nn step 42000, lr: 0.1.
	loss_policy_0: 0.28535
	accuracy_policy_0: 0.65461
	loss_value_0: 0.34089
	loss_policy_1: 0.05711
	accuracy_policy_1: 0.65137
	loss_value_1: 0.06948
	loss_reward_1: 0.00679
	loss_policy_2: 0.05713
	accuracy_policy_2: 0.6566
	loss_value_2: 0.07103
	loss_reward_2: 0.00912
	loss_policy_3: 0.05739
	accuracy_policy_3: 0.65793
	loss_value_3: 0.07233
	loss_reward_3: 0.0119
	loss_policy_4: 0.05747
	accuracy_policy_4: 0.66188
	loss_value_4: 0.0739
	loss_reward_4: 0.01403
	loss_policy_5: 0.05749
	accuracy_policy_5: 0.66375
	loss_value_5: 0.07554
	loss_reward_5: 0.01537
	loss_policy: 0.57195
	loss_value: 0.70316
	loss_reward: 0.05721
Optimization_Done 42000
[2025-05-07 22:32:51] [command] train weight_iter_42000.pkl 192 211
[2025-05-07 22:33:00] nn step 42050, lr: 0.1.
	loss_policy_0: 0.25706
	accuracy_policy_0: 0.66293
	loss_value_0: 0.31429
	loss_policy_1: 0.05178
	accuracy_policy_1: 0.64961
	loss_value_1: 0.06401
	loss_reward_1: 0.00645
	loss_policy_2: 0.05185
	accuracy_policy_2: 0.65465
	loss_value_2: 0.06522
	loss_reward_2: 0.00824
	loss_policy_3: 0.05205
	accuracy_policy_3: 0.65816
	loss_value_3: 0.0665
	loss_reward_3: 0.01092
	loss_policy_4: 0.05234
	accuracy_policy_4: 0.65824
	loss_value_4: 0.06764
	loss_reward_4: 0.01333
	loss_policy_5: 0.05205
	accuracy_policy_5: 0.67066
	loss_value_5: 0.06908
	loss_reward_5: 0.01419
	loss_policy: 0.51712
	loss_value: 0.64674
	loss_reward: 0.05312
[2025-05-07 22:33:08] nn step 42100, lr: 0.1.
	loss_policy_0: 0.26212
	accuracy_policy_0: 0.66922
	loss_value_0: 0.31852
	loss_policy_1: 0.05313
	accuracy_policy_1: 0.65332
	loss_value_1: 0.0648
	loss_reward_1: 0.00635
	loss_policy_2: 0.05332
	accuracy_policy_2: 0.65336
	loss_value_2: 0.06653
	loss_reward_2: 0.00832
	loss_policy_3: 0.05366
	accuracy_policy_3: 0.65645
	loss_value_3: 0.06767
	loss_reward_3: 0.01094
	loss_policy_4: 0.05322
	accuracy_policy_4: 0.66719
	loss_value_4: 0.06886
	loss_reward_4: 0.01314
	loss_policy_5: 0.05351
	accuracy_policy_5: 0.66832
	loss_value_5: 0.07038
	loss_reward_5: 0.01449
	loss_policy: 0.52896
	loss_value: 0.65676
	loss_reward: 0.05323
[2025-05-07 22:33:15] nn step 42150, lr: 0.1.
	loss_policy_0: 0.28318
	accuracy_policy_0: 0.66461
	loss_value_0: 0.33964
	loss_policy_1: 0.05733
	accuracy_policy_1: 0.65746
	loss_value_1: 0.06958
	loss_reward_1: 0.00678
	loss_policy_2: 0.05717
	accuracy_policy_2: 0.65965
	loss_value_2: 0.07108
	loss_reward_2: 0.009
	loss_policy_3: 0.05768
	accuracy_policy_3: 0.66059
	loss_value_3: 0.0728
	loss_reward_3: 0.0118
	loss_policy_4: 0.05763
	accuracy_policy_4: 0.66406
	loss_value_4: 0.07401
	loss_reward_4: 0.01416
	loss_policy_5: 0.05757
	accuracy_policy_5: 0.6693
	loss_value_5: 0.07566
	loss_reward_5: 0.01527
	loss_policy: 0.57055
	loss_value: 0.70277
	loss_reward: 0.05701
[2025-05-07 22:33:23] nn step 42200, lr: 0.1.
	loss_policy_0: 0.26626
	accuracy_policy_0: 0.66492
	loss_value_0: 0.31728
	loss_policy_1: 0.05384
	accuracy_policy_1: 0.65199
	loss_value_1: 0.06478
	loss_reward_1: 0.00663
	loss_policy_2: 0.05413
	accuracy_policy_2: 0.65406
	loss_value_2: 0.06591
	loss_reward_2: 0.00842
	loss_policy_3: 0.05416
	accuracy_policy_3: 0.65555
	loss_value_3: 0.06758
	loss_reward_3: 0.01093
	loss_policy_4: 0.05439
	accuracy_policy_4: 0.66117
	loss_value_4: 0.069
	loss_reward_4: 0.01331
	loss_policy_5: 0.05466
	accuracy_policy_5: 0.66254
	loss_value_5: 0.07037
	loss_reward_5: 0.01454
	loss_policy: 0.53744
	loss_value: 0.65491
	loss_reward: 0.05383
Optimization_Done 42200
[2025-05-07 22:36:44] [command] train weight_iter_42200.pkl 193 212
[2025-05-07 22:36:53] nn step 42250, lr: 0.1.
	loss_policy_0: 0.26386
	accuracy_policy_0: 0.66812
	loss_value_0: 0.32936
	loss_policy_1: 0.05352
	accuracy_policy_1: 0.65488
	loss_value_1: 0.06649
	loss_reward_1: 0.00609
	loss_policy_2: 0.05393
	accuracy_policy_2: 0.65703
	loss_value_2: 0.06805
	loss_reward_2: 0.00826
	loss_policy_3: 0.05432
	accuracy_policy_3: 0.65988
	loss_value_3: 0.06891
	loss_reward_3: 0.01107
	loss_policy_4: 0.05386
	accuracy_policy_4: 0.67066
	loss_value_4: 0.07025
	loss_reward_4: 0.01355
	loss_policy_5: 0.05388
	accuracy_policy_5: 0.67488
	loss_value_5: 0.07181
	loss_reward_5: 0.01487
	loss_policy: 0.53339
	loss_value: 0.67486
	loss_reward: 0.05383
[2025-05-07 22:36:59] nn step 42300, lr: 0.1.
	loss_policy_0: 0.26217
	accuracy_policy_0: 0.67102
	loss_value_0: 0.32076
	loss_policy_1: 0.05271
	accuracy_policy_1: 0.66199
	loss_value_1: 0.0654
	loss_reward_1: 0.00627
	loss_policy_2: 0.05307
	accuracy_policy_2: 0.65863
	loss_value_2: 0.06677
	loss_reward_2: 0.00821
	loss_policy_3: 0.05364
	accuracy_policy_3: 0.6616
	loss_value_3: 0.06812
	loss_reward_3: 0.01084
	loss_policy_4: 0.05361
	accuracy_policy_4: 0.66918
	loss_value_4: 0.06944
	loss_reward_4: 0.01326
	loss_policy_5: 0.0535
	accuracy_policy_5: 0.67293
	loss_value_5: 0.07084
	loss_reward_5: 0.01422
	loss_policy: 0.5287
	loss_value: 0.66132
	loss_reward: 0.0528
[2025-05-07 22:37:07] nn step 42350, lr: 0.1.
	loss_policy_0: 0.27048
	accuracy_policy_0: 0.67215
	loss_value_0: 0.32757
	loss_policy_1: 0.05447
	accuracy_policy_1: 0.66438
	loss_value_1: 0.06688
	loss_reward_1: 0.00649
	loss_policy_2: 0.05507
	accuracy_policy_2: 0.66312
	loss_value_2: 0.06816
	loss_reward_2: 0.00843
	loss_policy_3: 0.05502
	accuracy_policy_3: 0.66551
	loss_value_3: 0.06967
	loss_reward_3: 0.0112
	loss_policy_4: 0.05561
	accuracy_policy_4: 0.66465
	loss_value_4: 0.0713
	loss_reward_4: 0.01363
	loss_policy_5: 0.05518
	accuracy_policy_5: 0.67148
	loss_value_5: 0.07253
	loss_reward_5: 0.01489
	loss_policy: 0.54582
	loss_value: 0.67611
	loss_reward: 0.05463
[2025-05-07 22:37:15] nn step 42400, lr: 0.1.
	loss_policy_0: 0.28984
	accuracy_policy_0: 0.66453
	loss_value_0: 0.34937
	loss_policy_1: 0.05849
	accuracy_policy_1: 0.65555
	loss_value_1: 0.0714
	loss_reward_1: 0.0071
	loss_policy_2: 0.05848
	accuracy_policy_2: 0.65852
	loss_value_2: 0.07286
	loss_reward_2: 0.00907
	loss_policy_3: 0.05846
	accuracy_policy_3: 0.66324
	loss_value_3: 0.07456
	loss_reward_3: 0.01193
	loss_policy_4: 0.05852
	accuracy_policy_4: 0.67172
	loss_value_4: 0.07615
	loss_reward_4: 0.01507
	loss_policy_5: 0.05917
	accuracy_policy_5: 0.67195
	loss_value_5: 0.0776
	loss_reward_5: 0.01632
	loss_policy: 0.58295
	loss_value: 0.72195
	loss_reward: 0.05949
Optimization_Done 42400
[2025-05-07 22:40:47] [command] train weight_iter_42400.pkl 194 213
[2025-05-07 22:40:56] nn step 42450, lr: 0.1.
	loss_policy_0: 0.26304
	accuracy_policy_0: 0.66996
	loss_value_0: 0.33296
	loss_policy_1: 0.05291
	accuracy_policy_1: 0.66008
	loss_value_1: 0.06748
	loss_reward_1: 0.00641
	loss_policy_2: 0.05336
	accuracy_policy_2: 0.65594
	loss_value_2: 0.06885
	loss_reward_2: 0.00828
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.66094
	loss_value_3: 0.07022
	loss_reward_3: 0.01118
	loss_policy_4: 0.05401
	accuracy_policy_4: 0.66215
	loss_value_4: 0.071
	loss_reward_4: 0.01367
	loss_policy_5: 0.05328
	accuracy_policy_5: 0.67199
	loss_value_5: 0.07222
	loss_reward_5: 0.0143
	loss_policy: 0.53002
	loss_value: 0.68272
	loss_reward: 0.05385
[2025-05-07 22:41:04] nn step 42500, lr: 0.1.
	loss_policy_0: 0.27783
	accuracy_policy_0: 0.66168
	loss_value_0: 0.33833
	loss_policy_1: 0.05616
	accuracy_policy_1: 0.65691
	loss_value_1: 0.06894
	loss_reward_1: 0.00667
	loss_policy_2: 0.05636
	accuracy_policy_2: 0.65531
	loss_value_2: 0.07058
	loss_reward_2: 0.00852
	loss_policy_3: 0.05634
	accuracy_policy_3: 0.66066
	loss_value_3: 0.07202
	loss_reward_3: 0.01115
	loss_policy_4: 0.05647
	accuracy_policy_4: 0.66484
	loss_value_4: 0.07349
	loss_reward_4: 0.01392
	loss_policy_5: 0.05691
	accuracy_policy_5: 0.66836
	loss_value_5: 0.0751
	loss_reward_5: 0.01532
	loss_policy: 0.56007
	loss_value: 0.69846
	loss_reward: 0.05558
[2025-05-07 22:41:10] nn step 42550, lr: 0.1.
	loss_policy_0: 0.26201
	accuracy_policy_0: 0.66402
	loss_value_0: 0.31688
	loss_policy_1: 0.05273
	accuracy_policy_1: 0.65145
	loss_value_1: 0.06451
	loss_reward_1: 0.00607
	loss_policy_2: 0.05311
	accuracy_policy_2: 0.65605
	loss_value_2: 0.06616
	loss_reward_2: 0.00866
	loss_policy_3: 0.05273
	accuracy_policy_3: 0.66164
	loss_value_3: 0.06763
	loss_reward_3: 0.01119
	loss_policy_4: 0.0531
	accuracy_policy_4: 0.66285
	loss_value_4: 0.06895
	loss_reward_4: 0.01279
	loss_policy_5: 0.05295
	accuracy_policy_5: 0.67191
	loss_value_5: 0.07008
	loss_reward_5: 0.01444
	loss_policy: 0.52664
	loss_value: 0.65422
	loss_reward: 0.05314
[2025-05-07 22:41:18] nn step 42600, lr: 0.1.
	loss_policy_0: 0.27427
	accuracy_policy_0: 0.66781
	loss_value_0: 0.3363
	loss_policy_1: 0.05558
	accuracy_policy_1: 0.65992
	loss_value_1: 0.06852
	loss_reward_1: 0.00709
	loss_policy_2: 0.05581
	accuracy_policy_2: 0.65969
	loss_value_2: 0.07015
	loss_reward_2: 0.00882
	loss_policy_3: 0.05596
	accuracy_policy_3: 0.65668
	loss_value_3: 0.0714
	loss_reward_3: 0.01152
	loss_policy_4: 0.05607
	accuracy_policy_4: 0.66691
	loss_value_4: 0.07291
	loss_reward_4: 0.01447
	loss_policy_5: 0.05639
	accuracy_policy_5: 0.66797
	loss_value_5: 0.07486
	loss_reward_5: 0.0154
	loss_policy: 0.55406
	loss_value: 0.69414
	loss_reward: 0.0573
Optimization_Done 42600
[2025-05-07 22:44:47] [command] train weight_iter_42600.pkl 195 214
[2025-05-07 22:44:56] nn step 42650, lr: 0.1.
	loss_policy_0: 0.28549
	accuracy_policy_0: 0.6666
	loss_value_0: 0.35606
	loss_policy_1: 0.05745
	accuracy_policy_1: 0.66285
	loss_value_1: 0.07193
	loss_reward_1: 0.00709
	loss_policy_2: 0.05765
	accuracy_policy_2: 0.65559
	loss_value_2: 0.07373
	loss_reward_2: 0.00907
	loss_policy_3: 0.05773
	accuracy_policy_3: 0.66523
	loss_value_3: 0.07486
	loss_reward_3: 0.01184
	loss_policy_4: 0.05774
	accuracy_policy_4: 0.66504
	loss_value_4: 0.07609
	loss_reward_4: 0.01527
	loss_policy_5: 0.0577
	accuracy_policy_5: 0.67449
	loss_value_5: 0.07751
	loss_reward_5: 0.01593
	loss_policy: 0.57376
	loss_value: 0.73019
	loss_reward: 0.05921
[2025-05-07 22:45:02] nn step 42700, lr: 0.1.
	loss_policy_0: 0.27047
	accuracy_policy_0: 0.67
	loss_value_0: 0.33462
	loss_policy_1: 0.05484
	accuracy_policy_1: 0.65422
	loss_value_1: 0.06801
	loss_reward_1: 0.0064
	loss_policy_2: 0.05451
	accuracy_policy_2: 0.65828
	loss_value_2: 0.06948
	loss_reward_2: 0.00866
	loss_policy_3: 0.05477
	accuracy_policy_3: 0.66441
	loss_value_3: 0.07113
	loss_reward_3: 0.01109
	loss_policy_4: 0.05515
	accuracy_policy_4: 0.66543
	loss_value_4: 0.07276
	loss_reward_4: 0.01366
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.6723
	loss_value_5: 0.07423
	loss_reward_5: 0.01496
	loss_policy: 0.54517
	loss_value: 0.69023
	loss_reward: 0.05477
[2025-05-07 22:45:10] nn step 42750, lr: 0.1.
	loss_policy_0: 0.26749
	accuracy_policy_0: 0.6691
	loss_value_0: 0.33248
	loss_policy_1: 0.054
	accuracy_policy_1: 0.66371
	loss_value_1: 0.06774
	loss_reward_1: 0.00659
	loss_policy_2: 0.05434
	accuracy_policy_2: 0.66316
	loss_value_2: 0.0692
	loss_reward_2: 0.0086
	loss_policy_3: 0.05479
	accuracy_policy_3: 0.66539
	loss_value_3: 0.07058
	loss_reward_3: 0.01115
	loss_policy_4: 0.05473
	accuracy_policy_4: 0.66523
	loss_value_4: 0.07171
	loss_reward_4: 0.01381
	loss_policy_5: 0.05475
	accuracy_policy_5: 0.67527
	loss_value_5: 0.07322
	loss_reward_5: 0.01493
	loss_policy: 0.5401
	loss_value: 0.68492
	loss_reward: 0.05508
[2025-05-07 22:45:18] nn step 42800, lr: 0.1.
	loss_policy_0: 0.28042
	accuracy_policy_0: 0.67719
	loss_value_0: 0.34321
	loss_policy_1: 0.0565
	accuracy_policy_1: 0.66656
	loss_value_1: 0.06998
	loss_reward_1: 0.00662
	loss_policy_2: 0.05696
	accuracy_policy_2: 0.66336
	loss_value_2: 0.07158
	loss_reward_2: 0.00875
	loss_policy_3: 0.05693
	accuracy_policy_3: 0.66301
	loss_value_3: 0.07306
	loss_reward_3: 0.01167
	loss_policy_4: 0.05769
	accuracy_policy_4: 0.66406
	loss_value_4: 0.07467
	loss_reward_4: 0.01436
	loss_policy_5: 0.05716
	accuracy_policy_5: 0.67555
	loss_value_5: 0.07605
	loss_reward_5: 0.01517
	loss_policy: 0.56566
	loss_value: 0.70854
	loss_reward: 0.05657
Optimization_Done 42800
[2025-05-07 22:48:41] [command] train weight_iter_42800.pkl 196 215
[2025-05-07 22:48:49] nn step 42850, lr: 0.1.
	loss_policy_0: 0.26211
	accuracy_policy_0: 0.68305
	loss_value_0: 0.34352
	loss_policy_1: 0.05343
	accuracy_policy_1: 0.66668
	loss_value_1: 0.06957
	loss_reward_1: 0.00653
	loss_policy_2: 0.05384
	accuracy_policy_2: 0.6684
	loss_value_2: 0.07124
	loss_reward_2: 0.0085
	loss_policy_3: 0.05393
	accuracy_policy_3: 0.67461
	loss_value_3: 0.0725
	loss_reward_3: 0.01087
	loss_policy_4: 0.05408
	accuracy_policy_4: 0.67176
	loss_value_4: 0.07378
	loss_reward_4: 0.01374
	loss_policy_5: 0.05382
	accuracy_policy_5: 0.68645
	loss_value_5: 0.07505
	loss_reward_5: 0.01503
	loss_policy: 0.53122
	loss_value: 0.70565
	loss_reward: 0.05466
[2025-05-07 22:48:57] nn step 42900, lr: 0.1.
	loss_policy_0: 0.24735
	accuracy_policy_0: 0.68684
	loss_value_0: 0.3152
	loss_policy_1: 0.05015
	accuracy_policy_1: 0.67523
	loss_value_1: 0.06402
	loss_reward_1: 0.00597
	loss_policy_2: 0.05074
	accuracy_policy_2: 0.66984
	loss_value_2: 0.06523
	loss_reward_2: 0.00808
	loss_policy_3: 0.05063
	accuracy_policy_3: 0.67402
	loss_value_3: 0.06657
	loss_reward_3: 0.01031
	loss_policy_4: 0.05044
	accuracy_policy_4: 0.67871
	loss_value_4: 0.06794
	loss_reward_4: 0.01304
	loss_policy_5: 0.05054
	accuracy_policy_5: 0.68688
	loss_value_5: 0.06916
	loss_reward_5: 0.01375
	loss_policy: 0.49984
	loss_value: 0.64813
	loss_reward: 0.05116
[2025-05-07 22:49:05] nn step 42950, lr: 0.1.
	loss_policy_0: 0.26926
	accuracy_policy_0: 0.68398
	loss_value_0: 0.33642
	loss_policy_1: 0.05462
	accuracy_policy_1: 0.67266
	loss_value_1: 0.06882
	loss_reward_1: 0.00677
	loss_policy_2: 0.05462
	accuracy_policy_2: 0.67566
	loss_value_2: 0.07048
	loss_reward_2: 0.00859
	loss_policy_3: 0.05516
	accuracy_policy_3: 0.66949
	loss_value_3: 0.07154
	loss_reward_3: 0.01156
	loss_policy_4: 0.05558
	accuracy_policy_4: 0.67281
	loss_value_4: 0.07256
	loss_reward_4: 0.01439
	loss_policy_5: 0.05549
	accuracy_policy_5: 0.67852
	loss_value_5: 0.07413
	loss_reward_5: 0.01499
	loss_policy: 0.54474
	loss_value: 0.69395
	loss_reward: 0.0563
[2025-05-07 22:49:14] nn step 43000, lr: 0.1.
	loss_policy_0: 0.25433
	accuracy_policy_0: 0.68102
	loss_value_0: 0.31584
	loss_policy_1: 0.05174
	accuracy_policy_1: 0.6741
	loss_value_1: 0.06432
	loss_reward_1: 0.0063
	loss_policy_2: 0.0519
	accuracy_policy_2: 0.67273
	loss_value_2: 0.0661
	loss_reward_2: 0.00811
	loss_policy_3: 0.05225
	accuracy_policy_3: 0.67246
	loss_value_3: 0.06697
	loss_reward_3: 0.01055
	loss_policy_4: 0.05259
	accuracy_policy_4: 0.67125
	loss_value_4: 0.06844
	loss_reward_4: 0.01308
	loss_policy_5: 0.05261
	accuracy_policy_5: 0.67957
	loss_value_5: 0.07005
	loss_reward_5: 0.0139
	loss_policy: 0.51542
	loss_value: 0.65172
	loss_reward: 0.05194
Optimization_Done 43000
[2025-05-07 22:52:46] [command] train weight_iter_43000.pkl 197 216
[2025-05-07 22:52:55] nn step 43050, lr: 0.1.
	loss_policy_0: 0.27939
	accuracy_policy_0: 0.67902
	loss_value_0: 0.36101
	loss_policy_1: 0.0565
	accuracy_policy_1: 0.67238
	loss_value_1: 0.07327
	loss_reward_1: 0.00662
	loss_policy_2: 0.05673
	accuracy_policy_2: 0.67176
	loss_value_2: 0.07422
	loss_reward_2: 0.00914
	loss_policy_3: 0.05717
	accuracy_policy_3: 0.67047
	loss_value_3: 0.07522
	loss_reward_3: 0.01166
	loss_policy_4: 0.05691
	accuracy_policy_4: 0.67703
	loss_value_4: 0.07674
	loss_reward_4: 0.01416
	loss_policy_5: 0.05752
	accuracy_policy_5: 0.67832
	loss_value_5: 0.078
	loss_reward_5: 0.01582
	loss_policy: 0.56421
	loss_value: 0.73846
	loss_reward: 0.0574
[2025-05-07 22:53:01] nn step 43100, lr: 0.1.
	loss_policy_0: 0.25834
	accuracy_policy_0: 0.68551
	loss_value_0: 0.32611
	loss_policy_1: 0.05216
	accuracy_policy_1: 0.67461
	loss_value_1: 0.06606
	loss_reward_1: 0.00636
	loss_policy_2: 0.0528
	accuracy_policy_2: 0.66473
	loss_value_2: 0.06745
	loss_reward_2: 0.00847
	loss_policy_3: 0.05284
	accuracy_policy_3: 0.6693
	loss_value_3: 0.06875
	loss_reward_3: 0.01094
	loss_policy_4: 0.0529
	accuracy_policy_4: 0.67898
	loss_value_4: 0.06982
	loss_reward_4: 0.01319
	loss_policy_5: 0.05326
	accuracy_policy_5: 0.67891
	loss_value_5: 0.07142
	loss_reward_5: 0.01448
	loss_policy: 0.5223
	loss_value: 0.66961
	loss_reward: 0.05344
[2025-05-07 22:53:09] nn step 43150, lr: 0.1.
	loss_policy_0: 0.27432
	accuracy_policy_0: 0.68688
	loss_value_0: 0.34803
	loss_policy_1: 0.0555
	accuracy_policy_1: 0.6759
	loss_value_1: 0.07102
	loss_reward_1: 0.00667
	loss_policy_2: 0.05585
	accuracy_policy_2: 0.67703
	loss_value_2: 0.07239
	loss_reward_2: 0.00896
	loss_policy_3: 0.05624
	accuracy_policy_3: 0.67234
	loss_value_3: 0.07354
	loss_reward_3: 0.01132
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.67898
	loss_value_4: 0.07519
	loss_reward_4: 0.01445
	loss_policy_5: 0.05663
	accuracy_policy_5: 0.68473
	loss_value_5: 0.07663
	loss_reward_5: 0.01555
	loss_policy: 0.55497
	loss_value: 0.71681
	loss_reward: 0.05694
[2025-05-07 22:53:18] nn step 43200, lr: 0.1.
	loss_policy_0: 0.26778
	accuracy_policy_0: 0.68262
	loss_value_0: 0.33393
	loss_policy_1: 0.05415
	accuracy_policy_1: 0.67078
	loss_value_1: 0.06815
	loss_reward_1: 0.00676
	loss_policy_2: 0.05444
	accuracy_policy_2: 0.67438
	loss_value_2: 0.06932
	loss_reward_2: 0.00907
	loss_policy_3: 0.0551
	accuracy_policy_3: 0.6693
	loss_value_3: 0.07094
	loss_reward_3: 0.01149
	loss_policy_4: 0.05518
	accuracy_policy_4: 0.67094
	loss_value_4: 0.07241
	loss_reward_4: 0.01413
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.67984
	loss_value_5: 0.07403
	loss_reward_5: 0.01522
	loss_policy: 0.5419
	loss_value: 0.68878
	loss_reward: 0.05668
Optimization_Done 43200
[2025-05-07 22:56:46] [command] train weight_iter_43200.pkl 198 217
[2025-05-07 22:56:53] nn step 43250, lr: 0.1.
	loss_policy_0: 0.25109
	accuracy_policy_0: 0.68289
	loss_value_0: 0.31972
	loss_policy_1: 0.05078
	accuracy_policy_1: 0.67441
	loss_value_1: 0.06506
	loss_reward_1: 0.00627
	loss_policy_2: 0.05091
	accuracy_policy_2: 0.67203
	loss_value_2: 0.0659
	loss_reward_2: 0.00801
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.67145
	loss_value_3: 0.06705
	loss_reward_3: 0.01068
	loss_policy_4: 0.05148
	accuracy_policy_4: 0.67539
	loss_value_4: 0.06854
	loss_reward_4: 0.01345
	loss_policy_5: 0.0512
	accuracy_policy_5: 0.68781
	loss_value_5: 0.06965
	loss_reward_5: 0.01402
	loss_policy: 0.50705
	loss_value: 0.65591
	loss_reward: 0.05243
[2025-05-07 22:57:01] nn step 43300, lr: 0.1.
	loss_policy_0: 0.2663
	accuracy_policy_0: 0.68602
	loss_value_0: 0.33631
	loss_policy_1: 0.05437
	accuracy_policy_1: 0.6725
	loss_value_1: 0.06877
	loss_reward_1: 0.00645
	loss_policy_2: 0.0548
	accuracy_policy_2: 0.66719
	loss_value_2: 0.07014
	loss_reward_2: 0.00853
	loss_policy_3: 0.05484
	accuracy_policy_3: 0.66855
	loss_value_3: 0.07152
	loss_reward_3: 0.01115
	loss_policy_4: 0.05488
	accuracy_policy_4: 0.67539
	loss_value_4: 0.07293
	loss_reward_4: 0.01436
	loss_policy_5: 0.05452
	accuracy_policy_5: 0.69008
	loss_value_5: 0.074
	loss_reward_5: 0.01508
	loss_policy: 0.5397
	loss_value: 0.69367
	loss_reward: 0.05557
[2025-05-07 22:57:10] nn step 43350, lr: 0.1.
	loss_policy_0: 0.25259
	accuracy_policy_0: 0.67328
	loss_value_0: 0.30554
	loss_policy_1: 0.05049
	accuracy_policy_1: 0.66965
	loss_value_1: 0.06179
	loss_reward_1: 0.00638
	loss_policy_2: 0.05076
	accuracy_policy_2: 0.66664
	loss_value_2: 0.06327
	loss_reward_2: 0.00785
	loss_policy_3: 0.05098
	accuracy_policy_3: 0.66762
	loss_value_3: 0.06469
	loss_reward_3: 0.01038
	loss_policy_4: 0.05069
	accuracy_policy_4: 0.67359
	loss_value_4: 0.06624
	loss_reward_4: 0.01279
	loss_policy_5: 0.05069
	accuracy_policy_5: 0.67645
	loss_value_5: 0.06775
	loss_reward_5: 0.01386
	loss_policy: 0.50619
	loss_value: 0.62929
	loss_reward: 0.05124
[2025-05-07 22:57:16] nn step 43400, lr: 0.1.
	loss_policy_0: 0.27946
	accuracy_policy_0: 0.6841
	loss_value_0: 0.34655
	loss_policy_1: 0.05669
	accuracy_policy_1: 0.6693
	loss_value_1: 0.0709
	loss_reward_1: 0.00693
	loss_policy_2: 0.05674
	accuracy_policy_2: 0.66961
	loss_value_2: 0.07253
	loss_reward_2: 0.00877
	loss_policy_3: 0.05741
	accuracy_policy_3: 0.6632
	loss_value_3: 0.07356
	loss_reward_3: 0.01193
	loss_policy_4: 0.05753
	accuracy_policy_4: 0.67094
	loss_value_4: 0.07484
	loss_reward_4: 0.0147
	loss_policy_5: 0.05747
	accuracy_policy_5: 0.6798
	loss_value_5: 0.0765
	loss_reward_5: 0.01606
	loss_policy: 0.56531
	loss_value: 0.71487
	loss_reward: 0.05839
Optimization_Done 43400
[2025-05-07 23:00:40] [command] train weight_iter_43400.pkl 199 218
[2025-05-07 23:00:49] nn step 43450, lr: 0.1.
	loss_policy_0: 0.25893
	accuracy_policy_0: 0.68527
	loss_value_0: 0.32985
	loss_policy_1: 0.05296
	accuracy_policy_1: 0.67
	loss_value_1: 0.0664
	loss_reward_1: 0.00651
	loss_policy_2: 0.05334
	accuracy_policy_2: 0.66613
	loss_value_2: 0.0677
	loss_reward_2: 0.00875
	loss_policy_3: 0.05319
	accuracy_policy_3: 0.66266
	loss_value_3: 0.06901
	loss_reward_3: 0.01049
	loss_policy_4: 0.05305
	accuracy_policy_4: 0.67055
	loss_value_4: 0.07029
	loss_reward_4: 0.01352
	loss_policy_5: 0.05307
	accuracy_policy_5: 0.67918
	loss_value_5: 0.07158
	loss_reward_5: 0.01464
	loss_policy: 0.52454
	loss_value: 0.67484
	loss_reward: 0.05391
[2025-05-07 23:00:57] nn step 43500, lr: 0.1.
	loss_policy_0: 0.25059
	accuracy_policy_0: 0.68699
	loss_value_0: 0.31877
	loss_policy_1: 0.05105
	accuracy_policy_1: 0.66828
	loss_value_1: 0.0647
	loss_reward_1: 0.00619
	loss_policy_2: 0.05133
	accuracy_policy_2: 0.66754
	loss_value_2: 0.06621
	loss_reward_2: 0.00823
	loss_policy_3: 0.05165
	accuracy_policy_3: 0.66414
	loss_value_3: 0.06711
	loss_reward_3: 0.01082
	loss_policy_4: 0.0518
	accuracy_policy_4: 0.67223
	loss_value_4: 0.06858
	loss_reward_4: 0.01327
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.67539
	loss_value_5: 0.07015
	loss_reward_5: 0.01435
	loss_policy: 0.50823
	loss_value: 0.65552
	loss_reward: 0.05287
[2025-05-07 23:01:04] nn step 43550, lr: 0.1.
	loss_policy_0: 0.26378
	accuracy_policy_0: 0.68918
	loss_value_0: 0.32791
	loss_policy_1: 0.05355
	accuracy_policy_1: 0.6718
	loss_value_1: 0.06662
	loss_reward_1: 0.00661
	loss_policy_2: 0.05437
	accuracy_policy_2: 0.66102
	loss_value_2: 0.06836
	loss_reward_2: 0.00862
	loss_policy_3: 0.05441
	accuracy_policy_3: 0.6659
	loss_value_3: 0.06957
	loss_reward_3: 0.01121
	loss_policy_4: 0.05461
	accuracy_policy_4: 0.67301
	loss_value_4: 0.07092
	loss_reward_4: 0.0137
	loss_policy_5: 0.05398
	accuracy_policy_5: 0.68195
	loss_value_5: 0.07222
	loss_reward_5: 0.01534
	loss_policy: 0.5347
	loss_value: 0.6756
	loss_reward: 0.05548
[2025-05-07 23:01:12] nn step 43600, lr: 0.1.
	loss_policy_0: 0.2714
	accuracy_policy_0: 0.68273
	loss_value_0: 0.32791
	loss_policy_1: 0.05475
	accuracy_policy_1: 0.66875
	loss_value_1: 0.0668
	loss_reward_1: 0.00677
	loss_policy_2: 0.05499
	accuracy_policy_2: 0.66559
	loss_value_2: 0.06829
	loss_reward_2: 0.00865
	loss_policy_3: 0.05494
	accuracy_policy_3: 0.66488
	loss_value_3: 0.06981
	loss_reward_3: 0.01139
	loss_policy_4: 0.05499
	accuracy_policy_4: 0.67047
	loss_value_4: 0.07155
	loss_reward_4: 0.01454
	loss_policy_5: 0.05497
	accuracy_policy_5: 0.68637
	loss_value_5: 0.07316
	loss_reward_5: 0.0151
	loss_policy: 0.54605
	loss_value: 0.67752
	loss_reward: 0.05646
Optimization_Done 43600
[2025-05-07 23:04:40] [command] train weight_iter_43600.pkl 200 219
[2025-05-07 23:04:49] nn step 43650, lr: 0.1.
	loss_policy_0: 0.25822
	accuracy_policy_0: 0.69016
	loss_value_0: 0.33843
	loss_policy_1: 0.05206
	accuracy_policy_1: 0.67422
	loss_value_1: 0.06866
	loss_reward_1: 0.00649
	loss_policy_2: 0.0523
	accuracy_policy_2: 0.67137
	loss_value_2: 0.06963
	loss_reward_2: 0.00852
	loss_policy_3: 0.05252
	accuracy_policy_3: 0.67
	loss_value_3: 0.0709
	loss_reward_3: 0.01141
	loss_policy_4: 0.05266
	accuracy_policy_4: 0.67578
	loss_value_4: 0.07229
	loss_reward_4: 0.01362
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.68332
	loss_value_5: 0.07379
	loss_reward_5: 0.01453
	loss_policy: 0.52021
	loss_value: 0.69371
	loss_reward: 0.05457
[2025-05-07 23:04:55] nn step 43700, lr: 0.1.
	loss_policy_0: 0.24954
	accuracy_policy_0: 0.68262
	loss_value_0: 0.31388
	loss_policy_1: 0.05005
	accuracy_policy_1: 0.67312
	loss_value_1: 0.06361
	loss_reward_1: 0.00626
	loss_policy_2: 0.0502
	accuracy_policy_2: 0.67309
	loss_value_2: 0.06505
	loss_reward_2: 0.00772
	loss_policy_3: 0.05098
	accuracy_policy_3: 0.6693
	loss_value_3: 0.06625
	loss_reward_3: 0.01046
	loss_policy_4: 0.05091
	accuracy_policy_4: 0.6716
	loss_value_4: 0.06747
	loss_reward_4: 0.0131
	loss_policy_5: 0.05056
	accuracy_policy_5: 0.68082
	loss_value_5: 0.06888
	loss_reward_5: 0.01374
	loss_policy: 0.50223
	loss_value: 0.64514
	loss_reward: 0.05128
[2025-05-07 23:05:03] nn step 43750, lr: 0.1.
	loss_policy_0: 0.26453
	accuracy_policy_0: 0.69008
	loss_value_0: 0.32895
	loss_policy_1: 0.0537
	accuracy_policy_1: 0.67434
	loss_value_1: 0.06705
	loss_reward_1: 0.00659
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.67445
	loss_value_2: 0.06849
	loss_reward_2: 0.00851
	loss_policy_3: 0.05412
	accuracy_policy_3: 0.67066
	loss_value_3: 0.06989
	loss_reward_3: 0.01087
	loss_policy_4: 0.05423
	accuracy_policy_4: 0.67426
	loss_value_4: 0.07129
	loss_reward_4: 0.0139
	loss_policy_5: 0.05468
	accuracy_policy_5: 0.67461
	loss_value_5: 0.07281
	loss_reward_5: 0.01511
	loss_policy: 0.53532
	loss_value: 0.67849
	loss_reward: 0.05498
[2025-05-07 23:05:11] nn step 43800, lr: 0.1.
	loss_policy_0: 0.26553
	accuracy_policy_0: 0.68402
	loss_value_0: 0.32719
	loss_policy_1: 0.05331
	accuracy_policy_1: 0.67383
	loss_value_1: 0.06664
	loss_reward_1: 0.00658
	loss_policy_2: 0.05395
	accuracy_policy_2: 0.6723
	loss_value_2: 0.06814
	loss_reward_2: 0.00859
	loss_policy_3: 0.0542
	accuracy_policy_3: 0.66715
	loss_value_3: 0.06939
	loss_reward_3: 0.01152
	loss_policy_4: 0.05394
	accuracy_policy_4: 0.67656
	loss_value_4: 0.07088
	loss_reward_4: 0.0141
	loss_policy_5: 0.05382
	accuracy_policy_5: 0.68836
	loss_value_5: 0.07253
	loss_reward_5: 0.01518
	loss_policy: 0.53474
	loss_value: 0.67477
	loss_reward: 0.05597
Optimization_Done 43800
[2025-05-07 23:08:35] [command] train weight_iter_43800.pkl 201 220
[2025-05-07 23:08:43] nn step 43850, lr: 0.1.
	loss_policy_0: 0.24929
	accuracy_policy_0: 0.69105
	loss_value_0: 0.32628
	loss_policy_1: 0.05127
	accuracy_policy_1: 0.67391
	loss_value_1: 0.06586
	loss_reward_1: 0.00667
	loss_policy_2: 0.05131
	accuracy_policy_2: 0.67656
	loss_value_2: 0.06699
	loss_reward_2: 0.00786
	loss_policy_3: 0.05179
	accuracy_policy_3: 0.66977
	loss_value_3: 0.06813
	loss_reward_3: 0.0105
	loss_policy_4: 0.05179
	accuracy_policy_4: 0.67324
	loss_value_4: 0.06911
	loss_reward_4: 0.01311
	loss_policy_5: 0.05165
	accuracy_policy_5: 0.68176
	loss_value_5: 0.0704
	loss_reward_5: 0.0143
	loss_policy: 0.50709
	loss_value: 0.66677
	loss_reward: 0.05244
[2025-05-07 23:08:51] nn step 43900, lr: 0.1.
	loss_policy_0: 0.27101
	accuracy_policy_0: 0.69027
	loss_value_0: 0.34289
	loss_policy_1: 0.05504
	accuracy_policy_1: 0.67914
	loss_value_1: 0.06964
	loss_reward_1: 0.00667
	loss_policy_2: 0.05555
	accuracy_policy_2: 0.67344
	loss_value_2: 0.07116
	loss_reward_2: 0.00865
	loss_policy_3: 0.05595
	accuracy_policy_3: 0.67316
	loss_value_3: 0.07296
	loss_reward_3: 0.011
	loss_policy_4: 0.05614
	accuracy_policy_4: 0.67594
	loss_value_4: 0.07409
	loss_reward_4: 0.01439
	loss_policy_5: 0.05561
	accuracy_policy_5: 0.68387
	loss_value_5: 0.07588
	loss_reward_5: 0.01492
	loss_policy: 0.5493
	loss_value: 0.70663
	loss_reward: 0.05563
[2025-05-07 23:08:59] nn step 43950, lr: 0.1.
	loss_policy_0: 0.25796
	accuracy_policy_0: 0.69012
	loss_value_0: 0.32296
	loss_policy_1: 0.05196
	accuracy_policy_1: 0.67973
	loss_value_1: 0.06586
	loss_reward_1: 0.00647
	loss_policy_2: 0.05251
	accuracy_policy_2: 0.67297
	loss_value_2: 0.06726
	loss_reward_2: 0.00855
	loss_policy_3: 0.0525
	accuracy_policy_3: 0.67078
	loss_value_3: 0.06862
	loss_reward_3: 0.01136
	loss_policy_4: 0.0527
	accuracy_policy_4: 0.67383
	loss_value_4: 0.07031
	loss_reward_4: 0.01343
	loss_policy_5: 0.0527
	accuracy_policy_5: 0.67926
	loss_value_5: 0.07159
	loss_reward_5: 0.01498
	loss_policy: 0.52034
	loss_value: 0.66661
	loss_reward: 0.05479
[2025-05-07 23:09:05] nn step 44000, lr: 0.1.
	loss_policy_0: 0.26449
	accuracy_policy_0: 0.68766
	loss_value_0: 0.32746
	loss_policy_1: 0.05325
	accuracy_policy_1: 0.67586
	loss_value_1: 0.06649
	loss_reward_1: 0.00654
	loss_policy_2: 0.05321
	accuracy_policy_2: 0.67562
	loss_value_2: 0.06822
	loss_reward_2: 0.0086
	loss_policy_3: 0.05366
	accuracy_policy_3: 0.67367
	loss_value_3: 0.06968
	loss_reward_3: 0.01156
	loss_policy_4: 0.05421
	accuracy_policy_4: 0.67391
	loss_value_4: 0.07109
	loss_reward_4: 0.01389
	loss_policy_5: 0.05384
	accuracy_policy_5: 0.68031
	loss_value_5: 0.0728
	loss_reward_5: 0.01497
	loss_policy: 0.53266
	loss_value: 0.67574
	loss_reward: 0.05557
Optimization_Done 44000
[2025-05-07 23:12:34] [command] train weight_iter_44000.pkl 202 221
[2025-05-07 23:12:42] nn step 44050, lr: 0.1.
	loss_policy_0: 0.26915
	accuracy_policy_0: 0.68707
	loss_value_0: 0.34346
	loss_policy_1: 0.05445
	accuracy_policy_1: 0.67234
	loss_value_1: 0.06972
	loss_reward_1: 0.00675
	loss_policy_2: 0.05502
	accuracy_policy_2: 0.66891
	loss_value_2: 0.0708
	loss_reward_2: 0.00864
	loss_policy_3: 0.05539
	accuracy_policy_3: 0.66422
	loss_value_3: 0.07228
	loss_reward_3: 0.01151
	loss_policy_4: 0.0555
	accuracy_policy_4: 0.67
	loss_value_4: 0.07378
	loss_reward_4: 0.01435
	loss_policy_5: 0.05524
	accuracy_policy_5: 0.67793
	loss_value_5: 0.07541
	loss_reward_5: 0.01517
	loss_policy: 0.54474
	loss_value: 0.70544
	loss_reward: 0.05642
[2025-05-07 23:12:51] nn step 44100, lr: 0.1.
	loss_policy_0: 0.26237
	accuracy_policy_0: 0.69504
	loss_value_0: 0.33136
	loss_policy_1: 0.05354
	accuracy_policy_1: 0.67711
	loss_value_1: 0.06732
	loss_reward_1: 0.00645
	loss_policy_2: 0.05397
	accuracy_policy_2: 0.66918
	loss_value_2: 0.0688
	loss_reward_2: 0.00849
	loss_policy_3: 0.05396
	accuracy_policy_3: 0.66934
	loss_value_3: 0.07035
	loss_reward_3: 0.01081
	loss_policy_4: 0.05449
	accuracy_policy_4: 0.66457
	loss_value_4: 0.07163
	loss_reward_4: 0.01409
	loss_policy_5: 0.05425
	accuracy_policy_5: 0.67707
	loss_value_5: 0.07323
	loss_reward_5: 0.01481
	loss_policy: 0.53258
	loss_value: 0.68269
	loss_reward: 0.05464
[2025-05-07 23:12:57] nn step 44150, lr: 0.1.
	loss_policy_0: 0.26502
	accuracy_policy_0: 0.68938
	loss_value_0: 0.32875
	loss_policy_1: 0.05384
	accuracy_policy_1: 0.67516
	loss_value_1: 0.06723
	loss_reward_1: 0.00641
	loss_policy_2: 0.054
	accuracy_policy_2: 0.67359
	loss_value_2: 0.06866
	loss_reward_2: 0.00865
	loss_policy_3: 0.05451
	accuracy_policy_3: 0.67125
	loss_value_3: 0.07038
	loss_reward_3: 0.01116
	loss_policy_4: 0.05499
	accuracy_policy_4: 0.66906
	loss_value_4: 0.07183
	loss_reward_4: 0.01399
	loss_policy_5: 0.05414
	accuracy_policy_5: 0.68242
	loss_value_5: 0.07381
	loss_reward_5: 0.01526
	loss_policy: 0.53649
	loss_value: 0.68066
	loss_reward: 0.05547
[2025-05-07 23:13:05] nn step 44200, lr: 0.1.
	loss_policy_0: 0.26304
	accuracy_policy_0: 0.69027
	loss_value_0: 0.32575
	loss_policy_1: 0.0534
	accuracy_policy_1: 0.67422
	loss_value_1: 0.0663
	loss_reward_1: 0.00676
	loss_policy_2: 0.05376
	accuracy_policy_2: 0.6732
	loss_value_2: 0.06776
	loss_reward_2: 0.00845
	loss_policy_3: 0.05431
	accuracy_policy_3: 0.66613
	loss_value_3: 0.06937
	loss_reward_3: 0.01126
	loss_policy_4: 0.05411
	accuracy_policy_4: 0.67137
	loss_value_4: 0.07085
	loss_reward_4: 0.01399
	loss_policy_5: 0.05384
	accuracy_policy_5: 0.68707
	loss_value_5: 0.0725
	loss_reward_5: 0.01493
	loss_policy: 0.53245
	loss_value: 0.67254
	loss_reward: 0.05539
Optimization_Done 44200
[2025-05-07 23:16:27] [command] train weight_iter_44200.pkl 203 222
[2025-05-07 23:16:35] nn step 44250, lr: 0.1.
	loss_policy_0: 0.26308
	accuracy_policy_0: 0.69398
	loss_value_0: 0.33644
	loss_policy_1: 0.05338
	accuracy_policy_1: 0.67785
	loss_value_1: 0.06841
	loss_reward_1: 0.00658
	loss_policy_2: 0.05384
	accuracy_policy_2: 0.67559
	loss_value_2: 0.07003
	loss_reward_2: 0.00886
	loss_policy_3: 0.05398
	accuracy_policy_3: 0.6693
	loss_value_3: 0.0711
	loss_reward_3: 0.01149
	loss_policy_4: 0.05426
	accuracy_policy_4: 0.67109
	loss_value_4: 0.07245
	loss_reward_4: 0.01383
	loss_policy_5: 0.05401
	accuracy_policy_5: 0.68285
	loss_value_5: 0.07389
	loss_reward_5: 0.01505
	loss_policy: 0.53255
	loss_value: 0.69232
	loss_reward: 0.05581
[2025-05-07 23:16:42] nn step 44300, lr: 0.1.
	loss_policy_0: 0.27152
	accuracy_policy_0: 0.69254
	loss_value_0: 0.34182
	loss_policy_1: 0.05496
	accuracy_policy_1: 0.67785
	loss_value_1: 0.06976
	loss_reward_1: 0.00678
	loss_policy_2: 0.05558
	accuracy_policy_2: 0.67613
	loss_value_2: 0.07162
	loss_reward_2: 0.00888
	loss_policy_3: 0.05577
	accuracy_policy_3: 0.66965
	loss_value_3: 0.07303
	loss_reward_3: 0.01177
	loss_policy_4: 0.05596
	accuracy_policy_4: 0.67613
	loss_value_4: 0.07524
	loss_reward_4: 0.01435
	loss_policy_5: 0.05586
	accuracy_policy_5: 0.67949
	loss_value_5: 0.07685
	loss_reward_5: 0.01562
	loss_policy: 0.54965
	loss_value: 0.70832
	loss_reward: 0.0574
[2025-05-07 23:16:50] nn step 44350, lr: 0.1.
	loss_policy_0: 0.2284
	accuracy_policy_0: 0.68824
	loss_value_0: 0.28364
	loss_policy_1: 0.04615
	accuracy_policy_1: 0.67945
	loss_value_1: 0.05788
	loss_reward_1: 0.00564
	loss_policy_2: 0.0466
	accuracy_policy_2: 0.67484
	loss_value_2: 0.05932
	loss_reward_2: 0.00748
	loss_policy_3: 0.04662
	accuracy_policy_3: 0.67297
	loss_value_3: 0.06036
	loss_reward_3: 0.00966
	loss_policy_4: 0.04697
	accuracy_policy_4: 0.67559
	loss_value_4: 0.06182
	loss_reward_4: 0.01218
	loss_policy_5: 0.04694
	accuracy_policy_5: 0.6834
	loss_value_5: 0.06331
	loss_reward_5: 0.0133
	loss_policy: 0.46168
	loss_value: 0.58633
	loss_reward: 0.04826
[2025-05-07 23:16:58] nn step 44400, lr: 0.1.
	loss_policy_0: 0.2597
	accuracy_policy_0: 0.68828
	loss_value_0: 0.31871
	loss_policy_1: 0.05261
	accuracy_policy_1: 0.67848
	loss_value_1: 0.06503
	loss_reward_1: 0.00639
	loss_policy_2: 0.05292
	accuracy_policy_2: 0.67379
	loss_value_2: 0.06654
	loss_reward_2: 0.00868
	loss_policy_3: 0.05344
	accuracy_policy_3: 0.66973
	loss_value_3: 0.06827
	loss_reward_3: 0.01091
	loss_policy_4: 0.05319
	accuracy_policy_4: 0.67809
	loss_value_4: 0.0698
	loss_reward_4: 0.01362
	loss_policy_5: 0.05323
	accuracy_policy_5: 0.68152
	loss_value_5: 0.07168
	loss_reward_5: 0.01483
	loss_policy: 0.52509
	loss_value: 0.66002
	loss_reward: 0.05443
Optimization_Done 44400
[2025-05-07 23:20:28] [command] train weight_iter_44400.pkl 204 223
[2025-05-07 23:20:35] nn step 44450, lr: 0.1.
	loss_policy_0: 0.25185
	accuracy_policy_0: 0.68707
	loss_value_0: 0.31955
	loss_policy_1: 0.05084
	accuracy_policy_1: 0.67711
	loss_value_1: 0.06453
	loss_reward_1: 0.00619
	loss_policy_2: 0.05142
	accuracy_policy_2: 0.67004
	loss_value_2: 0.06598
	loss_reward_2: 0.00828
	loss_policy_3: 0.05184
	accuracy_policy_3: 0.66406
	loss_value_3: 0.06729
	loss_reward_3: 0.01057
	loss_policy_4: 0.05203
	accuracy_policy_4: 0.67129
	loss_value_4: 0.06888
	loss_reward_4: 0.01337
	loss_policy_5: 0.05169
	accuracy_policy_5: 0.68043
	loss_value_5: 0.07013
	loss_reward_5: 0.01403
	loss_policy: 0.50967
	loss_value: 0.65637
	loss_reward: 0.05244
[2025-05-07 23:20:43] nn step 44500, lr: 0.1.
	loss_policy_0: 0.25855
	accuracy_policy_0: 0.68906
	loss_value_0: 0.31869
	loss_policy_1: 0.05226
	accuracy_policy_1: 0.6743
	loss_value_1: 0.06466
	loss_reward_1: 0.00658
	loss_policy_2: 0.05226
	accuracy_policy_2: 0.67039
	loss_value_2: 0.06609
	loss_reward_2: 0.0084
	loss_policy_3: 0.05263
	accuracy_policy_3: 0.67453
	loss_value_3: 0.06797
	loss_reward_3: 0.01059
	loss_policy_4: 0.05291
	accuracy_policy_4: 0.6727
	loss_value_4: 0.06928
	loss_reward_4: 0.01389
	loss_policy_5: 0.053
	accuracy_policy_5: 0.68098
	loss_value_5: 0.071
	loss_reward_5: 0.01449
	loss_policy: 0.52161
	loss_value: 0.65768
	loss_reward: 0.05395
[2025-05-07 23:20:51] nn step 44550, lr: 0.1.
	loss_policy_0: 0.26808
	accuracy_policy_0: 0.69145
	loss_value_0: 0.33151
	loss_policy_1: 0.05392
	accuracy_policy_1: 0.67887
	loss_value_1: 0.06755
	loss_reward_1: 0.00699
	loss_policy_2: 0.05443
	accuracy_policy_2: 0.67617
	loss_value_2: 0.06921
	loss_reward_2: 0.00858
	loss_policy_3: 0.0546
	accuracy_policy_3: 0.67129
	loss_value_3: 0.07089
	loss_reward_3: 0.01142
	loss_policy_4: 0.0548
	accuracy_policy_4: 0.67168
	loss_value_4: 0.07248
	loss_reward_4: 0.01407
	loss_policy_5: 0.05481
	accuracy_policy_5: 0.6825
	loss_value_5: 0.07393
	loss_reward_5: 0.01559
	loss_policy: 0.54064
	loss_value: 0.68558
	loss_reward: 0.05664
[2025-05-07 23:20:57] nn step 44600, lr: 0.1.
	loss_policy_0: 0.24996
	accuracy_policy_0: 0.69234
	loss_value_0: 0.30617
	loss_policy_1: 0.05027
	accuracy_policy_1: 0.67938
	loss_value_1: 0.06217
	loss_reward_1: 0.00613
	loss_policy_2: 0.05099
	accuracy_policy_2: 0.67348
	loss_value_2: 0.06385
	loss_reward_2: 0.00783
	loss_policy_3: 0.05157
	accuracy_policy_3: 0.67121
	loss_value_3: 0.06543
	loss_reward_3: 0.01081
	loss_policy_4: 0.05146
	accuracy_policy_4: 0.67789
	loss_value_4: 0.06703
	loss_reward_4: 0.0133
	loss_policy_5: 0.0511
	accuracy_policy_5: 0.68426
	loss_value_5: 0.06845
	loss_reward_5: 0.01439
	loss_policy: 0.50535
	loss_value: 0.6331
	loss_reward: 0.05246
Optimization_Done 44600
[2025-05-07 23:24:21] [command] train weight_iter_44600.pkl 205 224
[2025-05-07 23:24:30] nn step 44650, lr: 0.1.
	loss_policy_0: 0.24606
	accuracy_policy_0: 0.6982
	loss_value_0: 0.31374
	loss_policy_1: 0.05022
	accuracy_policy_1: 0.68496
	loss_value_1: 0.06365
	loss_reward_1: 0.00626
	loss_policy_2: 0.0505
	accuracy_policy_2: 0.68133
	loss_value_2: 0.06506
	loss_reward_2: 0.00812
	loss_policy_3: 0.05053
	accuracy_policy_3: 0.68203
	loss_value_3: 0.0666
	loss_reward_3: 0.01003
	loss_policy_4: 0.05094
	accuracy_policy_4: 0.67758
	loss_value_4: 0.06841
	loss_reward_4: 0.01321
	loss_policy_5: 0.05066
	accuracy_policy_5: 0.68879
	loss_value_5: 0.06952
	loss_reward_5: 0.01403
	loss_policy: 0.49892
	loss_value: 0.64698
	loss_reward: 0.05164
[2025-05-07 23:24:38] nn step 44700, lr: 0.1.
	loss_policy_0: 0.26165
	accuracy_policy_0: 0.69324
	loss_value_0: 0.32331
	loss_policy_1: 0.0526
	accuracy_policy_1: 0.68531
	loss_value_1: 0.06591
	loss_reward_1: 0.00652
	loss_policy_2: 0.05317
	accuracy_policy_2: 0.67656
	loss_value_2: 0.06771
	loss_reward_2: 0.00836
	loss_policy_3: 0.05344
	accuracy_policy_3: 0.67324
	loss_value_3: 0.06961
	loss_reward_3: 0.01087
	loss_policy_4: 0.05352
	accuracy_policy_4: 0.6816
	loss_value_4: 0.07118
	loss_reward_4: 0.01397
	loss_policy_5: 0.05359
	accuracy_policy_5: 0.69039
	loss_value_5: 0.07266
	loss_reward_5: 0.0146
	loss_policy: 0.52798
	loss_value: 0.67038
	loss_reward: 0.05433
[2025-05-07 23:24:45] nn step 44750, lr: 0.1.
	loss_policy_0: 0.26779
	accuracy_policy_0: 0.69422
	loss_value_0: 0.32867
	loss_policy_1: 0.05371
	accuracy_policy_1: 0.68465
	loss_value_1: 0.06717
	loss_reward_1: 0.00669
	loss_policy_2: 0.05416
	accuracy_policy_2: 0.68301
	loss_value_2: 0.0691
	loss_reward_2: 0.00885
	loss_policy_3: 0.05476
	accuracy_policy_3: 0.67133
	loss_value_3: 0.07096
	loss_reward_3: 0.01167
	loss_policy_4: 0.05449
	accuracy_policy_4: 0.68117
	loss_value_4: 0.07241
	loss_reward_4: 0.01428
	loss_policy_5: 0.05452
	accuracy_policy_5: 0.68637
	loss_value_5: 0.07391
	loss_reward_5: 0.01543
	loss_policy: 0.53944
	loss_value: 0.68222
	loss_reward: 0.05691
[2025-05-07 23:24:53] nn step 44800, lr: 0.1.
	loss_policy_0: 0.25989
	accuracy_policy_0: 0.70047
	loss_value_0: 0.3194
	loss_policy_1: 0.0526
	accuracy_policy_1: 0.68617
	loss_value_1: 0.06506
	loss_reward_1: 0.00673
	loss_policy_2: 0.05287
	accuracy_policy_2: 0.67945
	loss_value_2: 0.06697
	loss_reward_2: 0.00836
	loss_policy_3: 0.05329
	accuracy_policy_3: 0.67715
	loss_value_3: 0.06857
	loss_reward_3: 0.01111
	loss_policy_4: 0.05367
	accuracy_policy_4: 0.68082
	loss_value_4: 0.07038
	loss_reward_4: 0.01424
	loss_policy_5: 0.05342
	accuracy_policy_5: 0.68402
	loss_value_5: 0.07227
	loss_reward_5: 0.01468
	loss_policy: 0.52574
	loss_value: 0.66265
	loss_reward: 0.05511
Optimization_Done 44800
[2025-05-07 23:28:21] [command] train weight_iter_44800.pkl 206 225
[2025-05-07 23:28:29] nn step 44850, lr: 0.1.
	loss_policy_0: 0.25299
	accuracy_policy_0: 0.69832
	loss_value_0: 0.32335
	loss_policy_1: 0.0514
	accuracy_policy_1: 0.6832
	loss_value_1: 0.06575
	loss_reward_1: 0.00651
	loss_policy_2: 0.05172
	accuracy_policy_2: 0.67898
	loss_value_2: 0.0675
	loss_reward_2: 0.00837
	loss_policy_3: 0.05203
	accuracy_policy_3: 0.67965
	loss_value_3: 0.06857
	loss_reward_3: 0.0107
	loss_policy_4: 0.05248
	accuracy_policy_4: 0.67746
	loss_value_4: 0.07004
	loss_reward_4: 0.01361
	loss_policy_5: 0.05193
	accuracy_policy_5: 0.68305
	loss_value_5: 0.07178
	loss_reward_5: 0.01482
	loss_policy: 0.51255
	loss_value: 0.66699
	loss_reward: 0.05401
[2025-05-07 23:28:36] nn step 44900, lr: 0.1.
	loss_policy_0: 0.27455
	accuracy_policy_0: 0.69781
	loss_value_0: 0.34543
	loss_policy_1: 0.05534
	accuracy_policy_1: 0.6868
	loss_value_1: 0.07012
	loss_reward_1: 0.0071
	loss_policy_2: 0.05625
	accuracy_policy_2: 0.67922
	loss_value_2: 0.07201
	loss_reward_2: 0.00961
	loss_policy_3: 0.05666
	accuracy_policy_3: 0.67754
	loss_value_3: 0.07365
	loss_reward_3: 0.01193
	loss_policy_4: 0.05649
	accuracy_policy_4: 0.67969
	loss_value_4: 0.07488
	loss_reward_4: 0.01479
	loss_policy_5: 0.05636
	accuracy_policy_5: 0.68879
	loss_value_5: 0.07717
	loss_reward_5: 0.01611
	loss_policy: 0.55564
	loss_value: 0.71324
	loss_reward: 0.05954
[2025-05-07 23:28:44] nn step 44950, lr: 0.1.
	loss_policy_0: 0.25577
	accuracy_policy_0: 0.69848
	loss_value_0: 0.31623
	loss_policy_1: 0.05203
	accuracy_policy_1: 0.6882
	loss_value_1: 0.06421
	loss_reward_1: 0.00668
	loss_policy_2: 0.05216
	accuracy_policy_2: 0.68238
	loss_value_2: 0.066
	loss_reward_2: 0.00831
	loss_policy_3: 0.05224
	accuracy_policy_3: 0.68031
	loss_value_3: 0.06771
	loss_reward_3: 0.01085
	loss_policy_4: 0.05262
	accuracy_policy_4: 0.6766
	loss_value_4: 0.06914
	loss_reward_4: 0.01388
	loss_policy_5: 0.05291
	accuracy_policy_5: 0.68336
	loss_value_5: 0.07072
	loss_reward_5: 0.01473
	loss_policy: 0.51773
	loss_value: 0.65402
	loss_reward: 0.05445
[2025-05-07 23:28:52] nn step 45000, lr: 0.1.
	loss_policy_0: 0.25433
	accuracy_policy_0: 0.7075
	loss_value_0: 0.3176
	loss_policy_1: 0.05173
	accuracy_policy_1: 0.69
	loss_value_1: 0.06485
	loss_reward_1: 0.00634
	loss_policy_2: 0.05244
	accuracy_policy_2: 0.6777
	loss_value_2: 0.06667
	loss_reward_2: 0.0084
	loss_policy_3: 0.05267
	accuracy_policy_3: 0.67828
	loss_value_3: 0.06831
	loss_reward_3: 0.01067
	loss_policy_4: 0.05299
	accuracy_policy_4: 0.68422
	loss_value_4: 0.0699
	loss_reward_4: 0.01373
	loss_policy_5: 0.05289
	accuracy_policy_5: 0.68887
	loss_value_5: 0.07158
	loss_reward_5: 0.01488
	loss_policy: 0.51705
	loss_value: 0.65891
	loss_reward: 0.05402
Optimization_Done 45000
[2025-05-07 23:32:13] [command] train weight_iter_45000.pkl 207 226
[2025-05-07 23:32:20] nn step 45050, lr: 0.1.
	loss_policy_0: 0.23477
	accuracy_policy_0: 0.70145
	loss_value_0: 0.30382
	loss_policy_1: 0.04742
	accuracy_policy_1: 0.69082
	loss_value_1: 0.06164
	loss_reward_1: 0.006
	loss_policy_2: 0.04775
	accuracy_policy_2: 0.6866
	loss_value_2: 0.06309
	loss_reward_2: 0.00748
	loss_policy_3: 0.04812
	accuracy_policy_3: 0.68551
	loss_value_3: 0.06471
	loss_reward_3: 0.01004
	loss_policy_4: 0.04828
	accuracy_policy_4: 0.68219
	loss_value_4: 0.06608
	loss_reward_4: 0.01265
	loss_policy_5: 0.04822
	accuracy_policy_5: 0.69719
	loss_value_5: 0.06722
	loss_reward_5: 0.01346
	loss_policy: 0.47456
	loss_value: 0.62656
	loss_reward: 0.04963
[2025-05-07 23:32:28] nn step 45100, lr: 0.1.
	loss_policy_0: 0.25232
	accuracy_policy_0: 0.70551
	loss_value_0: 0.32082
	loss_policy_1: 0.05177
	accuracy_policy_1: 0.68773
	loss_value_1: 0.06539
	loss_reward_1: 0.00658
	loss_policy_2: 0.05172
	accuracy_policy_2: 0.68461
	loss_value_2: 0.06689
	loss_reward_2: 0.00825
	loss_policy_3: 0.05209
	accuracy_policy_3: 0.68125
	loss_value_3: 0.06822
	loss_reward_3: 0.01029
	loss_policy_4: 0.05205
	accuracy_policy_4: 0.68453
	loss_value_4: 0.06981
	loss_reward_4: 0.01366
	loss_policy_5: 0.05211
	accuracy_policy_5: 0.69582
	loss_value_5: 0.07186
	loss_reward_5: 0.01455
	loss_policy: 0.51205
	loss_value: 0.66299
	loss_reward: 0.05333
[2025-05-07 23:32:36] nn step 45150, lr: 0.1.
	loss_policy_0: 0.23638
	accuracy_policy_0: 0.70875
	loss_value_0: 0.29698
	loss_policy_1: 0.04789
	accuracy_policy_1: 0.69492
	loss_value_1: 0.06066
	loss_reward_1: 0.00592
	loss_policy_2: 0.04827
	accuracy_policy_2: 0.69023
	loss_value_2: 0.06231
	loss_reward_2: 0.00775
	loss_policy_3: 0.04909
	accuracy_policy_3: 0.67922
	loss_value_3: 0.06379
	loss_reward_3: 0.00995
	loss_policy_4: 0.04876
	accuracy_policy_4: 0.68559
	loss_value_4: 0.06539
	loss_reward_4: 0.01244
	loss_policy_5: 0.04892
	accuracy_policy_5: 0.69215
	loss_value_5: 0.06662
	loss_reward_5: 0.01366
	loss_policy: 0.4793
	loss_value: 0.61575
	loss_reward: 0.04973
[2025-05-07 23:32:44] nn step 45200, lr: 0.1.
	loss_policy_0: 0.23698
	accuracy_policy_0: 0.7048
	loss_value_0: 0.29399
	loss_policy_1: 0.04817
	accuracy_policy_1: 0.68883
	loss_value_1: 0.06025
	loss_reward_1: 0.0059
	loss_policy_2: 0.04818
	accuracy_policy_2: 0.69062
	loss_value_2: 0.06204
	loss_reward_2: 0.00749
	loss_policy_3: 0.04853
	accuracy_policy_3: 0.67887
	loss_value_3: 0.06339
	loss_reward_3: 0.01006
	loss_policy_4: 0.04903
	accuracy_policy_4: 0.68215
	loss_value_4: 0.06511
	loss_reward_4: 0.01263
	loss_policy_5: 0.04851
	accuracy_policy_5: 0.69199
	loss_value_5: 0.06676
	loss_reward_5: 0.01342
	loss_policy: 0.47939
	loss_value: 0.61153
	loss_reward: 0.04951
Optimization_Done 45200
[2025-05-07 23:36:13] [command] train weight_iter_45200.pkl 208 227
[2025-05-07 23:36:22] nn step 45250, lr: 0.1.
	loss_policy_0: 0.25766
	accuracy_policy_0: 0.69246
	loss_value_0: 0.3315
	loss_policy_1: 0.0519
	accuracy_policy_1: 0.68422
	loss_value_1: 0.06745
	loss_reward_1: 0.00678
	loss_policy_2: 0.05232
	accuracy_policy_2: 0.67734
	loss_value_2: 0.069
	loss_reward_2: 0.00836
	loss_policy_3: 0.05287
	accuracy_policy_3: 0.67684
	loss_value_3: 0.07055
	loss_reward_3: 0.01159
	loss_policy_4: 0.05262
	accuracy_policy_4: 0.67887
	loss_value_4: 0.07193
	loss_reward_4: 0.01384
	loss_policy_5: 0.05269
	accuracy_policy_5: 0.68371
	loss_value_5: 0.07361
	loss_reward_5: 0.01458
	loss_policy: 0.52007
	loss_value: 0.68403
	loss_reward: 0.05515
[2025-05-07 23:36:30] nn step 45300, lr: 0.1.
	loss_policy_0: 0.24646
	accuracy_policy_0: 0.69875
	loss_value_0: 0.31216
	loss_policy_1: 0.05006
	accuracy_policy_1: 0.68352
	loss_value_1: 0.06324
	loss_reward_1: 0.00627
	loss_policy_2: 0.04997
	accuracy_policy_2: 0.68418
	loss_value_2: 0.06478
	loss_reward_2: 0.00788
	loss_policy_3: 0.05059
	accuracy_policy_3: 0.67711
	loss_value_3: 0.06632
	loss_reward_3: 0.01057
	loss_policy_4: 0.05069
	accuracy_policy_4: 0.68039
	loss_value_4: 0.06774
	loss_reward_4: 0.01308
	loss_policy_5: 0.05055
	accuracy_policy_5: 0.6857
	loss_value_5: 0.06951
	loss_reward_5: 0.01378
	loss_policy: 0.49833
	loss_value: 0.64375
	loss_reward: 0.05158
[2025-05-07 23:36:37] nn step 45350, lr: 0.1.
	loss_policy_0: 0.25351
	accuracy_policy_0: 0.69668
	loss_value_0: 0.31727
	loss_policy_1: 0.05135
	accuracy_policy_1: 0.68109
	loss_value_1: 0.06481
	loss_reward_1: 0.00639
	loss_policy_2: 0.05172
	accuracy_policy_2: 0.68344
	loss_value_2: 0.06642
	loss_reward_2: 0.00842
	loss_policy_3: 0.05196
	accuracy_policy_3: 0.67754
	loss_value_3: 0.0681
	loss_reward_3: 0.01095
	loss_policy_4: 0.05224
	accuracy_policy_4: 0.6782
	loss_value_4: 0.0694
	loss_reward_4: 0.01314
	loss_policy_5: 0.05196
	accuracy_policy_5: 0.69258
	loss_value_5: 0.07113
	loss_reward_5: 0.01472
	loss_policy: 0.51274
	loss_value: 0.65712
	loss_reward: 0.05364
[2025-05-07 23:36:45] nn step 45400, lr: 0.1.
	loss_policy_0: 0.25328
	accuracy_policy_0: 0.70312
	loss_value_0: 0.31981
	loss_policy_1: 0.05197
	accuracy_policy_1: 0.68141
	loss_value_1: 0.06517
	loss_reward_1: 0.00647
	loss_policy_2: 0.05236
	accuracy_policy_2: 0.68074
	loss_value_2: 0.06655
	loss_reward_2: 0.00826
	loss_policy_3: 0.05252
	accuracy_policy_3: 0.6802
	loss_value_3: 0.0683
	loss_reward_3: 0.01114
	loss_policy_4: 0.05261
	accuracy_policy_4: 0.67641
	loss_value_4: 0.0698
	loss_reward_4: 0.01348
	loss_policy_5: 0.05262
	accuracy_policy_5: 0.68844
	loss_value_5: 0.07139
	loss_reward_5: 0.01484
	loss_policy: 0.51536
	loss_value: 0.66101
	loss_reward: 0.05418
Optimization_Done 45400
[2025-05-07 23:40:03] [command] train weight_iter_45400.pkl 209 228
[2025-05-07 23:40:12] nn step 45450, lr: 0.1.
	loss_policy_0: 0.25147
	accuracy_policy_0: 0.70367
	loss_value_0: 0.32365
	loss_policy_1: 0.05124
	accuracy_policy_1: 0.68297
	loss_value_1: 0.06553
	loss_reward_1: 0.0064
	loss_policy_2: 0.05178
	accuracy_policy_2: 0.67641
	loss_value_2: 0.06698
	loss_reward_2: 0.00852
	loss_policy_3: 0.05195
	accuracy_policy_3: 0.67793
	loss_value_3: 0.06803
	loss_reward_3: 0.01083
	loss_policy_4: 0.05182
	accuracy_policy_4: 0.68535
	loss_value_4: 0.06986
	loss_reward_4: 0.01325
	loss_policy_5: 0.05175
	accuracy_policy_5: 0.69633
	loss_value_5: 0.07127
	loss_reward_5: 0.01436
	loss_policy: 0.51001
	loss_value: 0.66532
	loss_reward: 0.05336
[2025-05-07 23:40:20] nn step 45500, lr: 0.1.
	loss_policy_0: 0.24848
	accuracy_policy_0: 0.70168
	loss_value_0: 0.31254
	loss_policy_1: 0.05059
	accuracy_policy_1: 0.68156
	loss_value_1: 0.06368
	loss_reward_1: 0.00616
	loss_policy_2: 0.05064
	accuracy_policy_2: 0.68301
	loss_value_2: 0.06541
	loss_reward_2: 0.00822
	loss_policy_3: 0.05078
	accuracy_policy_3: 0.6807
	loss_value_3: 0.0669
	loss_reward_3: 0.01083
	loss_policy_4: 0.05122
	accuracy_policy_4: 0.67805
	loss_value_4: 0.06843
	loss_reward_4: 0.01337
	loss_policy_5: 0.05132
	accuracy_policy_5: 0.69098
	loss_value_5: 0.06983
	loss_reward_5: 0.01456
	loss_policy: 0.50303
	loss_value: 0.64679
	loss_reward: 0.05314
[2025-05-07 23:40:26] nn step 45550, lr: 0.1.
	loss_policy_0: 0.26287
	accuracy_policy_0: 0.69996
	loss_value_0: 0.32697
	loss_policy_1: 0.05346
	accuracy_policy_1: 0.68715
	loss_value_1: 0.06721
	loss_reward_1: 0.00679
	loss_policy_2: 0.05403
	accuracy_policy_2: 0.67516
	loss_value_2: 0.06893
	loss_reward_2: 0.00898
	loss_policy_3: 0.05373
	accuracy_policy_3: 0.67852
	loss_value_3: 0.07064
	loss_reward_3: 0.01153
	loss_policy_4: 0.05411
	accuracy_policy_4: 0.67898
	loss_value_4: 0.07193
	loss_reward_4: 0.01378
	loss_policy_5: 0.054
	accuracy_policy_5: 0.68652
	loss_value_5: 0.07365
	loss_reward_5: 0.0149
	loss_policy: 0.53218
	loss_value: 0.67932
	loss_reward: 0.05598
[2025-05-07 23:40:34] nn step 45600, lr: 0.1.
	loss_policy_0: 0.26846
	accuracy_policy_0: 0.70297
	loss_value_0: 0.33182
	loss_policy_1: 0.0542
	accuracy_policy_1: 0.68656
	loss_value_1: 0.06734
	loss_reward_1: 0.00674
	loss_policy_2: 0.05457
	accuracy_policy_2: 0.68426
	loss_value_2: 0.06908
	loss_reward_2: 0.00894
	loss_policy_3: 0.05537
	accuracy_policy_3: 0.67484
	loss_value_3: 0.07083
	loss_reward_3: 0.01157
	loss_policy_4: 0.0556
	accuracy_policy_4: 0.67477
	loss_value_4: 0.07251
	loss_reward_4: 0.0147
	loss_policy_5: 0.05505
	accuracy_policy_5: 0.68477
	loss_value_5: 0.07422
	loss_reward_5: 0.01557
	loss_policy: 0.54324
	loss_value: 0.6858
	loss_reward: 0.05752
Optimization_Done 45600
[2025-05-07 23:44:02] [command] train weight_iter_45600.pkl 210 229
[2025-05-07 23:44:10] nn step 45650, lr: 0.1.
	loss_policy_0: 0.25427
	accuracy_policy_0: 0.69559
	loss_value_0: 0.32738
	loss_policy_1: 0.05172
	accuracy_policy_1: 0.67914
	loss_value_1: 0.0667
	loss_reward_1: 0.00633
	loss_policy_2: 0.05192
	accuracy_policy_2: 0.67898
	loss_value_2: 0.06824
	loss_reward_2: 0.00875
	loss_policy_3: 0.05211
	accuracy_policy_3: 0.67902
	loss_value_3: 0.06968
	loss_reward_3: 0.0109
	loss_policy_4: 0.05214
	accuracy_policy_4: 0.67676
	loss_value_4: 0.071
	loss_reward_4: 0.01351
	loss_policy_5: 0.05224
	accuracy_policy_5: 0.68344
	loss_value_5: 0.07244
	loss_reward_5: 0.01494
	loss_policy: 0.5144
	loss_value: 0.67544
	loss_reward: 0.05443
[2025-05-07 23:44:18] nn step 45700, lr: 0.1.
	loss_policy_0: 0.25968
	accuracy_policy_0: 0.68961
	loss_value_0: 0.3259
	loss_policy_1: 0.05262
	accuracy_policy_1: 0.67984
	loss_value_1: 0.06618
	loss_reward_1: 0.0066
	loss_policy_2: 0.05277
	accuracy_policy_2: 0.67609
	loss_value_2: 0.06775
	loss_reward_2: 0.00852
	loss_policy_3: 0.0536
	accuracy_policy_3: 0.66473
	loss_value_3: 0.06941
	loss_reward_3: 0.01127
	loss_policy_4: 0.05336
	accuracy_policy_4: 0.67281
	loss_value_4: 0.07091
	loss_reward_4: 0.01398
	loss_policy_5: 0.05338
	accuracy_policy_5: 0.67566
	loss_value_5: 0.0723
	loss_reward_5: 0.01533
	loss_policy: 0.52541
	loss_value: 0.67245
	loss_reward: 0.05569
[2025-05-07 23:44:26] nn step 45750, lr: 0.1.
	loss_policy_0: 0.25847
	accuracy_policy_0: 0.69469
	loss_value_0: 0.32617
	loss_policy_1: 0.05245
	accuracy_policy_1: 0.6775
	loss_value_1: 0.06603
	loss_reward_1: 0.00688
	loss_policy_2: 0.05275
	accuracy_policy_2: 0.67582
	loss_value_2: 0.0677
	loss_reward_2: 0.00871
	loss_policy_3: 0.05321
	accuracy_policy_3: 0.67359
	loss_value_3: 0.06911
	loss_reward_3: 0.01124
	loss_policy_4: 0.05332
	accuracy_policy_4: 0.67262
	loss_value_4: 0.07039
	loss_reward_4: 0.01464
	loss_policy_5: 0.0535
	accuracy_policy_5: 0.68309
	loss_value_5: 0.0722
	loss_reward_5: 0.01572
	loss_policy: 0.52369
	loss_value: 0.67161
	loss_reward: 0.05719
[2025-05-07 23:44:34] nn step 45800, lr: 0.1.
	loss_policy_0: 0.24435
	accuracy_policy_0: 0.69973
	loss_value_0: 0.30228
	loss_policy_1: 0.04939
	accuracy_policy_1: 0.68781
	loss_value_1: 0.06188
	loss_reward_1: 0.00608
	loss_policy_2: 0.04993
	accuracy_policy_2: 0.67848
	loss_value_2: 0.06346
	loss_reward_2: 0.00804
	loss_policy_3: 0.05036
	accuracy_policy_3: 0.67641
	loss_value_3: 0.06488
	loss_reward_3: 0.01091
	loss_policy_4: 0.05069
	accuracy_policy_4: 0.67504
	loss_value_4: 0.06644
	loss_reward_4: 0.01308
	loss_policy_5: 0.05053
	accuracy_policy_5: 0.6843
	loss_value_5: 0.06794
	loss_reward_5: 0.01412
	loss_policy: 0.49524
	loss_value: 0.62688
	loss_reward: 0.05224
Optimization_Done 45800
[2025-05-07 23:47:54] [command] train weight_iter_45800.pkl 211 230
[2025-05-07 23:48:04] nn step 45850, lr: 0.1.
	loss_policy_0: 0.2515
	accuracy_policy_0: 0.69031
	loss_value_0: 0.31675
	loss_policy_1: 0.05125
	accuracy_policy_1: 0.67305
	loss_value_1: 0.06427
	loss_reward_1: 0.00626
	loss_policy_2: 0.05163
	accuracy_policy_2: 0.67008
	loss_value_2: 0.06563
	loss_reward_2: 0.00802
	loss_policy_3: 0.05154
	accuracy_policy_3: 0.67312
	loss_value_3: 0.06713
	loss_reward_3: 0.01003
	loss_policy_4: 0.05178
	accuracy_policy_4: 0.66797
	loss_value_4: 0.06831
	loss_reward_4: 0.01314
	loss_policy_5: 0.05158
	accuracy_policy_5: 0.67586
	loss_value_5: 0.06994
	loss_reward_5: 0.01441
	loss_policy: 0.50928
	loss_value: 0.65204
	loss_reward: 0.05184
[2025-05-07 23:48:12] nn step 45900, lr: 0.1.
	loss_policy_0: 0.24965
	accuracy_policy_0: 0.69395
	loss_value_0: 0.30956
	loss_policy_1: 0.05132
	accuracy_policy_1: 0.66914
	loss_value_1: 0.06282
	loss_reward_1: 0.00624
	loss_policy_2: 0.05137
	accuracy_policy_2: 0.66785
	loss_value_2: 0.06464
	loss_reward_2: 0.008
	loss_policy_3: 0.0515
	accuracy_policy_3: 0.66875
	loss_value_3: 0.06598
	loss_reward_3: 0.01016
	loss_policy_4: 0.05153
	accuracy_policy_4: 0.66895
	loss_value_4: 0.0675
	loss_reward_4: 0.01312
	loss_policy_5: 0.05177
	accuracy_policy_5: 0.67559
	loss_value_5: 0.06898
	loss_reward_5: 0.01423
	loss_policy: 0.50713
	loss_value: 0.63947
	loss_reward: 0.05174
[2025-05-07 23:48:20] nn step 45950, lr: 0.1.
	loss_policy_0: 0.26271
	accuracy_policy_0: 0.68844
	loss_value_0: 0.32467
	loss_policy_1: 0.05329
	accuracy_policy_1: 0.67582
	loss_value_1: 0.06604
	loss_reward_1: 0.00664
	loss_policy_2: 0.05402
	accuracy_policy_2: 0.66895
	loss_value_2: 0.06773
	loss_reward_2: 0.00863
	loss_policy_3: 0.05417
	accuracy_policy_3: 0.66488
	loss_value_3: 0.06936
	loss_reward_3: 0.01096
	loss_policy_4: 0.05416
	accuracy_policy_4: 0.67234
	loss_value_4: 0.07103
	loss_reward_4: 0.01394
	loss_policy_5: 0.05399
	accuracy_policy_5: 0.67832
	loss_value_5: 0.07258
	loss_reward_5: 0.01508
	loss_policy: 0.53234
	loss_value: 0.6714
	loss_reward: 0.05525
[2025-05-07 23:48:26] nn step 46000, lr: 0.1.
	loss_policy_0: 0.27185
	accuracy_policy_0: 0.68398
	loss_value_0: 0.32826
	loss_policy_1: 0.05445
	accuracy_policy_1: 0.67566
	loss_value_1: 0.06708
	loss_reward_1: 0.00687
	loss_policy_2: 0.05475
	accuracy_policy_2: 0.67062
	loss_value_2: 0.06853
	loss_reward_2: 0.00889
	loss_policy_3: 0.05544
	accuracy_policy_3: 0.67344
	loss_value_3: 0.07058
	loss_reward_3: 0.01154
	loss_policy_4: 0.05545
	accuracy_policy_4: 0.67035
	loss_value_4: 0.07245
	loss_reward_4: 0.01407
	loss_policy_5: 0.05562
	accuracy_policy_5: 0.67262
	loss_value_5: 0.07396
	loss_reward_5: 0.01537
	loss_policy: 0.54757
	loss_value: 0.68086
	loss_reward: 0.05674
Optimization_Done 46000
[2025-05-07 23:51:55] [command] train weight_iter_46000.pkl 212 231
[2025-05-07 23:52:04] nn step 46050, lr: 0.1.
	loss_policy_0: 0.25418
	accuracy_policy_0: 0.69129
	loss_value_0: 0.32532
	loss_policy_1: 0.05192
	accuracy_policy_1: 0.68141
	loss_value_1: 0.06578
	loss_reward_1: 0.00672
	loss_policy_2: 0.05207
	accuracy_policy_2: 0.67426
	loss_value_2: 0.06743
	loss_reward_2: 0.00833
	loss_policy_3: 0.05253
	accuracy_policy_3: 0.6723
	loss_value_3: 0.06888
	loss_reward_3: 0.01091
	loss_policy_4: 0.05257
	accuracy_policy_4: 0.67559
	loss_value_4: 0.07044
	loss_reward_4: 0.01342
	loss_policy_5: 0.05269
	accuracy_policy_5: 0.68098
	loss_value_5: 0.07152
	loss_reward_5: 0.01475
	loss_policy: 0.51595
	loss_value: 0.66937
	loss_reward: 0.05413
[2025-05-07 23:52:10] nn step 46100, lr: 0.1.
	loss_policy_0: 0.26366
	accuracy_policy_0: 0.69371
	loss_value_0: 0.33069
	loss_policy_1: 0.05378
	accuracy_policy_1: 0.67723
	loss_value_1: 0.06715
	loss_reward_1: 0.0066
	loss_policy_2: 0.05392
	accuracy_policy_2: 0.67484
	loss_value_2: 0.06938
	loss_reward_2: 0.00871
	loss_policy_3: 0.0542
	accuracy_policy_3: 0.67156
	loss_value_3: 0.07091
	loss_reward_3: 0.01144
	loss_policy_4: 0.05412
	accuracy_policy_4: 0.67809
	loss_value_4: 0.07236
	loss_reward_4: 0.01429
	loss_policy_5: 0.05445
	accuracy_policy_5: 0.67844
	loss_value_5: 0.07387
	loss_reward_5: 0.01519
	loss_policy: 0.53413
	loss_value: 0.68436
	loss_reward: 0.05624
[2025-05-07 23:52:18] nn step 46150, lr: 0.1.
	loss_policy_0: 0.26246
	accuracy_policy_0: 0.69148
	loss_value_0: 0.3247
	loss_policy_1: 0.05325
	accuracy_policy_1: 0.6777
	loss_value_1: 0.066
	loss_reward_1: 0.0066
	loss_policy_2: 0.05382
	accuracy_policy_2: 0.67145
	loss_value_2: 0.06765
	loss_reward_2: 0.00841
	loss_policy_3: 0.05415
	accuracy_policy_3: 0.67137
	loss_value_3: 0.0693
	loss_reward_3: 0.01127
	loss_policy_4: 0.05446
	accuracy_policy_4: 0.67328
	loss_value_4: 0.07108
	loss_reward_4: 0.0139
	loss_policy_5: 0.05438
	accuracy_policy_5: 0.67449
	loss_value_5: 0.07273
	loss_reward_5: 0.01509
	loss_policy: 0.53252
	loss_value: 0.67146
	loss_reward: 0.05527
[2025-05-07 23:52:26] nn step 46200, lr: 0.1.
	loss_policy_0: 0.27492
	accuracy_policy_0: 0.69375
	loss_value_0: 0.3352
	loss_policy_1: 0.05567
	accuracy_policy_1: 0.68062
	loss_value_1: 0.06835
	loss_reward_1: 0.00698
	loss_policy_2: 0.05623
	accuracy_policy_2: 0.67195
	loss_value_2: 0.07033
	loss_reward_2: 0.00899
	loss_policy_3: 0.0565
	accuracy_policy_3: 0.67477
	loss_value_3: 0.07203
	loss_reward_3: 0.01209
	loss_policy_4: 0.05656
	accuracy_policy_4: 0.67434
	loss_value_4: 0.07386
	loss_reward_4: 0.01437
	loss_policy_5: 0.05661
	accuracy_policy_5: 0.6809
	loss_value_5: 0.0754
	loss_reward_5: 0.01546
	loss_policy: 0.55649
	loss_value: 0.69516
	loss_reward: 0.05789
Optimization_Done 46200
[2025-05-07 23:55:43] [command] train weight_iter_46200.pkl 213 232
[2025-05-07 23:55:52] nn step 46250, lr: 0.1.
	loss_policy_0: 0.25927
	accuracy_policy_0: 0.68891
	loss_value_0: 0.33258
	loss_policy_1: 0.05311
	accuracy_policy_1: 0.67309
	loss_value_1: 0.06727
	loss_reward_1: 0.00657
	loss_policy_2: 0.05354
	accuracy_policy_2: 0.67051
	loss_value_2: 0.06884
	loss_reward_2: 0.00851
	loss_policy_3: 0.05322
	accuracy_policy_3: 0.67512
	loss_value_3: 0.07045
	loss_reward_3: 0.01094
	loss_policy_4: 0.05389
	accuracy_policy_4: 0.66684
	loss_value_4: 0.07215
	loss_reward_4: 0.01394
	loss_policy_5: 0.05378
	accuracy_policy_5: 0.67547
	loss_value_5: 0.07325
	loss_reward_5: 0.01488
	loss_policy: 0.52682
	loss_value: 0.68453
	loss_reward: 0.05484
[2025-05-07 23:56:01] nn step 46300, lr: 0.1.
	loss_policy_0: 0.2748
	accuracy_policy_0: 0.68676
	loss_value_0: 0.34155
	loss_policy_1: 0.0556
	accuracy_policy_1: 0.67676
	loss_value_1: 0.06945
	loss_reward_1: 0.00691
	loss_policy_2: 0.05572
	accuracy_policy_2: 0.66988
	loss_value_2: 0.07142
	loss_reward_2: 0.00916
	loss_policy_3: 0.05601
	accuracy_policy_3: 0.6698
	loss_value_3: 0.07256
	loss_reward_3: 0.01142
	loss_policy_4: 0.05587
	accuracy_policy_4: 0.66961
	loss_value_4: 0.07426
	loss_reward_4: 0.01434
	loss_policy_5: 0.05583
	accuracy_policy_5: 0.68082
	loss_value_5: 0.07584
	loss_reward_5: 0.01561
	loss_policy: 0.55383
	loss_value: 0.70508
	loss_reward: 0.05744
[2025-05-07 23:56:09] nn step 46350, lr: 0.1.
	loss_policy_0: 0.24654
	accuracy_policy_0: 0.6898
	loss_value_0: 0.30394
	loss_policy_1: 0.04941
	accuracy_policy_1: 0.68148
	loss_value_1: 0.0619
	loss_reward_1: 0.00608
	loss_policy_2: 0.0497
	accuracy_policy_2: 0.67133
	loss_value_2: 0.06364
	loss_reward_2: 0.0079
	loss_policy_3: 0.04987
	accuracy_policy_3: 0.6716
	loss_value_3: 0.06501
	loss_reward_3: 0.01045
	loss_policy_4: 0.0504
	accuracy_policy_4: 0.66637
	loss_value_4: 0.06628
	loss_reward_4: 0.01321
	loss_policy_5: 0.04991
	accuracy_policy_5: 0.6809
	loss_value_5: 0.06776
	loss_reward_5: 0.01387
	loss_policy: 0.49582
	loss_value: 0.62852
	loss_reward: 0.05152
[2025-05-07 23:56:18] nn step 46400, lr: 0.1.
	loss_policy_0: 0.24869
	accuracy_policy_0: 0.69727
	loss_value_0: 0.30602
	loss_policy_1: 0.0507
	accuracy_policy_1: 0.6777
	loss_value_1: 0.06248
	loss_reward_1: 0.00624
	loss_policy_2: 0.05099
	accuracy_policy_2: 0.67668
	loss_value_2: 0.06448
	loss_reward_2: 0.0081
	loss_policy_3: 0.05148
	accuracy_policy_3: 0.66656
	loss_value_3: 0.06641
	loss_reward_3: 0.01084
	loss_policy_4: 0.05162
	accuracy_policy_4: 0.67109
	loss_value_4: 0.06807
	loss_reward_4: 0.01326
	loss_policy_5: 0.05138
	accuracy_policy_5: 0.68141
	loss_value_5: 0.06964
	loss_reward_5: 0.0141
	loss_policy: 0.50487
	loss_value: 0.63709
	loss_reward: 0.05255
Optimization_Done 46400
[2025-05-07 23:59:20] [command] train weight_iter_46400.pkl 214 233
[2025-05-07 23:59:30] nn step 46450, lr: 0.1.
	loss_policy_0: 0.25335
	accuracy_policy_0: 0.69812
	loss_value_0: 0.32056
	loss_policy_1: 0.05164
	accuracy_policy_1: 0.68367
	loss_value_1: 0.06491
	loss_reward_1: 0.00644
	loss_policy_2: 0.0513
	accuracy_policy_2: 0.67879
	loss_value_2: 0.06665
	loss_reward_2: 0.00799
	loss_policy_3: 0.05184
	accuracy_policy_3: 0.67359
	loss_value_3: 0.06788
	loss_reward_3: 0.0111
	loss_policy_4: 0.05219
	accuracy_policy_4: 0.67531
	loss_value_4: 0.06918
	loss_reward_4: 0.01342
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.68188
	loss_value_5: 0.07052
	loss_reward_5: 0.01448
	loss_policy: 0.51214
	loss_value: 0.65969
	loss_reward: 0.05344
[2025-05-07 23:59:38] nn step 46500, lr: 0.1.
	loss_policy_0: 0.27212
	accuracy_policy_0: 0.69121
	loss_value_0: 0.33781
	loss_policy_1: 0.05511
	accuracy_policy_1: 0.68238
	loss_value_1: 0.06892
	loss_reward_1: 0.00702
	loss_policy_2: 0.05545
	accuracy_policy_2: 0.67785
	loss_value_2: 0.07074
	loss_reward_2: 0.00953
	loss_policy_3: 0.0554
	accuracy_policy_3: 0.67527
	loss_value_3: 0.0724
	loss_reward_3: 0.01207
	loss_policy_4: 0.05581
	accuracy_policy_4: 0.67789
	loss_value_4: 0.07372
	loss_reward_4: 0.01482
	loss_policy_5: 0.05584
	accuracy_policy_5: 0.6875
	loss_value_5: 0.07544
	loss_reward_5: 0.01557
	loss_policy: 0.54972
	loss_value: 0.69903
	loss_reward: 0.05901
[2025-05-07 23:59:45] nn step 46550, lr: 0.1.
	loss_policy_0: 0.2705
	accuracy_policy_0: 0.69586
	loss_value_0: 0.33215
	loss_policy_1: 0.05476
	accuracy_policy_1: 0.68441
	loss_value_1: 0.06801
	loss_reward_1: 0.00675
	loss_policy_2: 0.05524
	accuracy_policy_2: 0.67406
	loss_value_2: 0.06928
	loss_reward_2: 0.00909
	loss_policy_3: 0.05556
	accuracy_policy_3: 0.66961
	loss_value_3: 0.07102
	loss_reward_3: 0.01158
	loss_policy_4: 0.05565
	accuracy_policy_4: 0.67957
	loss_value_4: 0.07269
	loss_reward_4: 0.01442
	loss_policy_5: 0.05552
	accuracy_policy_5: 0.6823
	loss_value_5: 0.0745
	loss_reward_5: 0.01558
	loss_policy: 0.54721
	loss_value: 0.68764
	loss_reward: 0.05743
[2025-05-07 23:59:53] nn step 46600, lr: 0.1.
	loss_policy_0: 0.25631
	accuracy_policy_0: 0.69582
	loss_value_0: 0.31413
	loss_policy_1: 0.05227
	accuracy_policy_1: 0.68176
	loss_value_1: 0.06427
	loss_reward_1: 0.00646
	loss_policy_2: 0.05211
	accuracy_policy_2: 0.68023
	loss_value_2: 0.06601
	loss_reward_2: 0.00828
	loss_policy_3: 0.05251
	accuracy_policy_3: 0.67297
	loss_value_3: 0.06763
	loss_reward_3: 0.01114
	loss_policy_4: 0.05274
	accuracy_policy_4: 0.67672
	loss_value_4: 0.06922
	loss_reward_4: 0.01352
	loss_policy_5: 0.05268
	accuracy_policy_5: 0.6875
	loss_value_5: 0.07085
	loss_reward_5: 0.014
	loss_policy: 0.51863
	loss_value: 0.65211
	loss_reward: 0.05339
Optimization_Done 46600
[2025-05-08 00:02:58] [command] train weight_iter_46600.pkl 215 234
[2025-05-08 00:03:06] nn step 46650, lr: 0.1.
	loss_policy_0: 0.24807
	accuracy_policy_0: 0.70215
	loss_value_0: 0.31222
	loss_policy_1: 0.0506
	accuracy_policy_1: 0.68352
	loss_value_1: 0.06336
	loss_reward_1: 0.00607
	loss_policy_2: 0.05071
	accuracy_policy_2: 0.68215
	loss_value_2: 0.06486
	loss_reward_2: 0.00794
	loss_policy_3: 0.05122
	accuracy_policy_3: 0.67102
	loss_value_3: 0.06635
	loss_reward_3: 0.01054
	loss_policy_4: 0.05134
	accuracy_policy_4: 0.67773
	loss_value_4: 0.06783
	loss_reward_4: 0.0132
	loss_policy_5: 0.05143
	accuracy_policy_5: 0.68125
	loss_value_5: 0.06934
	loss_reward_5: 0.01372
	loss_policy: 0.50337
	loss_value: 0.64396
	loss_reward: 0.05146
[2025-05-08 00:03:14] nn step 46700, lr: 0.1.
	loss_policy_0: 0.26649
	accuracy_policy_0: 0.69738
	loss_value_0: 0.33089
	loss_policy_1: 0.05408
	accuracy_policy_1: 0.68305
	loss_value_1: 0.06759
	loss_reward_1: 0.00662
	loss_policy_2: 0.0542
	accuracy_policy_2: 0.68324
	loss_value_2: 0.06917
	loss_reward_2: 0.00874
	loss_policy_3: 0.05447
	accuracy_policy_3: 0.67629
	loss_value_3: 0.07084
	loss_reward_3: 0.01109
	loss_policy_4: 0.05484
	accuracy_policy_4: 0.67746
	loss_value_4: 0.07229
	loss_reward_4: 0.01394
	loss_policy_5: 0.05457
	accuracy_policy_5: 0.68594
	loss_value_5: 0.07357
	loss_reward_5: 0.01535
	loss_policy: 0.53865
	loss_value: 0.68434
	loss_reward: 0.05574
[2025-05-08 00:03:23] nn step 46750, lr: 0.1.
	loss_policy_0: 0.27511
	accuracy_policy_0: 0.69551
	loss_value_0: 0.33902
	loss_policy_1: 0.05569
	accuracy_policy_1: 0.68523
	loss_value_1: 0.06957
	loss_reward_1: 0.00681
	loss_policy_2: 0.05587
	accuracy_policy_2: 0.68047
	loss_value_2: 0.07142
	loss_reward_2: 0.00896
	loss_policy_3: 0.05654
	accuracy_policy_3: 0.67457
	loss_value_3: 0.0732
	loss_reward_3: 0.01171
	loss_policy_4: 0.05681
	accuracy_policy_4: 0.67148
	loss_value_4: 0.07514
	loss_reward_4: 0.01411
	loss_policy_5: 0.05673
	accuracy_policy_5: 0.68125
	loss_value_5: 0.07686
	loss_reward_5: 0.01601
	loss_policy: 0.55675
	loss_value: 0.70521
	loss_reward: 0.05761
[2025-05-08 00:03:29] nn step 46800, lr: 0.1.
	loss_policy_0: 0.25153
	accuracy_policy_0: 0.70062
	loss_value_0: 0.31039
	loss_policy_1: 0.05101
	accuracy_policy_1: 0.6841
	loss_value_1: 0.06296
	loss_reward_1: 0.00645
	loss_policy_2: 0.05152
	accuracy_policy_2: 0.67711
	loss_value_2: 0.06459
	loss_reward_2: 0.00835
	loss_policy_3: 0.05133
	accuracy_policy_3: 0.67887
	loss_value_3: 0.06617
	loss_reward_3: 0.01072
	loss_policy_4: 0.05177
	accuracy_policy_4: 0.67633
	loss_value_4: 0.06802
	loss_reward_4: 0.01381
	loss_policy_5: 0.05195
	accuracy_policy_5: 0.6807
	loss_value_5: 0.06933
	loss_reward_5: 0.01442
	loss_policy: 0.50912
	loss_value: 0.64146
	loss_reward: 0.05374
Optimization_Done 46800
[2025-05-08 00:06:36] [command] train weight_iter_46800.pkl 216 235
[2025-05-08 00:06:46] nn step 46850, lr: 0.1.
	loss_policy_0: 0.24835
	accuracy_policy_0: 0.70504
	loss_value_0: 0.31805
	loss_policy_1: 0.05033
	accuracy_policy_1: 0.69453
	loss_value_1: 0.06458
	loss_reward_1: 0.00641
	loss_policy_2: 0.05084
	accuracy_policy_2: 0.68633
	loss_value_2: 0.06613
	loss_reward_2: 0.00809
	loss_policy_3: 0.05122
	accuracy_policy_3: 0.68137
	loss_value_3: 0.06782
	loss_reward_3: 0.01042
	loss_policy_4: 0.05162
	accuracy_policy_4: 0.67863
	loss_value_4: 0.06903
	loss_reward_4: 0.01315
	loss_policy_5: 0.05155
	accuracy_policy_5: 0.68266
	loss_value_5: 0.07013
	loss_reward_5: 0.0141
	loss_policy: 0.50392
	loss_value: 0.65575
	loss_reward: 0.05217
[2025-05-08 00:06:53] nn step 46900, lr: 0.1.
	loss_policy_0: 0.26068
	accuracy_policy_0: 0.69328
	loss_value_0: 0.31629
	loss_policy_1: 0.05215
	accuracy_policy_1: 0.6832
	loss_value_1: 0.06437
	loss_reward_1: 0.00637
	loss_policy_2: 0.0522
	accuracy_policy_2: 0.68258
	loss_value_2: 0.06634
	loss_reward_2: 0.00814
	loss_policy_3: 0.05279
	accuracy_policy_3: 0.67508
	loss_value_3: 0.0677
	loss_reward_3: 0.01053
	loss_policy_4: 0.05314
	accuracy_policy_4: 0.67465
	loss_value_4: 0.06909
	loss_reward_4: 0.01393
	loss_policy_5: 0.05273
	accuracy_policy_5: 0.68289
	loss_value_5: 0.07052
	loss_reward_5: 0.0146
	loss_policy: 0.5237
	loss_value: 0.65432
	loss_reward: 0.05357
[2025-05-08 00:07:01] nn step 46950, lr: 0.1.
	loss_policy_0: 0.26631
	accuracy_policy_0: 0.69836
	loss_value_0: 0.32804
	loss_policy_1: 0.05337
	accuracy_policy_1: 0.695
	loss_value_1: 0.0668
	loss_reward_1: 0.00632
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.68082
	loss_value_2: 0.06871
	loss_reward_2: 0.00867
	loss_policy_3: 0.05435
	accuracy_policy_3: 0.67629
	loss_value_3: 0.07045
	loss_reward_3: 0.01123
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.67801
	loss_value_4: 0.07225
	loss_reward_4: 0.01415
	loss_policy_5: 0.05466
	accuracy_policy_5: 0.68246
	loss_value_5: 0.07364
	loss_reward_5: 0.01542
	loss_policy: 0.53725
	loss_value: 0.6799
	loss_reward: 0.05579
[2025-05-08 00:07:10] nn step 47000, lr: 0.1.
	loss_policy_0: 0.2637
	accuracy_policy_0: 0.69965
	loss_value_0: 0.32059
	loss_policy_1: 0.05304
	accuracy_policy_1: 0.68457
	loss_value_1: 0.06545
	loss_reward_1: 0.00646
	loss_policy_2: 0.05353
	accuracy_policy_2: 0.67875
	loss_value_2: 0.06713
	loss_reward_2: 0.00845
	loss_policy_3: 0.05365
	accuracy_policy_3: 0.67918
	loss_value_3: 0.06875
	loss_reward_3: 0.01101
	loss_policy_4: 0.0536
	accuracy_policy_4: 0.68199
	loss_value_4: 0.07036
	loss_reward_4: 0.01354
	loss_policy_5: 0.05368
	accuracy_policy_5: 0.68316
	loss_value_5: 0.07212
	loss_reward_5: 0.0148
	loss_policy: 0.5312
	loss_value: 0.6644
	loss_reward: 0.05425
Optimization_Done 47000
[2025-05-08 00:10:21] [command] train weight_iter_47000.pkl 217 236
[2025-05-08 00:10:32] nn step 47050, lr: 0.1.
	loss_policy_0: 0.25504
	accuracy_policy_0: 0.68914
	loss_value_0: 0.31837
	loss_policy_1: 0.051
	accuracy_policy_1: 0.68148
	loss_value_1: 0.06454
	loss_reward_1: 0.00605
	loss_policy_2: 0.05159
	accuracy_policy_2: 0.67367
	loss_value_2: 0.0658
	loss_reward_2: 0.00838
	loss_policy_3: 0.05193
	accuracy_policy_3: 0.6657
	loss_value_3: 0.06713
	loss_reward_3: 0.01047
	loss_policy_4: 0.05187
	accuracy_policy_4: 0.67211
	loss_value_4: 0.0684
	loss_reward_4: 0.01322
	loss_policy_5: 0.05179
	accuracy_policy_5: 0.68027
	loss_value_5: 0.06996
	loss_reward_5: 0.0141
	loss_policy: 0.51322
	loss_value: 0.6542
	loss_reward: 0.05222
[2025-05-08 00:10:39] nn step 47100, lr: 0.1.
	loss_policy_0: 0.25583
	accuracy_policy_0: 0.6907
	loss_value_0: 0.31454
	loss_policy_1: 0.05179
	accuracy_policy_1: 0.68148
	loss_value_1: 0.06436
	loss_reward_1: 0.00615
	loss_policy_2: 0.05224
	accuracy_policy_2: 0.67516
	loss_value_2: 0.06579
	loss_reward_2: 0.00793
	loss_policy_3: 0.05243
	accuracy_policy_3: 0.67691
	loss_value_3: 0.06746
	loss_reward_3: 0.01055
	loss_policy_4: 0.05264
	accuracy_policy_4: 0.67047
	loss_value_4: 0.06896
	loss_reward_4: 0.0132
	loss_policy_5: 0.05257
	accuracy_policy_5: 0.68012
	loss_value_5: 0.07065
	loss_reward_5: 0.01431
	loss_policy: 0.51751
	loss_value: 0.65177
	loss_reward: 0.05214
[2025-05-08 00:10:48] nn step 47150, lr: 0.1.
	loss_policy_0: 0.27139
	accuracy_policy_0: 0.69781
	loss_value_0: 0.32757
	loss_policy_1: 0.05472
	accuracy_policy_1: 0.68375
	loss_value_1: 0.06692
	loss_reward_1: 0.00649
	loss_policy_2: 0.05544
	accuracy_policy_2: 0.67219
	loss_value_2: 0.06872
	loss_reward_2: 0.00864
	loss_policy_3: 0.05558
	accuracy_policy_3: 0.67215
	loss_value_3: 0.07015
	loss_reward_3: 0.01117
	loss_policy_4: 0.0557
	accuracy_policy_4: 0.67297
	loss_value_4: 0.07206
	loss_reward_4: 0.01392
	loss_policy_5: 0.05572
	accuracy_policy_5: 0.67492
	loss_value_5: 0.07363
	loss_reward_5: 0.01513
	loss_policy: 0.54855
	loss_value: 0.67904
	loss_reward: 0.05535
[2025-05-08 00:10:56] nn step 47200, lr: 0.1.
	loss_policy_0: 0.26786
	accuracy_policy_0: 0.69516
	loss_value_0: 0.32649
	loss_policy_1: 0.05497
	accuracy_policy_1: 0.67641
	loss_value_1: 0.0669
	loss_reward_1: 0.00674
	loss_policy_2: 0.055
	accuracy_policy_2: 0.67582
	loss_value_2: 0.06855
	loss_reward_2: 0.00885
	loss_policy_3: 0.05554
	accuracy_policy_3: 0.6693
	loss_value_3: 0.07048
	loss_reward_3: 0.01134
	loss_policy_4: 0.05543
	accuracy_policy_4: 0.67184
	loss_value_4: 0.07244
	loss_reward_4: 0.0141
	loss_policy_5: 0.05574
	accuracy_policy_5: 0.67695
	loss_value_5: 0.07407
	loss_reward_5: 0.01526
	loss_policy: 0.54455
	loss_value: 0.67892
	loss_reward: 0.05628
Optimization_Done 47200
[2025-05-08 00:14:00] [command] train weight_iter_47200.pkl 218 237
[2025-05-08 00:14:10] nn step 47250, lr: 0.1.
	loss_policy_0: 0.25441
	accuracy_policy_0: 0.69773
	loss_value_0: 0.32942
	loss_policy_1: 0.05179
	accuracy_policy_1: 0.68477
	loss_value_1: 0.06652
	loss_reward_1: 0.00656
	loss_policy_2: 0.05203
	accuracy_policy_2: 0.67945
	loss_value_2: 0.06797
	loss_reward_2: 0.00816
	loss_policy_3: 0.05228
	accuracy_policy_3: 0.67578
	loss_value_3: 0.06944
	loss_reward_3: 0.01062
	loss_policy_4: 0.05269
	accuracy_policy_4: 0.67504
	loss_value_4: 0.07093
	loss_reward_4: 0.01355
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.68051
	loss_value_5: 0.07223
	loss_reward_5: 0.01458
	loss_policy: 0.51565
	loss_value: 0.67651
	loss_reward: 0.05347
[2025-05-08 00:14:19] nn step 47300, lr: 0.1.
	loss_policy_0: 0.2704
	accuracy_policy_0: 0.6975
	loss_value_0: 0.33544
	loss_policy_1: 0.05491
	accuracy_policy_1: 0.68094
	loss_value_1: 0.06857
	loss_reward_1: 0.00654
	loss_policy_2: 0.05507
	accuracy_policy_2: 0.67684
	loss_value_2: 0.06999
	loss_reward_2: 0.00891
	loss_policy_3: 0.05547
	accuracy_policy_3: 0.67188
	loss_value_3: 0.0717
	loss_reward_3: 0.01126
	loss_policy_4: 0.05567
	accuracy_policy_4: 0.67555
	loss_value_4: 0.07324
	loss_reward_4: 0.01417
	loss_policy_5: 0.05576
	accuracy_policy_5: 0.67926
	loss_value_5: 0.0751
	loss_reward_5: 0.01555
	loss_policy: 0.54728
	loss_value: 0.69405
	loss_reward: 0.05645
[2025-05-08 00:14:25] nn step 47350, lr: 0.1.
	loss_policy_0: 0.26685
	accuracy_policy_0: 0.69762
	loss_value_0: 0.33058
	loss_policy_1: 0.05415
	accuracy_policy_1: 0.6843
	loss_value_1: 0.06755
	loss_reward_1: 0.00689
	loss_policy_2: 0.05484
	accuracy_policy_2: 0.67785
	loss_value_2: 0.06932
	loss_reward_2: 0.0085
	loss_policy_3: 0.05511
	accuracy_policy_3: 0.67035
	loss_value_3: 0.07116
	loss_reward_3: 0.01125
	loss_policy_4: 0.05549
	accuracy_policy_4: 0.67039
	loss_value_4: 0.07276
	loss_reward_4: 0.0143
	loss_policy_5: 0.05544
	accuracy_policy_5: 0.67516
	loss_value_5: 0.07401
	loss_reward_5: 0.01522
	loss_policy: 0.54188
	loss_value: 0.68538
	loss_reward: 0.05615
[2025-05-08 00:14:34] nn step 47400, lr: 0.1.
	loss_policy_0: 0.25404
	accuracy_policy_0: 0.69086
	loss_value_0: 0.31362
	loss_policy_1: 0.05166
	accuracy_policy_1: 0.68219
	loss_value_1: 0.0643
	loss_reward_1: 0.00633
	loss_policy_2: 0.05178
	accuracy_policy_2: 0.67594
	loss_value_2: 0.06596
	loss_reward_2: 0.00787
	loss_policy_3: 0.05252
	accuracy_policy_3: 0.66984
	loss_value_3: 0.0679
	loss_reward_3: 0.01087
	loss_policy_4: 0.0528
	accuracy_policy_4: 0.67277
	loss_value_4: 0.06927
	loss_reward_4: 0.01369
	loss_policy_5: 0.05265
	accuracy_policy_5: 0.67773
	loss_value_5: 0.07088
	loss_reward_5: 0.0145
	loss_policy: 0.51545
	loss_value: 0.65193
	loss_reward: 0.05326
Optimization_Done 47400
[2025-05-08 00:17:38] [command] train weight_iter_47400.pkl 219 238
[2025-05-08 00:17:47] nn step 47450, lr: 0.1.
	loss_policy_0: 0.25895
	accuracy_policy_0: 0.68754
	loss_value_0: 0.3197
	loss_policy_1: 0.05251
	accuracy_policy_1: 0.67211
	loss_value_1: 0.06495
	loss_reward_1: 0.00627
	loss_policy_2: 0.05284
	accuracy_policy_2: 0.66617
	loss_value_2: 0.06669
	loss_reward_2: 0.00845
	loss_policy_3: 0.05343
	accuracy_policy_3: 0.66141
	loss_value_3: 0.06822
	loss_reward_3: 0.01075
	loss_policy_4: 0.05359
	accuracy_policy_4: 0.66109
	loss_value_4: 0.0697
	loss_reward_4: 0.01354
	loss_policy_5: 0.0536
	accuracy_policy_5: 0.66559
	loss_value_5: 0.0713
	loss_reward_5: 0.01459
	loss_policy: 0.52493
	loss_value: 0.66056
	loss_reward: 0.0536
[2025-05-08 00:17:56] nn step 47500, lr: 0.1.
	loss_policy_0: 0.25949
	accuracy_policy_0: 0.68738
	loss_value_0: 0.31468
	loss_policy_1: 0.05286
	accuracy_policy_1: 0.67312
	loss_value_1: 0.06392
	loss_reward_1: 0.00637
	loss_policy_2: 0.05326
	accuracy_policy_2: 0.66777
	loss_value_2: 0.06603
	loss_reward_2: 0.00824
	loss_policy_3: 0.05324
	accuracy_policy_3: 0.66113
	loss_value_3: 0.06765
	loss_reward_3: 0.01034
	loss_policy_4: 0.05371
	accuracy_policy_4: 0.66355
	loss_value_4: 0.06944
	loss_reward_4: 0.01364
	loss_policy_5: 0.05349
	accuracy_policy_5: 0.67094
	loss_value_5: 0.07092
	loss_reward_5: 0.01452
	loss_policy: 0.52605
	loss_value: 0.65266
	loss_reward: 0.05311
[2025-05-08 00:18:04] nn step 47550, lr: 0.1.
	loss_policy_0: 0.27178
	accuracy_policy_0: 0.68289
	loss_value_0: 0.32788
	loss_policy_1: 0.05484
	accuracy_policy_1: 0.67469
	loss_value_1: 0.06682
	loss_reward_1: 0.00633
	loss_policy_2: 0.05556
	accuracy_policy_2: 0.66453
	loss_value_2: 0.06869
	loss_reward_2: 0.00873
	loss_policy_3: 0.05613
	accuracy_policy_3: 0.66
	loss_value_3: 0.07054
	loss_reward_3: 0.01125
	loss_policy_4: 0.05623
	accuracy_policy_4: 0.66152
	loss_value_4: 0.07196
	loss_reward_4: 0.01371
	loss_policy_5: 0.05606
	accuracy_policy_5: 0.66609
	loss_value_5: 0.07356
	loss_reward_5: 0.01546
	loss_policy: 0.55061
	loss_value: 0.67946
	loss_reward: 0.05547
[2025-05-08 00:18:11] nn step 47600, lr: 0.1.
	loss_policy_0: 0.2677
	accuracy_policy_0: 0.68707
	loss_value_0: 0.32468
	loss_policy_1: 0.05416
	accuracy_policy_1: 0.68434
	loss_value_1: 0.06639
	loss_reward_1: 0.00686
	loss_policy_2: 0.05493
	accuracy_policy_2: 0.67051
	loss_value_2: 0.06792
	loss_reward_2: 0.00875
	loss_policy_3: 0.05489
	accuracy_policy_3: 0.66949
	loss_value_3: 0.06993
	loss_reward_3: 0.0113
	loss_policy_4: 0.05535
	accuracy_policy_4: 0.67238
	loss_value_4: 0.07162
	loss_reward_4: 0.01385
	loss_policy_5: 0.05544
	accuracy_policy_5: 0.67051
	loss_value_5: 0.07336
	loss_reward_5: 0.01551
	loss_policy: 0.54247
	loss_value: 0.6739
	loss_reward: 0.05627
Optimization_Done 47600
[2025-05-08 00:21:21] [command] train weight_iter_47600.pkl 220 239
[2025-05-08 00:21:30] nn step 47650, lr: 0.1.
	loss_policy_0: 0.25563
	accuracy_policy_0: 0.67086
	loss_value_0: 0.31465
	loss_policy_1: 0.05157
	accuracy_policy_1: 0.66301
	loss_value_1: 0.0639
	loss_reward_1: 0.006
	loss_policy_2: 0.05221
	accuracy_policy_2: 0.65723
	loss_value_2: 0.06542
	loss_reward_2: 0.00814
	loss_policy_3: 0.05246
	accuracy_policy_3: 0.65449
	loss_value_3: 0.06667
	loss_reward_3: 0.01029
	loss_policy_4: 0.05244
	accuracy_policy_4: 0.65707
	loss_value_4: 0.06834
	loss_reward_4: 0.01314
	loss_policy_5: 0.05265
	accuracy_policy_5: 0.6577
	loss_value_5: 0.06968
	loss_reward_5: 0.01439
	loss_policy: 0.51695
	loss_value: 0.64866
	loss_reward: 0.05197
[2025-05-08 00:21:38] nn step 47700, lr: 0.1.
	loss_policy_0: 0.25684
	accuracy_policy_0: 0.68414
	loss_value_0: 0.31338
	loss_policy_1: 0.05221
	accuracy_policy_1: 0.67309
	loss_value_1: 0.06385
	loss_reward_1: 0.00624
	loss_policy_2: 0.05241
	accuracy_policy_2: 0.66227
	loss_value_2: 0.06593
	loss_reward_2: 0.00791
	loss_policy_3: 0.05289
	accuracy_policy_3: 0.65609
	loss_value_3: 0.0675
	loss_reward_3: 0.01057
	loss_policy_4: 0.0534
	accuracy_policy_4: 0.65707
	loss_value_4: 0.069
	loss_reward_4: 0.0133
	loss_policy_5: 0.05376
	accuracy_policy_5: 0.65715
	loss_value_5: 0.07062
	loss_reward_5: 0.01409
	loss_policy: 0.52151
	loss_value: 0.65028
	loss_reward: 0.05212
[2025-05-08 00:21:47] nn step 47750, lr: 0.1.
	loss_policy_0: 0.26321
	accuracy_policy_0: 0.67367
	loss_value_0: 0.31217
	loss_policy_1: 0.05335
	accuracy_policy_1: 0.6627
	loss_value_1: 0.06396
	loss_reward_1: 0.00624
	loss_policy_2: 0.05358
	accuracy_policy_2: 0.66008
	loss_value_2: 0.06576
	loss_reward_2: 0.00826
	loss_policy_3: 0.05382
	accuracy_policy_3: 0.65527
	loss_value_3: 0.0675
	loss_reward_3: 0.01061
	loss_policy_4: 0.05397
	accuracy_policy_4: 0.6566
	loss_value_4: 0.06901
	loss_reward_4: 0.01341
	loss_policy_5: 0.05406
	accuracy_policy_5: 0.65828
	loss_value_5: 0.07041
	loss_reward_5: 0.01472
	loss_policy: 0.53199
	loss_value: 0.64881
	loss_reward: 0.05325
[2025-05-08 00:21:55] nn step 47800, lr: 0.1.
	loss_policy_0: 0.26017
	accuracy_policy_0: 0.68402
	loss_value_0: 0.31272
	loss_policy_1: 0.05316
	accuracy_policy_1: 0.66996
	loss_value_1: 0.06359
	loss_reward_1: 0.00613
	loss_policy_2: 0.0535
	accuracy_policy_2: 0.65715
	loss_value_2: 0.06556
	loss_reward_2: 0.00834
	loss_policy_3: 0.05369
	accuracy_policy_3: 0.65625
	loss_value_3: 0.06734
	loss_reward_3: 0.01066
	loss_policy_4: 0.05413
	accuracy_policy_4: 0.65441
	loss_value_4: 0.06895
	loss_reward_4: 0.01355
	loss_policy_5: 0.05352
	accuracy_policy_5: 0.66496
	loss_value_5: 0.07034
	loss_reward_5: 0.01494
	loss_policy: 0.52817
	loss_value: 0.6485
	loss_reward: 0.05363
Optimization_Done 47800
[2025-05-08 00:24:59] [command] train weight_iter_47800.pkl 221 240
[2025-05-08 00:25:10] nn step 47850, lr: 0.1.
	loss_policy_0: 0.26754
	accuracy_policy_0: 0.68285
	loss_value_0: 0.33824
	loss_policy_1: 0.05429
	accuracy_policy_1: 0.6709
	loss_value_1: 0.06864
	loss_reward_1: 0.00653
	loss_policy_2: 0.05506
	accuracy_policy_2: 0.66723
	loss_value_2: 0.07026
	loss_reward_2: 0.00861
	loss_policy_3: 0.05531
	accuracy_policy_3: 0.65945
	loss_value_3: 0.07155
	loss_reward_3: 0.01075
	loss_policy_4: 0.05564
	accuracy_policy_4: 0.65977
	loss_value_4: 0.07268
	loss_reward_4: 0.01388
	loss_policy_5: 0.0556
	accuracy_policy_5: 0.6602
	loss_value_5: 0.07425
	loss_reward_5: 0.01526
	loss_policy: 0.54344
	loss_value: 0.69561
	loss_reward: 0.05503
[2025-05-08 00:25:16] nn step 47900, lr: 0.1.
	loss_policy_0: 0.25906
	accuracy_policy_0: 0.68551
	loss_value_0: 0.31878
	loss_policy_1: 0.05282
	accuracy_policy_1: 0.66645
	loss_value_1: 0.06467
	loss_reward_1: 0.00652
	loss_policy_2: 0.05297
	accuracy_policy_2: 0.66492
	loss_value_2: 0.0666
	loss_reward_2: 0.00819
	loss_policy_3: 0.0534
	accuracy_policy_3: 0.66039
	loss_value_3: 0.06839
	loss_reward_3: 0.01116
	loss_policy_4: 0.05355
	accuracy_policy_4: 0.66016
	loss_value_4: 0.07002
	loss_reward_4: 0.01377
	loss_policy_5: 0.05372
	accuracy_policy_5: 0.65914
	loss_value_5: 0.07156
	loss_reward_5: 0.01518
	loss_policy: 0.52552
	loss_value: 0.66003
	loss_reward: 0.05482
[2025-05-08 00:25:25] nn step 47950, lr: 0.1.
	loss_policy_0: 0.27675
	accuracy_policy_0: 0.67375
	loss_value_0: 0.33004
	loss_policy_1: 0.05584
	accuracy_policy_1: 0.67078
	loss_value_1: 0.06767
	loss_reward_1: 0.00683
	loss_policy_2: 0.05642
	accuracy_policy_2: 0.66438
	loss_value_2: 0.0694
	loss_reward_2: 0.00874
	loss_policy_3: 0.05659
	accuracy_policy_3: 0.6627
	loss_value_3: 0.07142
	loss_reward_3: 0.01126
	loss_policy_4: 0.05734
	accuracy_policy_4: 0.65789
	loss_value_4: 0.07312
	loss_reward_4: 0.01434
	loss_policy_5: 0.05718
	accuracy_policy_5: 0.66074
	loss_value_5: 0.07481
	loss_reward_5: 0.01576
	loss_policy: 0.56011
	loss_value: 0.68646
	loss_reward: 0.05693
[2025-05-08 00:25:33] nn step 48000, lr: 0.1.
	loss_policy_0: 0.26751
	accuracy_policy_0: 0.67969
	loss_value_0: 0.31971
	loss_policy_1: 0.05425
	accuracy_policy_1: 0.66773
	loss_value_1: 0.06548
	loss_reward_1: 0.00642
	loss_policy_2: 0.05464
	accuracy_policy_2: 0.66641
	loss_value_2: 0.06766
	loss_reward_2: 0.00832
	loss_policy_3: 0.05483
	accuracy_policy_3: 0.65918
	loss_value_3: 0.06928
	loss_reward_3: 0.01074
	loss_policy_4: 0.05549
	accuracy_policy_4: 0.66031
	loss_value_4: 0.07092
	loss_reward_4: 0.01395
	loss_policy_5: 0.05522
	accuracy_policy_5: 0.65961
	loss_value_5: 0.07261
	loss_reward_5: 0.01497
	loss_policy: 0.54194
	loss_value: 0.66564
	loss_reward: 0.0544
Optimization_Done 48000
[2025-05-08 00:28:38] [command] train weight_iter_48000.pkl 222 241
[2025-05-08 00:28:49] nn step 48050, lr: 0.1.
	loss_policy_0: 0.26457
	accuracy_policy_0: 0.68176
	loss_value_0: 0.33194
	loss_policy_1: 0.05406
	accuracy_policy_1: 0.66961
	loss_value_1: 0.0678
	loss_reward_1: 0.00639
	loss_policy_2: 0.05411
	accuracy_policy_2: 0.66531
	loss_value_2: 0.06935
	loss_reward_2: 0.00842
	loss_policy_3: 0.05448
	accuracy_policy_3: 0.6598
	loss_value_3: 0.07082
	loss_reward_3: 0.01086
	loss_policy_4: 0.05504
	accuracy_policy_4: 0.65621
	loss_value_4: 0.07168
	loss_reward_4: 0.01367
	loss_policy_5: 0.05494
	accuracy_policy_5: 0.66098
	loss_value_5: 0.07294
	loss_reward_5: 0.01518
	loss_policy: 0.5372
	loss_value: 0.68455
	loss_reward: 0.05453
[2025-05-08 00:28:57] nn step 48100, lr: 0.1.
	loss_policy_0: 0.25205
	accuracy_policy_0: 0.68312
	loss_value_0: 0.30807
	loss_policy_1: 0.05101
	accuracy_policy_1: 0.67438
	loss_value_1: 0.06289
	loss_reward_1: 0.00605
	loss_policy_2: 0.05173
	accuracy_policy_2: 0.66691
	loss_value_2: 0.06442
	loss_reward_2: 0.00767
	loss_policy_3: 0.05205
	accuracy_policy_3: 0.66309
	loss_value_3: 0.06597
	loss_reward_3: 0.01041
	loss_policy_4: 0.05245
	accuracy_policy_4: 0.6566
	loss_value_4: 0.06739
	loss_reward_4: 0.01328
	loss_policy_5: 0.05246
	accuracy_policy_5: 0.66465
	loss_value_5: 0.06841
	loss_reward_5: 0.01452
	loss_policy: 0.51174
	loss_value: 0.63716
	loss_reward: 0.05192
[2025-05-08 00:29:04] nn step 48150, lr: 0.1.
	loss_policy_0: 0.27435
	accuracy_policy_0: 0.68812
	loss_value_0: 0.32864
	loss_policy_1: 0.05636
	accuracy_policy_1: 0.67281
	loss_value_1: 0.06736
	loss_reward_1: 0.00676
	loss_policy_2: 0.05606
	accuracy_policy_2: 0.66684
	loss_value_2: 0.06937
	loss_reward_2: 0.00861
	loss_policy_3: 0.05659
	accuracy_policy_3: 0.66098
	loss_value_3: 0.07115
	loss_reward_3: 0.01107
	loss_policy_4: 0.05677
	accuracy_policy_4: 0.66547
	loss_value_4: 0.07261
	loss_reward_4: 0.01451
	loss_policy_5: 0.05738
	accuracy_policy_5: 0.66445
	loss_value_5: 0.07391
	loss_reward_5: 0.01571
	loss_policy: 0.55751
	loss_value: 0.68302
	loss_reward: 0.05665
[2025-05-08 00:29:12] nn step 48200, lr: 0.1.
	loss_policy_0: 0.25914
	accuracy_policy_0: 0.68059
	loss_value_0: 0.30958
	loss_policy_1: 0.05258
	accuracy_policy_1: 0.66855
	loss_value_1: 0.0632
	loss_reward_1: 0.0062
	loss_policy_2: 0.05331
	accuracy_policy_2: 0.66418
	loss_value_2: 0.06506
	loss_reward_2: 0.00816
	loss_policy_3: 0.05315
	accuracy_policy_3: 0.6618
	loss_value_3: 0.06678
	loss_reward_3: 0.01058
	loss_policy_4: 0.05328
	accuracy_policy_4: 0.66176
	loss_value_4: 0.06851
	loss_reward_4: 0.01361
	loss_policy_5: 0.05346
	accuracy_policy_5: 0.66293
	loss_value_5: 0.07019
	loss_reward_5: 0.01439
	loss_policy: 0.52493
	loss_value: 0.64332
	loss_reward: 0.05293
Optimization_Done 48200
[2025-05-08 00:32:20] [command] train weight_iter_48200.pkl 223 242
[2025-05-08 00:32:31] nn step 48250, lr: 0.1.
	loss_policy_0: 0.25895
	accuracy_policy_0: 0.6923
	loss_value_0: 0.3312
	loss_policy_1: 0.0523
	accuracy_policy_1: 0.6793
	loss_value_1: 0.06689
	loss_reward_1: 0.00615
	loss_policy_2: 0.05283
	accuracy_policy_2: 0.67191
	loss_value_2: 0.06831
	loss_reward_2: 0.00845
	loss_policy_3: 0.05327
	accuracy_policy_3: 0.67215
	loss_value_3: 0.0698
	loss_reward_3: 0.01083
	loss_policy_4: 0.05365
	accuracy_policy_4: 0.66668
	loss_value_4: 0.07129
	loss_reward_4: 0.01345
	loss_policy_5: 0.0534
	accuracy_policy_5: 0.67121
	loss_value_5: 0.07318
	loss_reward_5: 0.01488
	loss_policy: 0.52441
	loss_value: 0.68068
	loss_reward: 0.05377
[2025-05-08 00:32:39] nn step 48300, lr: 0.1.
	loss_policy_0: 0.2669
	accuracy_policy_0: 0.68832
	loss_value_0: 0.32789
	loss_policy_1: 0.05401
	accuracy_policy_1: 0.67473
	loss_value_1: 0.06705
	loss_reward_1: 0.00651
	loss_policy_2: 0.05473
	accuracy_policy_2: 0.6677
	loss_value_2: 0.06928
	loss_reward_2: 0.00852
	loss_policy_3: 0.05499
	accuracy_policy_3: 0.66516
	loss_value_3: 0.07087
	loss_reward_3: 0.01105
	loss_policy_4: 0.05546
	accuracy_policy_4: 0.66574
	loss_value_4: 0.07227
	loss_reward_4: 0.01404
	loss_policy_5: 0.055
	accuracy_policy_5: 0.67387
	loss_value_5: 0.07393
	loss_reward_5: 0.01501
	loss_policy: 0.5411
	loss_value: 0.68129
	loss_reward: 0.05513
[2025-05-08 00:32:47] nn step 48350, lr: 0.1.
	loss_policy_0: 0.26264
	accuracy_policy_0: 0.69168
	loss_value_0: 0.32093
	loss_policy_1: 0.05327
	accuracy_policy_1: 0.68254
	loss_value_1: 0.06555
	loss_reward_1: 0.00692
	loss_policy_2: 0.05402
	accuracy_policy_2: 0.6677
	loss_value_2: 0.06759
	loss_reward_2: 0.0086
	loss_policy_3: 0.05416
	accuracy_policy_3: 0.66496
	loss_value_3: 0.06935
	loss_reward_3: 0.01119
	loss_policy_4: 0.05421
	accuracy_policy_4: 0.6673
	loss_value_4: 0.07091
	loss_reward_4: 0.01402
	loss_policy_5: 0.05443
	accuracy_policy_5: 0.66852
	loss_value_5: 0.07251
	loss_reward_5: 0.01523
	loss_policy: 0.53272
	loss_value: 0.66683
	loss_reward: 0.05596
[2025-05-08 00:32:54] nn step 48400, lr: 0.1.
	loss_policy_0: 0.2653
	accuracy_policy_0: 0.68594
	loss_value_0: 0.31745
	loss_policy_1: 0.05361
	accuracy_policy_1: 0.67461
	loss_value_1: 0.065
	loss_reward_1: 0.00659
	loss_policy_2: 0.05412
	accuracy_policy_2: 0.66988
	loss_value_2: 0.06665
	loss_reward_2: 0.00866
	loss_policy_3: 0.05443
	accuracy_policy_3: 0.6702
	loss_value_3: 0.06808
	loss_reward_3: 0.01108
	loss_policy_4: 0.05509
	accuracy_policy_4: 0.66461
	loss_value_4: 0.07021
	loss_reward_4: 0.01382
	loss_policy_5: 0.05451
	accuracy_policy_5: 0.6757
	loss_value_5: 0.07176
	loss_reward_5: 0.01488
	loss_policy: 0.53705
	loss_value: 0.65914
	loss_reward: 0.05502
Optimization_Done 48400
[2025-05-08 00:35:57] [command] train weight_iter_48400.pkl 224 243
[2025-05-08 00:36:08] nn step 48450, lr: 0.1.
	loss_policy_0: 0.27188
	accuracy_policy_0: 0.68293
	loss_value_0: 0.34047
	loss_policy_1: 0.05516
	accuracy_policy_1: 0.67289
	loss_value_1: 0.06912
	loss_reward_1: 0.00638
	loss_policy_2: 0.0563
	accuracy_policy_2: 0.6607
	loss_value_2: 0.07074
	loss_reward_2: 0.00864
	loss_policy_3: 0.05612
	accuracy_policy_3: 0.66066
	loss_value_3: 0.07202
	loss_reward_3: 0.01133
	loss_policy_4: 0.05619
	accuracy_policy_4: 0.66461
	loss_value_4: 0.07381
	loss_reward_4: 0.01403
	loss_policy_5: 0.05638
	accuracy_policy_5: 0.66348
	loss_value_5: 0.07532
	loss_reward_5: 0.01509
	loss_policy: 0.55202
	loss_value: 0.7015
	loss_reward: 0.05546
[2025-05-08 00:36:15] nn step 48500, lr: 0.1.
	loss_policy_0: 0.26632
	accuracy_policy_0: 0.68871
	loss_value_0: 0.32721
	loss_policy_1: 0.05457
	accuracy_policy_1: 0.66809
	loss_value_1: 0.06649
	loss_reward_1: 0.00643
	loss_policy_2: 0.05533
	accuracy_policy_2: 0.66426
	loss_value_2: 0.06822
	loss_reward_2: 0.00831
	loss_policy_3: 0.05564
	accuracy_policy_3: 0.65863
	loss_value_3: 0.06955
	loss_reward_3: 0.0112
	loss_policy_4: 0.05571
	accuracy_policy_4: 0.66152
	loss_value_4: 0.0712
	loss_reward_4: 0.01374
	loss_policy_5: 0.05606
	accuracy_policy_5: 0.65988
	loss_value_5: 0.07262
	loss_reward_5: 0.01522
	loss_policy: 0.54363
	loss_value: 0.67529
	loss_reward: 0.05489
[2025-05-08 00:36:23] nn step 48550, lr: 0.1.
	loss_policy_0: 0.28394
	accuracy_policy_0: 0.68082
	loss_value_0: 0.33992
	loss_policy_1: 0.05676
	accuracy_policy_1: 0.67785
	loss_value_1: 0.06927
	loss_reward_1: 0.00701
	loss_policy_2: 0.05745
	accuracy_policy_2: 0.66387
	loss_value_2: 0.07122
	loss_reward_2: 0.00899
	loss_policy_3: 0.05784
	accuracy_policy_3: 0.6632
	loss_value_3: 0.07289
	loss_reward_3: 0.01168
	loss_policy_4: 0.05795
	accuracy_policy_4: 0.6668
	loss_value_4: 0.07461
	loss_reward_4: 0.01441
	loss_policy_5: 0.05806
	accuracy_policy_5: 0.66488
	loss_value_5: 0.0761
	loss_reward_5: 0.01603
	loss_policy: 0.572
	loss_value: 0.70402
	loss_reward: 0.05813
[2025-05-08 00:36:31] nn step 48600, lr: 0.1.
	loss_policy_0: 0.26485
	accuracy_policy_0: 0.68395
	loss_value_0: 0.31489
	loss_policy_1: 0.05389
	accuracy_policy_1: 0.67117
	loss_value_1: 0.06407
	loss_reward_1: 0.00646
	loss_policy_2: 0.0545
	accuracy_policy_2: 0.66293
	loss_value_2: 0.0657
	loss_reward_2: 0.00798
	loss_policy_3: 0.05504
	accuracy_policy_3: 0.65062
	loss_value_3: 0.06735
	loss_reward_3: 0.01071
	loss_policy_4: 0.05495
	accuracy_policy_4: 0.66035
	loss_value_4: 0.06879
	loss_reward_4: 0.01342
	loss_policy_5: 0.05464
	accuracy_policy_5: 0.66785
	loss_value_5: 0.07053
	loss_reward_5: 0.01465
	loss_policy: 0.53788
	loss_value: 0.65133
	loss_reward: 0.05321
Optimization_Done 48600
[2025-05-08 00:39:37] [command] train weight_iter_48600.pkl 225 244
[2025-05-08 00:39:48] nn step 48650, lr: 0.1.
	loss_policy_0: 0.25165
	accuracy_policy_0: 0.67863
	loss_value_0: 0.30608
	loss_policy_1: 0.05126
	accuracy_policy_1: 0.66609
	loss_value_1: 0.06207
	loss_reward_1: 0.00598
	loss_policy_2: 0.05161
	accuracy_policy_2: 0.65812
	loss_value_2: 0.06329
	loss_reward_2: 0.00784
	loss_policy_3: 0.05187
	accuracy_policy_3: 0.65891
	loss_value_3: 0.06477
	loss_reward_3: 0.0103
	loss_policy_4: 0.05155
	accuracy_policy_4: 0.66188
	loss_value_4: 0.06611
	loss_reward_4: 0.0128
	loss_policy_5: 0.05145
	accuracy_policy_5: 0.66598
	loss_value_5: 0.06728
	loss_reward_5: 0.01384
	loss_policy: 0.50939
	loss_value: 0.62961
	loss_reward: 0.05076
[2025-05-08 00:39:55] nn step 48700, lr: 0.1.
	loss_policy_0: 0.26642
	accuracy_policy_0: 0.6818
	loss_value_0: 0.32068
	loss_policy_1: 0.05392
	accuracy_policy_1: 0.66883
	loss_value_1: 0.06567
	loss_reward_1: 0.00653
	loss_policy_2: 0.05453
	accuracy_policy_2: 0.65973
	loss_value_2: 0.06733
	loss_reward_2: 0.00863
	loss_policy_3: 0.05491
	accuracy_policy_3: 0.65488
	loss_value_3: 0.06892
	loss_reward_3: 0.01105
	loss_policy_4: 0.05499
	accuracy_policy_4: 0.65695
	loss_value_4: 0.0703
	loss_reward_4: 0.01414
	loss_policy_5: 0.05506
	accuracy_policy_5: 0.66391
	loss_value_5: 0.07163
	loss_reward_5: 0.01494
	loss_policy: 0.53982
	loss_value: 0.66453
	loss_reward: 0.05528
[2025-05-08 00:40:03] nn step 48750, lr: 0.1.
	loss_policy_0: 0.26242
	accuracy_policy_0: 0.68184
	loss_value_0: 0.31308
	loss_policy_1: 0.05284
	accuracy_policy_1: 0.67305
	loss_value_1: 0.06355
	loss_reward_1: 0.00635
	loss_policy_2: 0.05363
	accuracy_policy_2: 0.66164
	loss_value_2: 0.06546
	loss_reward_2: 0.00828
	loss_policy_3: 0.05377
	accuracy_policy_3: 0.66047
	loss_value_3: 0.06714
	loss_reward_3: 0.01065
	loss_policy_4: 0.05403
	accuracy_policy_4: 0.66137
	loss_value_4: 0.0685
	loss_reward_4: 0.0134
	loss_policy_5: 0.05391
	accuracy_policy_5: 0.66398
	loss_value_5: 0.07001
	loss_reward_5: 0.01502
	loss_policy: 0.5306
	loss_value: 0.64774
	loss_reward: 0.0537
[2025-05-08 00:40:11] nn step 48800, lr: 0.1.
	loss_policy_0: 0.26904
	accuracy_policy_0: 0.67797
	loss_value_0: 0.32038
	loss_policy_1: 0.05429
	accuracy_policy_1: 0.67359
	loss_value_1: 0.0654
	loss_reward_1: 0.00614
	loss_policy_2: 0.05461
	accuracy_policy_2: 0.66598
	loss_value_2: 0.0672
	loss_reward_2: 0.00815
	loss_policy_3: 0.05485
	accuracy_policy_3: 0.66301
	loss_value_3: 0.06885
	loss_reward_3: 0.01075
	loss_policy_4: 0.05509
	accuracy_policy_4: 0.66633
	loss_value_4: 0.07064
	loss_reward_4: 0.01367
	loss_policy_5: 0.05536
	accuracy_policy_5: 0.66934
	loss_value_5: 0.07227
	loss_reward_5: 0.01473
	loss_policy: 0.54325
	loss_value: 0.66474
	loss_reward: 0.05343
Optimization_Done 48800
[2025-05-08 00:43:18] [command] train weight_iter_48800.pkl 226 245
[2025-05-08 00:43:29] nn step 48850, lr: 0.1.
	loss_policy_0: 0.26289
	accuracy_policy_0: 0.69172
	loss_value_0: 0.33277
	loss_policy_1: 0.05357
	accuracy_policy_1: 0.67629
	loss_value_1: 0.06731
	loss_reward_1: 0.00648
	loss_policy_2: 0.05394
	accuracy_policy_2: 0.66691
	loss_value_2: 0.06899
	loss_reward_2: 0.00856
	loss_policy_3: 0.05416
	accuracy_policy_3: 0.67098
	loss_value_3: 0.07014
	loss_reward_3: 0.01114
	loss_policy_4: 0.05465
	accuracy_policy_4: 0.66289
	loss_value_4: 0.07145
	loss_reward_4: 0.01379
	loss_policy_5: 0.05479
	accuracy_policy_5: 0.66812
	loss_value_5: 0.07257
	loss_reward_5: 0.0151
	loss_policy: 0.53399
	loss_value: 0.68322
	loss_reward: 0.05508
[2025-05-08 00:43:38] nn step 48900, lr: 0.1.
	loss_policy_0: 0.26083
	accuracy_policy_0: 0.68719
	loss_value_0: 0.31974
	loss_policy_1: 0.05302
	accuracy_policy_1: 0.67758
	loss_value_1: 0.06491
	loss_reward_1: 0.00624
	loss_policy_2: 0.05383
	accuracy_policy_2: 0.67258
	loss_value_2: 0.06637
	loss_reward_2: 0.00811
	loss_policy_3: 0.0544
	accuracy_policy_3: 0.66207
	loss_value_3: 0.06834
	loss_reward_3: 0.01104
	loss_policy_4: 0.05403
	accuracy_policy_4: 0.66688
	loss_value_4: 0.06965
	loss_reward_4: 0.01354
	loss_policy_5: 0.0539
	accuracy_policy_5: 0.67156
	loss_value_5: 0.07121
	loss_reward_5: 0.01392
	loss_policy: 0.53001
	loss_value: 0.66021
	loss_reward: 0.05284
[2025-05-08 00:43:44] nn step 48950, lr: 0.1.
	loss_policy_0: 0.28634
	accuracy_policy_0: 0.68633
	loss_value_0: 0.34492
	loss_policy_1: 0.05805
	accuracy_policy_1: 0.67953
	loss_value_1: 0.07095
	loss_reward_1: 0.00703
	loss_policy_2: 0.05878
	accuracy_policy_2: 0.67098
	loss_value_2: 0.07268
	loss_reward_2: 0.00903
	loss_policy_3: 0.05964
	accuracy_policy_3: 0.66414
	loss_value_3: 0.07434
	loss_reward_3: 0.01165
	loss_policy_4: 0.05976
	accuracy_policy_4: 0.66566
	loss_value_4: 0.07597
	loss_reward_4: 0.01529
	loss_policy_5: 0.05979
	accuracy_policy_5: 0.66727
	loss_value_5: 0.07797
	loss_reward_5: 0.01612
	loss_policy: 0.58236
	loss_value: 0.71684
	loss_reward: 0.05912
[2025-05-08 00:43:53] nn step 49000, lr: 0.1.
	loss_policy_0: 0.25742
	accuracy_policy_0: 0.6877
	loss_value_0: 0.30961
	loss_policy_1: 0.05218
	accuracy_policy_1: 0.68078
	loss_value_1: 0.06293
	loss_reward_1: 0.00642
	loss_policy_2: 0.05277
	accuracy_policy_2: 0.67344
	loss_value_2: 0.06448
	loss_reward_2: 0.00806
	loss_policy_3: 0.05312
	accuracy_policy_3: 0.66355
	loss_value_3: 0.06593
	loss_reward_3: 0.01037
	loss_policy_4: 0.05367
	accuracy_policy_4: 0.66418
	loss_value_4: 0.0674
	loss_reward_4: 0.01356
	loss_policy_5: 0.05346
	accuracy_policy_5: 0.66402
	loss_value_5: 0.06893
	loss_reward_5: 0.01479
	loss_policy: 0.52262
	loss_value: 0.63927
	loss_reward: 0.05319
Optimization_Done 49000
[2025-05-08 00:47:01] [command] train weight_iter_49000.pkl 227 246
[2025-05-08 00:47:12] nn step 49050, lr: 0.1.
	loss_policy_0: 0.26077
	accuracy_policy_0: 0.69008
	loss_value_0: 0.32469
	loss_policy_1: 0.05296
	accuracy_policy_1: 0.6757
	loss_value_1: 0.06613
	loss_reward_1: 0.00644
	loss_policy_2: 0.05342
	accuracy_policy_2: 0.66766
	loss_value_2: 0.06754
	loss_reward_2: 0.0082
	loss_policy_3: 0.0539
	accuracy_policy_3: 0.6643
	loss_value_3: 0.06884
	loss_reward_3: 0.01084
	loss_policy_4: 0.05427
	accuracy_policy_4: 0.6584
	loss_value_4: 0.07042
	loss_reward_4: 0.01371
	loss_policy_5: 0.05421
	accuracy_policy_5: 0.66223
	loss_value_5: 0.07159
	loss_reward_5: 0.01453
	loss_policy: 0.52953
	loss_value: 0.66921
	loss_reward: 0.05372
[2025-05-08 00:47:20] nn step 49100, lr: 0.1.
	loss_policy_0: 0.25411
	accuracy_policy_0: 0.68773
	loss_value_0: 0.30555
	loss_policy_1: 0.05133
	accuracy_policy_1: 0.68203
	loss_value_1: 0.06191
	loss_reward_1: 0.00622
	loss_policy_2: 0.05175
	accuracy_policy_2: 0.66707
	loss_value_2: 0.06383
	loss_reward_2: 0.00808
	loss_policy_3: 0.05226
	accuracy_policy_3: 0.65645
	loss_value_3: 0.06513
	loss_reward_3: 0.01078
	loss_policy_4: 0.05233
	accuracy_policy_4: 0.66012
	loss_value_4: 0.06667
	loss_reward_4: 0.01325
	loss_policy_5: 0.05213
	accuracy_policy_5: 0.66863
	loss_value_5: 0.06787
	loss_reward_5: 0.0139
	loss_policy: 0.51391
	loss_value: 0.63097
	loss_reward: 0.05224
[2025-05-08 00:47:29] nn step 49150, lr: 0.1.
	loss_policy_0: 0.25546
	accuracy_policy_0: 0.6877
	loss_value_0: 0.30361
	loss_policy_1: 0.05238
	accuracy_policy_1: 0.67324
	loss_value_1: 0.0619
	loss_reward_1: 0.0063
	loss_policy_2: 0.05261
	accuracy_policy_2: 0.66434
	loss_value_2: 0.06349
	loss_reward_2: 0.00796
	loss_policy_3: 0.05267
	accuracy_policy_3: 0.66422
	loss_value_3: 0.06488
	loss_reward_3: 0.01073
	loss_policy_4: 0.05307
	accuracy_policy_4: 0.66547
	loss_value_4: 0.06623
	loss_reward_4: 0.01308
	loss_policy_5: 0.05334
	accuracy_policy_5: 0.66332
	loss_value_5: 0.06774
	loss_reward_5: 0.01414
	loss_policy: 0.51953
	loss_value: 0.62786
	loss_reward: 0.05221
[2025-05-08 00:47:35] nn step 49200, lr: 0.1.
	loss_policy_0: 0.27297
	accuracy_policy_0: 0.69375
	loss_value_0: 0.33063
	loss_policy_1: 0.05533
	accuracy_policy_1: 0.68145
	loss_value_1: 0.06713
	loss_reward_1: 0.00677
	loss_policy_2: 0.05624
	accuracy_policy_2: 0.66715
	loss_value_2: 0.06864
	loss_reward_2: 0.00907
	loss_policy_3: 0.05681
	accuracy_policy_3: 0.66188
	loss_value_3: 0.07032
	loss_reward_3: 0.01116
	loss_policy_4: 0.05672
	accuracy_policy_4: 0.6641
	loss_value_4: 0.07209
	loss_reward_4: 0.01419
	loss_policy_5: 0.05676
	accuracy_policy_5: 0.67035
	loss_value_5: 0.07383
	loss_reward_5: 0.01581
	loss_policy: 0.55482
	loss_value: 0.68264
	loss_reward: 0.05701
Optimization_Done 49200
[2025-05-08 00:50:39] [command] train weight_iter_49200.pkl 228 247
[2025-05-08 00:50:48] nn step 49250, lr: 0.1.
	loss_policy_0: 0.27559
	accuracy_policy_0: 0.68848
	loss_value_0: 0.34339
	loss_policy_1: 0.05559
	accuracy_policy_1: 0.68195
	loss_value_1: 0.06991
	loss_reward_1: 0.00679
	loss_policy_2: 0.05603
	accuracy_policy_2: 0.67359
	loss_value_2: 0.07149
	loss_reward_2: 0.00886
	loss_policy_3: 0.05663
	accuracy_policy_3: 0.66914
	loss_value_3: 0.07315
	loss_reward_3: 0.0114
	loss_policy_4: 0.05697
	accuracy_policy_4: 0.66938
	loss_value_4: 0.07412
	loss_reward_4: 0.01406
	loss_policy_5: 0.05674
	accuracy_policy_5: 0.6734
	loss_value_5: 0.07536
	loss_reward_5: 0.0151
	loss_policy: 0.55755
	loss_value: 0.70742
	loss_reward: 0.05622
[2025-05-08 00:50:56] nn step 49300, lr: 0.1.
	loss_policy_0: 0.28214
	accuracy_policy_0: 0.68719
	loss_value_0: 0.34241
	loss_policy_1: 0.05686
	accuracy_policy_1: 0.68215
	loss_value_1: 0.06934
	loss_reward_1: 0.00684
	loss_policy_2: 0.05732
	accuracy_policy_2: 0.67172
	loss_value_2: 0.07096
	loss_reward_2: 0.00887
	loss_policy_3: 0.05772
	accuracy_policy_3: 0.66906
	loss_value_3: 0.07282
	loss_reward_3: 0.01174
	loss_policy_4: 0.05807
	accuracy_policy_4: 0.66848
	loss_value_4: 0.07448
	loss_reward_4: 0.01436
	loss_policy_5: 0.05804
	accuracy_policy_5: 0.67039
	loss_value_5: 0.0759
	loss_reward_5: 0.01549
	loss_policy: 0.57013
	loss_value: 0.70591
	loss_reward: 0.05729
[2025-05-08 00:51:05] nn step 49350, lr: 0.1.
	loss_policy_0: 0.25826
	accuracy_policy_0: 0.6927
	loss_value_0: 0.30474
	loss_policy_1: 0.05191
	accuracy_policy_1: 0.68047
	loss_value_1: 0.0618
	loss_reward_1: 0.00617
	loss_policy_2: 0.05218
	accuracy_policy_2: 0.67754
	loss_value_2: 0.06357
	loss_reward_2: 0.00799
	loss_policy_3: 0.05249
	accuracy_policy_3: 0.66848
	loss_value_3: 0.0655
	loss_reward_3: 0.01083
	loss_policy_4: 0.05276
	accuracy_policy_4: 0.67332
	loss_value_4: 0.06722
	loss_reward_4: 0.013
	loss_policy_5: 0.05312
	accuracy_policy_5: 0.6682
	loss_value_5: 0.06836
	loss_reward_5: 0.01442
	loss_policy: 0.52071
	loss_value: 0.63119
	loss_reward: 0.05241
[2025-05-08 00:51:13] nn step 49400, lr: 0.1.
	loss_policy_0: 0.28093
	accuracy_policy_0: 0.68918
	loss_value_0: 0.33678
	loss_policy_1: 0.05696
	accuracy_policy_1: 0.67621
	loss_value_1: 0.06855
	loss_reward_1: 0.00695
	loss_policy_2: 0.05778
	accuracy_policy_2: 0.66902
	loss_value_2: 0.07018
	loss_reward_2: 0.00887
	loss_policy_3: 0.05806
	accuracy_policy_3: 0.66469
	loss_value_3: 0.07174
	loss_reward_3: 0.01155
	loss_policy_4: 0.05818
	accuracy_policy_4: 0.66801
	loss_value_4: 0.07346
	loss_reward_4: 0.01424
	loss_policy_5: 0.05776
	accuracy_policy_5: 0.67684
	loss_value_5: 0.07477
	loss_reward_5: 0.01536
	loss_policy: 0.56968
	loss_value: 0.69548
	loss_reward: 0.05697
Optimization_Done 49400
[2025-05-08 00:54:23] [command] train weight_iter_49400.pkl 229 248
[2025-05-08 00:54:33] nn step 49450, lr: 0.1.
	loss_policy_0: 0.25087
	accuracy_policy_0: 0.68957
	loss_value_0: 0.31565
	loss_policy_1: 0.05065
	accuracy_policy_1: 0.68027
	loss_value_1: 0.06382
	loss_reward_1: 0.00601
	loss_policy_2: 0.05133
	accuracy_policy_2: 0.67121
	loss_value_2: 0.06548
	loss_reward_2: 0.0079
	loss_policy_3: 0.05178
	accuracy_policy_3: 0.665
	loss_value_3: 0.06624
	loss_reward_3: 0.00976
	loss_policy_4: 0.05174
	accuracy_policy_4: 0.66672
	loss_value_4: 0.06735
	loss_reward_4: 0.01262
	loss_policy_5: 0.05207
	accuracy_policy_5: 0.65832
	loss_value_5: 0.06886
	loss_reward_5: 0.01408
	loss_policy: 0.50844
	loss_value: 0.64738
	loss_reward: 0.05037
[2025-05-08 00:54:41] nn step 49500, lr: 0.1.
	loss_policy_0: 0.26189
	accuracy_policy_0: 0.68906
	loss_value_0: 0.32343
	loss_policy_1: 0.05299
	accuracy_policy_1: 0.6798
	loss_value_1: 0.066
	loss_reward_1: 0.00647
	loss_policy_2: 0.0538
	accuracy_policy_2: 0.66984
	loss_value_2: 0.06733
	loss_reward_2: 0.00803
	loss_policy_3: 0.05405
	accuracy_policy_3: 0.66141
	loss_value_3: 0.0689
	loss_reward_3: 0.01075
	loss_policy_4: 0.05443
	accuracy_policy_4: 0.66656
	loss_value_4: 0.07051
	loss_reward_4: 0.01366
	loss_policy_5: 0.05423
	accuracy_policy_5: 0.67215
	loss_value_5: 0.07175
	loss_reward_5: 0.01435
	loss_policy: 0.53138
	loss_value: 0.66792
	loss_reward: 0.05326
[2025-05-08 00:54:50] nn step 49550, lr: 0.1.
	loss_policy_0: 0.25084
	accuracy_policy_0: 0.69402
	loss_value_0: 0.30623
	loss_policy_1: 0.05119
	accuracy_policy_1: 0.67523
	loss_value_1: 0.06241
	loss_reward_1: 0.00585
	loss_policy_2: 0.05147
	accuracy_policy_2: 0.67168
	loss_value_2: 0.06376
	loss_reward_2: 0.008
	loss_policy_3: 0.05188
	accuracy_policy_3: 0.66926
	loss_value_3: 0.06576
	loss_reward_3: 0.01028
	loss_policy_4: 0.05229
	accuracy_policy_4: 0.66781
	loss_value_4: 0.06698
	loss_reward_4: 0.01247
	loss_policy_5: 0.05242
	accuracy_policy_5: 0.67168
	loss_value_5: 0.06786
	loss_reward_5: 0.01385
	loss_policy: 0.51009
	loss_value: 0.63301
	loss_reward: 0.05044
[2025-05-08 00:54:58] nn step 49600, lr: 0.1.
	loss_policy_0: 0.24566
	accuracy_policy_0: 0.6975
	loss_value_0: 0.29554
	loss_policy_1: 0.0499
	accuracy_policy_1: 0.68152
	loss_value_1: 0.06028
	loss_reward_1: 0.00587
	loss_policy_2: 0.05062
	accuracy_policy_2: 0.66801
	loss_value_2: 0.06186
	loss_reward_2: 0.00754
	loss_policy_3: 0.05057
	accuracy_policy_3: 0.66598
	loss_value_3: 0.06308
	loss_reward_3: 0.01005
	loss_policy_4: 0.05105
	accuracy_policy_4: 0.67031
	loss_value_4: 0.06454
	loss_reward_4: 0.01253
	loss_policy_5: 0.05112
	accuracy_policy_5: 0.66844
	loss_value_5: 0.06584
	loss_reward_5: 0.01379
	loss_policy: 0.49893
	loss_value: 0.61114
	loss_reward: 0.04978
Optimization_Done 49600
[2025-05-08 00:58:05] [command] train weight_iter_49600.pkl 230 249
[2025-05-08 00:58:16] nn step 49650, lr: 0.1.
	loss_policy_0: 0.26204
	accuracy_policy_0: 0.66324
	loss_value_0: 0.31748
	loss_policy_1: 0.05231
	accuracy_policy_1: 0.66484
	loss_value_1: 0.0644
	loss_reward_1: 0.006
	loss_policy_2: 0.05277
	accuracy_policy_2: 0.65465
	loss_value_2: 0.06587
	loss_reward_2: 0.00744
	loss_policy_3: 0.05294
	accuracy_policy_3: 0.65211
	loss_value_3: 0.06733
	loss_reward_3: 0.01016
	loss_policy_4: 0.05278
	accuracy_policy_4: 0.65309
	loss_value_4: 0.06869
	loss_reward_4: 0.01261
	loss_policy_5: 0.05313
	accuracy_policy_5: 0.6502
	loss_value_5: 0.07008
	loss_reward_5: 0.01333
	loss_policy: 0.52595
	loss_value: 0.65387
	loss_reward: 0.04954
[2025-05-08 00:58:22] nn step 49700, lr: 0.1.
	loss_policy_0: 0.26805
	accuracy_policy_0: 0.68113
	loss_value_0: 0.32384
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.66777
	loss_value_1: 0.06605
	loss_reward_1: 0.0062
	loss_policy_2: 0.05495
	accuracy_policy_2: 0.65715
	loss_value_2: 0.06791
	loss_reward_2: 0.00834
	loss_policy_3: 0.05528
	accuracy_policy_3: 0.65578
	loss_value_3: 0.06939
	loss_reward_3: 0.01091
	loss_policy_4: 0.05605
	accuracy_policy_4: 0.64793
	loss_value_4: 0.07078
	loss_reward_4: 0.01385
	loss_policy_5: 0.056
	accuracy_policy_5: 0.65383
	loss_value_5: 0.07209
	loss_reward_5: 0.01508
	loss_policy: 0.54476
	loss_value: 0.67006
	loss_reward: 0.05437
[2025-05-08 00:58:30] nn step 49750, lr: 0.1.
	loss_policy_0: 0.26728
	accuracy_policy_0: 0.68238
	loss_value_0: 0.32004
	loss_policy_1: 0.05422
	accuracy_policy_1: 0.66375
	loss_value_1: 0.06522
	loss_reward_1: 0.00658
	loss_policy_2: 0.05447
	accuracy_policy_2: 0.65773
	loss_value_2: 0.06665
	loss_reward_2: 0.00816
	loss_policy_3: 0.05487
	accuracy_policy_3: 0.65625
	loss_value_3: 0.06804
	loss_reward_3: 0.01088
	loss_policy_4: 0.05535
	accuracy_policy_4: 0.65289
	loss_value_4: 0.06958
	loss_reward_4: 0.01318
	loss_policy_5: 0.05533
	accuracy_policy_5: 0.65629
	loss_value_5: 0.07137
	loss_reward_5: 0.01476
	loss_policy: 0.54151
	loss_value: 0.6609
	loss_reward: 0.05357
[2025-05-08 00:58:39] nn step 49800, lr: 0.1.
	loss_policy_0: 0.27058
	accuracy_policy_0: 0.68742
	loss_value_0: 0.3219
	loss_policy_1: 0.05524
	accuracy_policy_1: 0.66809
	loss_value_1: 0.06548
	loss_reward_1: 0.00646
	loss_policy_2: 0.05563
	accuracy_policy_2: 0.66621
	loss_value_2: 0.06709
	loss_reward_2: 0.00831
	loss_policy_3: 0.05659
	accuracy_policy_3: 0.65238
	loss_value_3: 0.06871
	loss_reward_3: 0.01074
	loss_policy_4: 0.05657
	accuracy_policy_4: 0.65461
	loss_value_4: 0.07036
	loss_reward_4: 0.01386
	loss_policy_5: 0.05654
	accuracy_policy_5: 0.65953
	loss_value_5: 0.07173
	loss_reward_5: 0.01513
	loss_policy: 0.55115
	loss_value: 0.66528
	loss_reward: 0.05451
Optimization_Done 49800
[2025-05-08 01:01:41] [command] train weight_iter_49800.pkl 231 250
[2025-05-08 01:01:52] nn step 49850, lr: 0.1.
	loss_policy_0: 0.2564
	accuracy_policy_0: 0.67535
	loss_value_0: 0.31212
	loss_policy_1: 0.05198
	accuracy_policy_1: 0.66477
	loss_value_1: 0.06338
	loss_reward_1: 0.00607
	loss_policy_2: 0.05258
	accuracy_policy_2: 0.65848
	loss_value_2: 0.06504
	loss_reward_2: 0.00799
	loss_policy_3: 0.05289
	accuracy_policy_3: 0.65234
	loss_value_3: 0.06611
	loss_reward_3: 0.01038
	loss_policy_4: 0.05302
	accuracy_policy_4: 0.65574
	loss_value_4: 0.06727
	loss_reward_4: 0.01286
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.65992
	loss_value_5: 0.06841
	loss_reward_5: 0.01424
	loss_policy: 0.51969
	loss_value: 0.64232
	loss_reward: 0.05154
[2025-05-08 01:02:00] nn step 49900, lr: 0.1.
	loss_policy_0: 0.26953
	accuracy_policy_0: 0.67535
	loss_value_0: 0.32308
	loss_policy_1: 0.05511
	accuracy_policy_1: 0.66156
	loss_value_1: 0.06572
	loss_reward_1: 0.00624
	loss_policy_2: 0.05552
	accuracy_policy_2: 0.65652
	loss_value_2: 0.06727
	loss_reward_2: 0.00815
	loss_policy_3: 0.05545
	accuracy_policy_3: 0.65527
	loss_value_3: 0.06852
	loss_reward_3: 0.01079
	loss_policy_4: 0.05573
	accuracy_policy_4: 0.65164
	loss_value_4: 0.07018
	loss_reward_4: 0.01387
	loss_policy_5: 0.05596
	accuracy_policy_5: 0.65797
	loss_value_5: 0.07146
	loss_reward_5: 0.01445
	loss_policy: 0.5473
	loss_value: 0.66623
	loss_reward: 0.0535
[2025-05-08 01:02:07] nn step 49950, lr: 0.1.
	loss_policy_0: 0.27539
	accuracy_policy_0: 0.67676
	loss_value_0: 0.32844
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.6698
	loss_value_1: 0.06704
	loss_reward_1: 0.00647
	loss_policy_2: 0.05619
	accuracy_policy_2: 0.66211
	loss_value_2: 0.06834
	loss_reward_2: 0.0081
	loss_policy_3: 0.05621
	accuracy_policy_3: 0.66094
	loss_value_3: 0.07016
	loss_reward_3: 0.01093
	loss_policy_4: 0.05646
	accuracy_policy_4: 0.66273
	loss_value_4: 0.07145
	loss_reward_4: 0.01384
	loss_policy_5: 0.05653
	accuracy_policy_5: 0.66273
	loss_value_5: 0.07305
	loss_reward_5: 0.01532
	loss_policy: 0.55613
	loss_value: 0.67849
	loss_reward: 0.05467
[2025-05-08 01:02:16] nn step 50000, lr: 0.1.
	loss_policy_0: 0.26668
	accuracy_policy_0: 0.68141
	loss_value_0: 0.31834
	loss_policy_1: 0.05405
	accuracy_policy_1: 0.6727
	loss_value_1: 0.06495
	loss_reward_1: 0.00653
	loss_policy_2: 0.05481
	accuracy_policy_2: 0.66105
	loss_value_2: 0.06668
	loss_reward_2: 0.00844
	loss_policy_3: 0.05525
	accuracy_policy_3: 0.65344
	loss_value_3: 0.06818
	loss_reward_3: 0.01079
	loss_policy_4: 0.05563
	accuracy_policy_4: 0.65367
	loss_value_4: 0.06956
	loss_reward_4: 0.01364
	loss_policy_5: 0.05549
	accuracy_policy_5: 0.65758
	loss_value_5: 0.07104
	loss_reward_5: 0.01497
	loss_policy: 0.54191
	loss_value: 0.65876
	loss_reward: 0.05437
Optimization_Done 50000
[2025-05-08 01:05:25] [command] train weight_iter_50000.pkl 232 251
[2025-05-08 01:05:34] nn step 50050, lr: 0.1.
	loss_policy_0: 0.26067
	accuracy_policy_0: 0.68348
	loss_value_0: 0.32245
	loss_policy_1: 0.05287
	accuracy_policy_1: 0.67211
	loss_value_1: 0.06556
	loss_reward_1: 0.00632
	loss_policy_2: 0.05325
	accuracy_policy_2: 0.66781
	loss_value_2: 0.06723
	loss_reward_2: 0.00799
	loss_policy_3: 0.05375
	accuracy_policy_3: 0.65715
	loss_value_3: 0.06865
	loss_reward_3: 0.01053
	loss_policy_4: 0.05438
	accuracy_policy_4: 0.65676
	loss_value_4: 0.06991
	loss_reward_4: 0.01357
	loss_policy_5: 0.05407
	accuracy_policy_5: 0.66094
	loss_value_5: 0.07126
	loss_reward_5: 0.01417
	loss_policy: 0.52899
	loss_value: 0.66506
	loss_reward: 0.05258
[2025-05-08 01:05:42] nn step 50100, lr: 0.1.
	loss_policy_0: 0.27756
	accuracy_policy_0: 0.68574
	loss_value_0: 0.34078
	loss_policy_1: 0.05595
	accuracy_policy_1: 0.67168
	loss_value_1: 0.06913
	loss_reward_1: 0.00657
	loss_policy_2: 0.05694
	accuracy_policy_2: 0.65949
	loss_value_2: 0.07067
	loss_reward_2: 0.00875
	loss_policy_3: 0.05712
	accuracy_policy_3: 0.65469
	loss_value_3: 0.07192
	loss_reward_3: 0.01154
	loss_policy_4: 0.05696
	accuracy_policy_4: 0.66223
	loss_value_4: 0.0731
	loss_reward_4: 0.01395
	loss_policy_5: 0.0568
	accuracy_policy_5: 0.66996
	loss_value_5: 0.07467
	loss_reward_5: 0.01517
	loss_policy: 0.56132
	loss_value: 0.70026
	loss_reward: 0.05598
[2025-05-08 01:05:51] nn step 50150, lr: 0.1.
	loss_policy_0: 0.27246
	accuracy_policy_0: 0.68652
	loss_value_0: 0.33387
	loss_policy_1: 0.05542
	accuracy_policy_1: 0.67523
	loss_value_1: 0.06816
	loss_reward_1: 0.00673
	loss_policy_2: 0.05581
	accuracy_policy_2: 0.66516
	loss_value_2: 0.06989
	loss_reward_2: 0.00835
	loss_policy_3: 0.05657
	accuracy_policy_3: 0.6593
	loss_value_3: 0.0713
	loss_reward_3: 0.01105
	loss_policy_4: 0.05675
	accuracy_policy_4: 0.65668
	loss_value_4: 0.0731
	loss_reward_4: 0.01382
	loss_policy_5: 0.05675
	accuracy_policy_5: 0.66051
	loss_value_5: 0.07448
	loss_reward_5: 0.0153
	loss_policy: 0.55376
	loss_value: 0.69081
	loss_reward: 0.05525
[2025-05-08 01:05:57] nn step 50200, lr: 0.1.
	loss_policy_0: 0.27491
	accuracy_policy_0: 0.68148
	loss_value_0: 0.33138
	loss_policy_1: 0.05565
	accuracy_policy_1: 0.66863
	loss_value_1: 0.06737
	loss_reward_1: 0.00662
	loss_policy_2: 0.05623
	accuracy_policy_2: 0.66328
	loss_value_2: 0.06901
	loss_reward_2: 0.00881
	loss_policy_3: 0.05657
	accuracy_policy_3: 0.65758
	loss_value_3: 0.07037
	loss_reward_3: 0.01095
	loss_policy_4: 0.05703
	accuracy_policy_4: 0.65973
	loss_value_4: 0.0721
	loss_reward_4: 0.01395
	loss_policy_5: 0.05718
	accuracy_policy_5: 0.65906
	loss_value_5: 0.0739
	loss_reward_5: 0.01513
	loss_policy: 0.55757
	loss_value: 0.68412
	loss_reward: 0.05547
Optimization_Done 50200
[2025-05-08 01:09:04] [command] train weight_iter_50200.pkl 233 252
[2025-05-08 01:09:13] nn step 50250, lr: 0.1.
	loss_policy_0: 0.2769
	accuracy_policy_0: 0.6848
	loss_value_0: 0.34727
	loss_policy_1: 0.05622
	accuracy_policy_1: 0.66906
	loss_value_1: 0.07024
	loss_reward_1: 0.00666
	loss_policy_2: 0.05706
	accuracy_policy_2: 0.6591
	loss_value_2: 0.07132
	loss_reward_2: 0.00875
	loss_policy_3: 0.05735
	accuracy_policy_3: 0.66055
	loss_value_3: 0.07295
	loss_reward_3: 0.01133
	loss_policy_4: 0.05772
	accuracy_policy_4: 0.65879
	loss_value_4: 0.0742
	loss_reward_4: 0.01445
	loss_policy_5: 0.05802
	accuracy_policy_5: 0.65676
	loss_value_5: 0.07558
	loss_reward_5: 0.01565
	loss_policy: 0.56325
	loss_value: 0.71157
	loss_reward: 0.05684
[2025-05-08 01:09:22] nn step 50300, lr: 0.1.
	loss_policy_0: 0.26778
	accuracy_policy_0: 0.68328
	loss_value_0: 0.32757
	loss_policy_1: 0.05429
	accuracy_policy_1: 0.66875
	loss_value_1: 0.06668
	loss_reward_1: 0.00652
	loss_policy_2: 0.05462
	accuracy_policy_2: 0.66703
	loss_value_2: 0.06845
	loss_reward_2: 0.0084
	loss_policy_3: 0.05525
	accuracy_policy_3: 0.66074
	loss_value_3: 0.06999
	loss_reward_3: 0.01089
	loss_policy_4: 0.05576
	accuracy_policy_4: 0.6577
	loss_value_4: 0.07114
	loss_reward_4: 0.01389
	loss_policy_5: 0.05534
	accuracy_policy_5: 0.66137
	loss_value_5: 0.07248
	loss_reward_5: 0.01479
	loss_policy: 0.54303
	loss_value: 0.67631
	loss_reward: 0.05449
[2025-05-08 01:09:30] nn step 50350, lr: 0.1.
	loss_policy_0: 0.27626
	accuracy_policy_0: 0.68059
	loss_value_0: 0.3359
	loss_policy_1: 0.0562
	accuracy_policy_1: 0.66953
	loss_value_1: 0.06844
	loss_reward_1: 0.00696
	loss_policy_2: 0.0563
	accuracy_policy_2: 0.66695
	loss_value_2: 0.07023
	loss_reward_2: 0.00882
	loss_policy_3: 0.05702
	accuracy_policy_3: 0.65883
	loss_value_3: 0.07168
	loss_reward_3: 0.0113
	loss_policy_4: 0.05732
	accuracy_policy_4: 0.65871
	loss_value_4: 0.07314
	loss_reward_4: 0.01465
	loss_policy_5: 0.05724
	accuracy_policy_5: 0.66527
	loss_value_5: 0.07493
	loss_reward_5: 0.01534
	loss_policy: 0.56034
	loss_value: 0.69432
	loss_reward: 0.05707
[2025-05-08 01:09:39] nn step 50400, lr: 0.1.
	loss_policy_0: 0.26162
	accuracy_policy_0: 0.68566
	loss_value_0: 0.31645
	loss_policy_1: 0.05302
	accuracy_policy_1: 0.6716
	loss_value_1: 0.06489
	loss_reward_1: 0.00614
	loss_policy_2: 0.05396
	accuracy_policy_2: 0.66152
	loss_value_2: 0.06683
	loss_reward_2: 0.00787
	loss_policy_3: 0.05428
	accuracy_policy_3: 0.65676
	loss_value_3: 0.06791
	loss_reward_3: 0.01078
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.65598
	loss_value_4: 0.06948
	loss_reward_4: 0.01299
	loss_policy_5: 0.05419
	accuracy_policy_5: 0.65863
	loss_value_5: 0.0708
	loss_reward_5: 0.01409
	loss_policy: 0.53155
	loss_value: 0.65637
	loss_reward: 0.05187
Optimization_Done 50400
[2025-05-08 01:12:49] [command] train weight_iter_50400.pkl 234 253
[2025-05-08 01:12:59] nn step 50450, lr: 0.1.
	loss_policy_0: 0.27019
	accuracy_policy_0: 0.68332
	loss_value_0: 0.33436
	loss_policy_1: 0.05514
	accuracy_policy_1: 0.67465
	loss_value_1: 0.06779
	loss_reward_1: 0.0063
	loss_policy_2: 0.05509
	accuracy_policy_2: 0.67125
	loss_value_2: 0.06919
	loss_reward_2: 0.00802
	loss_policy_3: 0.05522
	accuracy_policy_3: 0.66602
	loss_value_3: 0.07034
	loss_reward_3: 0.01097
	loss_policy_4: 0.05586
	accuracy_policy_4: 0.66039
	loss_value_4: 0.07192
	loss_reward_4: 0.01387
	loss_policy_5: 0.05565
	accuracy_policy_5: 0.66316
	loss_value_5: 0.07339
	loss_reward_5: 0.01488
	loss_policy: 0.54715
	loss_value: 0.68699
	loss_reward: 0.05405
[2025-05-08 01:13:05] nn step 50500, lr: 0.1.
	loss_policy_0: 0.27365
	accuracy_policy_0: 0.68488
	loss_value_0: 0.33713
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.67309
	loss_value_1: 0.06859
	loss_reward_1: 0.00646
	loss_policy_2: 0.05592
	accuracy_policy_2: 0.66859
	loss_value_2: 0.07036
	loss_reward_2: 0.00836
	loss_policy_3: 0.05636
	accuracy_policy_3: 0.66305
	loss_value_3: 0.07176
	loss_reward_3: 0.01115
	loss_policy_4: 0.05697
	accuracy_policy_4: 0.66062
	loss_value_4: 0.0731
	loss_reward_4: 0.0141
	loss_policy_5: 0.05667
	accuracy_policy_5: 0.66629
	loss_value_5: 0.07486
	loss_reward_5: 0.01487
	loss_policy: 0.55494
	loss_value: 0.6958
	loss_reward: 0.05494
[2025-05-08 01:13:13] nn step 50550, lr: 0.1.
	loss_policy_0: 0.27261
	accuracy_policy_0: 0.6775
	loss_value_0: 0.32626
	loss_policy_1: 0.05494
	accuracy_policy_1: 0.6718
	loss_value_1: 0.06638
	loss_reward_1: 0.00626
	loss_policy_2: 0.05551
	accuracy_policy_2: 0.66289
	loss_value_2: 0.06804
	loss_reward_2: 0.00826
	loss_policy_3: 0.05584
	accuracy_policy_3: 0.65793
	loss_value_3: 0.06969
	loss_reward_3: 0.01123
	loss_policy_4: 0.05581
	accuracy_policy_4: 0.66438
	loss_value_4: 0.07117
	loss_reward_4: 0.01315
	loss_policy_5: 0.05628
	accuracy_policy_5: 0.66238
	loss_value_5: 0.07234
	loss_reward_5: 0.01492
	loss_policy: 0.55097
	loss_value: 0.67387
	loss_reward: 0.05383
[2025-05-08 01:13:21] nn step 50600, lr: 0.1.
	loss_policy_0: 0.26844
	accuracy_policy_0: 0.67812
	loss_value_0: 0.32061
	loss_policy_1: 0.0541
	accuracy_policy_1: 0.6666
	loss_value_1: 0.06571
	loss_reward_1: 0.00604
	loss_policy_2: 0.05456
	accuracy_policy_2: 0.66449
	loss_value_2: 0.06732
	loss_reward_2: 0.00819
	loss_policy_3: 0.0549
	accuracy_policy_3: 0.65734
	loss_value_3: 0.06883
	loss_reward_3: 0.01081
	loss_policy_4: 0.05505
	accuracy_policy_4: 0.65957
	loss_value_4: 0.07041
	loss_reward_4: 0.0133
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.65914
	loss_value_5: 0.07168
	loss_reward_5: 0.01489
	loss_policy: 0.54231
	loss_value: 0.66455
	loss_reward: 0.05322
Optimization_Done 50600
[2025-05-08 01:16:21] [command] train weight_iter_50600.pkl 235 254
[2025-05-08 01:16:30] nn step 50650, lr: 0.1.
	loss_policy_0: 0.27134
	accuracy_policy_0: 0.68285
	loss_value_0: 0.32977
	loss_policy_1: 0.05494
	accuracy_policy_1: 0.66672
	loss_value_1: 0.06685
	loss_reward_1: 0.00629
	loss_policy_2: 0.05541
	accuracy_policy_2: 0.66832
	loss_value_2: 0.06857
	loss_reward_2: 0.00818
	loss_policy_3: 0.05589
	accuracy_policy_3: 0.65992
	loss_value_3: 0.07022
	loss_reward_3: 0.01094
	loss_policy_4: 0.05639
	accuracy_policy_4: 0.65625
	loss_value_4: 0.07177
	loss_reward_4: 0.01351
	loss_policy_5: 0.05654
	accuracy_policy_5: 0.66047
	loss_value_5: 0.07313
	loss_reward_5: 0.01449
	loss_policy: 0.55052
	loss_value: 0.68031
	loss_reward: 0.05342
[2025-05-08 01:16:38] nn step 50700, lr: 0.1.
	loss_policy_0: 0.27663
	accuracy_policy_0: 0.6825
	loss_value_0: 0.33598
	loss_policy_1: 0.05632
	accuracy_policy_1: 0.67152
	loss_value_1: 0.06848
	loss_reward_1: 0.00682
	loss_policy_2: 0.05672
	accuracy_policy_2: 0.66492
	loss_value_2: 0.0703
	loss_reward_2: 0.00849
	loss_policy_3: 0.05736
	accuracy_policy_3: 0.65941
	loss_value_3: 0.0718
	loss_reward_3: 0.01092
	loss_policy_4: 0.05775
	accuracy_policy_4: 0.65289
	loss_value_4: 0.07301
	loss_reward_4: 0.01412
	loss_policy_5: 0.0572
	accuracy_policy_5: 0.66426
	loss_value_5: 0.07459
	loss_reward_5: 0.01532
	loss_policy: 0.56198
	loss_value: 0.69418
	loss_reward: 0.05567
[2025-05-08 01:16:45] nn step 50750, lr: 0.1.
	loss_policy_0: 0.27577
	accuracy_policy_0: 0.67684
	loss_value_0: 0.32962
	loss_policy_1: 0.05573
	accuracy_policy_1: 0.66566
	loss_value_1: 0.06701
	loss_reward_1: 0.00669
	loss_policy_2: 0.05612
	accuracy_policy_2: 0.65891
	loss_value_2: 0.06881
	loss_reward_2: 0.00843
	loss_policy_3: 0.05654
	accuracy_policy_3: 0.65469
	loss_value_3: 0.07019
	loss_reward_3: 0.01115
	loss_policy_4: 0.05682
	accuracy_policy_4: 0.65836
	loss_value_4: 0.07171
	loss_reward_4: 0.01397
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.66199
	loss_value_5: 0.07269
	loss_reward_5: 0.01526
	loss_policy: 0.55781
	loss_value: 0.68002
	loss_reward: 0.0555
[2025-05-08 01:16:54] nn step 50800, lr: 0.1.
	loss_policy_0: 0.26884
	accuracy_policy_0: 0.67543
	loss_value_0: 0.31984
	loss_policy_1: 0.05432
	accuracy_policy_1: 0.66969
	loss_value_1: 0.06535
	loss_reward_1: 0.00639
	loss_policy_2: 0.05475
	accuracy_policy_2: 0.66426
	loss_value_2: 0.06684
	loss_reward_2: 0.00833
	loss_policy_3: 0.05494
	accuracy_policy_3: 0.65883
	loss_value_3: 0.06848
	loss_reward_3: 0.0107
	loss_policy_4: 0.05535
	accuracy_policy_4: 0.65156
	loss_value_4: 0.06986
	loss_reward_4: 0.01343
	loss_policy_5: 0.05499
	accuracy_policy_5: 0.66215
	loss_value_5: 0.07152
	loss_reward_5: 0.01516
	loss_policy: 0.54319
	loss_value: 0.66189
	loss_reward: 0.05401
Optimization_Done 50800
[2025-05-08 01:20:00] [command] train weight_iter_50800.pkl 236 255
[2025-05-08 01:20:08] nn step 50850, lr: 0.1.
	loss_policy_0: 0.28504
	accuracy_policy_0: 0.66918
	loss_value_0: 0.3458
	loss_policy_1: 0.05753
	accuracy_policy_1: 0.66633
	loss_value_1: 0.07021
	loss_reward_1: 0.00671
	loss_policy_2: 0.0579
	accuracy_policy_2: 0.66441
	loss_value_2: 0.07174
	loss_reward_2: 0.00859
	loss_policy_3: 0.05884
	accuracy_policy_3: 0.65047
	loss_value_3: 0.07302
	loss_reward_3: 0.0114
	loss_policy_4: 0.05911
	accuracy_policy_4: 0.64961
	loss_value_4: 0.07432
	loss_reward_4: 0.01462
	loss_policy_5: 0.05873
	accuracy_policy_5: 0.65809
	loss_value_5: 0.07572
	loss_reward_5: 0.0152
	loss_policy: 0.57715
	loss_value: 0.71083
	loss_reward: 0.05652
[2025-05-08 01:20:16] nn step 50900, lr: 0.1.
	loss_policy_0: 0.26488
	accuracy_policy_0: 0.67027
	loss_value_0: 0.31685
	loss_policy_1: 0.05314
	accuracy_policy_1: 0.66352
	loss_value_1: 0.06451
	loss_reward_1: 0.00624
	loss_policy_2: 0.05368
	accuracy_policy_2: 0.66062
	loss_value_2: 0.06598
	loss_reward_2: 0.00843
	loss_policy_3: 0.05375
	accuracy_policy_3: 0.65672
	loss_value_3: 0.06747
	loss_reward_3: 0.01048
	loss_policy_4: 0.05413
	accuracy_policy_4: 0.65887
	loss_value_4: 0.06888
	loss_reward_4: 0.01315
	loss_policy_5: 0.0543
	accuracy_policy_5: 0.65441
	loss_value_5: 0.07045
	loss_reward_5: 0.01455
	loss_policy: 0.53389
	loss_value: 0.65415
	loss_reward: 0.05285
[2025-05-08 01:20:25] nn step 50950, lr: 0.1.
	loss_policy_0: 0.26937
	accuracy_policy_0: 0.67285
	loss_value_0: 0.31855
	loss_policy_1: 0.05464
	accuracy_policy_1: 0.66344
	loss_value_1: 0.06519
	loss_reward_1: 0.00656
	loss_policy_2: 0.05472
	accuracy_policy_2: 0.66676
	loss_value_2: 0.06685
	loss_reward_2: 0.00827
	loss_policy_3: 0.0552
	accuracy_policy_3: 0.65406
	loss_value_3: 0.06824
	loss_reward_3: 0.01072
	loss_policy_4: 0.0556
	accuracy_policy_4: 0.65395
	loss_value_4: 0.06972
	loss_reward_4: 0.01318
	loss_policy_5: 0.05583
	accuracy_policy_5: 0.65812
	loss_value_5: 0.07102
	loss_reward_5: 0.01465
	loss_policy: 0.54537
	loss_value: 0.65957
	loss_reward: 0.05338
[2025-05-08 01:20:31] nn step 51000, lr: 0.1.
	loss_policy_0: 0.27895
	accuracy_policy_0: 0.6784
	loss_value_0: 0.32841
	loss_policy_1: 0.05655
	accuracy_policy_1: 0.66133
	loss_value_1: 0.06667
	loss_reward_1: 0.00636
	loss_policy_2: 0.05695
	accuracy_policy_2: 0.66055
	loss_value_2: 0.06814
	loss_reward_2: 0.00847
	loss_policy_3: 0.05761
	accuracy_policy_3: 0.65016
	loss_value_3: 0.06987
	loss_reward_3: 0.01126
	loss_policy_4: 0.05778
	accuracy_policy_4: 0.64895
	loss_value_4: 0.07098
	loss_reward_4: 0.01408
	loss_policy_5: 0.05751
	accuracy_policy_5: 0.65992
	loss_value_5: 0.07247
	loss_reward_5: 0.01506
	loss_policy: 0.56536
	loss_value: 0.67654
	loss_reward: 0.05523
Optimization_Done 51000
[2025-05-08 01:23:40] [command] train weight_iter_51000.pkl 237 256
[2025-05-08 01:23:49] nn step 51050, lr: 0.1.
	loss_policy_0: 0.28154
	accuracy_policy_0: 0.67234
	loss_value_0: 0.34705
	loss_policy_1: 0.05693
	accuracy_policy_1: 0.66598
	loss_value_1: 0.06996
	loss_reward_1: 0.00664
	loss_policy_2: 0.05741
	accuracy_policy_2: 0.65723
	loss_value_2: 0.07154
	loss_reward_2: 0.00877
	loss_policy_3: 0.05806
	accuracy_policy_3: 0.65781
	loss_value_3: 0.073
	loss_reward_3: 0.01169
	loss_policy_4: 0.05812
	accuracy_policy_4: 0.65078
	loss_value_4: 0.07403
	loss_reward_4: 0.01439
	loss_policy_5: 0.05834
	accuracy_policy_5: 0.65199
	loss_value_5: 0.07521
	loss_reward_5: 0.01498
	loss_policy: 0.5704
	loss_value: 0.71079
	loss_reward: 0.05648
[2025-05-08 01:23:56] nn step 51100, lr: 0.1.
	loss_policy_0: 0.2807
	accuracy_policy_0: 0.67074
	loss_value_0: 0.33356
	loss_policy_1: 0.05689
	accuracy_policy_1: 0.65973
	loss_value_1: 0.06779
	loss_reward_1: 0.00683
	loss_policy_2: 0.05753
	accuracy_policy_2: 0.65637
	loss_value_2: 0.06966
	loss_reward_2: 0.00846
	loss_policy_3: 0.05756
	accuracy_policy_3: 0.6477
	loss_value_3: 0.07113
	loss_reward_3: 0.01097
	loss_policy_4: 0.05805
	accuracy_policy_4: 0.65465
	loss_value_4: 0.07266
	loss_reward_4: 0.01394
	loss_policy_5: 0.05795
	accuracy_policy_5: 0.65406
	loss_value_5: 0.07429
	loss_reward_5: 0.0153
	loss_policy: 0.56868
	loss_value: 0.68909
	loss_reward: 0.0555
[2025-05-08 01:24:05] nn step 51150, lr: 0.1.
	loss_policy_0: 0.27786
	accuracy_policy_0: 0.67102
	loss_value_0: 0.3336
	loss_policy_1: 0.05656
	accuracy_policy_1: 0.66289
	loss_value_1: 0.06791
	loss_reward_1: 0.00664
	loss_policy_2: 0.0575
	accuracy_policy_2: 0.65328
	loss_value_2: 0.0696
	loss_reward_2: 0.00856
	loss_policy_3: 0.05787
	accuracy_policy_3: 0.64895
	loss_value_3: 0.07146
	loss_reward_3: 0.01105
	loss_policy_4: 0.05774
	accuracy_policy_4: 0.64938
	loss_value_4: 0.07273
	loss_reward_4: 0.014
	loss_policy_5: 0.05764
	accuracy_policy_5: 0.65676
	loss_value_5: 0.07419
	loss_reward_5: 0.01513
	loss_policy: 0.56518
	loss_value: 0.68949
	loss_reward: 0.05538
[2025-05-08 01:24:13] nn step 51200, lr: 0.1.
	loss_policy_0: 0.28222
	accuracy_policy_0: 0.67461
	loss_value_0: 0.33727
	loss_policy_1: 0.05713
	accuracy_policy_1: 0.66191
	loss_value_1: 0.06902
	loss_reward_1: 0.00669
	loss_policy_2: 0.0577
	accuracy_policy_2: 0.65664
	loss_value_2: 0.07055
	loss_reward_2: 0.00849
	loss_policy_3: 0.05808
	accuracy_policy_3: 0.65117
	loss_value_3: 0.07225
	loss_reward_3: 0.01118
	loss_policy_4: 0.05835
	accuracy_policy_4: 0.64965
	loss_value_4: 0.07407
	loss_reward_4: 0.01429
	loss_policy_5: 0.05833
	accuracy_policy_5: 0.65703
	loss_value_5: 0.07521
	loss_reward_5: 0.01507
	loss_policy: 0.57182
	loss_value: 0.69838
	loss_reward: 0.05573
Optimization_Done 51200
[2025-05-08 01:27:21] [command] train weight_iter_51200.pkl 238 257
[2025-05-08 01:27:30] nn step 51250, lr: 0.1.
	loss_policy_0: 0.25828
	accuracy_policy_0: 0.67293
	loss_value_0: 0.32099
	loss_policy_1: 0.05279
	accuracy_policy_1: 0.66297
	loss_value_1: 0.06496
	loss_reward_1: 0.00623
	loss_policy_2: 0.05363
	accuracy_policy_2: 0.65625
	loss_value_2: 0.0667
	loss_reward_2: 0.00787
	loss_policy_3: 0.05368
	accuracy_policy_3: 0.64973
	loss_value_3: 0.06792
	loss_reward_3: 0.01038
	loss_policy_4: 0.05357
	accuracy_policy_4: 0.65496
	loss_value_4: 0.069
	loss_reward_4: 0.0132
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.65508
	loss_value_5: 0.07032
	loss_reward_5: 0.01428
	loss_policy: 0.52568
	loss_value: 0.65987
	loss_reward: 0.05196
[2025-05-08 01:27:37] nn step 51300, lr: 0.1.
	loss_policy_0: 0.25893
	accuracy_policy_0: 0.67371
	loss_value_0: 0.30587
	loss_policy_1: 0.05222
	accuracy_policy_1: 0.66312
	loss_value_1: 0.06231
	loss_reward_1: 0.00629
	loss_policy_2: 0.05264
	accuracy_policy_2: 0.65773
	loss_value_2: 0.06381
	loss_reward_2: 0.00768
	loss_policy_3: 0.05268
	accuracy_policy_3: 0.6591
	loss_value_3: 0.06503
	loss_reward_3: 0.01004
	loss_policy_4: 0.05284
	accuracy_policy_4: 0.65762
	loss_value_4: 0.06621
	loss_reward_4: 0.01293
	loss_policy_5: 0.05318
	accuracy_policy_5: 0.65953
	loss_value_5: 0.06788
	loss_reward_5: 0.01463
	loss_policy: 0.52249
	loss_value: 0.63112
	loss_reward: 0.05157
[2025-05-08 01:27:45] nn step 51350, lr: 0.1.
	loss_policy_0: 0.27913
	accuracy_policy_0: 0.67414
	loss_value_0: 0.3315
	loss_policy_1: 0.05694
	accuracy_policy_1: 0.66488
	loss_value_1: 0.06771
	loss_reward_1: 0.00679
	loss_policy_2: 0.05722
	accuracy_policy_2: 0.65559
	loss_value_2: 0.0693
	loss_reward_2: 0.0086
	loss_policy_3: 0.05786
	accuracy_policy_3: 0.65172
	loss_value_3: 0.07106
	loss_reward_3: 0.0112
	loss_policy_4: 0.05808
	accuracy_policy_4: 0.65195
	loss_value_4: 0.07246
	loss_reward_4: 0.01385
	loss_policy_5: 0.05807
	accuracy_policy_5: 0.65516
	loss_value_5: 0.07402
	loss_reward_5: 0.01514
	loss_policy: 0.56729
	loss_value: 0.68605
	loss_reward: 0.05558
[2025-05-08 01:27:54] nn step 51400, lr: 0.1.
	loss_policy_0: 0.2948
	accuracy_policy_0: 0.67109
	loss_value_0: 0.3493
	loss_policy_1: 0.05966
	accuracy_policy_1: 0.6609
	loss_value_1: 0.07103
	loss_reward_1: 0.00708
	loss_policy_2: 0.06014
	accuracy_policy_2: 0.65746
	loss_value_2: 0.07306
	loss_reward_2: 0.00914
	loss_policy_3: 0.06063
	accuracy_policy_3: 0.65441
	loss_value_3: 0.07457
	loss_reward_3: 0.01222
	loss_policy_4: 0.06053
	accuracy_policy_4: 0.65391
	loss_value_4: 0.07643
	loss_reward_4: 0.01485
	loss_policy_5: 0.06088
	accuracy_policy_5: 0.65676
	loss_value_5: 0.0777
	loss_reward_5: 0.01642
	loss_policy: 0.59664
	loss_value: 0.7221
	loss_reward: 0.0597
Optimization_Done 51400
[2025-05-08 01:30:59] [command] train weight_iter_51400.pkl 239 258
[2025-05-08 01:31:08] nn step 51450, lr: 0.1.
	loss_policy_0: 0.27965
	accuracy_policy_0: 0.6707
	loss_value_0: 0.34201
	loss_policy_1: 0.0567
	accuracy_policy_1: 0.65926
	loss_value_1: 0.06941
	loss_reward_1: 0.0068
	loss_policy_2: 0.05724
	accuracy_policy_2: 0.6559
	loss_value_2: 0.07109
	loss_reward_2: 0.00886
	loss_policy_3: 0.05718
	accuracy_policy_3: 0.65449
	loss_value_3: 0.07246
	loss_reward_3: 0.01134
	loss_policy_4: 0.05762
	accuracy_policy_4: 0.65289
	loss_value_4: 0.07401
	loss_reward_4: 0.01397
	loss_policy_5: 0.05742
	accuracy_policy_5: 0.65621
	loss_value_5: 0.0754
	loss_reward_5: 0.01542
	loss_policy: 0.56581
	loss_value: 0.70439
	loss_reward: 0.0564
[2025-05-08 01:31:17] nn step 51500, lr: 0.1.
	loss_policy_0: 0.24988
	accuracy_policy_0: 0.6775
	loss_value_0: 0.3013
	loss_policy_1: 0.05048
	accuracy_policy_1: 0.66398
	loss_value_1: 0.06148
	loss_reward_1: 0.00586
	loss_policy_2: 0.05113
	accuracy_policy_2: 0.6575
	loss_value_2: 0.06296
	loss_reward_2: 0.00764
	loss_policy_3: 0.05151
	accuracy_policy_3: 0.65258
	loss_value_3: 0.06419
	loss_reward_3: 0.0101
	loss_policy_4: 0.0516
	accuracy_policy_4: 0.65113
	loss_value_4: 0.0654
	loss_reward_4: 0.01228
	loss_policy_5: 0.05188
	accuracy_policy_5: 0.65383
	loss_value_5: 0.06667
	loss_reward_5: 0.01335
	loss_policy: 0.50649
	loss_value: 0.622
	loss_reward: 0.04923
[2025-05-08 01:31:24] nn step 51550, lr: 0.1.
	loss_policy_0: 0.26611
	accuracy_policy_0: 0.67082
	loss_value_0: 0.31661
	loss_policy_1: 0.05423
	accuracy_policy_1: 0.65777
	loss_value_1: 0.0643
	loss_reward_1: 0.00592
	loss_policy_2: 0.05451
	accuracy_policy_2: 0.6532
	loss_value_2: 0.06622
	loss_reward_2: 0.00796
	loss_policy_3: 0.05479
	accuracy_policy_3: 0.65613
	loss_value_3: 0.06769
	loss_reward_3: 0.01076
	loss_policy_4: 0.05532
	accuracy_policy_4: 0.64496
	loss_value_4: 0.06925
	loss_reward_4: 0.01325
	loss_policy_5: 0.05496
	accuracy_policy_5: 0.64988
	loss_value_5: 0.0703
	loss_reward_5: 0.01429
	loss_policy: 0.53991
	loss_value: 0.65437
	loss_reward: 0.05217
[2025-05-08 01:31:32] nn step 51600, lr: 0.1.
	loss_policy_0: 0.26358
	accuracy_policy_0: 0.67145
	loss_value_0: 0.31472
	loss_policy_1: 0.05336
	accuracy_policy_1: 0.65758
	loss_value_1: 0.06393
	loss_reward_1: 0.00629
	loss_policy_2: 0.05417
	accuracy_policy_2: 0.6534
	loss_value_2: 0.06566
	loss_reward_2: 0.00779
	loss_policy_3: 0.05415
	accuracy_policy_3: 0.65402
	loss_value_3: 0.06721
	loss_reward_3: 0.01073
	loss_policy_4: 0.0545
	accuracy_policy_4: 0.6552
	loss_value_4: 0.0685
	loss_reward_4: 0.01304
	loss_policy_5: 0.05444
	accuracy_policy_5: 0.65344
	loss_value_5: 0.07003
	loss_reward_5: 0.01432
	loss_policy: 0.5342
	loss_value: 0.65004
	loss_reward: 0.05216
Optimization_Done 51600
[2025-05-08 01:34:40] [command] train weight_iter_51600.pkl 240 259
[2025-05-08 01:34:47] nn step 51650, lr: 0.1.
	loss_policy_0: 0.27838
	accuracy_policy_0: 0.65926
	loss_value_0: 0.34415
	loss_policy_1: 0.05587
	accuracy_policy_1: 0.65785
	loss_value_1: 0.06948
	loss_reward_1: 0.00644
	loss_policy_2: 0.05602
	accuracy_policy_2: 0.6527
	loss_value_2: 0.07064
	loss_reward_2: 0.00825
	loss_policy_3: 0.05648
	accuracy_policy_3: 0.65078
	loss_value_3: 0.07186
	loss_reward_3: 0.01086
	loss_policy_4: 0.05664
	accuracy_policy_4: 0.6523
	loss_value_4: 0.07318
	loss_reward_4: 0.01331
	loss_policy_5: 0.05667
	accuracy_policy_5: 0.65684
	loss_value_5: 0.07466
	loss_reward_5: 0.01513
	loss_policy: 0.56007
	loss_value: 0.70396
	loss_reward: 0.05398
[2025-05-08 01:34:56] nn step 51700, lr: 0.1.
	loss_policy_0: 0.26692
	accuracy_policy_0: 0.67215
	loss_value_0: 0.32154
	loss_policy_1: 0.05392
	accuracy_policy_1: 0.66133
	loss_value_1: 0.06543
	loss_reward_1: 0.00607
	loss_policy_2: 0.05444
	accuracy_policy_2: 0.65695
	loss_value_2: 0.06696
	loss_reward_2: 0.00811
	loss_policy_3: 0.05484
	accuracy_policy_3: 0.65582
	loss_value_3: 0.06866
	loss_reward_3: 0.01056
	loss_policy_4: 0.05495
	accuracy_policy_4: 0.6543
	loss_value_4: 0.07005
	loss_reward_4: 0.01312
	loss_policy_5: 0.05509
	accuracy_policy_5: 0.65461
	loss_value_5: 0.07127
	loss_reward_5: 0.01447
	loss_policy: 0.54016
	loss_value: 0.66391
	loss_reward: 0.05233
[2025-05-08 01:35:04] nn step 51750, lr: 0.1.
	loss_policy_0: 0.2801
	accuracy_policy_0: 0.67344
	loss_value_0: 0.33789
	loss_policy_1: 0.05703
	accuracy_policy_1: 0.66047
	loss_value_1: 0.06913
	loss_reward_1: 0.00643
	loss_policy_2: 0.05721
	accuracy_policy_2: 0.65672
	loss_value_2: 0.07087
	loss_reward_2: 0.00872
	loss_policy_3: 0.05804
	accuracy_policy_3: 0.6502
	loss_value_3: 0.07265
	loss_reward_3: 0.01128
	loss_policy_4: 0.05833
	accuracy_policy_4: 0.64941
	loss_value_4: 0.07401
	loss_reward_4: 0.01431
	loss_policy_5: 0.05807
	accuracy_policy_5: 0.65246
	loss_value_5: 0.07536
	loss_reward_5: 0.01498
	loss_policy: 0.56879
	loss_value: 0.69991
	loss_reward: 0.05573
[2025-05-08 01:35:11] nn step 51800, lr: 0.1.
	loss_policy_0: 0.28361
	accuracy_policy_0: 0.67223
	loss_value_0: 0.34272
	loss_policy_1: 0.05783
	accuracy_policy_1: 0.66766
	loss_value_1: 0.06977
	loss_reward_1: 0.00668
	loss_policy_2: 0.05829
	accuracy_policy_2: 0.66066
	loss_value_2: 0.07129
	loss_reward_2: 0.00823
	loss_policy_3: 0.0584
	accuracy_policy_3: 0.65465
	loss_value_3: 0.07275
	loss_reward_3: 0.01137
	loss_policy_4: 0.05894
	accuracy_policy_4: 0.65602
	loss_value_4: 0.07438
	loss_reward_4: 0.01439
	loss_policy_5: 0.0585
	accuracy_policy_5: 0.66461
	loss_value_5: 0.07609
	loss_reward_5: 0.01511
	loss_policy: 0.57557
	loss_value: 0.707
	loss_reward: 0.05578
Optimization_Done 51800
[2025-05-08 01:38:17] [command] train weight_iter_51800.pkl 241 260
[2025-05-08 01:38:27] nn step 51850, lr: 0.1.
	loss_policy_0: 0.2841
	accuracy_policy_0: 0.66738
	loss_value_0: 0.34956
	loss_policy_1: 0.05737
	accuracy_policy_1: 0.65879
	loss_value_1: 0.07086
	loss_reward_1: 0.00651
	loss_policy_2: 0.0578
	accuracy_policy_2: 0.65211
	loss_value_2: 0.0723
	loss_reward_2: 0.00848
	loss_policy_3: 0.05832
	accuracy_policy_3: 0.64902
	loss_value_3: 0.07342
	loss_reward_3: 0.01133
	loss_policy_4: 0.05842
	accuracy_policy_4: 0.65211
	loss_value_4: 0.07497
	loss_reward_4: 0.01413
	loss_policy_5: 0.05857
	accuracy_policy_5: 0.65547
	loss_value_5: 0.07658
	loss_reward_5: 0.01519
	loss_policy: 0.57458
	loss_value: 0.71769
	loss_reward: 0.05564
[2025-05-08 01:38:33] nn step 51900, lr: 0.1.
	loss_policy_0: 0.2832
	accuracy_policy_0: 0.67266
	loss_value_0: 0.34283
	loss_policy_1: 0.05709
	accuracy_policy_1: 0.66047
	loss_value_1: 0.06941
	loss_reward_1: 0.00671
	loss_policy_2: 0.0574
	accuracy_policy_2: 0.66148
	loss_value_2: 0.07128
	loss_reward_2: 0.00872
	loss_policy_3: 0.05805
	accuracy_policy_3: 0.65617
	loss_value_3: 0.07304
	loss_reward_3: 0.01164
	loss_policy_4: 0.05807
	accuracy_policy_4: 0.65645
	loss_value_4: 0.07425
	loss_reward_4: 0.01438
	loss_policy_5: 0.05815
	accuracy_policy_5: 0.65707
	loss_value_5: 0.07563
	loss_reward_5: 0.01551
	loss_policy: 0.57195
	loss_value: 0.70644
	loss_reward: 0.05697
[2025-05-08 01:38:42] nn step 51950, lr: 0.1.
	loss_policy_0: 0.27019
	accuracy_policy_0: 0.67125
	loss_value_0: 0.32228
	loss_policy_1: 0.05479
	accuracy_policy_1: 0.66148
	loss_value_1: 0.06533
	loss_reward_1: 0.00647
	loss_policy_2: 0.05507
	accuracy_policy_2: 0.65629
	loss_value_2: 0.0668
	loss_reward_2: 0.00842
	loss_policy_3: 0.0558
	accuracy_policy_3: 0.64695
	loss_value_3: 0.06832
	loss_reward_3: 0.0113
	loss_policy_4: 0.05576
	accuracy_policy_4: 0.65355
	loss_value_4: 0.06958
	loss_reward_4: 0.0143
	loss_policy_5: 0.05561
	accuracy_policy_5: 0.65766
	loss_value_5: 0.07119
	loss_reward_5: 0.01519
	loss_policy: 0.54722
	loss_value: 0.6635
	loss_reward: 0.05567
[2025-05-08 01:38:50] nn step 52000, lr: 0.1.
	loss_policy_0: 0.27419
	accuracy_policy_0: 0.67344
	loss_value_0: 0.32386
	loss_policy_1: 0.0557
	accuracy_policy_1: 0.65738
	loss_value_1: 0.06613
	loss_reward_1: 0.00674
	loss_policy_2: 0.05583
	accuracy_policy_2: 0.65684
	loss_value_2: 0.06778
	loss_reward_2: 0.00878
	loss_policy_3: 0.05606
	accuracy_policy_3: 0.65566
	loss_value_3: 0.06919
	loss_reward_3: 0.01118
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.65543
	loss_value_4: 0.07081
	loss_reward_4: 0.01349
	loss_policy_5: 0.05651
	accuracy_policy_5: 0.65734
	loss_value_5: 0.07193
	loss_reward_5: 0.01465
	loss_policy: 0.55472
	loss_value: 0.66969
	loss_reward: 0.05484
Optimization_Done 52000
[2025-05-08 01:41:55] [command] train weight_iter_52000.pkl 242 261
[2025-05-08 01:42:04] nn step 52050, lr: 0.1.
	loss_policy_0: 0.27162
	accuracy_policy_0: 0.66785
	loss_value_0: 0.3253
	loss_policy_1: 0.05483
	accuracy_policy_1: 0.65664
	loss_value_1: 0.06601
	loss_reward_1: 0.00625
	loss_policy_2: 0.05526
	accuracy_policy_2: 0.65105
	loss_value_2: 0.06756
	loss_reward_2: 0.00835
	loss_policy_3: 0.0554
	accuracy_policy_3: 0.64961
	loss_value_3: 0.06896
	loss_reward_3: 0.01114
	loss_policy_4: 0.05613
	accuracy_policy_4: 0.64527
	loss_value_4: 0.07016
	loss_reward_4: 0.01321
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.65598
	loss_value_5: 0.07158
	loss_reward_5: 0.01496
	loss_policy: 0.54867
	loss_value: 0.66956
	loss_reward: 0.05392
[2025-05-08 01:42:12] nn step 52100, lr: 0.1.
	loss_policy_0: 0.25747
	accuracy_policy_0: 0.66582
	loss_value_0: 0.30427
	loss_policy_1: 0.05166
	accuracy_policy_1: 0.65383
	loss_value_1: 0.06188
	loss_reward_1: 0.00579
	loss_policy_2: 0.05207
	accuracy_policy_2: 0.65445
	loss_value_2: 0.06337
	loss_reward_2: 0.0076
	loss_policy_3: 0.05252
	accuracy_policy_3: 0.65223
	loss_value_3: 0.06472
	loss_reward_3: 0.00998
	loss_policy_4: 0.05259
	accuracy_policy_4: 0.65098
	loss_value_4: 0.06613
	loss_reward_4: 0.01257
	loss_policy_5: 0.05275
	accuracy_policy_5: 0.65762
	loss_value_5: 0.06751
	loss_reward_5: 0.01355
	loss_policy: 0.51907
	loss_value: 0.62789
	loss_reward: 0.0495
[2025-05-08 01:42:19] nn step 52150, lr: 0.1.
	loss_policy_0: 0.27068
	accuracy_policy_0: 0.66902
	loss_value_0: 0.32623
	loss_policy_1: 0.05528
	accuracy_policy_1: 0.65488
	loss_value_1: 0.0663
	loss_reward_1: 0.00653
	loss_policy_2: 0.05564
	accuracy_policy_2: 0.65543
	loss_value_2: 0.06756
	loss_reward_2: 0.00801
	loss_policy_3: 0.05592
	accuracy_policy_3: 0.6484
	loss_value_3: 0.06911
	loss_reward_3: 0.01075
	loss_policy_4: 0.05626
	accuracy_policy_4: 0.64668
	loss_value_4: 0.07032
	loss_reward_4: 0.01322
	loss_policy_5: 0.05622
	accuracy_policy_5: 0.65348
	loss_value_5: 0.07206
	loss_reward_5: 0.0149
	loss_policy: 0.55
	loss_value: 0.67158
	loss_reward: 0.05341
[2025-05-08 01:42:27] nn step 52200, lr: 0.1.
	loss_policy_0: 0.28288
	accuracy_policy_0: 0.66957
	loss_value_0: 0.33371
	loss_policy_1: 0.05715
	accuracy_policy_1: 0.65535
	loss_value_1: 0.06777
	loss_reward_1: 0.00663
	loss_policy_2: 0.0577
	accuracy_policy_2: 0.65363
	loss_value_2: 0.0697
	loss_reward_2: 0.00858
	loss_policy_3: 0.05748
	accuracy_policy_3: 0.65543
	loss_value_3: 0.07112
	loss_reward_3: 0.01071
	loss_policy_4: 0.05796
	accuracy_policy_4: 0.65062
	loss_value_4: 0.07255
	loss_reward_4: 0.01415
	loss_policy_5: 0.05788
	accuracy_policy_5: 0.65762
	loss_value_5: 0.07403
	loss_reward_5: 0.01575
	loss_policy: 0.57105
	loss_value: 0.68887
	loss_reward: 0.05581
Optimization_Done 52200
[2025-05-08 01:45:34] [command] train weight_iter_52200.pkl 243 262
[2025-05-08 01:45:41] nn step 52250, lr: 0.1.
	loss_policy_0: 0.27966
	accuracy_policy_0: 0.65836
	loss_value_0: 0.33906
	loss_policy_1: 0.05647
	accuracy_policy_1: 0.65047
	loss_value_1: 0.06891
	loss_reward_1: 0.00655
	loss_policy_2: 0.05718
	accuracy_policy_2: 0.6534
	loss_value_2: 0.07053
	loss_reward_2: 0.00812
	loss_policy_3: 0.05716
	accuracy_policy_3: 0.64824
	loss_value_3: 0.07198
	loss_reward_3: 0.01095
	loss_policy_4: 0.05781
	accuracy_policy_4: 0.64469
	loss_value_4: 0.07293
	loss_reward_4: 0.01432
	loss_policy_5: 0.0574
	accuracy_policy_5: 0.64996
	loss_value_5: 0.07454
	loss_reward_5: 0.01504
	loss_policy: 0.56568
	loss_value: 0.69795
	loss_reward: 0.05497
[2025-05-08 01:45:50] nn step 52300, lr: 0.1.
	loss_policy_0: 0.2608
	accuracy_policy_0: 0.6618
	loss_value_0: 0.31268
	loss_policy_1: 0.05293
	accuracy_policy_1: 0.65215
	loss_value_1: 0.06354
	loss_reward_1: 0.00583
	loss_policy_2: 0.05354
	accuracy_policy_2: 0.64754
	loss_value_2: 0.06518
	loss_reward_2: 0.00792
	loss_policy_3: 0.05361
	accuracy_policy_3: 0.64465
	loss_value_3: 0.06657
	loss_reward_3: 0.0104
	loss_policy_4: 0.0541
	accuracy_policy_4: 0.64402
	loss_value_4: 0.06823
	loss_reward_4: 0.01284
	loss_policy_5: 0.0538
	accuracy_policy_5: 0.65285
	loss_value_5: 0.06921
	loss_reward_5: 0.01417
	loss_policy: 0.52879
	loss_value: 0.64542
	loss_reward: 0.05115
[2025-05-08 01:45:59] nn step 52350, lr: 0.1.
	loss_policy_0: 0.27459
	accuracy_policy_0: 0.66457
	loss_value_0: 0.32899
	loss_policy_1: 0.05588
	accuracy_policy_1: 0.6552
	loss_value_1: 0.06654
	loss_reward_1: 0.00638
	loss_policy_2: 0.05641
	accuracy_policy_2: 0.64707
	loss_value_2: 0.06855
	loss_reward_2: 0.00862
	loss_policy_3: 0.05676
	accuracy_policy_3: 0.64852
	loss_value_3: 0.0698
	loss_reward_3: 0.0113
	loss_policy_4: 0.05696
	accuracy_policy_4: 0.64527
	loss_value_4: 0.07131
	loss_reward_4: 0.01382
	loss_policy_5: 0.057
	accuracy_policy_5: 0.65078
	loss_value_5: 0.07264
	loss_reward_5: 0.01463
	loss_policy: 0.55761
	loss_value: 0.67783
	loss_reward: 0.05475
[2025-05-08 01:46:05] nn step 52400, lr: 0.1.
	loss_policy_0: 0.27553
	accuracy_policy_0: 0.67164
	loss_value_0: 0.32688
	loss_policy_1: 0.05595
	accuracy_policy_1: 0.66012
	loss_value_1: 0.06657
	loss_reward_1: 0.00656
	loss_policy_2: 0.05648
	accuracy_policy_2: 0.6534
	loss_value_2: 0.06828
	loss_reward_2: 0.00852
	loss_policy_3: 0.05656
	accuracy_policy_3: 0.64684
	loss_value_3: 0.06984
	loss_reward_3: 0.01076
	loss_policy_4: 0.0571
	accuracy_policy_4: 0.64398
	loss_value_4: 0.07117
	loss_reward_4: 0.01391
	loss_policy_5: 0.05698
	accuracy_policy_5: 0.6502
	loss_value_5: 0.07262
	loss_reward_5: 0.01523
	loss_policy: 0.55861
	loss_value: 0.67536
	loss_reward: 0.05499
Optimization_Done 52400
[2025-05-08 01:49:14] [command] train weight_iter_52400.pkl 244 263
[2025-05-08 01:49:22] nn step 52450, lr: 0.1.
	loss_policy_0: 0.27051
	accuracy_policy_0: 0.66832
	loss_value_0: 0.33167
	loss_policy_1: 0.05491
	accuracy_policy_1: 0.65266
	loss_value_1: 0.06748
	loss_reward_1: 0.00615
	loss_policy_2: 0.05546
	accuracy_policy_2: 0.65281
	loss_value_2: 0.06886
	loss_reward_2: 0.00823
	loss_policy_3: 0.05573
	accuracy_policy_3: 0.64559
	loss_value_3: 0.07003
	loss_reward_3: 0.01079
	loss_policy_4: 0.05622
	accuracy_policy_4: 0.63926
	loss_value_4: 0.07116
	loss_reward_4: 0.01352
	loss_policy_5: 0.05609
	accuracy_policy_5: 0.64777
	loss_value_5: 0.07236
	loss_reward_5: 0.01423
	loss_policy: 0.54892
	loss_value: 0.68157
	loss_reward: 0.05292
[2025-05-08 01:49:30] nn step 52500, lr: 0.1.
	loss_policy_0: 0.26708
	accuracy_policy_0: 0.66066
	loss_value_0: 0.32022
	loss_policy_1: 0.05393
	accuracy_policy_1: 0.65121
	loss_value_1: 0.06504
	loss_reward_1: 0.0062
	loss_policy_2: 0.05435
	accuracy_policy_2: 0.64523
	loss_value_2: 0.06676
	loss_reward_2: 0.00783
	loss_policy_3: 0.05474
	accuracy_policy_3: 0.64473
	loss_value_3: 0.06781
	loss_reward_3: 0.0104
	loss_policy_4: 0.0547
	accuracy_policy_4: 0.64492
	loss_value_4: 0.06906
	loss_reward_4: 0.0131
	loss_policy_5: 0.05463
	accuracy_policy_5: 0.65289
	loss_value_5: 0.07009
	loss_reward_5: 0.01405
	loss_policy: 0.53944
	loss_value: 0.65897
	loss_reward: 0.05158
[2025-05-08 01:49:38] nn step 52550, lr: 0.1.
	loss_policy_0: 0.26505
	accuracy_policy_0: 0.67062
	loss_value_0: 0.32024
	loss_policy_1: 0.05402
	accuracy_policy_1: 0.65211
	loss_value_1: 0.06537
	loss_reward_1: 0.00645
	loss_policy_2: 0.05432
	accuracy_policy_2: 0.65219
	loss_value_2: 0.06697
	loss_reward_2: 0.0079
	loss_policy_3: 0.0546
	accuracy_policy_3: 0.64629
	loss_value_3: 0.0685
	loss_reward_3: 0.01077
	loss_policy_4: 0.05518
	accuracy_policy_4: 0.64547
	loss_value_4: 0.07016
	loss_reward_4: 0.01343
	loss_policy_5: 0.05509
	accuracy_policy_5: 0.64887
	loss_value_5: 0.07134
	loss_reward_5: 0.01418
	loss_policy: 0.53827
	loss_value: 0.66259
	loss_reward: 0.05273
[2025-05-08 01:49:47] nn step 52600, lr: 0.1.
	loss_policy_0: 0.26863
	accuracy_policy_0: 0.66988
	loss_value_0: 0.31978
	loss_policy_1: 0.05462
	accuracy_policy_1: 0.6502
	loss_value_1: 0.06514
	loss_reward_1: 0.00643
	loss_policy_2: 0.05538
	accuracy_policy_2: 0.65133
	loss_value_2: 0.06667
	loss_reward_2: 0.00818
	loss_policy_3: 0.05543
	accuracy_policy_3: 0.64605
	loss_value_3: 0.06828
	loss_reward_3: 0.01059
	loss_policy_4: 0.05546
	accuracy_policy_4: 0.64289
	loss_value_4: 0.06952
	loss_reward_4: 0.01329
	loss_policy_5: 0.05531
	accuracy_policy_5: 0.65348
	loss_value_5: 0.07108
	loss_reward_5: 0.01482
	loss_policy: 0.54483
	loss_value: 0.66047
	loss_reward: 0.05331
Optimization_Done 52600
[2025-05-08 01:52:52] [command] train weight_iter_52600.pkl 245 264
[2025-05-08 01:53:02] nn step 52650, lr: 0.1.
	loss_policy_0: 0.26733
	accuracy_policy_0: 0.66738
	loss_value_0: 0.33597
	loss_policy_1: 0.05408
	accuracy_policy_1: 0.66055
	loss_value_1: 0.06804
	loss_reward_1: 0.00634
	loss_policy_2: 0.05426
	accuracy_policy_2: 0.66398
	loss_value_2: 0.06963
	loss_reward_2: 0.00815
	loss_policy_3: 0.0548
	accuracy_policy_3: 0.65582
	loss_value_3: 0.07072
	loss_reward_3: 0.01079
	loss_policy_4: 0.05516
	accuracy_policy_4: 0.65414
	loss_value_4: 0.07232
	loss_reward_4: 0.0135
	loss_policy_5: 0.05478
	accuracy_policy_5: 0.66164
	loss_value_5: 0.07323
	loss_reward_5: 0.0147
	loss_policy: 0.54041
	loss_value: 0.6899
	loss_reward: 0.05348
[2025-05-08 01:53:08] nn step 52700, lr: 0.1.
	loss_policy_0: 0.26788
	accuracy_policy_0: 0.67379
	loss_value_0: 0.3289
	loss_policy_1: 0.05452
	accuracy_policy_1: 0.66516
	loss_value_1: 0.06684
	loss_reward_1: 0.0064
	loss_policy_2: 0.05503
	accuracy_policy_2: 0.65871
	loss_value_2: 0.06864
	loss_reward_2: 0.00798
	loss_policy_3: 0.05569
	accuracy_policy_3: 0.65477
	loss_value_3: 0.06996
	loss_reward_3: 0.01066
	loss_policy_4: 0.05571
	accuracy_policy_4: 0.65402
	loss_value_4: 0.07133
	loss_reward_4: 0.01339
	loss_policy_5: 0.05534
	accuracy_policy_5: 0.65785
	loss_value_5: 0.07271
	loss_reward_5: 0.01456
	loss_policy: 0.54416
	loss_value: 0.67837
	loss_reward: 0.05298
[2025-05-08 01:53:17] nn step 52750, lr: 0.1.
	loss_policy_0: 0.28722
	accuracy_policy_0: 0.66965
	loss_value_0: 0.34661
	loss_policy_1: 0.05845
	accuracy_policy_1: 0.65957
	loss_value_1: 0.07075
	loss_reward_1: 0.00657
	loss_policy_2: 0.05857
	accuracy_policy_2: 0.6609
	loss_value_2: 0.07242
	loss_reward_2: 0.00868
	loss_policy_3: 0.05934
	accuracy_policy_3: 0.65387
	loss_value_3: 0.07412
	loss_reward_3: 0.01153
	loss_policy_4: 0.05958
	accuracy_policy_4: 0.65102
	loss_value_4: 0.07546
	loss_reward_4: 0.01471
	loss_policy_5: 0.05972
	accuracy_policy_5: 0.65824
	loss_value_5: 0.07667
	loss_reward_5: 0.01567
	loss_policy: 0.58288
	loss_value: 0.71602
	loss_reward: 0.05716
[2025-05-08 01:53:25] nn step 52800, lr: 0.1.
	loss_policy_0: 0.27088
	accuracy_policy_0: 0.66707
	loss_value_0: 0.3276
	loss_policy_1: 0.05499
	accuracy_policy_1: 0.65457
	loss_value_1: 0.06684
	loss_reward_1: 0.00656
	loss_policy_2: 0.05496
	accuracy_policy_2: 0.66027
	loss_value_2: 0.06834
	loss_reward_2: 0.00849
	loss_policy_3: 0.05544
	accuracy_policy_3: 0.65734
	loss_value_3: 0.06991
	loss_reward_3: 0.01123
	loss_policy_4: 0.05574
	accuracy_policy_4: 0.65406
	loss_value_4: 0.07152
	loss_reward_4: 0.01311
	loss_policy_5: 0.05571
	accuracy_policy_5: 0.65676
	loss_value_5: 0.07321
	loss_reward_5: 0.01464
	loss_policy: 0.54772
	loss_value: 0.67741
	loss_reward: 0.05402
Optimization_Done 52800
[2025-05-08 01:56:30] [command] train weight_iter_52800.pkl 246 265
[2025-05-08 01:56:39] nn step 52850, lr: 0.1.
	loss_policy_0: 0.27245
	accuracy_policy_0: 0.65723
	loss_value_0: 0.33196
	loss_policy_1: 0.05519
	accuracy_policy_1: 0.65254
	loss_value_1: 0.06742
	loss_reward_1: 0.00628
	loss_policy_2: 0.05563
	accuracy_policy_2: 0.65062
	loss_value_2: 0.0689
	loss_reward_2: 0.00794
	loss_policy_3: 0.05605
	accuracy_policy_3: 0.63875
	loss_value_3: 0.07003
	loss_reward_3: 0.01084
	loss_policy_4: 0.05685
	accuracy_policy_4: 0.63852
	loss_value_4: 0.0713
	loss_reward_4: 0.01334
	loss_policy_5: 0.05666
	accuracy_policy_5: 0.64535
	loss_value_5: 0.07224
	loss_reward_5: 0.01434
	loss_policy: 0.55283
	loss_value: 0.68185
	loss_reward: 0.05274
[2025-05-08 01:56:48] nn step 52900, lr: 0.1.
	loss_policy_0: 0.27609
	accuracy_policy_0: 0.66121
	loss_value_0: 0.32717
	loss_policy_1: 0.05573
	accuracy_policy_1: 0.65141
	loss_value_1: 0.06669
	loss_reward_1: 0.00631
	loss_policy_2: 0.05605
	accuracy_policy_2: 0.64656
	loss_value_2: 0.06812
	loss_reward_2: 0.00839
	loss_policy_3: 0.05639
	accuracy_policy_3: 0.6432
	loss_value_3: 0.06944
	loss_reward_3: 0.01067
	loss_policy_4: 0.05673
	accuracy_policy_4: 0.64438
	loss_value_4: 0.07138
	loss_reward_4: 0.0138
	loss_policy_5: 0.05653
	accuracy_policy_5: 0.64859
	loss_value_5: 0.07278
	loss_reward_5: 0.01513
	loss_policy: 0.55751
	loss_value: 0.67559
	loss_reward: 0.05431
[2025-05-08 01:56:54] nn step 52950, lr: 0.1.
	loss_policy_0: 0.29164
	accuracy_policy_0: 0.66258
	loss_value_0: 0.3474
	loss_policy_1: 0.05917
	accuracy_policy_1: 0.65375
	loss_value_1: 0.07071
	loss_reward_1: 0.00699
	loss_policy_2: 0.05945
	accuracy_policy_2: 0.65266
	loss_value_2: 0.07266
	loss_reward_2: 0.00873
	loss_policy_3: 0.06013
	accuracy_policy_3: 0.64238
	loss_value_3: 0.07416
	loss_reward_3: 0.01159
	loss_policy_4: 0.06036
	accuracy_policy_4: 0.64086
	loss_value_4: 0.07551
	loss_reward_4: 0.01471
	loss_policy_5: 0.0604
	accuracy_policy_5: 0.64703
	loss_value_5: 0.07658
	loss_reward_5: 0.01653
	loss_policy: 0.59115
	loss_value: 0.71701
	loss_reward: 0.05855
[2025-05-08 01:57:03] nn step 53000, lr: 0.1.
	loss_policy_0: 0.26604
	accuracy_policy_0: 0.66109
	loss_value_0: 0.31618
	loss_policy_1: 0.05368
	accuracy_policy_1: 0.64949
	loss_value_1: 0.0641
	loss_reward_1: 0.00645
	loss_policy_2: 0.05379
	accuracy_policy_2: 0.65215
	loss_value_2: 0.06583
	loss_reward_2: 0.0082
	loss_policy_3: 0.05429
	accuracy_policy_3: 0.64488
	loss_value_3: 0.06705
	loss_reward_3: 0.01086
	loss_policy_4: 0.05461
	accuracy_policy_4: 0.64637
	loss_value_4: 0.06832
	loss_reward_4: 0.01364
	loss_policy_5: 0.05453
	accuracy_policy_5: 0.65559
	loss_value_5: 0.0699
	loss_reward_5: 0.01468
	loss_policy: 0.53694
	loss_value: 0.65138
	loss_reward: 0.05383
Optimization_Done 53000
[2025-05-08 02:00:12] [command] train weight_iter_53000.pkl 247 266
[2025-05-08 02:00:21] nn step 53050, lr: 0.1.
	loss_policy_0: 0.2933
	accuracy_policy_0: 0.65934
	loss_value_0: 0.36194
	loss_policy_1: 0.0595
	accuracy_policy_1: 0.65215
	loss_value_1: 0.0729
	loss_reward_1: 0.0063
	loss_policy_2: 0.05971
	accuracy_policy_2: 0.64887
	loss_value_2: 0.0744
	loss_reward_2: 0.00861
	loss_policy_3: 0.0603
	accuracy_policy_3: 0.64312
	loss_value_3: 0.07558
	loss_reward_3: 0.01162
	loss_policy_4: 0.06054
	accuracy_policy_4: 0.64121
	loss_value_4: 0.07687
	loss_reward_4: 0.01426
	loss_policy_5: 0.06041
	accuracy_policy_5: 0.64613
	loss_value_5: 0.07792
	loss_reward_5: 0.01548
	loss_policy: 0.59376
	loss_value: 0.7396
	loss_reward: 0.05627
[2025-05-08 02:00:29] nn step 53100, lr: 0.1.
	loss_policy_0: 0.27769
	accuracy_policy_0: 0.65344
	loss_value_0: 0.33224
	loss_policy_1: 0.05616
	accuracy_policy_1: 0.6441
	loss_value_1: 0.06729
	loss_reward_1: 0.00641
	loss_policy_2: 0.05654
	accuracy_policy_2: 0.64602
	loss_value_2: 0.06872
	loss_reward_2: 0.00832
	loss_policy_3: 0.05718
	accuracy_policy_3: 0.64238
	loss_value_3: 0.07013
	loss_reward_3: 0.01077
	loss_policy_4: 0.0574
	accuracy_policy_4: 0.63945
	loss_value_4: 0.07161
	loss_reward_4: 0.01371
	loss_policy_5: 0.05754
	accuracy_policy_5: 0.64207
	loss_value_5: 0.07315
	loss_reward_5: 0.0148
	loss_policy: 0.5625
	loss_value: 0.68314
	loss_reward: 0.05401
[2025-05-08 02:00:38] nn step 53150, lr: 0.1.
	loss_policy_0: 0.27651
	accuracy_policy_0: 0.64988
	loss_value_0: 0.32556
	loss_policy_1: 0.05572
	accuracy_policy_1: 0.64492
	loss_value_1: 0.06605
	loss_reward_1: 0.00662
	loss_policy_2: 0.05562
	accuracy_policy_2: 0.64605
	loss_value_2: 0.06786
	loss_reward_2: 0.00804
	loss_policy_3: 0.05592
	accuracy_policy_3: 0.64254
	loss_value_3: 0.06914
	loss_reward_3: 0.01093
	loss_policy_4: 0.05619
	accuracy_policy_4: 0.63906
	loss_value_4: 0.07066
	loss_reward_4: 0.01377
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.6477
	loss_value_5: 0.07185
	loss_reward_5: 0.01502
	loss_policy: 0.55593
	loss_value: 0.67113
	loss_reward: 0.05438
[2025-05-08 02:00:44] nn step 53200, lr: 0.1.
	loss_policy_0: 0.27372
	accuracy_policy_0: 0.65922
	loss_value_0: 0.32176
	loss_policy_1: 0.05556
	accuracy_policy_1: 0.65016
	loss_value_1: 0.0657
	loss_reward_1: 0.00615
	loss_policy_2: 0.05539
	accuracy_policy_2: 0.65348
	loss_value_2: 0.06706
	loss_reward_2: 0.00836
	loss_policy_3: 0.05572
	accuracy_policy_3: 0.64633
	loss_value_3: 0.06854
	loss_reward_3: 0.01026
	loss_policy_4: 0.05619
	accuracy_policy_4: 0.64219
	loss_value_4: 0.06993
	loss_reward_4: 0.01337
	loss_policy_5: 0.05652
	accuracy_policy_5: 0.64551
	loss_value_5: 0.07136
	loss_reward_5: 0.01479
	loss_policy: 0.55311
	loss_value: 0.66434
	loss_reward: 0.05292
Optimization_Done 53200
[2025-05-08 02:03:49] [command] train weight_iter_53200.pkl 248 267
[2025-05-08 02:03:59] nn step 53250, lr: 0.1.
	loss_policy_0: 0.27433
	accuracy_policy_0: 0.66207
	loss_value_0: 0.33398
	loss_policy_1: 0.05575
	accuracy_policy_1: 0.64609
	loss_value_1: 0.068
	loss_reward_1: 0.00637
	loss_policy_2: 0.05616
	accuracy_policy_2: 0.64027
	loss_value_2: 0.06901
	loss_reward_2: 0.00805
	loss_policy_3: 0.05682
	accuracy_policy_3: 0.63879
	loss_value_3: 0.07056
	loss_reward_3: 0.011
	loss_policy_4: 0.05688
	accuracy_policy_4: 0.63484
	loss_value_4: 0.07175
	loss_reward_4: 0.01352
	loss_policy_5: 0.05702
	accuracy_policy_5: 0.63832
	loss_value_5: 0.07296
	loss_reward_5: 0.01435
	loss_policy: 0.55696
	loss_value: 0.68627
	loss_reward: 0.05328
[2025-05-08 02:04:05] nn step 53300, lr: 0.1.
	loss_policy_0: 0.28472
	accuracy_policy_0: 0.65434
	loss_value_0: 0.3375
	loss_policy_1: 0.05757
	accuracy_policy_1: 0.64629
	loss_value_1: 0.06849
	loss_reward_1: 0.00659
	loss_policy_2: 0.05812
	accuracy_policy_2: 0.64637
	loss_value_2: 0.07021
	loss_reward_2: 0.00832
	loss_policy_3: 0.05855
	accuracy_policy_3: 0.63992
	loss_value_3: 0.07171
	loss_reward_3: 0.01101
	loss_policy_4: 0.05891
	accuracy_policy_4: 0.6373
	loss_value_4: 0.07296
	loss_reward_4: 0.01329
	loss_policy_5: 0.05907
	accuracy_policy_5: 0.63977
	loss_value_5: 0.07414
	loss_reward_5: 0.01539
	loss_policy: 0.57694
	loss_value: 0.69501
	loss_reward: 0.05461
[2025-05-08 02:04:14] nn step 53350, lr: 0.1.
	loss_policy_0: 0.28898
	accuracy_policy_0: 0.65598
	loss_value_0: 0.33985
	loss_policy_1: 0.05846
	accuracy_policy_1: 0.64562
	loss_value_1: 0.06939
	loss_reward_1: 0.00662
	loss_policy_2: 0.05891
	accuracy_policy_2: 0.6427
	loss_value_2: 0.07092
	loss_reward_2: 0.00843
	loss_policy_3: 0.05933
	accuracy_policy_3: 0.63332
	loss_value_3: 0.07195
	loss_reward_3: 0.01161
	loss_policy_4: 0.05997
	accuracy_policy_4: 0.64074
	loss_value_4: 0.07359
	loss_reward_4: 0.01429
	loss_policy_5: 0.05944
	accuracy_policy_5: 0.64121
	loss_value_5: 0.07488
	loss_reward_5: 0.0157
	loss_policy: 0.58509
	loss_value: 0.70058
	loss_reward: 0.05666
[2025-05-08 02:04:22] nn step 53400, lr: 0.1.
	loss_policy_0: 0.30493
	accuracy_policy_0: 0.64508
	loss_value_0: 0.3501
	loss_policy_1: 0.06064
	accuracy_policy_1: 0.64074
	loss_value_1: 0.07155
	loss_reward_1: 0.00697
	loss_policy_2: 0.06088
	accuracy_policy_2: 0.64172
	loss_value_2: 0.07305
	loss_reward_2: 0.00895
	loss_policy_3: 0.06133
	accuracy_policy_3: 0.63418
	loss_value_3: 0.0746
	loss_reward_3: 0.01144
	loss_policy_4: 0.0617
	accuracy_policy_4: 0.63496
	loss_value_4: 0.07622
	loss_reward_4: 0.01495
	loss_policy_5: 0.06145
	accuracy_policy_5: 0.6409
	loss_value_5: 0.0773
	loss_reward_5: 0.01605
	loss_policy: 0.61092
	loss_value: 0.72283
	loss_reward: 0.05835
Optimization_Done 53400
[2025-05-08 02:07:37] [command] train weight_iter_53400.pkl 249 268
[2025-05-08 02:07:46] nn step 53450, lr: 0.1.
	loss_policy_0: 0.25521
	accuracy_policy_0: 0.65762
	loss_value_0: 0.30937
	loss_policy_1: 0.05137
	accuracy_policy_1: 0.64512
	loss_value_1: 0.06243
	loss_reward_1: 0.00624
	loss_policy_2: 0.05161
	accuracy_policy_2: 0.64719
	loss_value_2: 0.06393
	loss_reward_2: 0.00778
	loss_policy_3: 0.05199
	accuracy_policy_3: 0.6384
	loss_value_3: 0.065
	loss_reward_3: 0.00964
	loss_policy_4: 0.0522
	accuracy_policy_4: 0.63996
	loss_value_4: 0.06645
	loss_reward_4: 0.01253
	loss_policy_5: 0.0521
	accuracy_policy_5: 0.64219
	loss_value_5: 0.0675
	loss_reward_5: 0.01348
	loss_policy: 0.51448
	loss_value: 0.63469
	loss_reward: 0.04967
[2025-05-08 02:07:53] nn step 53500, lr: 0.1.
	loss_policy_0: 0.27533
	accuracy_policy_0: 0.65797
	loss_value_0: 0.32463
	loss_policy_1: 0.05539
	accuracy_policy_1: 0.64703
	loss_value_1: 0.06593
	loss_reward_1: 0.00624
	loss_policy_2: 0.05543
	accuracy_policy_2: 0.65098
	loss_value_2: 0.06754
	loss_reward_2: 0.00812
	loss_policy_3: 0.05591
	accuracy_policy_3: 0.64379
	loss_value_3: 0.06921
	loss_reward_3: 0.01042
	loss_policy_4: 0.05652
	accuracy_policy_4: 0.64086
	loss_value_4: 0.07036
	loss_reward_4: 0.01358
	loss_policy_5: 0.0566
	accuracy_policy_5: 0.63562
	loss_value_5: 0.07144
	loss_reward_5: 0.01473
	loss_policy: 0.55518
	loss_value: 0.66911
	loss_reward: 0.05309
[2025-05-08 02:08:01] nn step 53550, lr: 0.1.
	loss_policy_0: 0.24672
	accuracy_policy_0: 0.66316
	loss_value_0: 0.29525
	loss_policy_1: 0.05035
	accuracy_policy_1: 0.64957
	loss_value_1: 0.05978
	loss_reward_1: 0.00563
	loss_policy_2: 0.05077
	accuracy_policy_2: 0.64617
	loss_value_2: 0.06126
	loss_reward_2: 0.0075
	loss_policy_3: 0.05115
	accuracy_policy_3: 0.63875
	loss_value_3: 0.06256
	loss_reward_3: 0.00963
	loss_policy_4: 0.0515
	accuracy_policy_4: 0.63578
	loss_value_4: 0.06397
	loss_reward_4: 0.01199
	loss_policy_5: 0.0515
	accuracy_policy_5: 0.64133
	loss_value_5: 0.06516
	loss_reward_5: 0.01326
	loss_policy: 0.50199
	loss_value: 0.60798
	loss_reward: 0.04801
[2025-05-08 02:08:09] nn step 53600, lr: 0.1.
	loss_policy_0: 0.29054
	accuracy_policy_0: 0.65867
	loss_value_0: 0.34274
	loss_policy_1: 0.05905
	accuracy_policy_1: 0.64844
	loss_value_1: 0.06946
	loss_reward_1: 0.00686
	loss_policy_2: 0.05919
	accuracy_policy_2: 0.64965
	loss_value_2: 0.071
	loss_reward_2: 0.00865
	loss_policy_3: 0.05986
	accuracy_policy_3: 0.63934
	loss_value_3: 0.07267
	loss_reward_3: 0.01128
	loss_policy_4: 0.06007
	accuracy_policy_4: 0.63398
	loss_value_4: 0.07416
	loss_reward_4: 0.01407
	loss_policy_5: 0.05975
	accuracy_policy_5: 0.64527
	loss_value_5: 0.07558
	loss_reward_5: 0.01556
	loss_policy: 0.58844
	loss_value: 0.70561
	loss_reward: 0.05642
Optimization_Done 53600
[2025-05-08 02:11:09] [command] train weight_iter_53600.pkl 250 269
[2025-05-08 02:11:18] nn step 53650, lr: 0.1.
	loss_policy_0: 0.26807
	accuracy_policy_0: 0.6684
	loss_value_0: 0.33555
	loss_policy_1: 0.05431
	accuracy_policy_1: 0.65918
	loss_value_1: 0.06795
	loss_reward_1: 0.00621
	loss_policy_2: 0.0549
	accuracy_policy_2: 0.65289
	loss_value_2: 0.06941
	loss_reward_2: 0.00788
	loss_policy_3: 0.05485
	accuracy_policy_3: 0.65414
	loss_value_3: 0.07054
	loss_reward_3: 0.01037
	loss_policy_4: 0.05555
	accuracy_policy_4: 0.64309
	loss_value_4: 0.07189
	loss_reward_4: 0.0129
	loss_policy_5: 0.05513
	accuracy_policy_5: 0.6532
	loss_value_5: 0.07292
	loss_reward_5: 0.01419
	loss_policy: 0.54281
	loss_value: 0.68827
	loss_reward: 0.05155
[2025-05-08 02:11:26] nn step 53700, lr: 0.1.
	loss_policy_0: 0.29901
	accuracy_policy_0: 0.66934
	loss_value_0: 0.36287
	loss_policy_1: 0.06051
	accuracy_policy_1: 0.66188
	loss_value_1: 0.07378
	loss_reward_1: 0.007
	loss_policy_2: 0.06119
	accuracy_policy_2: 0.65516
	loss_value_2: 0.07517
	loss_reward_2: 0.00924
	loss_policy_3: 0.0614
	accuracy_policy_3: 0.65023
	loss_value_3: 0.07661
	loss_reward_3: 0.01163
	loss_policy_4: 0.06178
	accuracy_policy_4: 0.64309
	loss_value_4: 0.0782
	loss_reward_4: 0.01515
	loss_policy_5: 0.06155
	accuracy_policy_5: 0.64367
	loss_value_5: 0.07946
	loss_reward_5: 0.01602
	loss_policy: 0.60544
	loss_value: 0.74609
	loss_reward: 0.05903
[2025-05-08 02:11:33] nn step 53750, lr: 0.1.
	loss_policy_0: 0.29182
	accuracy_policy_0: 0.66266
	loss_value_0: 0.35118
	loss_policy_1: 0.05882
	accuracy_policy_1: 0.65543
	loss_value_1: 0.07113
	loss_reward_1: 0.00672
	loss_policy_2: 0.05945
	accuracy_policy_2: 0.65262
	loss_value_2: 0.07275
	loss_reward_2: 0.00879
	loss_policy_3: 0.06
	accuracy_policy_3: 0.64645
	loss_value_3: 0.07411
	loss_reward_3: 0.01189
	loss_policy_4: 0.0602
	accuracy_policy_4: 0.6484
	loss_value_4: 0.07567
	loss_reward_4: 0.01417
	loss_policy_5: 0.06055
	accuracy_policy_5: 0.64949
	loss_value_5: 0.0771
	loss_reward_5: 0.01582
	loss_policy: 0.59085
	loss_value: 0.72194
	loss_reward: 0.0574
[2025-05-08 02:11:41] nn step 53800, lr: 0.1.
	loss_policy_0: 0.28088
	accuracy_policy_0: 0.66145
	loss_value_0: 0.33338
	loss_policy_1: 0.05714
	accuracy_policy_1: 0.64832
	loss_value_1: 0.06776
	loss_reward_1: 0.00658
	loss_policy_2: 0.0572
	accuracy_policy_2: 0.6516
	loss_value_2: 0.06941
	loss_reward_2: 0.00865
	loss_policy_3: 0.0575
	accuracy_policy_3: 0.64695
	loss_value_3: 0.07078
	loss_reward_3: 0.01132
	loss_policy_4: 0.0576
	accuracy_policy_4: 0.64605
	loss_value_4: 0.0725
	loss_reward_4: 0.0138
	loss_policy_5: 0.05739
	accuracy_policy_5: 0.6559
	loss_value_5: 0.07384
	loss_reward_5: 0.01514
	loss_policy: 0.56771
	loss_value: 0.68767
	loss_reward: 0.0555
Optimization_Done 53800
[2025-05-08 02:14:51] [command] train weight_iter_53800.pkl 251 270
[2025-05-08 02:15:01] nn step 53850, lr: 0.1.
	loss_policy_0: 0.27319
	accuracy_policy_0: 0.65734
	loss_value_0: 0.3322
	loss_policy_1: 0.05528
	accuracy_policy_1: 0.64984
	loss_value_1: 0.06736
	loss_reward_1: 0.00649
	loss_policy_2: 0.05545
	accuracy_policy_2: 0.65031
	loss_value_2: 0.06914
	loss_reward_2: 0.00855
	loss_policy_3: 0.05572
	accuracy_policy_3: 0.64516
	loss_value_3: 0.0704
	loss_reward_3: 0.01092
	loss_policy_4: 0.05594
	accuracy_policy_4: 0.64379
	loss_value_4: 0.07148
	loss_reward_4: 0.01353
	loss_policy_5: 0.05595
	accuracy_policy_5: 0.6473
	loss_value_5: 0.07272
	loss_reward_5: 0.01456
	loss_policy: 0.55155
	loss_value: 0.68331
	loss_reward: 0.05404
[2025-05-08 02:15:09] nn step 53900, lr: 0.1.
	loss_policy_0: 0.27714
	accuracy_policy_0: 0.65855
	loss_value_0: 0.33732
	loss_policy_1: 0.05655
	accuracy_policy_1: 0.64652
	loss_value_1: 0.06875
	loss_reward_1: 0.00659
	loss_policy_2: 0.05711
	accuracy_policy_2: 0.64641
	loss_value_2: 0.07058
	loss_reward_2: 0.00871
	loss_policy_3: 0.05682
	accuracy_policy_3: 0.6491
	loss_value_3: 0.07195
	loss_reward_3: 0.01117
	loss_policy_4: 0.05753
	accuracy_policy_4: 0.64516
	loss_value_4: 0.07337
	loss_reward_4: 0.01395
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.64852
	loss_value_5: 0.07474
	loss_reward_5: 0.01494
	loss_policy: 0.56236
	loss_value: 0.69672
	loss_reward: 0.05536
[2025-05-08 02:15:17] nn step 53950, lr: 0.1.
	loss_policy_0: 0.27927
	accuracy_policy_0: 0.65777
	loss_value_0: 0.33232
	loss_policy_1: 0.05654
	accuracy_policy_1: 0.64941
	loss_value_1: 0.06742
	loss_reward_1: 0.0061
	loss_policy_2: 0.0568
	accuracy_policy_2: 0.64844
	loss_value_2: 0.06914
	loss_reward_2: 0.00818
	loss_policy_3: 0.05731
	accuracy_policy_3: 0.64297
	loss_value_3: 0.07101
	loss_reward_3: 0.011
	loss_policy_4: 0.05792
	accuracy_policy_4: 0.64117
	loss_value_4: 0.07248
	loss_reward_4: 0.01373
	loss_policy_5: 0.05767
	accuracy_policy_5: 0.64367
	loss_value_5: 0.07383
	loss_reward_5: 0.01507
	loss_policy: 0.56551
	loss_value: 0.6862
	loss_reward: 0.05407
[2025-05-08 02:15:24] nn step 54000, lr: 0.1.
	loss_policy_0: 0.27779
	accuracy_policy_0: 0.65898
	loss_value_0: 0.32932
	loss_policy_1: 0.0561
	accuracy_policy_1: 0.65078
	loss_value_1: 0.0671
	loss_reward_1: 0.00664
	loss_policy_2: 0.05609
	accuracy_policy_2: 0.65414
	loss_value_2: 0.06902
	loss_reward_2: 0.00836
	loss_policy_3: 0.0569
	accuracy_policy_3: 0.63953
	loss_value_3: 0.0706
	loss_reward_3: 0.01096
	loss_policy_4: 0.05679
	accuracy_policy_4: 0.64645
	loss_value_4: 0.07188
	loss_reward_4: 0.01381
	loss_policy_5: 0.05717
	accuracy_policy_5: 0.64801
	loss_value_5: 0.07335
	loss_reward_5: 0.01483
	loss_policy: 0.56085
	loss_value: 0.68127
	loss_reward: 0.0546
Optimization_Done 54000
[2025-05-08 02:18:32] [command] train weight_iter_54000.pkl 252 271
[2025-05-08 02:18:40] nn step 54050, lr: 0.1.
	loss_policy_0: 0.25245
	accuracy_policy_0: 0.67367
	loss_value_0: 0.3172
	loss_policy_1: 0.05105
	accuracy_policy_1: 0.65973
	loss_value_1: 0.06422
	loss_reward_1: 0.00622
	loss_policy_2: 0.05168
	accuracy_policy_2: 0.65238
	loss_value_2: 0.06522
	loss_reward_2: 0.00776
	loss_policy_3: 0.05221
	accuracy_policy_3: 0.65289
	loss_value_3: 0.06676
	loss_reward_3: 0.0101
	loss_policy_4: 0.05227
	accuracy_policy_4: 0.64949
	loss_value_4: 0.06795
	loss_reward_4: 0.01311
	loss_policy_5: 0.05217
	accuracy_policy_5: 0.65406
	loss_value_5: 0.06915
	loss_reward_5: 0.01399
	loss_policy: 0.51182
	loss_value: 0.65051
	loss_reward: 0.05118
[2025-05-08 02:18:48] nn step 54100, lr: 0.1.
	loss_policy_0: 0.26712
	accuracy_policy_0: 0.6666
	loss_value_0: 0.32567
	loss_policy_1: 0.05397
	accuracy_policy_1: 0.66336
	loss_value_1: 0.06616
	loss_reward_1: 0.00629
	loss_policy_2: 0.05463
	accuracy_policy_2: 0.65492
	loss_value_2: 0.06778
	loss_reward_2: 0.00797
	loss_policy_3: 0.05498
	accuracy_policy_3: 0.65246
	loss_value_3: 0.06892
	loss_reward_3: 0.01068
	loss_policy_4: 0.05515
	accuracy_policy_4: 0.65219
	loss_value_4: 0.07056
	loss_reward_4: 0.01329
	loss_policy_5: 0.05541
	accuracy_policy_5: 0.64855
	loss_value_5: 0.07183
	loss_reward_5: 0.01425
	loss_policy: 0.54126
	loss_value: 0.67092
	loss_reward: 0.05248
[2025-05-08 02:18:57] nn step 54150, lr: 0.1.
	loss_policy_0: 0.27825
	accuracy_policy_0: 0.66922
	loss_value_0: 0.33443
	loss_policy_1: 0.05667
	accuracy_policy_1: 0.65453
	loss_value_1: 0.06828
	loss_reward_1: 0.00688
	loss_policy_2: 0.05671
	accuracy_policy_2: 0.65633
	loss_value_2: 0.06983
	loss_reward_2: 0.00852
	loss_policy_3: 0.05749
	accuracy_policy_3: 0.64832
	loss_value_3: 0.07118
	loss_reward_3: 0.0114
	loss_policy_4: 0.05788
	accuracy_policy_4: 0.64301
	loss_value_4: 0.07301
	loss_reward_4: 0.01379
	loss_policy_5: 0.05811
	accuracy_policy_5: 0.64973
	loss_value_5: 0.07411
	loss_reward_5: 0.01526
	loss_policy: 0.56511
	loss_value: 0.69085
	loss_reward: 0.05585
[2025-05-08 02:19:05] nn step 54200, lr: 0.1.
	loss_policy_0: 0.26418
	accuracy_policy_0: 0.66812
	loss_value_0: 0.31739
	loss_policy_1: 0.05357
	accuracy_policy_1: 0.66008
	loss_value_1: 0.06461
	loss_reward_1: 0.00649
	loss_policy_2: 0.05416
	accuracy_policy_2: 0.6534
	loss_value_2: 0.06629
	loss_reward_2: 0.00821
	loss_policy_3: 0.05406
	accuracy_policy_3: 0.65383
	loss_value_3: 0.06763
	loss_reward_3: 0.01057
	loss_policy_4: 0.05493
	accuracy_policy_4: 0.64453
	loss_value_4: 0.06922
	loss_reward_4: 0.01331
	loss_policy_5: 0.05471
	accuracy_policy_5: 0.65512
	loss_value_5: 0.07044
	loss_reward_5: 0.01407
	loss_policy: 0.53562
	loss_value: 0.65558
	loss_reward: 0.05266
Optimization_Done 54200
[2025-05-08 02:22:10] [command] train weight_iter_54200.pkl 253 272
[2025-05-08 02:22:19] nn step 54250, lr: 0.1.
	loss_policy_0: 0.28116
	accuracy_policy_0: 0.67273
	loss_value_0: 0.35004
	loss_policy_1: 0.05689
	accuracy_policy_1: 0.65977
	loss_value_1: 0.07109
	loss_reward_1: 0.00657
	loss_policy_2: 0.05753
	accuracy_policy_2: 0.65508
	loss_value_2: 0.07282
	loss_reward_2: 0.00854
	loss_policy_3: 0.05788
	accuracy_policy_3: 0.6507
	loss_value_3: 0.07402
	loss_reward_3: 0.01115
	loss_policy_4: 0.05811
	accuracy_policy_4: 0.64914
	loss_value_4: 0.07564
	loss_reward_4: 0.01381
	loss_policy_5: 0.05835
	accuracy_policy_5: 0.65156
	loss_value_5: 0.07693
	loss_reward_5: 0.0153
	loss_policy: 0.56993
	loss_value: 0.72054
	loss_reward: 0.05537
[2025-05-08 02:22:26] nn step 54300, lr: 0.1.
	loss_policy_0: 0.28277
	accuracy_policy_0: 0.65754
	loss_value_0: 0.33792
	loss_policy_1: 0.05652
	accuracy_policy_1: 0.65477
	loss_value_1: 0.06857
	loss_reward_1: 0.00651
	loss_policy_2: 0.05671
	accuracy_policy_2: 0.65109
	loss_value_2: 0.07053
	loss_reward_2: 0.00866
	loss_policy_3: 0.05722
	accuracy_policy_3: 0.64609
	loss_value_3: 0.07207
	loss_reward_3: 0.01122
	loss_policy_4: 0.05762
	accuracy_policy_4: 0.64547
	loss_value_4: 0.07326
	loss_reward_4: 0.01414
	loss_policy_5: 0.05752
	accuracy_policy_5: 0.64918
	loss_value_5: 0.07455
	loss_reward_5: 0.01503
	loss_policy: 0.56836
	loss_value: 0.69688
	loss_reward: 0.05556
[2025-05-08 02:22:34] nn step 54350, lr: 0.1.
	loss_policy_0: 0.28476
	accuracy_policy_0: 0.66062
	loss_value_0: 0.34194
	loss_policy_1: 0.05777
	accuracy_policy_1: 0.65258
	loss_value_1: 0.06958
	loss_reward_1: 0.00646
	loss_policy_2: 0.05807
	accuracy_policy_2: 0.64906
	loss_value_2: 0.07128
	loss_reward_2: 0.00881
	loss_policy_3: 0.05843
	accuracy_policy_3: 0.64691
	loss_value_3: 0.073
	loss_reward_3: 0.01159
	loss_policy_4: 0.05864
	accuracy_policy_4: 0.64254
	loss_value_4: 0.07443
	loss_reward_4: 0.01425
	loss_policy_5: 0.05845
	accuracy_policy_5: 0.64469
	loss_value_5: 0.07582
	loss_reward_5: 0.01568
	loss_policy: 0.57613
	loss_value: 0.70605
	loss_reward: 0.0568
[2025-05-08 02:22:43] nn step 54400, lr: 0.1.
	loss_policy_0: 0.27074
	accuracy_policy_0: 0.66125
	loss_value_0: 0.32145
	loss_policy_1: 0.05472
	accuracy_policy_1: 0.65184
	loss_value_1: 0.06541
	loss_reward_1: 0.00629
	loss_policy_2: 0.05481
	accuracy_policy_2: 0.65324
	loss_value_2: 0.06705
	loss_reward_2: 0.00835
	loss_policy_3: 0.05505
	accuracy_policy_3: 0.64809
	loss_value_3: 0.06857
	loss_reward_3: 0.01074
	loss_policy_4: 0.05539
	accuracy_policy_4: 0.6493
	loss_value_4: 0.07004
	loss_reward_4: 0.01366
	loss_policy_5: 0.05545
	accuracy_policy_5: 0.64918
	loss_value_5: 0.07129
	loss_reward_5: 0.01476
	loss_policy: 0.54617
	loss_value: 0.66381
	loss_reward: 0.05381
Optimization_Done 54400
[2025-05-08 02:25:52] [command] train weight_iter_54400.pkl 254 273
[2025-05-08 02:26:01] nn step 54450, lr: 0.1.
	loss_policy_0: 0.26253
	accuracy_policy_0: 0.66984
	loss_value_0: 0.32304
	loss_policy_1: 0.05347
	accuracy_policy_1: 0.65566
	loss_value_1: 0.06568
	loss_reward_1: 0.00623
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.6518
	loss_value_2: 0.0671
	loss_reward_2: 0.00783
	loss_policy_3: 0.05368
	accuracy_policy_3: 0.64762
	loss_value_3: 0.06848
	loss_reward_3: 0.01076
	loss_policy_4: 0.05439
	accuracy_policy_4: 0.64738
	loss_value_4: 0.06975
	loss_reward_4: 0.01302
	loss_policy_5: 0.05427
	accuracy_policy_5: 0.65156
	loss_value_5: 0.07077
	loss_reward_5: 0.01459
	loss_policy: 0.53241
	loss_value: 0.66482
	loss_reward: 0.05243
[2025-05-08 02:26:10] nn step 54500, lr: 0.1.
	loss_policy_0: 0.28175
	accuracy_policy_0: 0.665
	loss_value_0: 0.33932
	loss_policy_1: 0.057
	accuracy_policy_1: 0.65121
	loss_value_1: 0.06873
	loss_reward_1: 0.00636
	loss_policy_2: 0.05749
	accuracy_policy_2: 0.65074
	loss_value_2: 0.07051
	loss_reward_2: 0.00893
	loss_policy_3: 0.05753
	accuracy_policy_3: 0.64766
	loss_value_3: 0.07201
	loss_reward_3: 0.01149
	loss_policy_4: 0.05816
	accuracy_policy_4: 0.64695
	loss_value_4: 0.07372
	loss_reward_4: 0.01399
	loss_policy_5: 0.05796
	accuracy_policy_5: 0.6507
	loss_value_5: 0.07528
	loss_reward_5: 0.01505
	loss_policy: 0.56989
	loss_value: 0.69956
	loss_reward: 0.05582
[2025-05-08 02:26:17] nn step 54550, lr: 0.1.
	loss_policy_0: 0.27301
	accuracy_policy_0: 0.66934
	loss_value_0: 0.32885
	loss_policy_1: 0.05589
	accuracy_policy_1: 0.6502
	loss_value_1: 0.06677
	loss_reward_1: 0.00639
	loss_policy_2: 0.05602
	accuracy_policy_2: 0.65531
	loss_value_2: 0.06818
	loss_reward_2: 0.00811
	loss_policy_3: 0.05643
	accuracy_policy_3: 0.6534
	loss_value_3: 0.06957
	loss_reward_3: 0.01115
	loss_policy_4: 0.05717
	accuracy_policy_4: 0.64688
	loss_value_4: 0.07106
	loss_reward_4: 0.01395
	loss_policy_5: 0.05694
	accuracy_policy_5: 0.65188
	loss_value_5: 0.07272
	loss_reward_5: 0.01524
	loss_policy: 0.55545
	loss_value: 0.67715
	loss_reward: 0.05485
[2025-05-08 02:26:25] nn step 54600, lr: 0.1.
	loss_policy_0: 0.2701
	accuracy_policy_0: 0.67168
	loss_value_0: 0.32304
	loss_policy_1: 0.05478
	accuracy_policy_1: 0.65852
	loss_value_1: 0.06576
	loss_reward_1: 0.00664
	loss_policy_2: 0.0551
	accuracy_policy_2: 0.65715
	loss_value_2: 0.06768
	loss_reward_2: 0.00828
	loss_policy_3: 0.05599
	accuracy_policy_3: 0.64918
	loss_value_3: 0.06926
	loss_reward_3: 0.0111
	loss_policy_4: 0.05607
	accuracy_policy_4: 0.65074
	loss_value_4: 0.07069
	loss_reward_4: 0.01384
	loss_policy_5: 0.05615
	accuracy_policy_5: 0.65625
	loss_value_5: 0.07248
	loss_reward_5: 0.01497
	loss_policy: 0.54817
	loss_value: 0.6689
	loss_reward: 0.05482
Optimization_Done 54600
[2025-05-08 02:29:29] [command] train weight_iter_54600.pkl 255 274
[2025-05-08 02:29:38] nn step 54650, lr: 0.1.
	loss_policy_0: 0.27065
	accuracy_policy_0: 0.66918
	loss_value_0: 0.32826
	loss_policy_1: 0.05497
	accuracy_policy_1: 0.6573
	loss_value_1: 0.06651
	loss_reward_1: 0.00651
	loss_policy_2: 0.05521
	accuracy_policy_2: 0.65699
	loss_value_2: 0.06816
	loss_reward_2: 0.00838
	loss_policy_3: 0.05525
	accuracy_policy_3: 0.65348
	loss_value_3: 0.06949
	loss_reward_3: 0.01118
	loss_policy_4: 0.05581
	accuracy_policy_4: 0.65008
	loss_value_4: 0.07091
	loss_reward_4: 0.01362
	loss_policy_5: 0.05556
	accuracy_policy_5: 0.65512
	loss_value_5: 0.07217
	loss_reward_5: 0.01503
	loss_policy: 0.54745
	loss_value: 0.67549
	loss_reward: 0.05472
[2025-05-08 02:29:46] nn step 54700, lr: 0.1.
	loss_policy_0: 0.28187
	accuracy_policy_0: 0.67141
	loss_value_0: 0.34125
	loss_policy_1: 0.05726
	accuracy_policy_1: 0.65871
	loss_value_1: 0.06945
	loss_reward_1: 0.00664
	loss_policy_2: 0.05806
	accuracy_policy_2: 0.65309
	loss_value_2: 0.07116
	loss_reward_2: 0.00865
	loss_policy_3: 0.05788
	accuracy_policy_3: 0.65555
	loss_value_3: 0.07254
	loss_reward_3: 0.01138
	loss_policy_4: 0.05849
	accuracy_policy_4: 0.65367
	loss_value_4: 0.07429
	loss_reward_4: 0.01432
	loss_policy_5: 0.05822
	accuracy_policy_5: 0.66078
	loss_value_5: 0.07558
	loss_reward_5: 0.01571
	loss_policy: 0.57179
	loss_value: 0.70428
	loss_reward: 0.05671
[2025-05-08 02:29:55] nn step 54750, lr: 0.1.
	loss_policy_0: 0.27284
	accuracy_policy_0: 0.67141
	loss_value_0: 0.33115
	loss_policy_1: 0.05574
	accuracy_policy_1: 0.66039
	loss_value_1: 0.0673
	loss_reward_1: 0.0063
	loss_policy_2: 0.056
	accuracy_policy_2: 0.65082
	loss_value_2: 0.06857
	loss_reward_2: 0.00851
	loss_policy_3: 0.05647
	accuracy_policy_3: 0.65281
	loss_value_3: 0.07004
	loss_reward_3: 0.01092
	loss_policy_4: 0.05685
	accuracy_policy_4: 0.64836
	loss_value_4: 0.07184
	loss_reward_4: 0.01392
	loss_policy_5: 0.0568
	accuracy_policy_5: 0.65434
	loss_value_5: 0.07337
	loss_reward_5: 0.01491
	loss_policy: 0.5547
	loss_value: 0.68228
	loss_reward: 0.05455
[2025-05-08 02:30:01] nn step 54800, lr: 0.1.
	loss_policy_0: 0.27143
	accuracy_policy_0: 0.66598
	loss_value_0: 0.32343
	loss_policy_1: 0.05506
	accuracy_policy_1: 0.65586
	loss_value_1: 0.06602
	loss_reward_1: 0.00635
	loss_policy_2: 0.05494
	accuracy_policy_2: 0.65602
	loss_value_2: 0.06736
	loss_reward_2: 0.0086
	loss_policy_3: 0.05556
	accuracy_policy_3: 0.64945
	loss_value_3: 0.06925
	loss_reward_3: 0.01112
	loss_policy_4: 0.05614
	accuracy_policy_4: 0.64758
	loss_value_4: 0.07073
	loss_reward_4: 0.01356
	loss_policy_5: 0.05598
	accuracy_policy_5: 0.64898
	loss_value_5: 0.07208
	loss_reward_5: 0.01484
	loss_policy: 0.54911
	loss_value: 0.66887
	loss_reward: 0.05448
Optimization_Done 54800
[2025-05-08 02:33:12] [command] train weight_iter_54800.pkl 256 275
[2025-05-08 02:33:20] nn step 54850, lr: 0.1.
	loss_policy_0: 0.27118
	accuracy_policy_0: 0.67492
	loss_value_0: 0.33916
	loss_policy_1: 0.05491
	accuracy_policy_1: 0.66145
	loss_value_1: 0.0688
	loss_reward_1: 0.00664
	loss_policy_2: 0.05531
	accuracy_policy_2: 0.66523
	loss_value_2: 0.07043
	loss_reward_2: 0.00816
	loss_policy_3: 0.056
	accuracy_policy_3: 0.65883
	loss_value_3: 0.07132
	loss_reward_3: 0.01093
	loss_policy_4: 0.05615
	accuracy_policy_4: 0.6534
	loss_value_4: 0.07277
	loss_reward_4: 0.01384
	loss_policy_5: 0.05611
	accuracy_policy_5: 0.6598
	loss_value_5: 0.07427
	loss_reward_5: 0.0148
	loss_policy: 0.54966
	loss_value: 0.69675
	loss_reward: 0.05437
[2025-05-08 02:33:28] nn step 54900, lr: 0.1.
	loss_policy_0: 0.27193
	accuracy_policy_0: 0.67539
	loss_value_0: 0.33215
	loss_policy_1: 0.05504
	accuracy_policy_1: 0.66621
	loss_value_1: 0.06732
	loss_reward_1: 0.00625
	loss_policy_2: 0.05539
	accuracy_policy_2: 0.66277
	loss_value_2: 0.0691
	loss_reward_2: 0.00821
	loss_policy_3: 0.05594
	accuracy_policy_3: 0.66094
	loss_value_3: 0.07075
	loss_reward_3: 0.01134
	loss_policy_4: 0.05635
	accuracy_policy_4: 0.65195
	loss_value_4: 0.07208
	loss_reward_4: 0.01371
	loss_policy_5: 0.05627
	accuracy_policy_5: 0.66062
	loss_value_5: 0.07349
	loss_reward_5: 0.01459
	loss_policy: 0.55093
	loss_value: 0.6849
	loss_reward: 0.05411
[2025-05-08 02:33:36] nn step 54950, lr: 0.1.
	loss_policy_0: 0.27493
	accuracy_policy_0: 0.67539
	loss_value_0: 0.33524
	loss_policy_1: 0.05586
	accuracy_policy_1: 0.66121
	loss_value_1: 0.06772
	loss_reward_1: 0.0067
	loss_policy_2: 0.05636
	accuracy_policy_2: 0.65793
	loss_value_2: 0.06954
	loss_reward_2: 0.00876
	loss_policy_3: 0.05684
	accuracy_policy_3: 0.65789
	loss_value_3: 0.0708
	loss_reward_3: 0.01068
	loss_policy_4: 0.0572
	accuracy_policy_4: 0.64996
	loss_value_4: 0.07208
	loss_reward_4: 0.01372
	loss_policy_5: 0.05725
	accuracy_policy_5: 0.65797
	loss_value_5: 0.07374
	loss_reward_5: 0.01536
	loss_policy: 0.55844
	loss_value: 0.68912
	loss_reward: 0.05523
[2025-05-08 02:33:44] nn step 55000, lr: 0.1.
	loss_policy_0: 0.25323
	accuracy_policy_0: 0.67406
	loss_value_0: 0.30595
	loss_policy_1: 0.05132
	accuracy_policy_1: 0.66043
	loss_value_1: 0.06228
	loss_reward_1: 0.00585
	loss_policy_2: 0.05162
	accuracy_policy_2: 0.66223
	loss_value_2: 0.06378
	loss_reward_2: 0.0076
	loss_policy_3: 0.05208
	accuracy_policy_3: 0.65391
	loss_value_3: 0.06508
	loss_reward_3: 0.01015
	loss_policy_4: 0.05235
	accuracy_policy_4: 0.65496
	loss_value_4: 0.06632
	loss_reward_4: 0.01255
	loss_policy_5: 0.05206
	accuracy_policy_5: 0.66152
	loss_value_5: 0.06763
	loss_reward_5: 0.01338
	loss_policy: 0.51265
	loss_value: 0.63103
	loss_reward: 0.04954
Optimization_Done 55000
[2025-05-08 02:36:50] [command] train weight_iter_55000.pkl 257 276
[2025-05-08 02:36:59] nn step 55050, lr: 0.1.
	loss_policy_0: 0.25317
	accuracy_policy_0: 0.68117
	loss_value_0: 0.31727
	loss_policy_1: 0.05083
	accuracy_policy_1: 0.67109
	loss_value_1: 0.06421
	loss_reward_1: 0.00608
	loss_policy_2: 0.05168
	accuracy_policy_2: 0.66555
	loss_value_2: 0.06539
	loss_reward_2: 0.00793
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.66461
	loss_value_3: 0.06686
	loss_reward_3: 0.00979
	loss_policy_4: 0.05196
	accuracy_policy_4: 0.66039
	loss_value_4: 0.06787
	loss_reward_4: 0.01254
	loss_policy_5: 0.0521
	accuracy_policy_5: 0.66859
	loss_value_5: 0.06906
	loss_reward_5: 0.01383
	loss_policy: 0.51133
	loss_value: 0.65065
	loss_reward: 0.05017
[2025-05-08 02:37:05] nn step 55100, lr: 0.1.
	loss_policy_0: 0.26658
	accuracy_policy_0: 0.66059
	loss_value_0: 0.31741
	loss_policy_1: 0.05239
	accuracy_policy_1: 0.6607
	loss_value_1: 0.0644
	loss_reward_1: 0.00613
	loss_policy_2: 0.05239
	accuracy_policy_2: 0.65938
	loss_value_2: 0.06568
	loss_reward_2: 0.00819
	loss_policy_3: 0.0526
	accuracy_policy_3: 0.6618
	loss_value_3: 0.06714
	loss_reward_3: 0.01037
	loss_policy_4: 0.0527
	accuracy_policy_4: 0.66164
	loss_value_4: 0.06838
	loss_reward_4: 0.01337
	loss_policy_5: 0.05282
	accuracy_policy_5: 0.66414
	loss_value_5: 0.06969
	loss_reward_5: 0.01465
	loss_policy: 0.52947
	loss_value: 0.65269
	loss_reward: 0.0527
[2025-05-08 02:37:14] nn step 55150, lr: 0.1.
	loss_policy_0: 0.26902
	accuracy_policy_0: 0.6759
	loss_value_0: 0.3274
	loss_policy_1: 0.05379
	accuracy_policy_1: 0.66656
	loss_value_1: 0.06658
	loss_reward_1: 0.00635
	loss_policy_2: 0.05425
	accuracy_policy_2: 0.67188
	loss_value_2: 0.06816
	loss_reward_2: 0.00829
	loss_policy_3: 0.0546
	accuracy_policy_3: 0.66172
	loss_value_3: 0.06968
	loss_reward_3: 0.01079
	loss_policy_4: 0.05499
	accuracy_policy_4: 0.66336
	loss_value_4: 0.07107
	loss_reward_4: 0.0138
	loss_policy_5: 0.05499
	accuracy_policy_5: 0.66137
	loss_value_5: 0.07198
	loss_reward_5: 0.01475
	loss_policy: 0.54164
	loss_value: 0.67487
	loss_reward: 0.05399
[2025-05-08 02:37:22] nn step 55200, lr: 0.1.
	loss_policy_0: 0.26172
	accuracy_policy_0: 0.6807
	loss_value_0: 0.31914
	loss_policy_1: 0.05308
	accuracy_policy_1: 0.66289
	loss_value_1: 0.06495
	loss_reward_1: 0.00665
	loss_policy_2: 0.05362
	accuracy_policy_2: 0.66023
	loss_value_2: 0.06636
	loss_reward_2: 0.00781
	loss_policy_3: 0.05381
	accuracy_policy_3: 0.66387
	loss_value_3: 0.06799
	loss_reward_3: 0.01043
	loss_policy_4: 0.05471
	accuracy_policy_4: 0.6575
	loss_value_4: 0.06937
	loss_reward_4: 0.01306
	loss_policy_5: 0.05461
	accuracy_policy_5: 0.66547
	loss_value_5: 0.07071
	loss_reward_5: 0.01473
	loss_policy: 0.53156
	loss_value: 0.65852
	loss_reward: 0.05268
Optimization_Done 55200
[2025-05-08 02:40:26] [command] train weight_iter_55200.pkl 258 277
[2025-05-08 02:40:35] nn step 55250, lr: 0.1.
	loss_policy_0: 0.27668
	accuracy_policy_0: 0.67195
	loss_value_0: 0.34009
	loss_policy_1: 0.05551
	accuracy_policy_1: 0.66129
	loss_value_1: 0.06913
	loss_reward_1: 0.00637
	loss_policy_2: 0.05599
	accuracy_policy_2: 0.66027
	loss_value_2: 0.07098
	loss_reward_2: 0.00836
	loss_policy_3: 0.05623
	accuracy_policy_3: 0.65625
	loss_value_3: 0.0723
	loss_reward_3: 0.01121
	loss_policy_4: 0.05643
	accuracy_policy_4: 0.6577
	loss_value_4: 0.07398
	loss_reward_4: 0.01371
	loss_policy_5: 0.05681
	accuracy_policy_5: 0.65777
	loss_value_5: 0.0755
	loss_reward_5: 0.01516
	loss_policy: 0.55765
	loss_value: 0.70198
	loss_reward: 0.05481
[2025-05-08 02:40:43] nn step 55300, lr: 0.1.
	loss_policy_0: 0.27484
	accuracy_policy_0: 0.67594
	loss_value_0: 0.33888
	loss_policy_1: 0.0559
	accuracy_policy_1: 0.65859
	loss_value_1: 0.06905
	loss_reward_1: 0.00627
	loss_policy_2: 0.05667
	accuracy_policy_2: 0.65656
	loss_value_2: 0.07089
	loss_reward_2: 0.00838
	loss_policy_3: 0.05646
	accuracy_policy_3: 0.65445
	loss_value_3: 0.07235
	loss_reward_3: 0.01102
	loss_policy_4: 0.05725
	accuracy_policy_4: 0.65488
	loss_value_4: 0.07386
	loss_reward_4: 0.01375
	loss_policy_5: 0.05699
	accuracy_policy_5: 0.66105
	loss_value_5: 0.07528
	loss_reward_5: 0.0148
	loss_policy: 0.55812
	loss_value: 0.70032
	loss_reward: 0.05422
[2025-05-08 02:40:50] nn step 55350, lr: 0.1.
	loss_policy_0: 0.26831
	accuracy_policy_0: 0.67484
	loss_value_0: 0.32802
	loss_policy_1: 0.05441
	accuracy_policy_1: 0.65949
	loss_value_1: 0.06672
	loss_reward_1: 0.00641
	loss_policy_2: 0.05485
	accuracy_policy_2: 0.65957
	loss_value_2: 0.06871
	loss_reward_2: 0.00819
	loss_policy_3: 0.05523
	accuracy_policy_3: 0.65711
	loss_value_3: 0.07006
	loss_reward_3: 0.01087
	loss_policy_4: 0.05563
	accuracy_policy_4: 0.65812
	loss_value_4: 0.07185
	loss_reward_4: 0.01348
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.65922
	loss_value_5: 0.07285
	loss_reward_5: 0.01517
	loss_policy: 0.54423
	loss_value: 0.67821
	loss_reward: 0.05411
[2025-05-08 02:40:58] nn step 55400, lr: 0.1.
	loss_policy_0: 0.28031
	accuracy_policy_0: 0.67348
	loss_value_0: 0.34419
	loss_policy_1: 0.05703
	accuracy_policy_1: 0.66043
	loss_value_1: 0.06998
	loss_reward_1: 0.00688
	loss_policy_2: 0.05731
	accuracy_policy_2: 0.65898
	loss_value_2: 0.07157
	loss_reward_2: 0.00868
	loss_policy_3: 0.05774
	accuracy_policy_3: 0.65625
	loss_value_3: 0.07289
	loss_reward_3: 0.0115
	loss_policy_4: 0.05846
	accuracy_policy_4: 0.65363
	loss_value_4: 0.07469
	loss_reward_4: 0.01462
	loss_policy_5: 0.05828
	accuracy_policy_5: 0.65883
	loss_value_5: 0.07604
	loss_reward_5: 0.01564
	loss_policy: 0.56913
	loss_value: 0.70935
	loss_reward: 0.05733
Optimization_Done 55400
[2025-05-08 02:44:03] [command] train weight_iter_55400.pkl 259 278
[2025-05-08 02:44:10] nn step 55450, lr: 0.1.
	loss_policy_0: 0.24944
	accuracy_policy_0: 0.68379
	loss_value_0: 0.31304
	loss_policy_1: 0.05071
	accuracy_policy_1: 0.66406
	loss_value_1: 0.06361
	loss_reward_1: 0.00613
	loss_policy_2: 0.05087
	accuracy_policy_2: 0.66656
	loss_value_2: 0.06526
	loss_reward_2: 0.00755
	loss_policy_3: 0.05169
	accuracy_policy_3: 0.65637
	loss_value_3: 0.06642
	loss_reward_3: 0.01043
	loss_policy_4: 0.05212
	accuracy_policy_4: 0.65316
	loss_value_4: 0.06776
	loss_reward_4: 0.01309
	loss_policy_5: 0.05167
	accuracy_policy_5: 0.65719
	loss_value_5: 0.06883
	loss_reward_5: 0.01375
	loss_policy: 0.50651
	loss_value: 0.64492
	loss_reward: 0.05095
[2025-05-08 02:44:19] nn step 55500, lr: 0.1.
	loss_policy_0: 0.26633
	accuracy_policy_0: 0.6766
	loss_value_0: 0.32845
	loss_policy_1: 0.05355
	accuracy_policy_1: 0.66344
	loss_value_1: 0.06671
	loss_reward_1: 0.0062
	loss_policy_2: 0.05435
	accuracy_policy_2: 0.66168
	loss_value_2: 0.06849
	loss_reward_2: 0.00808
	loss_policy_3: 0.05464
	accuracy_policy_3: 0.6573
	loss_value_3: 0.06998
	loss_reward_3: 0.01048
	loss_policy_4: 0.05501
	accuracy_policy_4: 0.65934
	loss_value_4: 0.07147
	loss_reward_4: 0.01396
	loss_policy_5: 0.0553
	accuracy_policy_5: 0.66027
	loss_value_5: 0.0727
	loss_reward_5: 0.01478
	loss_policy: 0.53918
	loss_value: 0.67781
	loss_reward: 0.0535
[2025-05-08 02:44:27] nn step 55550, lr: 0.1.
	loss_policy_0: 0.26449
	accuracy_policy_0: 0.68035
	loss_value_0: 0.32591
	loss_policy_1: 0.05369
	accuracy_policy_1: 0.66637
	loss_value_1: 0.06615
	loss_reward_1: 0.00654
	loss_policy_2: 0.05435
	accuracy_policy_2: 0.66039
	loss_value_2: 0.06753
	loss_reward_2: 0.00845
	loss_policy_3: 0.05477
	accuracy_policy_3: 0.65633
	loss_value_3: 0.06915
	loss_reward_3: 0.0111
	loss_policy_4: 0.05545
	accuracy_policy_4: 0.65332
	loss_value_4: 0.0708
	loss_reward_4: 0.01371
	loss_policy_5: 0.0555
	accuracy_policy_5: 0.65996
	loss_value_5: 0.07216
	loss_reward_5: 0.01515
	loss_policy: 0.53826
	loss_value: 0.67169
	loss_reward: 0.05494
[2025-05-08 02:44:36] nn step 55600, lr: 0.1.
	loss_policy_0: 0.27605
	accuracy_policy_0: 0.66949
	loss_value_0: 0.33318
	loss_policy_1: 0.05554
	accuracy_policy_1: 0.66227
	loss_value_1: 0.06791
	loss_reward_1: 0.00649
	loss_policy_2: 0.0557
	accuracy_policy_2: 0.6625
	loss_value_2: 0.06985
	loss_reward_2: 0.00819
	loss_policy_3: 0.0559
	accuracy_policy_3: 0.65957
	loss_value_3: 0.07125
	loss_reward_3: 0.01114
	loss_policy_4: 0.05618
	accuracy_policy_4: 0.65398
	loss_value_4: 0.07261
	loss_reward_4: 0.01404
	loss_policy_5: 0.05642
	accuracy_policy_5: 0.665
	loss_value_5: 0.07415
	loss_reward_5: 0.0151
	loss_policy: 0.55578
	loss_value: 0.68894
	loss_reward: 0.05495
Optimization_Done 55600
[2025-05-08 02:47:49] [command] train weight_iter_55600.pkl 260 279
[2025-05-08 02:47:56] nn step 55650, lr: 0.1.
	loss_policy_0: 0.27946
	accuracy_policy_0: 0.66387
	loss_value_0: 0.33855
	loss_policy_1: 0.05578
	accuracy_policy_1: 0.6575
	loss_value_1: 0.06865
	loss_reward_1: 0.00649
	loss_policy_2: 0.05629
	accuracy_policy_2: 0.6573
	loss_value_2: 0.07009
	loss_reward_2: 0.00854
	loss_policy_3: 0.0562
	accuracy_policy_3: 0.65746
	loss_value_3: 0.07153
	loss_reward_3: 0.01078
	loss_policy_4: 0.05668
	accuracy_policy_4: 0.65391
	loss_value_4: 0.0732
	loss_reward_4: 0.01393
	loss_policy_5: 0.05686
	accuracy_policy_5: 0.66145
	loss_value_5: 0.07441
	loss_reward_5: 0.0154
	loss_policy: 0.56126
	loss_value: 0.69643
	loss_reward: 0.05516
[2025-05-08 02:48:04] nn step 55700, lr: 0.1.
	loss_policy_0: 0.25749
	accuracy_policy_0: 0.67031
	loss_value_0: 0.31132
	loss_policy_1: 0.05175
	accuracy_policy_1: 0.65746
	loss_value_1: 0.06366
	loss_reward_1: 0.00624
	loss_policy_2: 0.05143
	accuracy_policy_2: 0.6584
	loss_value_2: 0.06503
	loss_reward_2: 0.00803
	loss_policy_3: 0.05218
	accuracy_policy_3: 0.65906
	loss_value_3: 0.06637
	loss_reward_3: 0.01028
	loss_policy_4: 0.05275
	accuracy_policy_4: 0.65328
	loss_value_4: 0.06755
	loss_reward_4: 0.01273
	loss_policy_5: 0.05258
	accuracy_policy_5: 0.65621
	loss_value_5: 0.06872
	loss_reward_5: 0.01425
	loss_policy: 0.51819
	loss_value: 0.64265
	loss_reward: 0.05153
[2025-05-08 02:48:13] nn step 55750, lr: 0.1.
	loss_policy_0: 0.2753
	accuracy_policy_0: 0.66723
	loss_value_0: 0.32913
	loss_policy_1: 0.05551
	accuracy_policy_1: 0.66008
	loss_value_1: 0.06689
	loss_reward_1: 0.00666
	loss_policy_2: 0.0556
	accuracy_policy_2: 0.65887
	loss_value_2: 0.06869
	loss_reward_2: 0.00804
	loss_policy_3: 0.05585
	accuracy_policy_3: 0.65742
	loss_value_3: 0.06998
	loss_reward_3: 0.01108
	loss_policy_4: 0.05667
	accuracy_policy_4: 0.65051
	loss_value_4: 0.0718
	loss_reward_4: 0.01417
	loss_policy_5: 0.05679
	accuracy_policy_5: 0.65617
	loss_value_5: 0.07305
	loss_reward_5: 0.01531
	loss_policy: 0.55572
	loss_value: 0.67954
	loss_reward: 0.05526
[2025-05-08 02:48:21] nn step 55800, lr: 0.1.
	loss_policy_0: 0.27018
	accuracy_policy_0: 0.65684
	loss_value_0: 0.31326
	loss_policy_1: 0.05358
	accuracy_policy_1: 0.65566
	loss_value_1: 0.06421
	loss_reward_1: 0.00597
	loss_policy_2: 0.05374
	accuracy_policy_2: 0.6568
	loss_value_2: 0.0658
	loss_reward_2: 0.00774
	loss_policy_3: 0.05388
	accuracy_policy_3: 0.65766
	loss_value_3: 0.06714
	loss_reward_3: 0.0103
	loss_policy_4: 0.05456
	accuracy_policy_4: 0.65332
	loss_value_4: 0.06833
	loss_reward_4: 0.01314
	loss_policy_5: 0.05434
	accuracy_policy_5: 0.66
	loss_value_5: 0.06966
	loss_reward_5: 0.01448
	loss_policy: 0.54028
	loss_value: 0.6484
	loss_reward: 0.05163
Optimization_Done 55800
[2025-05-08 02:51:22] [command] train weight_iter_55800.pkl 261 280
[2025-05-08 02:51:31] nn step 55850, lr: 0.1.
	loss_policy_0: 0.26158
	accuracy_policy_0: 0.67004
	loss_value_0: 0.32391
	loss_policy_1: 0.053
	accuracy_policy_1: 0.65891
	loss_value_1: 0.06587
	loss_reward_1: 0.00614
	loss_policy_2: 0.05341
	accuracy_policy_2: 0.65027
	loss_value_2: 0.06753
	loss_reward_2: 0.00777
	loss_policy_3: 0.05366
	accuracy_policy_3: 0.6518
	loss_value_3: 0.06877
	loss_reward_3: 0.0104
	loss_policy_4: 0.05392
	accuracy_policy_4: 0.64672
	loss_value_4: 0.06997
	loss_reward_4: 0.01328
	loss_policy_5: 0.05387
	accuracy_policy_5: 0.65605
	loss_value_5: 0.07118
	loss_reward_5: 0.01463
	loss_policy: 0.52944
	loss_value: 0.66723
	loss_reward: 0.05221
[2025-05-08 02:51:39] nn step 55900, lr: 0.1.
	loss_policy_0: 0.26747
	accuracy_policy_0: 0.67637
	loss_value_0: 0.32164
	loss_policy_1: 0.05435
	accuracy_policy_1: 0.66035
	loss_value_1: 0.0658
	loss_reward_1: 0.0064
	loss_policy_2: 0.0548
	accuracy_policy_2: 0.65359
	loss_value_2: 0.06758
	loss_reward_2: 0.00801
	loss_policy_3: 0.05521
	accuracy_policy_3: 0.65492
	loss_value_3: 0.0691
	loss_reward_3: 0.0112
	loss_policy_4: 0.0557
	accuracy_policy_4: 0.65512
	loss_value_4: 0.07078
	loss_reward_4: 0.01365
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.65746
	loss_value_5: 0.07201
	loss_reward_5: 0.01491
	loss_policy: 0.54334
	loss_value: 0.6669
	loss_reward: 0.05418
[2025-05-08 02:51:46] nn step 55950, lr: 0.1.
	loss_policy_0: 0.26973
	accuracy_policy_0: 0.67504
	loss_value_0: 0.32099
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.65535
	loss_value_1: 0.06529
	loss_reward_1: 0.0062
	loss_policy_2: 0.05527
	accuracy_policy_2: 0.65734
	loss_value_2: 0.0671
	loss_reward_2: 0.00815
	loss_policy_3: 0.0554
	accuracy_policy_3: 0.6582
	loss_value_3: 0.06864
	loss_reward_3: 0.01092
	loss_policy_4: 0.05561
	accuracy_policy_4: 0.65168
	loss_value_4: 0.07038
	loss_reward_4: 0.01342
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.65262
	loss_value_5: 0.0718
	loss_reward_5: 0.01462
	loss_policy: 0.54641
	loss_value: 0.6642
	loss_reward: 0.05331
[2025-05-08 02:51:54] nn step 56000, lr: 0.1.
	loss_policy_0: 0.27253
	accuracy_policy_0: 0.67434
	loss_value_0: 0.33188
	loss_policy_1: 0.05541
	accuracy_policy_1: 0.65855
	loss_value_1: 0.06774
	loss_reward_1: 0.0065
	loss_policy_2: 0.05536
	accuracy_policy_2: 0.66242
	loss_value_2: 0.0699
	loss_reward_2: 0.00834
	loss_policy_3: 0.05641
	accuracy_policy_3: 0.65129
	loss_value_3: 0.07135
	loss_reward_3: 0.01105
	loss_policy_4: 0.05679
	accuracy_policy_4: 0.65098
	loss_value_4: 0.07265
	loss_reward_4: 0.01359
	loss_policy_5: 0.05699
	accuracy_policy_5: 0.6502
	loss_value_5: 0.0739
	loss_reward_5: 0.01526
	loss_policy: 0.55348
	loss_value: 0.68743
	loss_reward: 0.05473
Optimization_Done 56000
[2025-05-08 02:54:57] [command] train weight_iter_56000.pkl 262 281
[2025-05-08 02:55:04] nn step 56050, lr: 0.1.
	loss_policy_0: 0.25382
	accuracy_policy_0: 0.67207
	loss_value_0: 0.31904
	loss_policy_1: 0.05169
	accuracy_policy_1: 0.65566
	loss_value_1: 0.06474
	loss_reward_1: 0.0059
	loss_policy_2: 0.0523
	accuracy_policy_2: 0.65352
	loss_value_2: 0.06645
	loss_reward_2: 0.00793
	loss_policy_3: 0.05225
	accuracy_policy_3: 0.65492
	loss_value_3: 0.0678
	loss_reward_3: 0.00998
	loss_policy_4: 0.05292
	accuracy_policy_4: 0.64844
	loss_value_4: 0.06862
	loss_reward_4: 0.01274
	loss_policy_5: 0.05307
	accuracy_policy_5: 0.65262
	loss_value_5: 0.0699
	loss_reward_5: 0.01434
	loss_policy: 0.51606
	loss_value: 0.65655
	loss_reward: 0.05088
[2025-05-08 02:55:13] nn step 56100, lr: 0.1.
	loss_policy_0: 0.27066
	accuracy_policy_0: 0.67758
	loss_value_0: 0.33352
	loss_policy_1: 0.05504
	accuracy_policy_1: 0.65797
	loss_value_1: 0.06783
	loss_reward_1: 0.00651
	loss_policy_2: 0.05503
	accuracy_policy_2: 0.65957
	loss_value_2: 0.06953
	loss_reward_2: 0.00821
	loss_policy_3: 0.05589
	accuracy_policy_3: 0.65582
	loss_value_3: 0.07112
	loss_reward_3: 0.01096
	loss_policy_4: 0.05591
	accuracy_policy_4: 0.65801
	loss_value_4: 0.07261
	loss_reward_4: 0.01368
	loss_policy_5: 0.0565
	accuracy_policy_5: 0.65676
	loss_value_5: 0.07408
	loss_reward_5: 0.01478
	loss_policy: 0.54903
	loss_value: 0.68868
	loss_reward: 0.05415
[2025-05-08 02:55:21] nn step 56150, lr: 0.1.
	loss_policy_0: 0.27006
	accuracy_policy_0: 0.67168
	loss_value_0: 0.32789
	loss_policy_1: 0.05473
	accuracy_policy_1: 0.66008
	loss_value_1: 0.06693
	loss_reward_1: 0.00612
	loss_policy_2: 0.05505
	accuracy_policy_2: 0.65738
	loss_value_2: 0.06875
	loss_reward_2: 0.00808
	loss_policy_3: 0.05523
	accuracy_policy_3: 0.65703
	loss_value_3: 0.07012
	loss_reward_3: 0.01081
	loss_policy_4: 0.05586
	accuracy_policy_4: 0.65113
	loss_value_4: 0.07141
	loss_reward_4: 0.01368
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.6577
	loss_value_5: 0.07285
	loss_reward_5: 0.01523
	loss_policy: 0.54682
	loss_value: 0.67795
	loss_reward: 0.0539
[2025-05-08 02:55:28] nn step 56200, lr: 0.1.
	loss_policy_0: 0.2574
	accuracy_policy_0: 0.67074
	loss_value_0: 0.31159
	loss_policy_1: 0.05254
	accuracy_policy_1: 0.65312
	loss_value_1: 0.06346
	loss_reward_1: 0.00631
	loss_policy_2: 0.05283
	accuracy_policy_2: 0.6541
	loss_value_2: 0.06542
	loss_reward_2: 0.00791
	loss_policy_3: 0.05336
	accuracy_policy_3: 0.64828
	loss_value_3: 0.06666
	loss_reward_3: 0.0104
	loss_policy_4: 0.05376
	accuracy_policy_4: 0.64664
	loss_value_4: 0.06771
	loss_reward_4: 0.01292
	loss_policy_5: 0.05416
	accuracy_policy_5: 0.65254
	loss_value_5: 0.06889
	loss_reward_5: 0.01451
	loss_policy: 0.52406
	loss_value: 0.64374
	loss_reward: 0.05205
Optimization_Done 56200
[2025-05-08 02:58:39] [command] train weight_iter_56200.pkl 263 282
[2025-05-08 02:58:47] nn step 56250, lr: 0.1.
	loss_policy_0: 0.25557
	accuracy_policy_0: 0.66898
	loss_value_0: 0.31451
	loss_policy_1: 0.05181
	accuracy_policy_1: 0.65688
	loss_value_1: 0.06405
	loss_reward_1: 0.00585
	loss_policy_2: 0.05231
	accuracy_policy_2: 0.65469
	loss_value_2: 0.06555
	loss_reward_2: 0.00755
	loss_policy_3: 0.05286
	accuracy_policy_3: 0.64969
	loss_value_3: 0.06661
	loss_reward_3: 0.01004
	loss_policy_4: 0.05288
	accuracy_policy_4: 0.65113
	loss_value_4: 0.06803
	loss_reward_4: 0.01283
	loss_policy_5: 0.05307
	accuracy_policy_5: 0.65672
	loss_value_5: 0.06933
	loss_reward_5: 0.01432
	loss_policy: 0.51851
	loss_value: 0.64808
	loss_reward: 0.05058
[2025-05-08 02:58:55] nn step 56300, lr: 0.1.
	loss_policy_0: 0.25303
	accuracy_policy_0: 0.66957
	loss_value_0: 0.30535
	loss_policy_1: 0.05091
	accuracy_policy_1: 0.66043
	loss_value_1: 0.06236
	loss_reward_1: 0.00581
	loss_policy_2: 0.05137
	accuracy_policy_2: 0.65883
	loss_value_2: 0.06415
	loss_reward_2: 0.00758
	loss_policy_3: 0.052
	accuracy_policy_3: 0.65852
	loss_value_3: 0.06556
	loss_reward_3: 0.01004
	loss_policy_4: 0.052
	accuracy_policy_4: 0.65738
	loss_value_4: 0.06679
	loss_reward_4: 0.01284
	loss_policy_5: 0.05211
	accuracy_policy_5: 0.65559
	loss_value_5: 0.06799
	loss_reward_5: 0.01395
	loss_policy: 0.51143
	loss_value: 0.63222
	loss_reward: 0.05022
[2025-05-08 02:59:03] nn step 56350, lr: 0.1.
	loss_policy_0: 0.24558
	accuracy_policy_0: 0.68129
	loss_value_0: 0.29494
	loss_policy_1: 0.04983
	accuracy_policy_1: 0.66832
	loss_value_1: 0.06012
	loss_reward_1: 0.00574
	loss_policy_2: 0.05023
	accuracy_policy_2: 0.66
	loss_value_2: 0.06199
	loss_reward_2: 0.00714
	loss_policy_3: 0.05112
	accuracy_policy_3: 0.65691
	loss_value_3: 0.06328
	loss_reward_3: 0.00984
	loss_policy_4: 0.05118
	accuracy_policy_4: 0.65703
	loss_value_4: 0.06465
	loss_reward_4: 0.01297
	loss_policy_5: 0.05102
	accuracy_policy_5: 0.66309
	loss_value_5: 0.06559
	loss_reward_5: 0.01328
	loss_policy: 0.49894
	loss_value: 0.61058
	loss_reward: 0.04895
[2025-05-08 02:59:12] nn step 56400, lr: 0.1.
	loss_policy_0: 0.27517
	accuracy_policy_0: 0.67973
	loss_value_0: 0.33122
	loss_policy_1: 0.05546
	accuracy_policy_1: 0.65992
	loss_value_1: 0.06767
	loss_reward_1: 0.00653
	loss_policy_2: 0.05616
	accuracy_policy_2: 0.65746
	loss_value_2: 0.06956
	loss_reward_2: 0.00841
	loss_policy_3: 0.05651
	accuracy_policy_3: 0.65727
	loss_value_3: 0.07116
	loss_reward_3: 0.01095
	loss_policy_4: 0.05673
	accuracy_policy_4: 0.65152
	loss_value_4: 0.07248
	loss_reward_4: 0.01406
	loss_policy_5: 0.05679
	accuracy_policy_5: 0.66078
	loss_value_5: 0.07417
	loss_reward_5: 0.01508
	loss_policy: 0.55683
	loss_value: 0.68626
	loss_reward: 0.05502
Optimization_Done 56400
[2025-05-08 03:02:15] [command] train weight_iter_56400.pkl 264 283
[2025-05-08 03:02:25] nn step 56450, lr: 0.1.
	loss_policy_0: 0.24363
	accuracy_policy_0: 0.67891
	loss_value_0: 0.30624
	loss_policy_1: 0.0495
	accuracy_policy_1: 0.65223
	loss_value_1: 0.06232
	loss_reward_1: 0.00595
	loss_policy_2: 0.04988
	accuracy_policy_2: 0.65449
	loss_value_2: 0.06365
	loss_reward_2: 0.00732
	loss_policy_3: 0.05014
	accuracy_policy_3: 0.65344
	loss_value_3: 0.06484
	loss_reward_3: 0.0095
	loss_policy_4: 0.05072
	accuracy_policy_4: 0.65363
	loss_value_4: 0.06628
	loss_reward_4: 0.01236
	loss_policy_5: 0.05076
	accuracy_policy_5: 0.65996
	loss_value_5: 0.06738
	loss_reward_5: 0.01377
	loss_policy: 0.49463
	loss_value: 0.6307
	loss_reward: 0.04889
[2025-05-08 03:02:31] nn step 56500, lr: 0.1.
	loss_policy_0: 0.26654
	accuracy_policy_0: 0.6759
	loss_value_0: 0.32679
	loss_policy_1: 0.0545
	accuracy_policy_1: 0.65949
	loss_value_1: 0.06654
	loss_reward_1: 0.00643
	loss_policy_2: 0.05463
	accuracy_policy_2: 0.65855
	loss_value_2: 0.06843
	loss_reward_2: 0.00794
	loss_policy_3: 0.05537
	accuracy_policy_3: 0.65496
	loss_value_3: 0.06949
	loss_reward_3: 0.01039
	loss_policy_4: 0.05588
	accuracy_policy_4: 0.65098
	loss_value_4: 0.07125
	loss_reward_4: 0.01316
	loss_policy_5: 0.05577
	accuracy_policy_5: 0.66055
	loss_value_5: 0.07247
	loss_reward_5: 0.01467
	loss_policy: 0.54269
	loss_value: 0.67498
	loss_reward: 0.05259
[2025-05-08 03:02:40] nn step 56550, lr: 0.1.
	loss_policy_0: 0.27328
	accuracy_policy_0: 0.6777
	loss_value_0: 0.33212
	loss_policy_1: 0.05567
	accuracy_policy_1: 0.65176
	loss_value_1: 0.06755
	loss_reward_1: 0.00658
	loss_policy_2: 0.05543
	accuracy_policy_2: 0.65941
	loss_value_2: 0.06895
	loss_reward_2: 0.00847
	loss_policy_3: 0.0562
	accuracy_policy_3: 0.65426
	loss_value_3: 0.07055
	loss_reward_3: 0.01096
	loss_policy_4: 0.05642
	accuracy_policy_4: 0.65531
	loss_value_4: 0.07186
	loss_reward_4: 0.01369
	loss_policy_5: 0.05663
	accuracy_policy_5: 0.65715
	loss_value_5: 0.07315
	loss_reward_5: 0.01535
	loss_policy: 0.55362
	loss_value: 0.68418
	loss_reward: 0.05505
[2025-05-08 03:02:48] nn step 56600, lr: 0.1.
	loss_policy_0: 0.26566
	accuracy_policy_0: 0.67625
	loss_value_0: 0.32356
	loss_policy_1: 0.05386
	accuracy_policy_1: 0.65898
	loss_value_1: 0.06585
	loss_reward_1: 0.00655
	loss_policy_2: 0.05439
	accuracy_policy_2: 0.65684
	loss_value_2: 0.06715
	loss_reward_2: 0.00824
	loss_policy_3: 0.055
	accuracy_policy_3: 0.6543
	loss_value_3: 0.06877
	loss_reward_3: 0.01069
	loss_policy_4: 0.05521
	accuracy_policy_4: 0.65332
	loss_value_4: 0.07039
	loss_reward_4: 0.01382
	loss_policy_5: 0.05513
	accuracy_policy_5: 0.65945
	loss_value_5: 0.07194
	loss_reward_5: 0.01466
	loss_policy: 0.53924
	loss_value: 0.66765
	loss_reward: 0.05397
Optimization_Done 56600
[2025-05-08 03:05:51] [command] train weight_iter_56600.pkl 265 284
[2025-05-08 03:06:00] nn step 56650, lr: 0.1.
	loss_policy_0: 0.267
	accuracy_policy_0: 0.67465
	loss_value_0: 0.34274
	loss_policy_1: 0.05401
	accuracy_policy_1: 0.65746
	loss_value_1: 0.06915
	loss_reward_1: 0.00604
	loss_policy_2: 0.05433
	accuracy_policy_2: 0.65539
	loss_value_2: 0.07047
	loss_reward_2: 0.00771
	loss_policy_3: 0.05416
	accuracy_policy_3: 0.66098
	loss_value_3: 0.0716
	loss_reward_3: 0.01058
	loss_policy_4: 0.05467
	accuracy_policy_4: 0.66082
	loss_value_4: 0.07266
	loss_reward_4: 0.01375
	loss_policy_5: 0.05498
	accuracy_policy_5: 0.66332
	loss_value_5: 0.07385
	loss_reward_5: 0.01429
	loss_policy: 0.53916
	loss_value: 0.70048
	loss_reward: 0.05237
[2025-05-08 03:06:09] nn step 56700, lr: 0.1.
	loss_policy_0: 0.24968
	accuracy_policy_0: 0.67695
	loss_value_0: 0.3102
	loss_policy_1: 0.05065
	accuracy_policy_1: 0.66145
	loss_value_1: 0.06298
	loss_reward_1: 0.00574
	loss_policy_2: 0.05098
	accuracy_policy_2: 0.66246
	loss_value_2: 0.06468
	loss_reward_2: 0.00756
	loss_policy_3: 0.05125
	accuracy_policy_3: 0.65996
	loss_value_3: 0.0659
	loss_reward_3: 0.00964
	loss_policy_4: 0.05189
	accuracy_policy_4: 0.65875
	loss_value_4: 0.06736
	loss_reward_4: 0.01251
	loss_policy_5: 0.05222
	accuracy_policy_5: 0.66324
	loss_value_5: 0.06867
	loss_reward_5: 0.0139
	loss_policy: 0.50667
	loss_value: 0.63979
	loss_reward: 0.04935
[2025-05-08 03:06:17] nn step 56750, lr: 0.1.
	loss_policy_0: 0.25224
	accuracy_policy_0: 0.6807
	loss_value_0: 0.31199
	loss_policy_1: 0.05174
	accuracy_policy_1: 0.66668
	loss_value_1: 0.06364
	loss_reward_1: 0.00609
	loss_policy_2: 0.05178
	accuracy_policy_2: 0.66305
	loss_value_2: 0.06488
	loss_reward_2: 0.00783
	loss_policy_3: 0.05211
	accuracy_policy_3: 0.66379
	loss_value_3: 0.06646
	loss_reward_3: 0.01009
	loss_policy_4: 0.05268
	accuracy_policy_4: 0.66219
	loss_value_4: 0.06777
	loss_reward_4: 0.01239
	loss_policy_5: 0.05278
	accuracy_policy_5: 0.6659
	loss_value_5: 0.06864
	loss_reward_5: 0.01356
	loss_policy: 0.51333
	loss_value: 0.64338
	loss_reward: 0.04996
[2025-05-08 03:06:24] nn step 56800, lr: 0.1.
	loss_policy_0: 0.25216
	accuracy_policy_0: 0.67824
	loss_value_0: 0.30738
	loss_policy_1: 0.05129
	accuracy_policy_1: 0.65949
	loss_value_1: 0.06304
	loss_reward_1: 0.00649
	loss_policy_2: 0.0512
	accuracy_policy_2: 0.66602
	loss_value_2: 0.06442
	loss_reward_2: 0.00747
	loss_policy_3: 0.05136
	accuracy_policy_3: 0.66301
	loss_value_3: 0.06566
	loss_reward_3: 0.0098
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.65488
	loss_value_4: 0.06739
	loss_reward_4: 0.01279
	loss_policy_5: 0.05226
	accuracy_policy_5: 0.66211
	loss_value_5: 0.06857
	loss_reward_5: 0.01404
	loss_policy: 0.51065
	loss_value: 0.63646
	loss_reward: 0.0506
Optimization_Done 56800
[2025-05-08 03:09:26] [command] train weight_iter_56800.pkl 266 285
[2025-05-08 03:09:36] nn step 56850, lr: 0.1.
	loss_policy_0: 0.26001
	accuracy_policy_0: 0.67781
	loss_value_0: 0.32547
	loss_policy_1: 0.05287
	accuracy_policy_1: 0.66074
	loss_value_1: 0.06629
	loss_reward_1: 0.00625
	loss_policy_2: 0.05333
	accuracy_policy_2: 0.65773
	loss_value_2: 0.06778
	loss_reward_2: 0.00773
	loss_policy_3: 0.05379
	accuracy_policy_3: 0.6618
	loss_value_3: 0.0694
	loss_reward_3: 0.01044
	loss_policy_4: 0.05385
	accuracy_policy_4: 0.6607
	loss_value_4: 0.0704
	loss_reward_4: 0.01285
	loss_policy_5: 0.05435
	accuracy_policy_5: 0.66316
	loss_value_5: 0.07156
	loss_reward_5: 0.01411
	loss_policy: 0.52819
	loss_value: 0.67091
	loss_reward: 0.05138
[2025-05-08 03:09:42] nn step 56900, lr: 0.1.
	loss_policy_0: 0.25349
	accuracy_policy_0: 0.67926
	loss_value_0: 0.31588
	loss_policy_1: 0.05202
	accuracy_policy_1: 0.65438
	loss_value_1: 0.06459
	loss_reward_1: 0.00622
	loss_policy_2: 0.05175
	accuracy_policy_2: 0.65875
	loss_value_2: 0.06618
	loss_reward_2: 0.00769
	loss_policy_3: 0.052
	accuracy_policy_3: 0.66184
	loss_value_3: 0.06737
	loss_reward_3: 0.0102
	loss_policy_4: 0.0525
	accuracy_policy_4: 0.66016
	loss_value_4: 0.06885
	loss_reward_4: 0.01324
	loss_policy_5: 0.05302
	accuracy_policy_5: 0.65977
	loss_value_5: 0.0699
	loss_reward_5: 0.01397
	loss_policy: 0.51478
	loss_value: 0.65277
	loss_reward: 0.05132
[2025-05-08 03:09:50] nn step 56950, lr: 0.1.
	loss_policy_0: 0.26674
	accuracy_policy_0: 0.67773
	loss_value_0: 0.32721
	loss_policy_1: 0.05386
	accuracy_policy_1: 0.66629
	loss_value_1: 0.0669
	loss_reward_1: 0.00631
	loss_policy_2: 0.05453
	accuracy_policy_2: 0.66078
	loss_value_2: 0.06864
	loss_reward_2: 0.00776
	loss_policy_3: 0.0551
	accuracy_policy_3: 0.65797
	loss_value_3: 0.06991
	loss_reward_3: 0.01025
	loss_policy_4: 0.05539
	accuracy_policy_4: 0.65852
	loss_value_4: 0.07106
	loss_reward_4: 0.01357
	loss_policy_5: 0.0557
	accuracy_policy_5: 0.66188
	loss_value_5: 0.07273
	loss_reward_5: 0.01466
	loss_policy: 0.54131
	loss_value: 0.67645
	loss_reward: 0.05255
[2025-05-08 03:09:59] nn step 57000, lr: 0.1.
	loss_policy_0: 0.26992
	accuracy_policy_0: 0.67898
	loss_value_0: 0.33207
	loss_policy_1: 0.05487
	accuracy_policy_1: 0.66051
	loss_value_1: 0.06778
	loss_reward_1: 0.0065
	loss_policy_2: 0.05521
	accuracy_policy_2: 0.66176
	loss_value_2: 0.06913
	loss_reward_2: 0.00839
	loss_policy_3: 0.05599
	accuracy_policy_3: 0.65445
	loss_value_3: 0.07028
	loss_reward_3: 0.01078
	loss_policy_4: 0.05613
	accuracy_policy_4: 0.65465
	loss_value_4: 0.07168
	loss_reward_4: 0.01366
	loss_policy_5: 0.05608
	accuracy_policy_5: 0.66152
	loss_value_5: 0.07304
	loss_reward_5: 0.01495
	loss_policy: 0.5482
	loss_value: 0.68398
	loss_reward: 0.05427
Optimization_Done 57000
[2025-05-08 03:13:05] [command] train weight_iter_57000.pkl 267 286
[2025-05-08 03:13:15] nn step 57050, lr: 0.1.
	loss_policy_0: 0.25568
	accuracy_policy_0: 0.6893
	loss_value_0: 0.33666
	loss_policy_1: 0.05225
	accuracy_policy_1: 0.66871
	loss_value_1: 0.06781
	loss_reward_1: 0.00619
	loss_policy_2: 0.05299
	accuracy_policy_2: 0.66508
	loss_value_2: 0.06939
	loss_reward_2: 0.00762
	loss_policy_3: 0.05337
	accuracy_policy_3: 0.66301
	loss_value_3: 0.07048
	loss_reward_3: 0.01035
	loss_policy_4: 0.05362
	accuracy_policy_4: 0.66523
	loss_value_4: 0.07148
	loss_reward_4: 0.01264
	loss_policy_5: 0.05388
	accuracy_policy_5: 0.6677
	loss_value_5: 0.07267
	loss_reward_5: 0.01417
	loss_policy: 0.52178
	loss_value: 0.68848
	loss_reward: 0.05097
[2025-05-08 03:13:23] nn step 57100, lr: 0.1.
	loss_policy_0: 0.26243
	accuracy_policy_0: 0.68535
	loss_value_0: 0.33031
	loss_policy_1: 0.05354
	accuracy_policy_1: 0.66594
	loss_value_1: 0.06718
	loss_reward_1: 0.00618
	loss_policy_2: 0.05383
	accuracy_policy_2: 0.66777
	loss_value_2: 0.06873
	loss_reward_2: 0.00794
	loss_policy_3: 0.05426
	accuracy_policy_3: 0.66379
	loss_value_3: 0.07028
	loss_reward_3: 0.01033
	loss_policy_4: 0.0549
	accuracy_policy_4: 0.66168
	loss_value_4: 0.07163
	loss_reward_4: 0.01354
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.67199
	loss_value_5: 0.07276
	loss_reward_5: 0.01425
	loss_policy: 0.53359
	loss_value: 0.68088
	loss_reward: 0.05224
[2025-05-08 03:13:29] nn step 57150, lr: 0.1.
	loss_policy_0: 0.2571
	accuracy_policy_0: 0.68668
	loss_value_0: 0.31817
	loss_policy_1: 0.05253
	accuracy_policy_1: 0.66527
	loss_value_1: 0.06487
	loss_reward_1: 0.00631
	loss_policy_2: 0.05268
	accuracy_policy_2: 0.66379
	loss_value_2: 0.06656
	loss_reward_2: 0.00821
	loss_policy_3: 0.05306
	accuracy_policy_3: 0.66035
	loss_value_3: 0.06829
	loss_reward_3: 0.0106
	loss_policy_4: 0.05357
	accuracy_policy_4: 0.66121
	loss_value_4: 0.06938
	loss_reward_4: 0.01297
	loss_policy_5: 0.05366
	accuracy_policy_5: 0.66504
	loss_value_5: 0.07064
	loss_reward_5: 0.01447
	loss_policy: 0.5226
	loss_value: 0.65792
	loss_reward: 0.05257
[2025-05-08 03:13:38] nn step 57200, lr: 0.1.
	loss_policy_0: 0.26366
	accuracy_policy_0: 0.67988
	loss_value_0: 0.32602
	loss_policy_1: 0.05351
	accuracy_policy_1: 0.66453
	loss_value_1: 0.06621
	loss_reward_1: 0.00602
	loss_policy_2: 0.05371
	accuracy_policy_2: 0.6657
	loss_value_2: 0.06801
	loss_reward_2: 0.00794
	loss_policy_3: 0.05431
	accuracy_policy_3: 0.66258
	loss_value_3: 0.06927
	loss_reward_3: 0.01047
	loss_policy_4: 0.05487
	accuracy_policy_4: 0.66191
	loss_value_4: 0.07031
	loss_reward_4: 0.01332
	loss_policy_5: 0.05497
	accuracy_policy_5: 0.66262
	loss_value_5: 0.07176
	loss_reward_5: 0.01435
	loss_policy: 0.53503
	loss_value: 0.67158
	loss_reward: 0.0521
Optimization_Done 57200
[2025-05-08 03:16:41] [command] train weight_iter_57200.pkl 268 287
[2025-05-08 03:16:50] nn step 57250, lr: 0.1.
	loss_policy_0: 0.26176
	accuracy_policy_0: 0.67668
	loss_value_0: 0.33108
	loss_policy_1: 0.0535
	accuracy_policy_1: 0.65676
	loss_value_1: 0.06735
	loss_reward_1: 0.00638
	loss_policy_2: 0.05343
	accuracy_policy_2: 0.65965
	loss_value_2: 0.06859
	loss_reward_2: 0.00789
	loss_policy_3: 0.05413
	accuracy_policy_3: 0.65254
	loss_value_3: 0.06983
	loss_reward_3: 0.01034
	loss_policy_4: 0.05427
	accuracy_policy_4: 0.65227
	loss_value_4: 0.07069
	loss_reward_4: 0.01319
	loss_policy_5: 0.05411
	accuracy_policy_5: 0.65996
	loss_value_5: 0.07188
	loss_reward_5: 0.01424
	loss_policy: 0.5312
	loss_value: 0.67942
	loss_reward: 0.05204
[2025-05-08 03:16:59] nn step 57300, lr: 0.1.
	loss_policy_0: 0.26668
	accuracy_policy_0: 0.67023
	loss_value_0: 0.3286
	loss_policy_1: 0.05409
	accuracy_policy_1: 0.65879
	loss_value_1: 0.06666
	loss_reward_1: 0.00638
	loss_policy_2: 0.05434
	accuracy_policy_2: 0.65191
	loss_value_2: 0.06831
	loss_reward_2: 0.00806
	loss_policy_3: 0.05438
	accuracy_policy_3: 0.65824
	loss_value_3: 0.06938
	loss_reward_3: 0.01063
	loss_policy_4: 0.05464
	accuracy_policy_4: 0.65844
	loss_value_4: 0.0708
	loss_reward_4: 0.01299
	loss_policy_5: 0.05477
	accuracy_policy_5: 0.66309
	loss_value_5: 0.07203
	loss_reward_5: 0.01503
	loss_policy: 0.53889
	loss_value: 0.67579
	loss_reward: 0.05309
[2025-05-08 03:17:07] nn step 57350, lr: 0.1.
	loss_policy_0: 0.24426
	accuracy_policy_0: 0.6775
	loss_value_0: 0.30078
	loss_policy_1: 0.0497
	accuracy_policy_1: 0.66391
	loss_value_1: 0.06136
	loss_reward_1: 0.00568
	loss_policy_2: 0.04998
	accuracy_policy_2: 0.66254
	loss_value_2: 0.06251
	loss_reward_2: 0.00729
	loss_policy_3: 0.05032
	accuracy_policy_3: 0.66219
	loss_value_3: 0.06378
	loss_reward_3: 0.01004
	loss_policy_4: 0.05089
	accuracy_policy_4: 0.6552
	loss_value_4: 0.06505
	loss_reward_4: 0.01235
	loss_policy_5: 0.05086
	accuracy_policy_5: 0.65949
	loss_value_5: 0.06638
	loss_reward_5: 0.01371
	loss_policy: 0.49602
	loss_value: 0.61986
	loss_reward: 0.04906
[2025-05-08 03:17:14] nn step 57400, lr: 0.1.
	loss_policy_0: 0.27433
	accuracy_policy_0: 0.67539
	loss_value_0: 0.33294
	loss_policy_1: 0.0555
	accuracy_policy_1: 0.66074
	loss_value_1: 0.06781
	loss_reward_1: 0.00651
	loss_policy_2: 0.05584
	accuracy_policy_2: 0.66047
	loss_value_2: 0.06927
	loss_reward_2: 0.00845
	loss_policy_3: 0.05596
	accuracy_policy_3: 0.65566
	loss_value_3: 0.07046
	loss_reward_3: 0.0106
	loss_policy_4: 0.05672
	accuracy_policy_4: 0.65332
	loss_value_4: 0.07194
	loss_reward_4: 0.01374
	loss_policy_5: 0.05679
	accuracy_policy_5: 0.65645
	loss_value_5: 0.07342
	loss_reward_5: 0.01535
	loss_policy: 0.55513
	loss_value: 0.68583
	loss_reward: 0.05466
Optimization_Done 57400
[2025-05-08 03:20:18] [command] train weight_iter_57400.pkl 269 288
[2025-05-08 03:20:27] nn step 57450, lr: 0.1.
	loss_policy_0: 0.27179
	accuracy_policy_0: 0.67828
	loss_value_0: 0.34087
	loss_policy_1: 0.05543
	accuracy_policy_1: 0.6602
	loss_value_1: 0.06909
	loss_reward_1: 0.00636
	loss_policy_2: 0.05595
	accuracy_policy_2: 0.65277
	loss_value_2: 0.07088
	loss_reward_2: 0.00816
	loss_policy_3: 0.05644
	accuracy_policy_3: 0.6516
	loss_value_3: 0.07186
	loss_reward_3: 0.01067
	loss_policy_4: 0.0566
	accuracy_policy_4: 0.65641
	loss_value_4: 0.07303
	loss_reward_4: 0.01343
	loss_policy_5: 0.05653
	accuracy_policy_5: 0.65938
	loss_value_5: 0.07408
	loss_reward_5: 0.01471
	loss_policy: 0.55273
	loss_value: 0.6998
	loss_reward: 0.05333
[2025-05-08 03:20:34] nn step 57500, lr: 0.1.
	loss_policy_0: 0.25838
	accuracy_policy_0: 0.65219
	loss_value_0: 0.30658
	loss_policy_1: 0.05148
	accuracy_policy_1: 0.65234
	loss_value_1: 0.06226
	loss_reward_1: 0.00586
	loss_policy_2: 0.05127
	accuracy_policy_2: 0.65887
	loss_value_2: 0.0634
	loss_reward_2: 0.00749
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.65012
	loss_value_3: 0.06459
	loss_reward_3: 0.00965
	loss_policy_4: 0.05177
	accuracy_policy_4: 0.65348
	loss_value_4: 0.06596
	loss_reward_4: 0.01266
	loss_policy_5: 0.05193
	accuracy_policy_5: 0.65871
	loss_value_5: 0.06709
	loss_reward_5: 0.01351
	loss_policy: 0.51641
	loss_value: 0.62987
	loss_reward: 0.04918
[2025-05-08 03:20:42] nn step 57550, lr: 0.1.
	loss_policy_0: 0.26158
	accuracy_policy_0: 0.67578
	loss_value_0: 0.31466
	loss_policy_1: 0.05299
	accuracy_policy_1: 0.65812
	loss_value_1: 0.06399
	loss_reward_1: 0.00606
	loss_policy_2: 0.0534
	accuracy_policy_2: 0.65754
	loss_value_2: 0.066
	loss_reward_2: 0.00781
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.65828
	loss_value_3: 0.06742
	loss_reward_3: 0.01063
	loss_policy_4: 0.05381
	accuracy_policy_4: 0.65281
	loss_value_4: 0.06885
	loss_reward_4: 0.01323
	loss_policy_5: 0.05385
	accuracy_policy_5: 0.66066
	loss_value_5: 0.07029
	loss_reward_5: 0.01456
	loss_policy: 0.52905
	loss_value: 0.6512
	loss_reward: 0.05229
[2025-05-08 03:20:51] nn step 57600, lr: 0.1.
	loss_policy_0: 0.26879
	accuracy_policy_0: 0.67566
	loss_value_0: 0.32381
	loss_policy_1: 0.05438
	accuracy_policy_1: 0.65949
	loss_value_1: 0.06596
	loss_reward_1: 0.0064
	loss_policy_2: 0.05458
	accuracy_policy_2: 0.65801
	loss_value_2: 0.06767
	loss_reward_2: 0.00806
	loss_policy_3: 0.05501
	accuracy_policy_3: 0.65246
	loss_value_3: 0.06908
	loss_reward_3: 0.01048
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.65242
	loss_value_4: 0.07057
	loss_reward_4: 0.01361
	loss_policy_5: 0.05568
	accuracy_policy_5: 0.66051
	loss_value_5: 0.07178
	loss_reward_5: 0.01451
	loss_policy: 0.54407
	loss_value: 0.66888
	loss_reward: 0.05305
Optimization_Done 57600
[2025-05-08 03:23:55] [command] train weight_iter_57600.pkl 270 289
[2025-05-08 03:24:05] nn step 57650, lr: 0.1.
	loss_policy_0: 0.25966
	accuracy_policy_0: 0.67582
	loss_value_0: 0.33027
	loss_policy_1: 0.05325
	accuracy_policy_1: 0.65879
	loss_value_1: 0.06712
	loss_reward_1: 0.00605
	loss_policy_2: 0.0533
	accuracy_policy_2: 0.66078
	loss_value_2: 0.06827
	loss_reward_2: 0.00778
	loss_policy_3: 0.05341
	accuracy_policy_3: 0.66273
	loss_value_3: 0.06943
	loss_reward_3: 0.01046
	loss_policy_4: 0.05374
	accuracy_policy_4: 0.66422
	loss_value_4: 0.07065
	loss_reward_4: 0.01272
	loss_policy_5: 0.05389
	accuracy_policy_5: 0.66508
	loss_value_5: 0.07169
	loss_reward_5: 0.0138
	loss_policy: 0.52725
	loss_value: 0.67742
	loss_reward: 0.0508
[2025-05-08 03:24:13] nn step 57700, lr: 0.1.
	loss_policy_0: 0.27924
	accuracy_policy_0: 0.68148
	loss_value_0: 0.34948
	loss_policy_1: 0.05676
	accuracy_policy_1: 0.66215
	loss_value_1: 0.07106
	loss_reward_1: 0.00663
	loss_policy_2: 0.05671
	accuracy_policy_2: 0.66777
	loss_value_2: 0.0723
	loss_reward_2: 0.00834
	loss_policy_3: 0.05731
	accuracy_policy_3: 0.66684
	loss_value_3: 0.07382
	loss_reward_3: 0.01123
	loss_policy_4: 0.05782
	accuracy_policy_4: 0.66398
	loss_value_4: 0.07553
	loss_reward_4: 0.01402
	loss_policy_5: 0.05794
	accuracy_policy_5: 0.66633
	loss_value_5: 0.07663
	loss_reward_5: 0.01566
	loss_policy: 0.56577
	loss_value: 0.71881
	loss_reward: 0.05588
[2025-05-08 03:24:20] nn step 57750, lr: 0.1.
	loss_policy_0: 0.26895
	accuracy_policy_0: 0.68238
	loss_value_0: 0.33465
	loss_policy_1: 0.05495
	accuracy_policy_1: 0.66484
	loss_value_1: 0.06811
	loss_reward_1: 0.00655
	loss_policy_2: 0.05518
	accuracy_policy_2: 0.6634
	loss_value_2: 0.06944
	loss_reward_2: 0.00823
	loss_policy_3: 0.05557
	accuracy_policy_3: 0.65996
	loss_value_3: 0.07085
	loss_reward_3: 0.01064
	loss_policy_4: 0.05572
	accuracy_policy_4: 0.66926
	loss_value_4: 0.0721
	loss_reward_4: 0.01319
	loss_policy_5: 0.05573
	accuracy_policy_5: 0.66875
	loss_value_5: 0.07329
	loss_reward_5: 0.01488
	loss_policy: 0.54612
	loss_value: 0.68844
	loss_reward: 0.05348
[2025-05-08 03:24:28] nn step 57800, lr: 0.1.
	loss_policy_0: 0.26377
	accuracy_policy_0: 0.66547
	loss_value_0: 0.32047
	loss_policy_1: 0.0533
	accuracy_policy_1: 0.65707
	loss_value_1: 0.06531
	loss_reward_1: 0.00631
	loss_policy_2: 0.05344
	accuracy_policy_2: 0.65988
	loss_value_2: 0.06678
	loss_reward_2: 0.00778
	loss_policy_3: 0.05321
	accuracy_policy_3: 0.66258
	loss_value_3: 0.06809
	loss_reward_3: 0.01017
	loss_policy_4: 0.05357
	accuracy_policy_4: 0.66176
	loss_value_4: 0.06939
	loss_reward_4: 0.01293
	loss_policy_5: 0.05359
	accuracy_policy_5: 0.67055
	loss_value_5: 0.07029
	loss_reward_5: 0.01418
	loss_policy: 0.53089
	loss_value: 0.66033
	loss_reward: 0.05136
Optimization_Done 57800
[2025-05-08 03:27:37] [command] train weight_iter_57800.pkl 271 290
[2025-05-08 03:27:47] nn step 57850, lr: 0.1.
	loss_policy_0: 0.24518
	accuracy_policy_0: 0.68301
	loss_value_0: 0.309
	loss_policy_1: 0.05005
	accuracy_policy_1: 0.66594
	loss_value_1: 0.06257
	loss_reward_1: 0.0059
	loss_policy_2: 0.05019
	accuracy_policy_2: 0.6659
	loss_value_2: 0.06356
	loss_reward_2: 0.00718
	loss_policy_3: 0.05021
	accuracy_policy_3: 0.66246
	loss_value_3: 0.06463
	loss_reward_3: 0.00942
	loss_policy_4: 0.05065
	accuracy_policy_4: 0.66645
	loss_value_4: 0.06573
	loss_reward_4: 0.0118
	loss_policy_5: 0.05079
	accuracy_policy_5: 0.67336
	loss_value_5: 0.06689
	loss_reward_5: 0.01339
	loss_policy: 0.49707
	loss_value: 0.63237
	loss_reward: 0.04769
[2025-05-08 03:27:55] nn step 57900, lr: 0.1.
	loss_policy_0: 0.26627
	accuracy_policy_0: 0.67402
	loss_value_0: 0.32661
	loss_policy_1: 0.05342
	accuracy_policy_1: 0.66242
	loss_value_1: 0.06661
	loss_reward_1: 0.00616
	loss_policy_2: 0.05369
	accuracy_policy_2: 0.66934
	loss_value_2: 0.0681
	loss_reward_2: 0.00767
	loss_policy_3: 0.0542
	accuracy_policy_3: 0.66352
	loss_value_3: 0.0692
	loss_reward_3: 0.01014
	loss_policy_4: 0.05447
	accuracy_policy_4: 0.65641
	loss_value_4: 0.07041
	loss_reward_4: 0.01302
	loss_policy_5: 0.05435
	accuracy_policy_5: 0.67066
	loss_value_5: 0.07156
	loss_reward_5: 0.01415
	loss_policy: 0.53641
	loss_value: 0.67251
	loss_reward: 0.05115
[2025-05-08 03:28:02] nn step 57950, lr: 0.1.
	loss_policy_0: 0.25875
	accuracy_policy_0: 0.68008
	loss_value_0: 0.31758
	loss_policy_1: 0.05234
	accuracy_policy_1: 0.66051
	loss_value_1: 0.0645
	loss_reward_1: 0.00616
	loss_policy_2: 0.0526
	accuracy_policy_2: 0.66457
	loss_value_2: 0.0658
	loss_reward_2: 0.00775
	loss_policy_3: 0.05286
	accuracy_policy_3: 0.6616
	loss_value_3: 0.06702
	loss_reward_3: 0.01015
	loss_policy_4: 0.05314
	accuracy_policy_4: 0.66562
	loss_value_4: 0.06837
	loss_reward_4: 0.01249
	loss_policy_5: 0.05319
	accuracy_policy_5: 0.66441
	loss_value_5: 0.06939
	loss_reward_5: 0.01425
	loss_policy: 0.5229
	loss_value: 0.65267
	loss_reward: 0.0508
[2025-05-08 03:28:10] nn step 58000, lr: 0.1.
	loss_policy_0: 0.25159
	accuracy_policy_0: 0.68258
	loss_value_0: 0.3042
	loss_policy_1: 0.05148
	accuracy_policy_1: 0.66578
	loss_value_1: 0.06215
	loss_reward_1: 0.00604
	loss_policy_2: 0.05171
	accuracy_policy_2: 0.6616
	loss_value_2: 0.06358
	loss_reward_2: 0.00784
	loss_policy_3: 0.05162
	accuracy_policy_3: 0.66719
	loss_value_3: 0.06499
	loss_reward_3: 0.00979
	loss_policy_4: 0.05213
	accuracy_policy_4: 0.66621
	loss_value_4: 0.06648
	loss_reward_4: 0.01232
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.67152
	loss_value_5: 0.06754
	loss_reward_5: 0.01383
	loss_policy: 0.51099
	loss_value: 0.62893
	loss_reward: 0.04981
Optimization_Done 58000
[2025-05-08 03:31:12] [command] train weight_iter_58000.pkl 272 291
[2025-05-08 03:31:20] nn step 58050, lr: 0.1.
	loss_policy_0: 0.2559
	accuracy_policy_0: 0.67742
	loss_value_0: 0.31865
	loss_policy_1: 0.05199
	accuracy_policy_1: 0.65488
	loss_value_1: 0.06434
	loss_reward_1: 0.00598
	loss_policy_2: 0.05187
	accuracy_policy_2: 0.65672
	loss_value_2: 0.06565
	loss_reward_2: 0.00705
	loss_policy_3: 0.05234
	accuracy_policy_3: 0.65789
	loss_value_3: 0.06676
	loss_reward_3: 0.00998
	loss_policy_4: 0.05245
	accuracy_policy_4: 0.65785
	loss_value_4: 0.0682
	loss_reward_4: 0.01215
	loss_policy_5: 0.05262
	accuracy_policy_5: 0.66719
	loss_value_5: 0.06934
	loss_reward_5: 0.01316
	loss_policy: 0.51718
	loss_value: 0.65295
	loss_reward: 0.04832
[2025-05-08 03:31:28] nn step 58100, lr: 0.1.
	loss_policy_0: 0.26464
	accuracy_policy_0: 0.67898
	loss_value_0: 0.3267
	loss_policy_1: 0.05433
	accuracy_policy_1: 0.66184
	loss_value_1: 0.06609
	loss_reward_1: 0.00617
	loss_policy_2: 0.0544
	accuracy_policy_2: 0.66352
	loss_value_2: 0.06786
	loss_reward_2: 0.0079
	loss_policy_3: 0.05462
	accuracy_policy_3: 0.66297
	loss_value_3: 0.0687
	loss_reward_3: 0.01008
	loss_policy_4: 0.05508
	accuracy_policy_4: 0.66012
	loss_value_4: 0.07012
	loss_reward_4: 0.01267
	loss_policy_5: 0.05487
	accuracy_policy_5: 0.67156
	loss_value_5: 0.07163
	loss_reward_5: 0.01386
	loss_policy: 0.53793
	loss_value: 0.6711
	loss_reward: 0.05068
[2025-05-08 03:31:37] nn step 58150, lr: 0.1.
	loss_policy_0: 0.26894
	accuracy_policy_0: 0.68207
	loss_value_0: 0.32677
	loss_policy_1: 0.05501
	accuracy_policy_1: 0.65918
	loss_value_1: 0.06622
	loss_reward_1: 0.0063
	loss_policy_2: 0.05545
	accuracy_policy_2: 0.65941
	loss_value_2: 0.06789
	loss_reward_2: 0.00788
	loss_policy_3: 0.05561
	accuracy_policy_3: 0.66191
	loss_value_3: 0.06943
	loss_reward_3: 0.01034
	loss_policy_4: 0.05622
	accuracy_policy_4: 0.65543
	loss_value_4: 0.07052
	loss_reward_4: 0.01353
	loss_policy_5: 0.05599
	accuracy_policy_5: 0.66281
	loss_value_5: 0.07179
	loss_reward_5: 0.01424
	loss_policy: 0.54722
	loss_value: 0.67261
	loss_reward: 0.05229
[2025-05-08 03:31:45] nn step 58200, lr: 0.1.
	loss_policy_0: 0.26247
	accuracy_policy_0: 0.68055
	loss_value_0: 0.32267
	loss_policy_1: 0.05327
	accuracy_policy_1: 0.66609
	loss_value_1: 0.06566
	loss_reward_1: 0.00612
	loss_policy_2: 0.05397
	accuracy_policy_2: 0.6602
	loss_value_2: 0.06706
	loss_reward_2: 0.00812
	loss_policy_3: 0.05428
	accuracy_policy_3: 0.66281
	loss_value_3: 0.06841
	loss_reward_3: 0.01037
	loss_policy_4: 0.05454
	accuracy_policy_4: 0.66215
	loss_value_4: 0.06944
	loss_reward_4: 0.01273
	loss_policy_5: 0.0546
	accuracy_policy_5: 0.66836
	loss_value_5: 0.07045
	loss_reward_5: 0.0145
	loss_policy: 0.53312
	loss_value: 0.66368
	loss_reward: 0.05184
Optimization_Done 58200
[2025-05-08 03:34:54] [command] train weight_iter_58200.pkl 273 292
[2025-05-08 03:35:03] nn step 58250, lr: 0.1.
	loss_policy_0: 0.23571
	accuracy_policy_0: 0.68496
	loss_value_0: 0.29898
	loss_policy_1: 0.04817
	accuracy_policy_1: 0.67082
	loss_value_1: 0.06055
	loss_reward_1: 0.00583
	loss_policy_2: 0.04829
	accuracy_policy_2: 0.67031
	loss_value_2: 0.06193
	loss_reward_2: 0.007
	loss_policy_3: 0.04865
	accuracy_policy_3: 0.66977
	loss_value_3: 0.06328
	loss_reward_3: 0.00939
	loss_policy_4: 0.04907
	accuracy_policy_4: 0.66992
	loss_value_4: 0.06436
	loss_reward_4: 0.01191
	loss_policy_5: 0.04884
	accuracy_policy_5: 0.67535
	loss_value_5: 0.06556
	loss_reward_5: 0.01322
	loss_policy: 0.47872
	loss_value: 0.61465
	loss_reward: 0.04736
[2025-05-08 03:35:10] nn step 58300, lr: 0.1.
	loss_policy_0: 0.26848
	accuracy_policy_0: 0.68328
	loss_value_0: 0.33055
	loss_policy_1: 0.05477
	accuracy_policy_1: 0.65922
	loss_value_1: 0.06765
	loss_reward_1: 0.00634
	loss_policy_2: 0.05525
	accuracy_policy_2: 0.6659
	loss_value_2: 0.06894
	loss_reward_2: 0.0084
	loss_policy_3: 0.05555
	accuracy_policy_3: 0.66062
	loss_value_3: 0.07032
	loss_reward_3: 0.01044
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.66047
	loss_value_4: 0.07153
	loss_reward_4: 0.01292
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.67199
	loss_value_5: 0.07261
	loss_reward_5: 0.01448
	loss_policy: 0.54555
	loss_value: 0.68161
	loss_reward: 0.05258
[2025-05-08 03:35:18] nn step 58350, lr: 0.1.
	loss_policy_0: 0.25024
	accuracy_policy_0: 0.68219
	loss_value_0: 0.30583
	loss_policy_1: 0.0509
	accuracy_policy_1: 0.66332
	loss_value_1: 0.06202
	loss_reward_1: 0.00589
	loss_policy_2: 0.05126
	accuracy_policy_2: 0.66773
	loss_value_2: 0.06367
	loss_reward_2: 0.00739
	loss_policy_3: 0.05124
	accuracy_policy_3: 0.67117
	loss_value_3: 0.06467
	loss_reward_3: 0.00986
	loss_policy_4: 0.05171
	accuracy_policy_4: 0.65816
	loss_value_4: 0.06597
	loss_reward_4: 0.01233
	loss_policy_5: 0.05178
	accuracy_policy_5: 0.67027
	loss_value_5: 0.06705
	loss_reward_5: 0.01352
	loss_policy: 0.50712
	loss_value: 0.6292
	loss_reward: 0.04898
[2025-05-08 03:35:26] nn step 58400, lr: 0.1.
	loss_policy_0: 0.25339
	accuracy_policy_0: 0.6793
	loss_value_0: 0.30906
	loss_policy_1: 0.05146
	accuracy_policy_1: 0.66309
	loss_value_1: 0.0632
	loss_reward_1: 0.00632
	loss_policy_2: 0.0517
	accuracy_policy_2: 0.66883
	loss_value_2: 0.06459
	loss_reward_2: 0.00781
	loss_policy_3: 0.05213
	accuracy_policy_3: 0.66191
	loss_value_3: 0.06587
	loss_reward_3: 0.00998
	loss_policy_4: 0.05229
	accuracy_policy_4: 0.66102
	loss_value_4: 0.06695
	loss_reward_4: 0.01259
	loss_policy_5: 0.05225
	accuracy_policy_5: 0.6709
	loss_value_5: 0.06798
	loss_reward_5: 0.01402
	loss_policy: 0.51321
	loss_value: 0.63764
	loss_reward: 0.05071
Optimization_Done 58400
[2025-05-08 03:38:26] [command] train weight_iter_58400.pkl 274 293
[2025-05-08 03:38:36] nn step 58450, lr: 0.1.
	loss_policy_0: 0.2561
	accuracy_policy_0: 0.68105
	loss_value_0: 0.32919
	loss_policy_1: 0.05262
	accuracy_policy_1: 0.66375
	loss_value_1: 0.06639
	loss_reward_1: 0.00606
	loss_policy_2: 0.05255
	accuracy_policy_2: 0.66559
	loss_value_2: 0.06736
	loss_reward_2: 0.00768
	loss_policy_3: 0.05303
	accuracy_policy_3: 0.66324
	loss_value_3: 0.06865
	loss_reward_3: 0.00999
	loss_policy_4: 0.05351
	accuracy_policy_4: 0.65844
	loss_value_4: 0.06957
	loss_reward_4: 0.01288
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.66574
	loss_value_5: 0.07051
	loss_reward_5: 0.0138
	loss_policy: 0.52109
	loss_value: 0.67167
	loss_reward: 0.05041
[2025-05-08 03:38:44] nn step 58500, lr: 0.1.
	loss_policy_0: 0.24524
	accuracy_policy_0: 0.6877
	loss_value_0: 0.30301
	loss_policy_1: 0.05008
	accuracy_policy_1: 0.66266
	loss_value_1: 0.0614
	loss_reward_1: 0.00567
	loss_policy_2: 0.0502
	accuracy_policy_2: 0.66723
	loss_value_2: 0.06266
	loss_reward_2: 0.00734
	loss_policy_3: 0.05053
	accuracy_policy_3: 0.66594
	loss_value_3: 0.06388
	loss_reward_3: 0.00944
	loss_policy_4: 0.05142
	accuracy_policy_4: 0.6559
	loss_value_4: 0.06494
	loss_reward_4: 0.01212
	loss_policy_5: 0.05103
	accuracy_policy_5: 0.66836
	loss_value_5: 0.06623
	loss_reward_5: 0.01342
	loss_policy: 0.49849
	loss_value: 0.62213
	loss_reward: 0.04799
[2025-05-08 03:38:51] nn step 58550, lr: 0.1.
	loss_policy_0: 0.28005
	accuracy_policy_0: 0.67316
	loss_value_0: 0.33627
	loss_policy_1: 0.05614
	accuracy_policy_1: 0.66312
	loss_value_1: 0.06888
	loss_reward_1: 0.00656
	loss_policy_2: 0.05638
	accuracy_policy_2: 0.6652
	loss_value_2: 0.07041
	loss_reward_2: 0.00836
	loss_policy_3: 0.05645
	accuracy_policy_3: 0.6677
	loss_value_3: 0.0718
	loss_reward_3: 0.01089
	loss_policy_4: 0.05691
	accuracy_policy_4: 0.66332
	loss_value_4: 0.07278
	loss_reward_4: 0.01401
	loss_policy_5: 0.05707
	accuracy_policy_5: 0.67133
	loss_value_5: 0.07388
	loss_reward_5: 0.01472
	loss_policy: 0.563
	loss_value: 0.69402
	loss_reward: 0.05455
[2025-05-08 03:38:59] nn step 58600, lr: 0.1.
	loss_policy_0: 0.26135
	accuracy_policy_0: 0.68598
	loss_value_0: 0.3172
	loss_policy_1: 0.05365
	accuracy_policy_1: 0.6623
	loss_value_1: 0.06445
	loss_reward_1: 0.00601
	loss_policy_2: 0.0538
	accuracy_policy_2: 0.66172
	loss_value_2: 0.06614
	loss_reward_2: 0.00809
	loss_policy_3: 0.05386
	accuracy_policy_3: 0.66652
	loss_value_3: 0.06771
	loss_reward_3: 0.01047
	loss_policy_4: 0.05466
	accuracy_policy_4: 0.65914
	loss_value_4: 0.06897
	loss_reward_4: 0.01278
	loss_policy_5: 0.05429
	accuracy_policy_5: 0.6677
	loss_value_5: 0.07017
	loss_reward_5: 0.01456
	loss_policy: 0.53162
	loss_value: 0.65464
	loss_reward: 0.05192
Optimization_Done 58600
[2025-05-08 03:42:08] [command] train weight_iter_58600.pkl 275 294
[2025-05-08 03:42:18] nn step 58650, lr: 0.1.
	loss_policy_0: 0.25537
	accuracy_policy_0: 0.67238
	loss_value_0: 0.31368
	loss_policy_1: 0.05148
	accuracy_policy_1: 0.65996
	loss_value_1: 0.06374
	loss_reward_1: 0.00591
	loss_policy_2: 0.05189
	accuracy_policy_2: 0.66363
	loss_value_2: 0.06493
	loss_reward_2: 0.00756
	loss_policy_3: 0.0521
	accuracy_policy_3: 0.66004
	loss_value_3: 0.06621
	loss_reward_3: 0.01003
	loss_policy_4: 0.05272
	accuracy_policy_4: 0.65793
	loss_value_4: 0.06713
	loss_reward_4: 0.01245
	loss_policy_5: 0.05271
	accuracy_policy_5: 0.66062
	loss_value_5: 0.06823
	loss_reward_5: 0.01354
	loss_policy: 0.51628
	loss_value: 0.64392
	loss_reward: 0.04949
[2025-05-08 03:42:26] nn step 58700, lr: 0.1.
	loss_policy_0: 0.25135
	accuracy_policy_0: 0.6773
	loss_value_0: 0.3035
	loss_policy_1: 0.05132
	accuracy_policy_1: 0.65602
	loss_value_1: 0.0618
	loss_reward_1: 0.00584
	loss_policy_2: 0.05109
	accuracy_policy_2: 0.66332
	loss_value_2: 0.06311
	loss_reward_2: 0.00724
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.66113
	loss_value_3: 0.06443
	loss_reward_3: 0.00967
	loss_policy_4: 0.05192
	accuracy_policy_4: 0.65961
	loss_value_4: 0.06561
	loss_reward_4: 0.01245
	loss_policy_5: 0.052
	accuracy_policy_5: 0.6623
	loss_value_5: 0.06671
	loss_reward_5: 0.01355
	loss_policy: 0.50927
	loss_value: 0.62515
	loss_reward: 0.04876
[2025-05-08 03:42:35] nn step 58750, lr: 0.1.
	loss_policy_0: 0.25411
	accuracy_policy_0: 0.68016
	loss_value_0: 0.30469
	loss_policy_1: 0.05183
	accuracy_policy_1: 0.66355
	loss_value_1: 0.0618
	loss_reward_1: 0.0058
	loss_policy_2: 0.05197
	accuracy_policy_2: 0.66105
	loss_value_2: 0.06313
	loss_reward_2: 0.00778
	loss_policy_3: 0.05242
	accuracy_policy_3: 0.65781
	loss_value_3: 0.06461
	loss_reward_3: 0.00963
	loss_policy_4: 0.05308
	accuracy_policy_4: 0.65922
	loss_value_4: 0.06596
	loss_reward_4: 0.01225
	loss_policy_5: 0.0534
	accuracy_policy_5: 0.6616
	loss_value_5: 0.0672
	loss_reward_5: 0.01341
	loss_policy: 0.5168
	loss_value: 0.62739
	loss_reward: 0.04888
[2025-05-08 03:42:42] nn step 58800, lr: 0.1.
	loss_policy_0: 0.24028
	accuracy_policy_0: 0.68336
	loss_value_0: 0.28513
	loss_policy_1: 0.04904
	accuracy_policy_1: 0.66543
	loss_value_1: 0.05844
	loss_reward_1: 0.00546
	loss_policy_2: 0.04963
	accuracy_policy_2: 0.66
	loss_value_2: 0.05966
	loss_reward_2: 0.0071
	loss_policy_3: 0.04991
	accuracy_policy_3: 0.66031
	loss_value_3: 0.06099
	loss_reward_3: 0.00978
	loss_policy_4: 0.04994
	accuracy_policy_4: 0.65918
	loss_value_4: 0.06215
	loss_reward_4: 0.01214
	loss_policy_5: 0.04963
	accuracy_policy_5: 0.66648
	loss_value_5: 0.06333
	loss_reward_5: 0.01299
	loss_policy: 0.48843
	loss_value: 0.5897
	loss_reward: 0.04747
Optimization_Done 58800
[2025-05-08 03:45:42] [command] train weight_iter_58800.pkl 276 295
[2025-05-08 03:45:52] nn step 58850, lr: 0.1.
	loss_policy_0: 0.26157
	accuracy_policy_0: 0.6777
	loss_value_0: 0.31755
	loss_policy_1: 0.0527
	accuracy_policy_1: 0.66324
	loss_value_1: 0.06438
	loss_reward_1: 0.00603
	loss_policy_2: 0.05307
	accuracy_policy_2: 0.66543
	loss_value_2: 0.06589
	loss_reward_2: 0.00741
	loss_policy_3: 0.05384
	accuracy_policy_3: 0.65688
	loss_value_3: 0.06717
	loss_reward_3: 0.00988
	loss_policy_4: 0.05404
	accuracy_policy_4: 0.65793
	loss_value_4: 0.06843
	loss_reward_4: 0.0127
	loss_policy_5: 0.05424
	accuracy_policy_5: 0.65887
	loss_value_5: 0.06928
	loss_reward_5: 0.01372
	loss_policy: 0.52946
	loss_value: 0.65268
	loss_reward: 0.04974
[2025-05-08 03:45:59] nn step 58900, lr: 0.1.
	loss_policy_0: 0.266
	accuracy_policy_0: 0.67668
	loss_value_0: 0.32231
	loss_policy_1: 0.05454
	accuracy_policy_1: 0.66012
	loss_value_1: 0.06565
	loss_reward_1: 0.0061
	loss_policy_2: 0.05493
	accuracy_policy_2: 0.66191
	loss_value_2: 0.06738
	loss_reward_2: 0.00783
	loss_policy_3: 0.05477
	accuracy_policy_3: 0.66102
	loss_value_3: 0.06861
	loss_reward_3: 0.01031
	loss_policy_4: 0.05575
	accuracy_policy_4: 0.65883
	loss_value_4: 0.07006
	loss_reward_4: 0.01345
	loss_policy_5: 0.05547
	accuracy_policy_5: 0.6675
	loss_value_5: 0.07128
	loss_reward_5: 0.01459
	loss_policy: 0.54148
	loss_value: 0.66529
	loss_reward: 0.05228
[2025-05-08 03:46:07] nn step 58950, lr: 0.1.
	loss_policy_0: 0.27168
	accuracy_policy_0: 0.68242
	loss_value_0: 0.32783
	loss_policy_1: 0.05545
	accuracy_policy_1: 0.66051
	loss_value_1: 0.06656
	loss_reward_1: 0.00657
	loss_policy_2: 0.05609
	accuracy_policy_2: 0.65738
	loss_value_2: 0.06807
	loss_reward_2: 0.00812
	loss_policy_3: 0.05602
	accuracy_policy_3: 0.65707
	loss_value_3: 0.06921
	loss_reward_3: 0.01097
	loss_policy_4: 0.05636
	accuracy_policy_4: 0.66023
	loss_value_4: 0.07081
	loss_reward_4: 0.01364
	loss_policy_5: 0.05623
	accuracy_policy_5: 0.66898
	loss_value_5: 0.07222
	loss_reward_5: 0.01511
	loss_policy: 0.55182
	loss_value: 0.6747
	loss_reward: 0.05441
[2025-05-08 03:46:15] nn step 59000, lr: 0.1.
	loss_policy_0: 0.26644
	accuracy_policy_0: 0.67773
	loss_value_0: 0.31779
	loss_policy_1: 0.05433
	accuracy_policy_1: 0.6609
	loss_value_1: 0.0647
	loss_reward_1: 0.0064
	loss_policy_2: 0.0543
	accuracy_policy_2: 0.66043
	loss_value_2: 0.06652
	loss_reward_2: 0.00818
	loss_policy_3: 0.05509
	accuracy_policy_3: 0.65785
	loss_value_3: 0.06788
	loss_reward_3: 0.01034
	loss_policy_4: 0.05556
	accuracy_policy_4: 0.6552
	loss_value_4: 0.06895
	loss_reward_4: 0.01316
	loss_policy_5: 0.05533
	accuracy_policy_5: 0.66387
	loss_value_5: 0.07037
	loss_reward_5: 0.015
	loss_policy: 0.54104
	loss_value: 0.65621
	loss_reward: 0.05308
Optimization_Done 59000
[2025-05-08 03:49:20] [command] train weight_iter_59000.pkl 277 296
[2025-05-08 03:49:30] nn step 59050, lr: 0.1.
	loss_policy_0: 0.26415
	accuracy_policy_0: 0.66832
	loss_value_0: 0.32452
	loss_policy_1: 0.05307
	accuracy_policy_1: 0.65977
	loss_value_1: 0.06579
	loss_reward_1: 0.00613
	loss_policy_2: 0.05361
	accuracy_policy_2: 0.65641
	loss_value_2: 0.06719
	loss_reward_2: 0.0078
	loss_policy_3: 0.05394
	accuracy_policy_3: 0.65379
	loss_value_3: 0.06858
	loss_reward_3: 0.0103
	loss_policy_4: 0.05424
	accuracy_policy_4: 0.65668
	loss_value_4: 0.06949
	loss_reward_4: 0.01274
	loss_policy_5: 0.05421
	accuracy_policy_5: 0.6559
	loss_value_5: 0.07047
	loss_reward_5: 0.01407
	loss_policy: 0.53321
	loss_value: 0.66604
	loss_reward: 0.05104
[2025-05-08 03:49:38] nn step 59100, lr: 0.1.
	loss_policy_0: 0.26164
	accuracy_policy_0: 0.66777
	loss_value_0: 0.31216
	loss_policy_1: 0.05281
	accuracy_policy_1: 0.65645
	loss_value_1: 0.06337
	loss_reward_1: 0.00612
	loss_policy_2: 0.05287
	accuracy_policy_2: 0.65973
	loss_value_2: 0.06478
	loss_reward_2: 0.00771
	loss_policy_3: 0.05362
	accuracy_policy_3: 0.6575
	loss_value_3: 0.06599
	loss_reward_3: 0.01
	loss_policy_4: 0.05399
	accuracy_policy_4: 0.64699
	loss_value_4: 0.06705
	loss_reward_4: 0.01283
	loss_policy_5: 0.05396
	accuracy_policy_5: 0.66004
	loss_value_5: 0.06831
	loss_reward_5: 0.01409
	loss_policy: 0.52889
	loss_value: 0.64166
	loss_reward: 0.05074
[2025-05-08 03:49:45] nn step 59150, lr: 0.1.
	loss_policy_0: 0.26084
	accuracy_policy_0: 0.67625
	loss_value_0: 0.31251
	loss_policy_1: 0.05289
	accuracy_policy_1: 0.66129
	loss_value_1: 0.06367
	loss_reward_1: 0.00611
	loss_policy_2: 0.05338
	accuracy_policy_2: 0.65828
	loss_value_2: 0.06535
	loss_reward_2: 0.00791
	loss_policy_3: 0.05363
	accuracy_policy_3: 0.65883
	loss_value_3: 0.06623
	loss_reward_3: 0.01023
	loss_policy_4: 0.05422
	accuracy_policy_4: 0.65238
	loss_value_4: 0.06727
	loss_reward_4: 0.01279
	loss_policy_5: 0.05409
	accuracy_policy_5: 0.66191
	loss_value_5: 0.06845
	loss_reward_5: 0.01446
	loss_policy: 0.52906
	loss_value: 0.64347
	loss_reward: 0.05149
[2025-05-08 03:49:53] nn step 59200, lr: 0.1.
	loss_policy_0: 0.2568
	accuracy_policy_0: 0.67895
	loss_value_0: 0.30823
	loss_policy_1: 0.05244
	accuracy_policy_1: 0.65961
	loss_value_1: 0.06292
	loss_reward_1: 0.00634
	loss_policy_2: 0.05264
	accuracy_policy_2: 0.66434
	loss_value_2: 0.06424
	loss_reward_2: 0.00774
	loss_policy_3: 0.05282
	accuracy_policy_3: 0.66008
	loss_value_3: 0.06558
	loss_reward_3: 0.00978
	loss_policy_4: 0.05322
	accuracy_policy_4: 0.65586
	loss_value_4: 0.06707
	loss_reward_4: 0.01284
	loss_policy_5: 0.05337
	accuracy_policy_5: 0.66195
	loss_value_5: 0.06812
	loss_reward_5: 0.01411
	loss_policy: 0.52129
	loss_value: 0.63616
	loss_reward: 0.05081
Optimization_Done 59200
[2025-05-08 03:53:00] [command] train weight_iter_59200.pkl 278 297
[2025-05-08 03:53:08] nn step 59250, lr: 0.1.
	loss_policy_0: 0.26516
	accuracy_policy_0: 0.67984
	loss_value_0: 0.33406
	loss_policy_1: 0.05447
	accuracy_policy_1: 0.66445
	loss_value_1: 0.06782
	loss_reward_1: 0.00643
	loss_policy_2: 0.05476
	accuracy_policy_2: 0.65707
	loss_value_2: 0.06919
	loss_reward_2: 0.00777
	loss_policy_3: 0.05505
	accuracy_policy_3: 0.65641
	loss_value_3: 0.07065
	loss_reward_3: 0.0109
	loss_policy_4: 0.05509
	accuracy_policy_4: 0.6557
	loss_value_4: 0.07138
	loss_reward_4: 0.01307
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.6677
	loss_value_5: 0.07242
	loss_reward_5: 0.01455
	loss_policy: 0.53914
	loss_value: 0.68552
	loss_reward: 0.05271
[2025-05-08 03:53:17] nn step 59300, lr: 0.1.
	loss_policy_0: 0.26193
	accuracy_policy_0: 0.68289
	loss_value_0: 0.32061
	loss_policy_1: 0.05324
	accuracy_policy_1: 0.66562
	loss_value_1: 0.06509
	loss_reward_1: 0.00621
	loss_policy_2: 0.05348
	accuracy_policy_2: 0.67254
	loss_value_2: 0.06681
	loss_reward_2: 0.00779
	loss_policy_3: 0.05417
	accuracy_policy_3: 0.66477
	loss_value_3: 0.0682
	loss_reward_3: 0.01038
	loss_policy_4: 0.05445
	accuracy_policy_4: 0.65895
	loss_value_4: 0.06951
	loss_reward_4: 0.01318
	loss_policy_5: 0.05456
	accuracy_policy_5: 0.66523
	loss_value_5: 0.07062
	loss_reward_5: 0.01387
	loss_policy: 0.53183
	loss_value: 0.66083
	loss_reward: 0.05142
[2025-05-08 03:53:25] nn step 59350, lr: 0.1.
	loss_policy_0: 0.25918
	accuracy_policy_0: 0.68605
	loss_value_0: 0.32154
	loss_policy_1: 0.05353
	accuracy_policy_1: 0.66152
	loss_value_1: 0.06553
	loss_reward_1: 0.00625
	loss_policy_2: 0.05393
	accuracy_policy_2: 0.66383
	loss_value_2: 0.06691
	loss_reward_2: 0.0079
	loss_policy_3: 0.05423
	accuracy_policy_3: 0.6627
	loss_value_3: 0.06846
	loss_reward_3: 0.01019
	loss_policy_4: 0.05477
	accuracy_policy_4: 0.65887
	loss_value_4: 0.06995
	loss_reward_4: 0.01314
	loss_policy_5: 0.0548
	accuracy_policy_5: 0.66422
	loss_value_5: 0.07137
	loss_reward_5: 0.01457
	loss_policy: 0.53044
	loss_value: 0.66375
	loss_reward: 0.05206
[2025-05-08 03:53:32] nn step 59400, lr: 0.1.
	loss_policy_0: 0.24182
	accuracy_policy_0: 0.68148
	loss_value_0: 0.29207
	loss_policy_1: 0.04902
	accuracy_policy_1: 0.67094
	loss_value_1: 0.05935
	loss_reward_1: 0.00591
	loss_policy_2: 0.04965
	accuracy_policy_2: 0.66898
	loss_value_2: 0.0608
	loss_reward_2: 0.00713
	loss_policy_3: 0.05014
	accuracy_policy_3: 0.66035
	loss_value_3: 0.06203
	loss_reward_3: 0.00967
	loss_policy_4: 0.05074
	accuracy_policy_4: 0.66164
	loss_value_4: 0.06343
	loss_reward_4: 0.01217
	loss_policy_5: 0.05043
	accuracy_policy_5: 0.66957
	loss_value_5: 0.06458
	loss_reward_5: 0.01306
	loss_policy: 0.49181
	loss_value: 0.60226
	loss_reward: 0.04794
Optimization_Done 59400
[2025-05-08 03:56:36] [command] train weight_iter_59400.pkl 279 298
[2025-05-08 03:56:46] nn step 59450, lr: 0.1.
	loss_policy_0: 0.24422
	accuracy_policy_0: 0.68621
	loss_value_0: 0.3051
	loss_policy_1: 0.04956
	accuracy_policy_1: 0.66461
	loss_value_1: 0.06176
	loss_reward_1: 0.00589
	loss_policy_2: 0.04998
	accuracy_policy_2: 0.66984
	loss_value_2: 0.06293
	loss_reward_2: 0.00747
	loss_policy_3: 0.05051
	accuracy_policy_3: 0.66066
	loss_value_3: 0.06449
	loss_reward_3: 0.00967
	loss_policy_4: 0.05084
	accuracy_policy_4: 0.66102
	loss_value_4: 0.06565
	loss_reward_4: 0.01212
	loss_policy_5: 0.05094
	accuracy_policy_5: 0.65801
	loss_value_5: 0.06695
	loss_reward_5: 0.01359
	loss_policy: 0.49604
	loss_value: 0.62688
	loss_reward: 0.04874
[2025-05-08 03:56:52] nn step 59500, lr: 0.1.
	loss_policy_0: 0.25372
	accuracy_policy_0: 0.68348
	loss_value_0: 0.31001
	loss_policy_1: 0.05171
	accuracy_policy_1: 0.66027
	loss_value_1: 0.06304
	loss_reward_1: 0.0059
	loss_policy_2: 0.05193
	accuracy_policy_2: 0.66332
	loss_value_2: 0.06454
	loss_reward_2: 0.00738
	loss_policy_3: 0.05217
	accuracy_policy_3: 0.66039
	loss_value_3: 0.06585
	loss_reward_3: 0.0099
	loss_policy_4: 0.05275
	accuracy_policy_4: 0.65988
	loss_value_4: 0.06727
	loss_reward_4: 0.01249
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.66082
	loss_value_5: 0.06826
	loss_reward_5: 0.01378
	loss_policy: 0.51538
	loss_value: 0.63897
	loss_reward: 0.04946
[2025-05-08 03:57:01] nn step 59550, lr: 0.1.
	loss_policy_0: 0.26896
	accuracy_policy_0: 0.67945
	loss_value_0: 0.32327
	loss_policy_1: 0.05457
	accuracy_policy_1: 0.66301
	loss_value_1: 0.06598
	loss_reward_1: 0.00648
	loss_policy_2: 0.05473
	accuracy_policy_2: 0.66734
	loss_value_2: 0.06734
	loss_reward_2: 0.00814
	loss_policy_3: 0.05542
	accuracy_policy_3: 0.66617
	loss_value_3: 0.0688
	loss_reward_3: 0.01054
	loss_policy_4: 0.0558
	accuracy_policy_4: 0.66059
	loss_value_4: 0.07014
	loss_reward_4: 0.01328
	loss_policy_5: 0.05596
	accuracy_policy_5: 0.66449
	loss_value_5: 0.07139
	loss_reward_5: 0.01485
	loss_policy: 0.54543
	loss_value: 0.66691
	loss_reward: 0.0533
[2025-05-08 03:57:09] nn step 59600, lr: 0.1.
	loss_policy_0: 0.25161
	accuracy_policy_0: 0.68402
	loss_value_0: 0.30385
	loss_policy_1: 0.05129
	accuracy_policy_1: 0.66855
	loss_value_1: 0.06159
	loss_reward_1: 0.00579
	loss_policy_2: 0.05141
	accuracy_policy_2: 0.66672
	loss_value_2: 0.06318
	loss_reward_2: 0.0076
	loss_policy_3: 0.052
	accuracy_policy_3: 0.66641
	loss_value_3: 0.06451
	loss_reward_3: 0.00992
	loss_policy_4: 0.05262
	accuracy_policy_4: 0.65758
	loss_value_4: 0.06573
	loss_reward_4: 0.01244
	loss_policy_5: 0.05227
	accuracy_policy_5: 0.6691
	loss_value_5: 0.06716
	loss_reward_5: 0.01379
	loss_policy: 0.51119
	loss_value: 0.62604
	loss_reward: 0.04953
Optimization_Done 59600
[2025-05-08 04:00:19] [command] train weight_iter_59600.pkl 280 299
[2025-05-08 04:00:28] nn step 59650, lr: 0.1.
	loss_policy_0: 0.25263
	accuracy_policy_0: 0.68895
	loss_value_0: 0.32693
	loss_policy_1: 0.05182
	accuracy_policy_1: 0.6691
	loss_value_1: 0.06609
	loss_reward_1: 0.00596
	loss_policy_2: 0.0523
	accuracy_policy_2: 0.66727
	loss_value_2: 0.06709
	loss_reward_2: 0.00747
	loss_policy_3: 0.05241
	accuracy_policy_3: 0.66508
	loss_value_3: 0.06855
	loss_reward_3: 0.00961
	loss_policy_4: 0.05276
	accuracy_policy_4: 0.66684
	loss_value_4: 0.06975
	loss_reward_4: 0.0123
	loss_policy_5: 0.05321
	accuracy_policy_5: 0.66762
	loss_value_5: 0.07087
	loss_reward_5: 0.01369
	loss_policy: 0.51515
	loss_value: 0.66928
	loss_reward: 0.04903
[2025-05-08 04:00:35] nn step 59700, lr: 0.1.
	loss_policy_0: 0.25613
	accuracy_policy_0: 0.69617
	loss_value_0: 0.32024
	loss_policy_1: 0.05281
	accuracy_policy_1: 0.67176
	loss_value_1: 0.06486
	loss_reward_1: 0.00623
	loss_policy_2: 0.05324
	accuracy_policy_2: 0.66457
	loss_value_2: 0.0662
	loss_reward_2: 0.00755
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.66922
	loss_value_3: 0.06758
	loss_reward_3: 0.01012
	loss_policy_4: 0.05436
	accuracy_policy_4: 0.65848
	loss_value_4: 0.06906
	loss_reward_4: 0.01295
	loss_policy_5: 0.05418
	accuracy_policy_5: 0.66523
	loss_value_5: 0.07019
	loss_reward_5: 0.01427
	loss_policy: 0.52413
	loss_value: 0.65812
	loss_reward: 0.05112
[2025-05-08 04:00:43] nn step 59750, lr: 0.1.
	loss_policy_0: 0.25587
	accuracy_policy_0: 0.68449
	loss_value_0: 0.30777
	loss_policy_1: 0.05199
	accuracy_policy_1: 0.67031
	loss_value_1: 0.06284
	loss_reward_1: 0.00593
	loss_policy_2: 0.05198
	accuracy_policy_2: 0.67434
	loss_value_2: 0.06417
	loss_reward_2: 0.00754
	loss_policy_3: 0.05263
	accuracy_policy_3: 0.66277
	loss_value_3: 0.06552
	loss_reward_3: 0.00992
	loss_policy_4: 0.05332
	accuracy_policy_4: 0.6577
	loss_value_4: 0.06678
	loss_reward_4: 0.01267
	loss_policy_5: 0.05296
	accuracy_policy_5: 0.66852
	loss_value_5: 0.06779
	loss_reward_5: 0.01389
	loss_policy: 0.51876
	loss_value: 0.63487
	loss_reward: 0.04995
[2025-05-08 04:00:51] nn step 59800, lr: 0.1.
	loss_policy_0: 0.26753
	accuracy_policy_0: 0.68512
	loss_value_0: 0.3211
	loss_policy_1: 0.05424
	accuracy_policy_1: 0.66742
	loss_value_1: 0.06521
	loss_reward_1: 0.00645
	loss_policy_2: 0.05451
	accuracy_policy_2: 0.6618
	loss_value_2: 0.06704
	loss_reward_2: 0.00816
	loss_policy_3: 0.05505
	accuracy_policy_3: 0.6609
	loss_value_3: 0.06862
	loss_reward_3: 0.01083
	loss_policy_4: 0.05533
	accuracy_policy_4: 0.65523
	loss_value_4: 0.07033
	loss_reward_4: 0.01335
	loss_policy_5: 0.05519
	accuracy_policy_5: 0.66887
	loss_value_5: 0.07157
	loss_reward_5: 0.01509
	loss_policy: 0.54184
	loss_value: 0.66387
	loss_reward: 0.05389
Optimization_Done 59800
[2025-05-08 04:03:51] [command] train weight_iter_59800.pkl 281 300
[2025-05-08 04:04:01] nn step 59850, lr: 0.1.
	loss_policy_0: 0.26021
	accuracy_policy_0: 0.68727
	loss_value_0: 0.33618
	loss_policy_1: 0.05276
	accuracy_policy_1: 0.67754
	loss_value_1: 0.0679
	loss_reward_1: 0.00592
	loss_policy_2: 0.05342
	accuracy_policy_2: 0.67148
	loss_value_2: 0.06924
	loss_reward_2: 0.00765
	loss_policy_3: 0.05374
	accuracy_policy_3: 0.66902
	loss_value_3: 0.07046
	loss_reward_3: 0.01014
	loss_policy_4: 0.05428
	accuracy_policy_4: 0.66508
	loss_value_4: 0.07157
	loss_reward_4: 0.01268
	loss_policy_5: 0.05449
	accuracy_policy_5: 0.66785
	loss_value_5: 0.07271
	loss_reward_5: 0.01415
	loss_policy: 0.5289
	loss_value: 0.68805
	loss_reward: 0.05053
[2025-05-08 04:04:09] nn step 59900, lr: 0.1.
	loss_policy_0: 0.26282
	accuracy_policy_0: 0.68867
	loss_value_0: 0.32751
	loss_policy_1: 0.05356
	accuracy_policy_1: 0.67629
	loss_value_1: 0.06668
	loss_reward_1: 0.00637
	loss_policy_2: 0.05448
	accuracy_policy_2: 0.66594
	loss_value_2: 0.06822
	loss_reward_2: 0.00801
	loss_policy_3: 0.05455
	accuracy_policy_3: 0.66367
	loss_value_3: 0.06997
	loss_reward_3: 0.01028
	loss_policy_4: 0.05523
	accuracy_policy_4: 0.6607
	loss_value_4: 0.07094
	loss_reward_4: 0.01306
	loss_policy_5: 0.05508
	accuracy_policy_5: 0.66387
	loss_value_5: 0.07219
	loss_reward_5: 0.0146
	loss_policy: 0.53572
	loss_value: 0.67552
	loss_reward: 0.05231
[2025-05-08 04:04:18] nn step 59950, lr: 0.1.
	loss_policy_0: 0.25627
	accuracy_policy_0: 0.68328
	loss_value_0: 0.31245
	loss_policy_1: 0.05192
	accuracy_policy_1: 0.67316
	loss_value_1: 0.0632
	loss_reward_1: 0.00604
	loss_policy_2: 0.05264
	accuracy_policy_2: 0.66953
	loss_value_2: 0.06477
	loss_reward_2: 0.00768
	loss_policy_3: 0.05306
	accuracy_policy_3: 0.66254
	loss_value_3: 0.06581
	loss_reward_3: 0.01008
	loss_policy_4: 0.05342
	accuracy_policy_4: 0.6602
	loss_value_4: 0.06701
	loss_reward_4: 0.01232
	loss_policy_5: 0.05308
	accuracy_policy_5: 0.66695
	loss_value_5: 0.06822
	loss_reward_5: 0.01373
	loss_policy: 0.52038
	loss_value: 0.64146
	loss_reward: 0.04985
[2025-05-08 04:04:24] nn step 60000, lr: 0.1.
	loss_policy_0: 0.24233
	accuracy_policy_0: 0.69035
	loss_value_0: 0.29239
	loss_policy_1: 0.04953
	accuracy_policy_1: 0.68254
	loss_value_1: 0.05936
	loss_reward_1: 0.0058
	loss_policy_2: 0.04997
	accuracy_policy_2: 0.67723
	loss_value_2: 0.06096
	loss_reward_2: 0.00733
	loss_policy_3: 0.0505
	accuracy_policy_3: 0.66367
	loss_value_3: 0.06239
	loss_reward_3: 0.0098
	loss_policy_4: 0.05096
	accuracy_policy_4: 0.66098
	loss_value_4: 0.06362
	loss_reward_4: 0.01211
	loss_policy_5: 0.05054
	accuracy_policy_5: 0.66938
	loss_value_5: 0.0647
	loss_reward_5: 0.01331
	loss_policy: 0.49384
	loss_value: 0.60341
	loss_reward: 0.04836
Optimization_Done 60000
