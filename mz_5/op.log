A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2025-05-11 11:02:55] [command] train weight_iter_0.pkl 1 1
[2025-05-11 11:03:09] nn step 50, lr: 0.1.
	loss_policy_0: 0.25362
	accuracy_policy_0: 0.19203
	loss_value_0: 0.28296
	loss_policy_1: 0.05127
	accuracy_policy_1: 0.19316
	loss_value_1: 0.05789
	loss_reward_1: 0.03224
	loss_policy_2: 0.05139
	accuracy_policy_2: 0.18691
	loss_value_2: 0.05778
	loss_reward_2: 0.03106
	loss_policy_3: 0.05151
	accuracy_policy_3: 0.18492
	loss_value_3: 0.05812
	loss_reward_3: 0.02864
	loss_policy_4: 0.05161
	accuracy_policy_4: 0.17832
	loss_value_4: 0.05786
	loss_reward_4: 0.03034
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.1791
	loss_value_5: 0.05718
	loss_reward_5: 0.03163
	loss_policy: 0.51121
	loss_value: 0.57179
	loss_reward: 0.15391
[2025-05-11 11:03:17] nn step 100, lr: 0.1.
	loss_policy_0: 0.25787
	accuracy_policy_0: 0.20137
	loss_value_0: 0.21793
	loss_policy_1: 0.05183
	accuracy_policy_1: 0.20469
	loss_value_1: 0.04339
	loss_reward_1: 0.01637
	loss_policy_2: 0.05206
	accuracy_policy_2: 0.19414
	loss_value_2: 0.04357
	loss_reward_2: 0.01522
	loss_policy_3: 0.05227
	accuracy_policy_3: 0.19551
	loss_value_3: 0.04402
	loss_reward_3: 0.01378
	loss_policy_4: 0.05236
	accuracy_policy_4: 0.19277
	loss_value_4: 0.04398
	loss_reward_4: 0.01511
	loss_policy_5: 0.05271
	accuracy_policy_5: 0.1907
	loss_value_5: 0.04292
	loss_reward_5: 0.01626
	loss_policy: 0.5191
	loss_value: 0.43581
	loss_reward: 0.07674
[2025-05-11 11:03:25] nn step 150, lr: 0.1.
	loss_policy_0: 0.28437
	accuracy_policy_0: 0.20629
	loss_value_0: 0.23864
	loss_policy_1: 0.05715
	accuracy_policy_1: 0.20172
	loss_value_1: 0.04752
	loss_reward_1: 0.01734
	loss_policy_2: 0.05759
	accuracy_policy_2: 0.1916
	loss_value_2: 0.04736
	loss_reward_2: 0.01529
	loss_policy_3: 0.05773
	accuracy_policy_3: 0.18996
	loss_value_3: 0.04779
	loss_reward_3: 0.01355
	loss_policy_4: 0.05805
	accuracy_policy_4: 0.19781
	loss_value_4: 0.04767
	loss_reward_4: 0.01556
	loss_policy_5: 0.0585
	accuracy_policy_5: 0.18617
	loss_value_5: 0.04623
	loss_reward_5: 0.01695
	loss_policy: 0.5734
	loss_value: 0.47521
	loss_reward: 0.07868
[2025-05-11 11:03:31] nn step 200, lr: 0.1.
	loss_policy_0: 0.28153
	accuracy_policy_0: 0.20836
	loss_value_0: 0.22813
	loss_policy_1: 0.05675
	accuracy_policy_1: 0.19922
	loss_value_1: 0.04677
	loss_reward_1: 0.01612
	loss_policy_2: 0.0571
	accuracy_policy_2: 0.19039
	loss_value_2: 0.04648
	loss_reward_2: 0.01465
	loss_policy_3: 0.05736
	accuracy_policy_3: 0.19234
	loss_value_3: 0.04705
	loss_reward_3: 0.01274
	loss_policy_4: 0.0576
	accuracy_policy_4: 0.19617
	loss_value_4: 0.04661
	loss_reward_4: 0.01424
	loss_policy_5: 0.05816
	accuracy_policy_5: 0.18469
	loss_value_5: 0.04531
	loss_reward_5: 0.01634
	loss_policy: 0.5685
	loss_value: 0.46035
	loss_reward: 0.0741
Optimization_Done 200
[2025-05-11 11:04:46] [command] train weight_iter_200.pkl 1 2
[2025-05-11 11:04:54] nn step 250, lr: 0.1.
	loss_policy_0: 0.24545
	accuracy_policy_0: 0.51941
	loss_value_0: 0.25046
	loss_policy_1: 0.0493
	accuracy_policy_1: 0.51496
	loss_value_1: 0.05024
	loss_reward_1: 0.0186
	loss_policy_2: 0.04931
	accuracy_policy_2: 0.51316
	loss_value_2: 0.04963
	loss_reward_2: 0.0173
	loss_policy_3: 0.04944
	accuracy_policy_3: 0.51324
	loss_value_3: 0.04904
	loss_reward_3: 0.01547
	loss_policy_4: 0.0495
	accuracy_policy_4: 0.51555
	loss_value_4: 0.04811
	loss_reward_4: 0.01713
	loss_policy_5: 0.05046
	accuracy_policy_5: 0.49652
	loss_value_5: 0.04636
	loss_reward_5: 0.01857
	loss_policy: 0.49347
	loss_value: 0.49384
	loss_reward: 0.08707
[2025-05-11 11:05:01] nn step 300, lr: 0.1.
	loss_policy_0: 0.23038
	accuracy_policy_0: 0.52535
	loss_value_0: 0.22876
	loss_policy_1: 0.04656
	accuracy_policy_1: 0.51797
	loss_value_1: 0.04689
	loss_reward_1: 0.01678
	loss_policy_2: 0.04643
	accuracy_policy_2: 0.52238
	loss_value_2: 0.04712
	loss_reward_2: 0.01569
	loss_policy_3: 0.0464
	accuracy_policy_3: 0.525
	loss_value_3: 0.04706
	loss_reward_3: 0.0143
	loss_policy_4: 0.04663
	accuracy_policy_4: 0.52258
	loss_value_4: 0.04671
	loss_reward_4: 0.01569
	loss_policy_5: 0.04743
	accuracy_policy_5: 0.4991
	loss_value_5: 0.04547
	loss_reward_5: 0.01679
	loss_policy: 0.46384
	loss_value: 0.46202
	loss_reward: 0.07926
[2025-05-11 11:05:09] nn step 350, lr: 0.1.
	loss_policy_0: 0.22627
	accuracy_policy_0: 0.52508
	loss_value_0: 0.22067
	loss_policy_1: 0.04559
	accuracy_policy_1: 0.51316
	loss_value_1: 0.04592
	loss_reward_1: 0.01543
	loss_policy_2: 0.04547
	accuracy_policy_2: 0.51602
	loss_value_2: 0.04634
	loss_reward_2: 0.01475
	loss_policy_3: 0.04544
	accuracy_policy_3: 0.51527
	loss_value_3: 0.04657
	loss_reward_3: 0.01333
	loss_policy_4: 0.04578
	accuracy_policy_4: 0.51723
	loss_value_4: 0.04646
	loss_reward_4: 0.01489
	loss_policy_5: 0.04666
	accuracy_policy_5: 0.4927
	loss_value_5: 0.04589
	loss_reward_5: 0.01581
	loss_policy: 0.45521
	loss_value: 0.45185
	loss_reward: 0.07421
[2025-05-11 11:05:17] nn step 400, lr: 0.1.
	loss_policy_0: 0.23129
	accuracy_policy_0: 0.5143
	loss_value_0: 0.22302
	loss_policy_1: 0.0463
	accuracy_policy_1: 0.50953
	loss_value_1: 0.04638
	loss_reward_1: 0.01529
	loss_policy_2: 0.04653
	accuracy_policy_2: 0.50426
	loss_value_2: 0.04688
	loss_reward_2: 0.01451
	loss_policy_3: 0.04624
	accuracy_policy_3: 0.50676
	loss_value_3: 0.04744
	loss_reward_3: 0.01291
	loss_policy_4: 0.04636
	accuracy_policy_4: 0.5068
	loss_value_4: 0.04751
	loss_reward_4: 0.01464
	loss_policy_5: 0.04717
	accuracy_policy_5: 0.48949
	loss_value_5: 0.04726
	loss_reward_5: 0.01559
	loss_policy: 0.4639
	loss_value: 0.45849
	loss_reward: 0.07295
Optimization_Done 400
[2025-05-11 11:06:30] [command] train weight_iter_400.pkl 1 3
[2025-05-11 11:06:39] nn step 450, lr: 0.1.
	loss_policy_0: 0.255
	accuracy_policy_0: 0.7202
	loss_value_0: 0.28218
	loss_policy_1: 0.05125
	accuracy_policy_1: 0.71805
	loss_value_1: 0.05826
	loss_reward_1: 0.0175
	loss_policy_2: 0.05113
	accuracy_policy_2: 0.71637
	loss_value_2: 0.05867
	loss_reward_2: 0.01646
	loss_policy_3: 0.05098
	accuracy_policy_3: 0.71879
	loss_value_3: 0.05939
	loss_reward_3: 0.01494
	loss_policy_4: 0.05153
	accuracy_policy_4: 0.70984
	loss_value_4: 0.0599
	loss_reward_4: 0.01658
	loss_policy_5: 0.05238
	accuracy_policy_5: 0.69621
	loss_value_5: 0.05992
	loss_reward_5: 0.01777
	loss_policy: 0.51227
	loss_value: 0.57833
	loss_reward: 0.08326
[2025-05-11 11:06:47] nn step 500, lr: 0.1.
	loss_policy_0: 0.22763
	accuracy_policy_0: 0.70039
	loss_value_0: 0.23648
	loss_policy_1: 0.04523
	accuracy_policy_1: 0.70559
	loss_value_1: 0.04935
	loss_reward_1: 0.01518
	loss_policy_2: 0.04551
	accuracy_policy_2: 0.70262
	loss_value_2: 0.04997
	loss_reward_2: 0.01429
	loss_policy_3: 0.04507
	accuracy_policy_3: 0.70301
	loss_value_3: 0.05082
	loss_reward_3: 0.013
	loss_policy_4: 0.04539
	accuracy_policy_4: 0.70234
	loss_value_4: 0.05155
	loss_reward_4: 0.0146
	loss_policy_5: 0.04637
	accuracy_policy_5: 0.6827
	loss_value_5: 0.05187
	loss_reward_5: 0.01569
	loss_policy: 0.45519
	loss_value: 0.49003
	loss_reward: 0.07275
[2025-05-11 11:06:54] nn step 550, lr: 0.1.
	loss_policy_0: 0.23726
	accuracy_policy_0: 0.6866
	loss_value_0: 0.24628
	loss_policy_1: 0.04716
	accuracy_policy_1: 0.69465
	loss_value_1: 0.05143
	loss_reward_1: 0.01576
	loss_policy_2: 0.04708
	accuracy_policy_2: 0.68805
	loss_value_2: 0.05205
	loss_reward_2: 0.01476
	loss_policy_3: 0.04712
	accuracy_policy_3: 0.69191
	loss_value_3: 0.05327
	loss_reward_3: 0.01329
	loss_policy_4: 0.04725
	accuracy_policy_4: 0.68816
	loss_value_4: 0.05405
	loss_reward_4: 0.01488
	loss_policy_5: 0.04783
	accuracy_policy_5: 0.67816
	loss_value_5: 0.05454
	loss_reward_5: 0.01628
	loss_policy: 0.47371
	loss_value: 0.51163
	loss_reward: 0.07498
[2025-05-11 11:07:02] nn step 600, lr: 0.1.
	loss_policy_0: 0.2434
	accuracy_policy_0: 0.67891
	loss_value_0: 0.25107
	loss_policy_1: 0.0483
	accuracy_policy_1: 0.68176
	loss_value_1: 0.05252
	loss_reward_1: 0.01518
	loss_policy_2: 0.04822
	accuracy_policy_2: 0.6841
	loss_value_2: 0.05324
	loss_reward_2: 0.01477
	loss_policy_3: 0.048
	accuracy_policy_3: 0.68754
	loss_value_3: 0.05444
	loss_reward_3: 0.01338
	loss_policy_4: 0.04813
	accuracy_policy_4: 0.68617
	loss_value_4: 0.05534
	loss_reward_4: 0.0149
	loss_policy_5: 0.04883
	accuracy_policy_5: 0.67227
	loss_value_5: 0.05607
	loss_reward_5: 0.01565
	loss_policy: 0.48488
	loss_value: 0.52269
	loss_reward: 0.07388
Optimization_Done 600
[2025-05-11 11:08:14] [command] train weight_iter_600.pkl 1 4
[2025-05-11 11:08:22] nn step 650, lr: 0.1.
	loss_policy_0: 0.22174
	accuracy_policy_0: 0.76605
	loss_value_0: 0.2685
	loss_policy_1: 0.04426
	accuracy_policy_1: 0.76855
	loss_value_1: 0.05551
	loss_reward_1: 0.01364
	loss_policy_2: 0.04441
	accuracy_policy_2: 0.76543
	loss_value_2: 0.05629
	loss_reward_2: 0.01313
	loss_policy_3: 0.04401
	accuracy_policy_3: 0.77059
	loss_value_3: 0.05718
	loss_reward_3: 0.01204
	loss_policy_4: 0.044
	accuracy_policy_4: 0.77293
	loss_value_4: 0.05795
	loss_reward_4: 0.01349
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.76375
	loss_value_5: 0.05871
	loss_reward_5: 0.01442
	loss_policy: 0.44289
	loss_value: 0.55414
	loss_reward: 0.06672
[2025-05-11 11:08:30] nn step 700, lr: 0.1.
	loss_policy_0: 0.22833
	accuracy_policy_0: 0.75543
	loss_value_0: 0.26085
	loss_policy_1: 0.04575
	accuracy_policy_1: 0.75527
	loss_value_1: 0.05475
	loss_reward_1: 0.01405
	loss_policy_2: 0.04573
	accuracy_policy_2: 0.75258
	loss_value_2: 0.05586
	loss_reward_2: 0.0141
	loss_policy_3: 0.04517
	accuracy_policy_3: 0.76207
	loss_value_3: 0.05695
	loss_reward_3: 0.01252
	loss_policy_4: 0.04546
	accuracy_policy_4: 0.76254
	loss_value_4: 0.05793
	loss_reward_4: 0.0137
	loss_policy_5: 0.04594
	accuracy_policy_5: 0.75301
	loss_value_5: 0.059
	loss_reward_5: 0.01542
	loss_policy: 0.45637
	loss_value: 0.54534
	loss_reward: 0.06978
[2025-05-11 11:08:38] nn step 750, lr: 0.1.
	loss_policy_0: 0.23967
	accuracy_policy_0: 0.74441
	loss_value_0: 0.26766
	loss_policy_1: 0.04763
	accuracy_policy_1: 0.7502
	loss_value_1: 0.05572
	loss_reward_1: 0.01466
	loss_policy_2: 0.04768
	accuracy_policy_2: 0.74703
	loss_value_2: 0.05681
	loss_reward_2: 0.01401
	loss_policy_3: 0.04751
	accuracy_policy_3: 0.75223
	loss_value_3: 0.05842
	loss_reward_3: 0.01298
	loss_policy_4: 0.04752
	accuracy_policy_4: 0.75367
	loss_value_4: 0.0596
	loss_reward_4: 0.01429
	loss_policy_5: 0.04798
	accuracy_policy_5: 0.74285
	loss_value_5: 0.06059
	loss_reward_5: 0.01545
	loss_policy: 0.478
	loss_value: 0.5588
	loss_reward: 0.0714
[2025-05-11 11:08:46] nn step 800, lr: 0.1.
	loss_policy_0: 0.24647
	accuracy_policy_0: 0.74453
	loss_value_0: 0.27773
	loss_policy_1: 0.04929
	accuracy_policy_1: 0.7398
	loss_value_1: 0.05843
	loss_reward_1: 0.0148
	loss_policy_2: 0.04881
	accuracy_policy_2: 0.74496
	loss_value_2: 0.0598
	loss_reward_2: 0.01419
	loss_policy_3: 0.04878
	accuracy_policy_3: 0.74695
	loss_value_3: 0.06102
	loss_reward_3: 0.01321
	loss_policy_4: 0.0488
	accuracy_policy_4: 0.74551
	loss_value_4: 0.06216
	loss_reward_4: 0.01422
	loss_policy_5: 0.04917
	accuracy_policy_5: 0.74316
	loss_value_5: 0.06305
	loss_reward_5: 0.01562
	loss_policy: 0.49133
	loss_value: 0.58219
	loss_reward: 0.07204
Optimization_Done 800
[2025-05-11 11:10:01] [command] train weight_iter_800.pkl 1 5
[2025-05-11 11:10:10] nn step 850, lr: 0.1.
	loss_policy_0: 0.25177
	accuracy_policy_0: 0.72508
	loss_value_0: 0.30858
	loss_policy_1: 0.05065
	accuracy_policy_1: 0.72645
	loss_value_1: 0.06331
	loss_reward_1: 0.01456
	loss_policy_2: 0.05058
	accuracy_policy_2: 0.72293
	loss_value_2: 0.06412
	loss_reward_2: 0.01406
	loss_policy_3: 0.05031
	accuracy_policy_3: 0.72875
	loss_value_3: 0.06508
	loss_reward_3: 0.01253
	loss_policy_4: 0.05056
	accuracy_policy_4: 0.72656
	loss_value_4: 0.06598
	loss_reward_4: 0.0144
	loss_policy_5: 0.05122
	accuracy_policy_5: 0.71633
	loss_value_5: 0.06673
	loss_reward_5: 0.01509
	loss_policy: 0.5051
	loss_value: 0.63381
	loss_reward: 0.07063
[2025-05-11 11:10:17] nn step 900, lr: 0.1.
	loss_policy_0: 0.25859
	accuracy_policy_0: 0.71879
	loss_value_0: 0.2873
	loss_policy_1: 0.05185
	accuracy_policy_1: 0.72098
	loss_value_1: 0.05953
	loss_reward_1: 0.01473
	loss_policy_2: 0.05168
	accuracy_policy_2: 0.72379
	loss_value_2: 0.06041
	loss_reward_2: 0.01463
	loss_policy_3: 0.05174
	accuracy_policy_3: 0.72398
	loss_value_3: 0.06192
	loss_reward_3: 0.01342
	loss_policy_4: 0.05183
	accuracy_policy_4: 0.72488
	loss_value_4: 0.06336
	loss_reward_4: 0.0143
	loss_policy_5: 0.05231
	accuracy_policy_5: 0.72133
	loss_value_5: 0.06424
	loss_reward_5: 0.01622
	loss_policy: 0.518
	loss_value: 0.59675
	loss_reward: 0.0733
[2025-05-11 11:10:25] nn step 950, lr: 0.1.
	loss_policy_0: 0.24411
	accuracy_policy_0: 0.71566
	loss_value_0: 0.2621
	loss_policy_1: 0.04854
	accuracy_policy_1: 0.72238
	loss_value_1: 0.05492
	loss_reward_1: 0.01381
	loss_policy_2: 0.04858
	accuracy_policy_2: 0.71871
	loss_value_2: 0.05619
	loss_reward_2: 0.01353
	loss_policy_3: 0.04826
	accuracy_policy_3: 0.72254
	loss_value_3: 0.05746
	loss_reward_3: 0.01246
	loss_policy_4: 0.04854
	accuracy_policy_4: 0.7241
	loss_value_4: 0.0585
	loss_reward_4: 0.01392
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.71297
	loss_value_5: 0.05942
	loss_reward_5: 0.01508
	loss_policy: 0.48731
	loss_value: 0.54858
	loss_reward: 0.0688
[2025-05-11 11:10:33] nn step 1000, lr: 0.1.
	loss_policy_0: 0.2585
	accuracy_policy_0: 0.71188
	loss_value_0: 0.27115
	loss_policy_1: 0.05163
	accuracy_policy_1: 0.71484
	loss_value_1: 0.05681
	loss_reward_1: 0.01431
	loss_policy_2: 0.0517
	accuracy_policy_2: 0.71113
	loss_value_2: 0.05802
	loss_reward_2: 0.01414
	loss_policy_3: 0.0513
	accuracy_policy_3: 0.71848
	loss_value_3: 0.0591
	loss_reward_3: 0.01288
	loss_policy_4: 0.05135
	accuracy_policy_4: 0.72062
	loss_value_4: 0.06082
	loss_reward_4: 0.01425
	loss_policy_5: 0.0519
	accuracy_policy_5: 0.70699
	loss_value_5: 0.06182
	loss_reward_5: 0.01544
	loss_policy: 0.51638
	loss_value: 0.56771
	loss_reward: 0.07101
Optimization_Done 1000
[2025-05-11 11:11:44] [command] train weight_iter_1000.pkl 1 6
[2025-05-11 11:11:53] nn step 1050, lr: 0.1.
	loss_policy_0: 0.24772
	accuracy_policy_0: 0.7048
	loss_value_0: 0.27403
	loss_policy_1: 0.0497
	accuracy_policy_1: 0.70242
	loss_value_1: 0.05658
	loss_reward_1: 0.01313
	loss_policy_2: 0.04975
	accuracy_policy_2: 0.7016
	loss_value_2: 0.05779
	loss_reward_2: 0.0129
	loss_policy_3: 0.04946
	accuracy_policy_3: 0.7091
	loss_value_3: 0.05905
	loss_reward_3: 0.01251
	loss_policy_4: 0.04956
	accuracy_policy_4: 0.70676
	loss_value_4: 0.06022
	loss_reward_4: 0.01318
	loss_policy_5: 0.05001
	accuracy_policy_5: 0.70211
	loss_value_5: 0.061
	loss_reward_5: 0.0144
	loss_policy: 0.4962
	loss_value: 0.56867
	loss_reward: 0.06613
[2025-05-11 11:12:02] nn step 1100, lr: 0.1.
	loss_policy_0: 0.26901
	accuracy_policy_0: 0.70277
	loss_value_0: 0.28113
	loss_policy_1: 0.05375
	accuracy_policy_1: 0.7041
	loss_value_1: 0.05879
	loss_reward_1: 0.01409
	loss_policy_2: 0.05393
	accuracy_policy_2: 0.70094
	loss_value_2: 0.06012
	loss_reward_2: 0.01426
	loss_policy_3: 0.05386
	accuracy_policy_3: 0.70961
	loss_value_3: 0.06152
	loss_reward_3: 0.01293
	loss_policy_4: 0.05381
	accuracy_policy_4: 0.70586
	loss_value_4: 0.06294
	loss_reward_4: 0.01455
	loss_policy_5: 0.05454
	accuracy_policy_5: 0.69656
	loss_value_5: 0.06431
	loss_reward_5: 0.01537
	loss_policy: 0.5389
	loss_value: 0.58881
	loss_reward: 0.0712
[2025-05-11 11:12:09] nn step 1150, lr: 0.1.
	loss_policy_0: 0.29025
	accuracy_policy_0: 0.70492
	loss_value_0: 0.29777
	loss_policy_1: 0.05804
	accuracy_policy_1: 0.70617
	loss_value_1: 0.06286
	loss_reward_1: 0.01543
	loss_policy_2: 0.05777
	accuracy_policy_2: 0.70891
	loss_value_2: 0.06407
	loss_reward_2: 0.01534
	loss_policy_3: 0.05799
	accuracy_policy_3: 0.7043
	loss_value_3: 0.06572
	loss_reward_3: 0.01408
	loss_policy_4: 0.05763
	accuracy_policy_4: 0.71215
	loss_value_4: 0.0675
	loss_reward_4: 0.01532
	loss_policy_5: 0.05844
	accuracy_policy_5: 0.70656
	loss_value_5: 0.06892
	loss_reward_5: 0.01729
	loss_policy: 0.58012
	loss_value: 0.62685
	loss_reward: 0.07747
[2025-05-11 11:12:17] nn step 1200, lr: 0.1.
	loss_policy_0: 0.27886
	accuracy_policy_0: 0.70074
	loss_value_0: 0.28813
	loss_policy_1: 0.05585
	accuracy_policy_1: 0.70211
	loss_value_1: 0.06021
	loss_reward_1: 0.01444
	loss_policy_2: 0.05564
	accuracy_policy_2: 0.70352
	loss_value_2: 0.06181
	loss_reward_2: 0.01475
	loss_policy_3: 0.05539
	accuracy_policy_3: 0.70918
	loss_value_3: 0.06339
	loss_reward_3: 0.01378
	loss_policy_4: 0.05571
	accuracy_policy_4: 0.70371
	loss_value_4: 0.06496
	loss_reward_4: 0.01458
	loss_policy_5: 0.05583
	accuracy_policy_5: 0.70375
	loss_value_5: 0.06644
	loss_reward_5: 0.01625
	loss_policy: 0.55727
	loss_value: 0.60493
	loss_reward: 0.07379
Optimization_Done 1200
[2025-05-11 11:13:33] [command] train weight_iter_1200.pkl 1 7
[2025-05-11 11:13:42] nn step 1250, lr: 0.1.
	loss_policy_0: 0.26012
	accuracy_policy_0: 0.76336
	loss_value_0: 0.28874
	loss_policy_1: 0.05185
	accuracy_policy_1: 0.7641
	loss_value_1: 0.06027
	loss_reward_1: 0.01343
	loss_policy_2: 0.05214
	accuracy_policy_2: 0.75887
	loss_value_2: 0.06178
	loss_reward_2: 0.0141
	loss_policy_3: 0.05203
	accuracy_policy_3: 0.76258
	loss_value_3: 0.06308
	loss_reward_3: 0.01282
	loss_policy_4: 0.05227
	accuracy_policy_4: 0.76219
	loss_value_4: 0.06431
	loss_reward_4: 0.01431
	loss_policy_5: 0.0526
	accuracy_policy_5: 0.75559
	loss_value_5: 0.06544
	loss_reward_5: 0.01476
	loss_policy: 0.52101
	loss_value: 0.60362
	loss_reward: 0.06943
[2025-05-11 11:13:50] nn step 1300, lr: 0.1.
	loss_policy_0: 0.27461
	accuracy_policy_0: 0.74984
	loss_value_0: 0.29549
	loss_policy_1: 0.05477
	accuracy_policy_1: 0.75512
	loss_value_1: 0.0622
	loss_reward_1: 0.0141
	loss_policy_2: 0.05483
	accuracy_policy_2: 0.755
	loss_value_2: 0.06393
	loss_reward_2: 0.01481
	loss_policy_3: 0.05491
	accuracy_policy_3: 0.75461
	loss_value_3: 0.06552
	loss_reward_3: 0.01403
	loss_policy_4: 0.05503
	accuracy_policy_4: 0.75469
	loss_value_4: 0.06695
	loss_reward_4: 0.0149
	loss_policy_5: 0.05556
	accuracy_policy_5: 0.75199
	loss_value_5: 0.06854
	loss_reward_5: 0.01639
	loss_policy: 0.5497
	loss_value: 0.62263
	loss_reward: 0.07422
[2025-05-11 11:13:58] nn step 1350, lr: 0.1.
	loss_policy_0: 0.25062
	accuracy_policy_0: 0.75055
	loss_value_0: 0.26423
	loss_policy_1: 0.05011
	accuracy_policy_1: 0.75004
	loss_value_1: 0.0556
	loss_reward_1: 0.01275
	loss_policy_2: 0.05033
	accuracy_policy_2: 0.74688
	loss_value_2: 0.05709
	loss_reward_2: 0.01312
	loss_policy_3: 0.05036
	accuracy_policy_3: 0.75078
	loss_value_3: 0.05835
	loss_reward_3: 0.01253
	loss_policy_4: 0.05043
	accuracy_policy_4: 0.74926
	loss_value_4: 0.05979
	loss_reward_4: 0.01323
	loss_policy_5: 0.05083
	accuracy_policy_5: 0.7416
	loss_value_5: 0.0611
	loss_reward_5: 0.01445
	loss_policy: 0.50268
	loss_value: 0.55616
	loss_reward: 0.06608
[2025-05-11 11:14:05] nn step 1400, lr: 0.1.
	loss_policy_0: 0.25454
	accuracy_policy_0: 0.73508
	loss_value_0: 0.2659
	loss_policy_1: 0.05055
	accuracy_policy_1: 0.74066
	loss_value_1: 0.05597
	loss_reward_1: 0.01255
	loss_policy_2: 0.05093
	accuracy_policy_2: 0.73805
	loss_value_2: 0.05775
	loss_reward_2: 0.01327
	loss_policy_3: 0.05062
	accuracy_policy_3: 0.7448
	loss_value_3: 0.05914
	loss_reward_3: 0.01258
	loss_policy_4: 0.05098
	accuracy_policy_4: 0.7398
	loss_value_4: 0.06039
	loss_reward_4: 0.01336
	loss_policy_5: 0.05095
	accuracy_policy_5: 0.73922
	loss_value_5: 0.06183
	loss_reward_5: 0.01454
	loss_policy: 0.50856
	loss_value: 0.56098
	loss_reward: 0.0663
Optimization_Done 1400
[2025-05-11 11:15:17] [command] train weight_iter_1400.pkl 1 8
[2025-05-11 11:15:25] nn step 1450, lr: 0.1.
	loss_policy_0: 0.27541
	accuracy_policy_0: 0.76129
	loss_value_0: 0.30404
	loss_policy_1: 0.05487
	accuracy_policy_1: 0.76836
	loss_value_1: 0.06342
	loss_reward_1: 0.01354
	loss_policy_2: 0.05526
	accuracy_policy_2: 0.76289
	loss_value_2: 0.06482
	loss_reward_2: 0.01546
	loss_policy_3: 0.05494
	accuracy_policy_3: 0.76691
	loss_value_3: 0.06658
	loss_reward_3: 0.01399
	loss_policy_4: 0.05507
	accuracy_policy_4: 0.76293
	loss_value_4: 0.06818
	loss_reward_4: 0.01467
	loss_policy_5: 0.05542
	accuracy_policy_5: 0.76309
	loss_value_5: 0.06953
	loss_reward_5: 0.01651
	loss_policy: 0.55097
	loss_value: 0.63658
	loss_reward: 0.07416
[2025-05-11 11:15:33] nn step 1500, lr: 0.1.
	loss_policy_0: 0.2798
	accuracy_policy_0: 0.76031
	loss_value_0: 0.30488
	loss_policy_1: 0.05601
	accuracy_policy_1: 0.75703
	loss_value_1: 0.06353
	loss_reward_1: 0.01349
	loss_policy_2: 0.05601
	accuracy_policy_2: 0.75871
	loss_value_2: 0.06516
	loss_reward_2: 0.01481
	loss_policy_3: 0.0561
	accuracy_policy_3: 0.75676
	loss_value_3: 0.06655
	loss_reward_3: 0.01428
	loss_policy_4: 0.0562
	accuracy_policy_4: 0.75805
	loss_value_4: 0.06825
	loss_reward_4: 0.01524
	loss_policy_5: 0.05663
	accuracy_policy_5: 0.75641
	loss_value_5: 0.06981
	loss_reward_5: 0.01617
	loss_policy: 0.56075
	loss_value: 0.63818
	loss_reward: 0.07399
[2025-05-11 11:15:41] nn step 1550, lr: 0.1.
	loss_policy_0: 0.2745
	accuracy_policy_0: 0.7573
	loss_value_0: 0.29438
	loss_policy_1: 0.05453
	accuracy_policy_1: 0.75656
	loss_value_1: 0.06165
	loss_reward_1: 0.0132
	loss_policy_2: 0.05496
	accuracy_policy_2: 0.75508
	loss_value_2: 0.06343
	loss_reward_2: 0.015
	loss_policy_3: 0.055
	accuracy_policy_3: 0.7566
	loss_value_3: 0.06494
	loss_reward_3: 0.01388
	loss_policy_4: 0.05472
	accuracy_policy_4: 0.7584
	loss_value_4: 0.06658
	loss_reward_4: 0.01487
	loss_policy_5: 0.05506
	accuracy_policy_5: 0.75051
	loss_value_5: 0.0684
	loss_reward_5: 0.0163
	loss_policy: 0.54878
	loss_value: 0.61938
	loss_reward: 0.07325
[2025-05-11 11:15:49] nn step 1600, lr: 0.1.
	loss_policy_0: 0.28649
	accuracy_policy_0: 0.75676
	loss_value_0: 0.30852
	loss_policy_1: 0.05786
	accuracy_policy_1: 0.75148
	loss_value_1: 0.06432
	loss_reward_1: 0.01347
	loss_policy_2: 0.05776
	accuracy_policy_2: 0.75219
	loss_value_2: 0.06626
	loss_reward_2: 0.01556
	loss_policy_3: 0.05777
	accuracy_policy_3: 0.75008
	loss_value_3: 0.06827
	loss_reward_3: 0.01398
	loss_policy_4: 0.05788
	accuracy_policy_4: 0.74711
	loss_value_4: 0.07023
	loss_reward_4: 0.01526
	loss_policy_5: 0.05799
	accuracy_policy_5: 0.74887
	loss_value_5: 0.07197
	loss_reward_5: 0.01712
	loss_policy: 0.57575
	loss_value: 0.64958
	loss_reward: 0.07539
Optimization_Done 1600
[2025-05-11 11:17:02] [command] train weight_iter_1600.pkl 1 9
[2025-05-11 11:17:11] nn step 1650, lr: 0.1.
	loss_policy_0: 0.25539
	accuracy_policy_0: 0.80168
	loss_value_0: 0.29281
	loss_policy_1: 0.05138
	accuracy_policy_1: 0.79734
	loss_value_1: 0.06132
	loss_reward_1: 0.0114
	loss_policy_2: 0.05147
	accuracy_policy_2: 0.79395
	loss_value_2: 0.06307
	loss_reward_2: 0.01312
	loss_policy_3: 0.05149
	accuracy_policy_3: 0.7991
	loss_value_3: 0.06427
	loss_reward_3: 0.01248
	loss_policy_4: 0.05149
	accuracy_policy_4: 0.79543
	loss_value_4: 0.06575
	loss_reward_4: 0.01351
	loss_policy_5: 0.05183
	accuracy_policy_5: 0.79496
	loss_value_5: 0.06733
	loss_reward_5: 0.01477
	loss_policy: 0.51306
	loss_value: 0.61455
	loss_reward: 0.06528
[2025-05-11 11:17:18] nn step 1700, lr: 0.1.
	loss_policy_0: 0.25441
	accuracy_policy_0: 0.79449
	loss_value_0: 0.28364
	loss_policy_1: 0.05094
	accuracy_policy_1: 0.79457
	loss_value_1: 0.05903
	loss_reward_1: 0.01087
	loss_policy_2: 0.05109
	accuracy_policy_2: 0.79379
	loss_value_2: 0.06102
	loss_reward_2: 0.01306
	loss_policy_3: 0.05087
	accuracy_policy_3: 0.79289
	loss_value_3: 0.06239
	loss_reward_3: 0.01207
	loss_policy_4: 0.05118
	accuracy_policy_4: 0.78895
	loss_value_4: 0.06419
	loss_reward_4: 0.01277
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.79125
	loss_value_5: 0.06598
	loss_reward_5: 0.01429
	loss_policy: 0.50988
	loss_value: 0.59625
	loss_reward: 0.06306
[2025-05-11 11:17:26] nn step 1750, lr: 0.1.
	loss_policy_0: 0.25704
	accuracy_policy_0: 0.78594
	loss_value_0: 0.28262
	loss_policy_1: 0.05152
	accuracy_policy_1: 0.78828
	loss_value_1: 0.05891
	loss_reward_1: 0.0112
	loss_policy_2: 0.05155
	accuracy_policy_2: 0.78598
	loss_value_2: 0.0611
	loss_reward_2: 0.01313
	loss_policy_3: 0.05136
	accuracy_policy_3: 0.78879
	loss_value_3: 0.06238
	loss_reward_3: 0.01203
	loss_policy_4: 0.05147
	accuracy_policy_4: 0.78453
	loss_value_4: 0.06387
	loss_reward_4: 0.01323
	loss_policy_5: 0.05184
	accuracy_policy_5: 0.7884
	loss_value_5: 0.06563
	loss_reward_5: 0.01422
	loss_policy: 0.51477
	loss_value: 0.59451
	loss_reward: 0.06381
[2025-05-11 11:17:34] nn step 1800, lr: 0.1.
	loss_policy_0: 0.27331
	accuracy_policy_0: 0.77613
	loss_value_0: 0.29338
	loss_policy_1: 0.05422
	accuracy_policy_1: 0.78285
	loss_value_1: 0.06152
	loss_reward_1: 0.01145
	loss_policy_2: 0.05485
	accuracy_policy_2: 0.77727
	loss_value_2: 0.06339
	loss_reward_2: 0.01376
	loss_policy_3: 0.05473
	accuracy_policy_3: 0.77996
	loss_value_3: 0.06464
	loss_reward_3: 0.013
	loss_policy_4: 0.05463
	accuracy_policy_4: 0.7793
	loss_value_4: 0.06662
	loss_reward_4: 0.01413
	loss_policy_5: 0.0551
	accuracy_policy_5: 0.77906
	loss_value_5: 0.06821
	loss_reward_5: 0.01496
	loss_policy: 0.54683
	loss_value: 0.61774
	loss_reward: 0.0673
Optimization_Done 1800
[2025-05-11 11:18:48] [command] train weight_iter_1800.pkl 1 10
[2025-05-11 11:18:57] nn step 1850, lr: 0.1.
	loss_policy_0: 0.2641
	accuracy_policy_0: 0.78367
	loss_value_0: 0.30277
	loss_policy_1: 0.05262
	accuracy_policy_1: 0.79238
	loss_value_1: 0.06289
	loss_reward_1: 0.0113
	loss_policy_2: 0.05297
	accuracy_policy_2: 0.78754
	loss_value_2: 0.06468
	loss_reward_2: 0.01343
	loss_policy_3: 0.05297
	accuracy_policy_3: 0.7884
	loss_value_3: 0.06581
	loss_reward_3: 0.01302
	loss_policy_4: 0.05343
	accuracy_policy_4: 0.78023
	loss_value_4: 0.06755
	loss_reward_4: 0.01387
	loss_policy_5: 0.05307
	accuracy_policy_5: 0.78965
	loss_value_5: 0.06871
	loss_reward_5: 0.01506
	loss_policy: 0.52917
	loss_value: 0.63241
	loss_reward: 0.06667
[2025-05-11 11:19:05] nn step 1900, lr: 0.1.
	loss_policy_0: 0.25608
	accuracy_policy_0: 0.7798
	loss_value_0: 0.28194
	loss_policy_1: 0.05133
	accuracy_policy_1: 0.77941
	loss_value_1: 0.05892
	loss_reward_1: 0.01087
	loss_policy_2: 0.05146
	accuracy_policy_2: 0.77984
	loss_value_2: 0.06076
	loss_reward_2: 0.01312
	loss_policy_3: 0.05129
	accuracy_policy_3: 0.78082
	loss_value_3: 0.06227
	loss_reward_3: 0.01216
	loss_policy_4: 0.05176
	accuracy_policy_4: 0.77617
	loss_value_4: 0.06407
	loss_reward_4: 0.01334
	loss_policy_5: 0.05169
	accuracy_policy_5: 0.78203
	loss_value_5: 0.06576
	loss_reward_5: 0.01443
	loss_policy: 0.51361
	loss_value: 0.59372
	loss_reward: 0.06392
[2025-05-11 11:19:14] nn step 1950, lr: 0.1.
	loss_policy_0: 0.26201
	accuracy_policy_0: 0.78324
	loss_value_0: 0.28129
	loss_policy_1: 0.05229
	accuracy_policy_1: 0.78152
	loss_value_1: 0.05865
	loss_reward_1: 0.01088
	loss_policy_2: 0.05253
	accuracy_policy_2: 0.78141
	loss_value_2: 0.06054
	loss_reward_2: 0.01355
	loss_policy_3: 0.05252
	accuracy_policy_3: 0.78188
	loss_value_3: 0.06212
	loss_reward_3: 0.0124
	loss_policy_4: 0.05279
	accuracy_policy_4: 0.77934
	loss_value_4: 0.06368
	loss_reward_4: 0.01355
	loss_policy_5: 0.05317
	accuracy_policy_5: 0.77676
	loss_value_5: 0.06554
	loss_reward_5: 0.01508
	loss_policy: 0.52531
	loss_value: 0.59181
	loss_reward: 0.06545
[2025-05-11 11:19:20] nn step 2000, lr: 0.1.
	loss_policy_0: 0.25738
	accuracy_policy_0: 0.77766
	loss_value_0: 0.27776
	loss_policy_1: 0.05153
	accuracy_policy_1: 0.77934
	loss_value_1: 0.05805
	loss_reward_1: 0.0105
	loss_policy_2: 0.05174
	accuracy_policy_2: 0.77605
	loss_value_2: 0.05985
	loss_reward_2: 0.01359
	loss_policy_3: 0.05171
	accuracy_policy_3: 0.78051
	loss_value_3: 0.06174
	loss_reward_3: 0.01233
	loss_policy_4: 0.05167
	accuracy_policy_4: 0.77477
	loss_value_4: 0.06342
	loss_reward_4: 0.01355
	loss_policy_5: 0.05185
	accuracy_policy_5: 0.77531
	loss_value_5: 0.06516
	loss_reward_5: 0.01496
	loss_policy: 0.51588
	loss_value: 0.58597
	loss_reward: 0.06492
Optimization_Done 2000
[2025-05-11 11:20:33] [command] train weight_iter_2000.pkl 1 11
[2025-05-11 11:20:41] nn step 2050, lr: 0.1.
	loss_policy_0: 0.26224
	accuracy_policy_0: 0.76094
	loss_value_0: 0.29646
	loss_policy_1: 0.05254
	accuracy_policy_1: 0.76434
	loss_value_1: 0.06176
	loss_reward_1: 0.01055
	loss_policy_2: 0.05222
	accuracy_policy_2: 0.76625
	loss_value_2: 0.06338
	loss_reward_2: 0.01303
	loss_policy_3: 0.05268
	accuracy_policy_3: 0.75914
	loss_value_3: 0.06481
	loss_reward_3: 0.01238
	loss_policy_4: 0.05271
	accuracy_policy_4: 0.7607
	loss_value_4: 0.06628
	loss_reward_4: 0.01376
	loss_policy_5: 0.05273
	accuracy_policy_5: 0.76727
	loss_value_5: 0.06763
	loss_reward_5: 0.01465
	loss_policy: 0.52511
	loss_value: 0.62032
	loss_reward: 0.06437
[2025-05-11 11:20:49] nn step 2100, lr: 0.1.
	loss_policy_0: 0.24876
	accuracy_policy_0: 0.76773
	loss_value_0: 0.27368
	loss_policy_1: 0.04953
	accuracy_policy_1: 0.77082
	loss_value_1: 0.05697
	loss_reward_1: 0.01004
	loss_policy_2: 0.04982
	accuracy_policy_2: 0.76641
	loss_value_2: 0.05881
	loss_reward_2: 0.01231
	loss_policy_3: 0.04991
	accuracy_policy_3: 0.76645
	loss_value_3: 0.06037
	loss_reward_3: 0.01166
	loss_policy_4: 0.04986
	accuracy_policy_4: 0.76535
	loss_value_4: 0.06182
	loss_reward_4: 0.01282
	loss_policy_5: 0.04996
	accuracy_policy_5: 0.76898
	loss_value_5: 0.06327
	loss_reward_5: 0.01357
	loss_policy: 0.49783
	loss_value: 0.57491
	loss_reward: 0.0604
[2025-05-11 11:20:57] nn step 2150, lr: 0.1.
	loss_policy_0: 0.26807
	accuracy_policy_0: 0.75621
	loss_value_0: 0.29218
	loss_policy_1: 0.05372
	accuracy_policy_1: 0.76285
	loss_value_1: 0.06076
	loss_reward_1: 0.01056
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.76059
	loss_value_2: 0.06265
	loss_reward_2: 0.01322
	loss_policy_3: 0.05382
	accuracy_policy_3: 0.7598
	loss_value_3: 0.06391
	loss_reward_3: 0.01294
	loss_policy_4: 0.05382
	accuracy_policy_4: 0.75922
	loss_value_4: 0.06578
	loss_reward_4: 0.01386
	loss_policy_5: 0.05399
	accuracy_policy_5: 0.76406
	loss_value_5: 0.06755
	loss_reward_5: 0.01497
	loss_policy: 0.53749
	loss_value: 0.61284
	loss_reward: 0.06556
[2025-05-11 11:21:06] nn step 2200, lr: 0.1.
	loss_policy_0: 0.24571
	accuracy_policy_0: 0.76273
	loss_value_0: 0.26511
	loss_policy_1: 0.04918
	accuracy_policy_1: 0.76301
	loss_value_1: 0.05535
	loss_reward_1: 0.00952
	loss_policy_2: 0.04911
	accuracy_policy_2: 0.76586
	loss_value_2: 0.05703
	loss_reward_2: 0.0117
	loss_policy_3: 0.04909
	accuracy_policy_3: 0.76301
	loss_value_3: 0.05831
	loss_reward_3: 0.0115
	loss_policy_4: 0.04921
	accuracy_policy_4: 0.76676
	loss_value_4: 0.05973
	loss_reward_4: 0.01226
	loss_policy_5: 0.04949
	accuracy_policy_5: 0.76844
	loss_value_5: 0.06149
	loss_reward_5: 0.01343
	loss_policy: 0.49179
	loss_value: 0.55702
	loss_reward: 0.05842
Optimization_Done 2200
[2025-05-11 11:22:19] [command] train weight_iter_2200.pkl 1 12
[2025-05-11 11:22:28] nn step 2250, lr: 0.1.
	loss_policy_0: 0.26105
	accuracy_policy_0: 0.72898
	loss_value_0: 0.29698
	loss_policy_1: 0.05206
	accuracy_policy_1: 0.73406
	loss_value_1: 0.06112
	loss_reward_1: 0.00975
	loss_policy_2: 0.05217
	accuracy_policy_2: 0.73418
	loss_value_2: 0.06264
	loss_reward_2: 0.01231
	loss_policy_3: 0.05244
	accuracy_policy_3: 0.73449
	loss_value_3: 0.06442
	loss_reward_3: 0.01202
	loss_policy_4: 0.05221
	accuracy_policy_4: 0.73285
	loss_value_4: 0.06617
	loss_reward_4: 0.0132
	loss_policy_5: 0.05244
	accuracy_policy_5: 0.73418
	loss_value_5: 0.06766
	loss_reward_5: 0.01422
	loss_policy: 0.52237
	loss_value: 0.619
	loss_reward: 0.06149
[2025-05-11 11:22:35] nn step 2300, lr: 0.1.
	loss_policy_0: 0.27226
	accuracy_policy_0: 0.7398
	loss_value_0: 0.2986
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.74223
	loss_value_1: 0.06191
	loss_reward_1: 0.01003
	loss_policy_2: 0.05442
	accuracy_policy_2: 0.73734
	loss_value_2: 0.06403
	loss_reward_2: 0.01319
	loss_policy_3: 0.05451
	accuracy_policy_3: 0.74004
	loss_value_3: 0.06584
	loss_reward_3: 0.01261
	loss_policy_4: 0.05451
	accuracy_policy_4: 0.73863
	loss_value_4: 0.06788
	loss_reward_4: 0.01388
	loss_policy_5: 0.05477
	accuracy_policy_5: 0.74098
	loss_value_5: 0.06953
	loss_reward_5: 0.01479
	loss_policy: 0.54489
	loss_value: 0.6278
	loss_reward: 0.0645
[2025-05-11 11:22:43] nn step 2350, lr: 0.1.
	loss_policy_0: 0.25996
	accuracy_policy_0: 0.74066
	loss_value_0: 0.28233
	loss_policy_1: 0.05207
	accuracy_policy_1: 0.74324
	loss_value_1: 0.05871
	loss_reward_1: 0.0097
	loss_policy_2: 0.05225
	accuracy_policy_2: 0.73977
	loss_value_2: 0.06033
	loss_reward_2: 0.01252
	loss_policy_3: 0.05231
	accuracy_policy_3: 0.74539
	loss_value_3: 0.06179
	loss_reward_3: 0.01174
	loss_policy_4: 0.05245
	accuracy_policy_4: 0.73828
	loss_value_4: 0.06346
	loss_reward_4: 0.01301
	loss_policy_5: 0.05248
	accuracy_policy_5: 0.74145
	loss_value_5: 0.06551
	loss_reward_5: 0.01425
	loss_policy: 0.52151
	loss_value: 0.59214
	loss_reward: 0.06122
[2025-05-11 11:22:51] nn step 2400, lr: 0.1.
	loss_policy_0: 0.29024
	accuracy_policy_0: 0.74578
	loss_value_0: 0.31351
	loss_policy_1: 0.05808
	accuracy_policy_1: 0.74469
	loss_value_1: 0.06536
	loss_reward_1: 0.01063
	loss_policy_2: 0.05811
	accuracy_policy_2: 0.74391
	loss_value_2: 0.06733
	loss_reward_2: 0.01377
	loss_policy_3: 0.05836
	accuracy_policy_3: 0.74492
	loss_value_3: 0.06922
	loss_reward_3: 0.01369
	loss_policy_4: 0.05832
	accuracy_policy_4: 0.7416
	loss_value_4: 0.07102
	loss_reward_4: 0.01464
	loss_policy_5: 0.05847
	accuracy_policy_5: 0.74641
	loss_value_5: 0.07314
	loss_reward_5: 0.01566
	loss_policy: 0.58158
	loss_value: 0.65958
	loss_reward: 0.06838
Optimization_Done 2400
[2025-05-11 11:24:05] [command] train weight_iter_2400.pkl 1 13
[2025-05-11 11:24:14] nn step 2450, lr: 0.1.
	loss_policy_0: 0.28751
	accuracy_policy_0: 0.75082
	loss_value_0: 0.32879
	loss_policy_1: 0.05787
	accuracy_policy_1: 0.74559
	loss_value_1: 0.0681
	loss_reward_1: 0.01027
	loss_policy_2: 0.05784
	accuracy_policy_2: 0.74695
	loss_value_2: 0.07032
	loss_reward_2: 0.01423
	loss_policy_3: 0.05806
	accuracy_policy_3: 0.7468
	loss_value_3: 0.07202
	loss_reward_3: 0.01331
	loss_policy_4: 0.05823
	accuracy_policy_4: 0.74633
	loss_value_4: 0.07374
	loss_reward_4: 0.01492
	loss_policy_5: 0.05841
	accuracy_policy_5: 0.74711
	loss_value_5: 0.07552
	loss_reward_5: 0.01638
	loss_policy: 0.57791
	loss_value: 0.68849
	loss_reward: 0.06911
[2025-05-11 11:24:22] nn step 2500, lr: 0.1.
	loss_policy_0: 0.26557
	accuracy_policy_0: 0.74344
	loss_value_0: 0.29138
	loss_policy_1: 0.05304
	accuracy_policy_1: 0.75117
	loss_value_1: 0.06049
	loss_reward_1: 0.00952
	loss_policy_2: 0.05321
	accuracy_policy_2: 0.74879
	loss_value_2: 0.06219
	loss_reward_2: 0.01261
	loss_policy_3: 0.05339
	accuracy_policy_3: 0.7493
	loss_value_3: 0.06364
	loss_reward_3: 0.01211
	loss_policy_4: 0.05313
	accuracy_policy_4: 0.74867
	loss_value_4: 0.06528
	loss_reward_4: 0.01305
	loss_policy_5: 0.05358
	accuracy_policy_5: 0.74648
	loss_value_5: 0.06687
	loss_reward_5: 0.01438
	loss_policy: 0.53192
	loss_value: 0.60985
	loss_reward: 0.06167
[2025-05-11 11:24:29] nn step 2550, lr: 0.1.
	loss_policy_0: 0.26238
	accuracy_policy_0: 0.74648
	loss_value_0: 0.28663
	loss_policy_1: 0.05256
	accuracy_policy_1: 0.74316
	loss_value_1: 0.05951
	loss_reward_1: 0.0091
	loss_policy_2: 0.05256
	accuracy_policy_2: 0.74352
	loss_value_2: 0.06141
	loss_reward_2: 0.01214
	loss_policy_3: 0.05256
	accuracy_policy_3: 0.7434
	loss_value_3: 0.06303
	loss_reward_3: 0.01179
	loss_policy_4: 0.0525
	accuracy_policy_4: 0.74441
	loss_value_4: 0.06456
	loss_reward_4: 0.01289
	loss_policy_5: 0.05259
	accuracy_policy_5: 0.74664
	loss_value_5: 0.06612
	loss_reward_5: 0.01398
	loss_policy: 0.52515
	loss_value: 0.60126
	loss_reward: 0.05989
[2025-05-11 11:24:37] nn step 2600, lr: 0.1.
	loss_policy_0: 0.27675
	accuracy_policy_0: 0.74781
	loss_value_0: 0.3002
	loss_policy_1: 0.05529
	accuracy_policy_1: 0.74934
	loss_value_1: 0.06205
	loss_reward_1: 0.00921
	loss_policy_2: 0.05498
	accuracy_policy_2: 0.75312
	loss_value_2: 0.06407
	loss_reward_2: 0.01298
	loss_policy_3: 0.05529
	accuracy_policy_3: 0.74754
	loss_value_3: 0.06607
	loss_reward_3: 0.01244
	loss_policy_4: 0.05552
	accuracy_policy_4: 0.7432
	loss_value_4: 0.0675
	loss_reward_4: 0.01396
	loss_policy_5: 0.05548
	accuracy_policy_5: 0.74844
	loss_value_5: 0.06922
	loss_reward_5: 0.01461
	loss_policy: 0.5533
	loss_value: 0.62911
	loss_reward: 0.06319
Optimization_Done 2600
[2025-05-11 11:25:52] [command] train weight_iter_2600.pkl 1 14
[2025-05-11 11:26:00] nn step 2650, lr: 0.1.
	loss_policy_0: 0.24555
	accuracy_policy_0: 0.78039
	loss_value_0: 0.29303
	loss_policy_1: 0.04894
	accuracy_policy_1: 0.78375
	loss_value_1: 0.06069
	loss_reward_1: 0.00869
	loss_policy_2: 0.04919
	accuracy_policy_2: 0.78242
	loss_value_2: 0.06261
	loss_reward_2: 0.012
	loss_policy_3: 0.04906
	accuracy_policy_3: 0.78629
	loss_value_3: 0.06407
	loss_reward_3: 0.01189
	loss_policy_4: 0.04948
	accuracy_policy_4: 0.77855
	loss_value_4: 0.06589
	loss_reward_4: 0.01252
	loss_policy_5: 0.04934
	accuracy_policy_5: 0.78191
	loss_value_5: 0.06758
	loss_reward_5: 0.01378
	loss_policy: 0.49157
	loss_value: 0.61387
	loss_reward: 0.05887
[2025-05-11 11:26:08] nn step 2700, lr: 0.1.
	loss_policy_0: 0.25524
	accuracy_policy_0: 0.77406
	loss_value_0: 0.29164
	loss_policy_1: 0.05093
	accuracy_policy_1: 0.77641
	loss_value_1: 0.06071
	loss_reward_1: 0.00872
	loss_policy_2: 0.05105
	accuracy_policy_2: 0.77773
	loss_value_2: 0.0627
	loss_reward_2: 0.01212
	loss_policy_3: 0.05106
	accuracy_policy_3: 0.77863
	loss_value_3: 0.0644
	loss_reward_3: 0.01152
	loss_policy_4: 0.05136
	accuracy_policy_4: 0.77195
	loss_value_4: 0.06602
	loss_reward_4: 0.01292
	loss_policy_5: 0.0514
	accuracy_policy_5: 0.77566
	loss_value_5: 0.06782
	loss_reward_5: 0.01389
	loss_policy: 0.51104
	loss_value: 0.61328
	loss_reward: 0.05916
[2025-05-11 11:26:16] nn step 2750, lr: 0.1.
	loss_policy_0: 0.26772
	accuracy_policy_0: 0.77883
	loss_value_0: 0.30162
	loss_policy_1: 0.05325
	accuracy_policy_1: 0.77934
	loss_value_1: 0.06215
	loss_reward_1: 0.00915
	loss_policy_2: 0.05384
	accuracy_policy_2: 0.77559
	loss_value_2: 0.06437
	loss_reward_2: 0.01303
	loss_policy_3: 0.05361
	accuracy_policy_3: 0.77961
	loss_value_3: 0.06593
	loss_reward_3: 0.01239
	loss_policy_4: 0.05391
	accuracy_policy_4: 0.77438
	loss_value_4: 0.06783
	loss_reward_4: 0.01349
	loss_policy_5: 0.05402
	accuracy_policy_5: 0.77441
	loss_value_5: 0.06978
	loss_reward_5: 0.01476
	loss_policy: 0.53636
	loss_value: 0.63168
	loss_reward: 0.06283
[2025-05-11 11:26:23] nn step 2800, lr: 0.1.
	loss_policy_0: 0.27845
	accuracy_policy_0: 0.77266
	loss_value_0: 0.30913
	loss_policy_1: 0.05585
	accuracy_policy_1: 0.77613
	loss_value_1: 0.06416
	loss_reward_1: 0.00912
	loss_policy_2: 0.05574
	accuracy_policy_2: 0.77293
	loss_value_2: 0.06604
	loss_reward_2: 0.01277
	loss_policy_3: 0.05586
	accuracy_policy_3: 0.77629
	loss_value_3: 0.06799
	loss_reward_3: 0.01274
	loss_policy_4: 0.056
	accuracy_policy_4: 0.77176
	loss_value_4: 0.06966
	loss_reward_4: 0.01373
	loss_policy_5: 0.05576
	accuracy_policy_5: 0.7782
	loss_value_5: 0.07163
	loss_reward_5: 0.01527
	loss_policy: 0.55765
	loss_value: 0.64862
	loss_reward: 0.06362
Optimization_Done 2800
[2025-05-11 11:27:41] [command] train weight_iter_2800.pkl 1 15
[2025-05-11 11:27:48] nn step 2850, lr: 0.1.
	loss_policy_0: 0.24502
	accuracy_policy_0: 0.75148
	loss_value_0: 0.28856
	loss_policy_1: 0.04897
	accuracy_policy_1: 0.75977
	loss_value_1: 0.05969
	loss_reward_1: 0.00817
	loss_policy_2: 0.04943
	accuracy_policy_2: 0.75289
	loss_value_2: 0.06121
	loss_reward_2: 0.01131
	loss_policy_3: 0.04906
	accuracy_policy_3: 0.7566
	loss_value_3: 0.06292
	loss_reward_3: 0.01127
	loss_policy_4: 0.04942
	accuracy_policy_4: 0.7548
	loss_value_4: 0.06456
	loss_reward_4: 0.01234
	loss_policy_5: 0.04897
	accuracy_policy_5: 0.76129
	loss_value_5: 0.06608
	loss_reward_5: 0.01283
	loss_policy: 0.49088
	loss_value: 0.60301
	loss_reward: 0.05592
[2025-05-11 11:27:57] nn step 2900, lr: 0.1.
	loss_policy_0: 0.25482
	accuracy_policy_0: 0.7632
	loss_value_0: 0.29317
	loss_policy_1: 0.0512
	accuracy_policy_1: 0.76176
	loss_value_1: 0.06062
	loss_reward_1: 0.00807
	loss_policy_2: 0.05143
	accuracy_policy_2: 0.76008
	loss_value_2: 0.06233
	loss_reward_2: 0.01125
	loss_policy_3: 0.05129
	accuracy_policy_3: 0.75934
	loss_value_3: 0.06397
	loss_reward_3: 0.01134
	loss_policy_4: 0.05153
	accuracy_policy_4: 0.75828
	loss_value_4: 0.06578
	loss_reward_4: 0.01254
	loss_policy_5: 0.0515
	accuracy_policy_5: 0.76121
	loss_value_5: 0.06771
	loss_reward_5: 0.01364
	loss_policy: 0.51177
	loss_value: 0.61357
	loss_reward: 0.05684
[2025-05-11 11:28:05] nn step 2950, lr: 0.1.
	loss_policy_0: 0.27369
	accuracy_policy_0: 0.75672
	loss_value_0: 0.30786
	loss_policy_1: 0.05508
	accuracy_policy_1: 0.75875
	loss_value_1: 0.06381
	loss_reward_1: 0.00862
	loss_policy_2: 0.05492
	accuracy_policy_2: 0.76004
	loss_value_2: 0.06592
	loss_reward_2: 0.01205
	loss_policy_3: 0.055
	accuracy_policy_3: 0.75801
	loss_value_3: 0.06763
	loss_reward_3: 0.01225
	loss_policy_4: 0.05503
	accuracy_policy_4: 0.7602
	loss_value_4: 0.06951
	loss_reward_4: 0.0139
	loss_policy_5: 0.05513
	accuracy_policy_5: 0.75855
	loss_value_5: 0.07124
	loss_reward_5: 0.01469
	loss_policy: 0.54886
	loss_value: 0.64597
	loss_reward: 0.06151
[2025-05-11 11:28:14] nn step 3000, lr: 0.1.
	loss_policy_0: 0.24908
	accuracy_policy_0: 0.75918
	loss_value_0: 0.28201
	loss_policy_1: 0.04981
	accuracy_policy_1: 0.76047
	loss_value_1: 0.05847
	loss_reward_1: 0.00822
	loss_policy_2: 0.04984
	accuracy_policy_2: 0.76164
	loss_value_2: 0.06049
	loss_reward_2: 0.01169
	loss_policy_3: 0.04983
	accuracy_policy_3: 0.76129
	loss_value_3: 0.06207
	loss_reward_3: 0.01174
	loss_policy_4: 0.04999
	accuracy_policy_4: 0.76051
	loss_value_4: 0.06373
	loss_reward_4: 0.01254
	loss_policy_5: 0.05
	accuracy_policy_5: 0.76207
	loss_value_5: 0.06541
	loss_reward_5: 0.01388
	loss_policy: 0.49854
	loss_value: 0.59217
	loss_reward: 0.05807
Optimization_Done 3000
[2025-05-11 11:29:26] [command] train weight_iter_3000.pkl 1 16
[2025-05-11 11:29:36] nn step 3050, lr: 0.1.
	loss_policy_0: 0.27748
	accuracy_policy_0: 0.76793
	loss_value_0: 0.32994
	loss_policy_1: 0.05584
	accuracy_policy_1: 0.76551
	loss_value_1: 0.06808
	loss_reward_1: 0.00862
	loss_policy_2: 0.05575
	accuracy_policy_2: 0.76465
	loss_value_2: 0.07003
	loss_reward_2: 0.01285
	loss_policy_3: 0.05561
	accuracy_policy_3: 0.76555
	loss_value_3: 0.07167
	loss_reward_3: 0.01244
	loss_policy_4: 0.05601
	accuracy_policy_4: 0.76383
	loss_value_4: 0.07341
	loss_reward_4: 0.01424
	loss_policy_5: 0.05593
	accuracy_policy_5: 0.76477
	loss_value_5: 0.07519
	loss_reward_5: 0.01529
	loss_policy: 0.55662
	loss_value: 0.68833
	loss_reward: 0.06343
[2025-05-11 11:29:43] nn step 3100, lr: 0.1.
	loss_policy_0: 0.2617
	accuracy_policy_0: 0.76648
	loss_value_0: 0.30126
	loss_policy_1: 0.05214
	accuracy_policy_1: 0.77082
	loss_value_1: 0.0621
	loss_reward_1: 0.00813
	loss_policy_2: 0.05245
	accuracy_policy_2: 0.76773
	loss_value_2: 0.06378
	loss_reward_2: 0.01147
	loss_policy_3: 0.05234
	accuracy_policy_3: 0.76754
	loss_value_3: 0.06546
	loss_reward_3: 0.01165
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.7659
	loss_value_4: 0.0672
	loss_reward_4: 0.01262
	loss_policy_5: 0.05273
	accuracy_policy_5: 0.76699
	loss_value_5: 0.06896
	loss_reward_5: 0.01374
	loss_policy: 0.52373
	loss_value: 0.62876
	loss_reward: 0.05761
[2025-05-11 11:29:51] nn step 3150, lr: 0.1.
	loss_policy_0: 0.26586
	accuracy_policy_0: 0.7648
	loss_value_0: 0.30514
	loss_policy_1: 0.05309
	accuracy_policy_1: 0.76652
	loss_value_1: 0.06314
	loss_reward_1: 0.00854
	loss_policy_2: 0.05341
	accuracy_policy_2: 0.7627
	loss_value_2: 0.06515
	loss_reward_2: 0.01224
	loss_policy_3: 0.05361
	accuracy_policy_3: 0.76352
	loss_value_3: 0.06673
	loss_reward_3: 0.01173
	loss_policy_4: 0.0536
	accuracy_policy_4: 0.7616
	loss_value_4: 0.06872
	loss_reward_4: 0.01334
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.76305
	loss_value_5: 0.07066
	loss_reward_5: 0.01432
	loss_policy: 0.53326
	loss_value: 0.63954
	loss_reward: 0.06018
[2025-05-11 11:29:59] nn step 3200, lr: 0.1.
	loss_policy_0: 0.25628
	accuracy_policy_0: 0.76438
	loss_value_0: 0.28696
	loss_policy_1: 0.05119
	accuracy_policy_1: 0.76578
	loss_value_1: 0.05921
	loss_reward_1: 0.00772
	loss_policy_2: 0.05113
	accuracy_policy_2: 0.7677
	loss_value_2: 0.0613
	loss_reward_2: 0.01103
	loss_policy_3: 0.05126
	accuracy_policy_3: 0.76727
	loss_value_3: 0.06309
	loss_reward_3: 0.01157
	loss_policy_4: 0.05141
	accuracy_policy_4: 0.7632
	loss_value_4: 0.06472
	loss_reward_4: 0.01246
	loss_policy_5: 0.05159
	accuracy_policy_5: 0.76582
	loss_value_5: 0.06627
	loss_reward_5: 0.01332
	loss_policy: 0.51286
	loss_value: 0.60155
	loss_reward: 0.05611
Optimization_Done 3200
[2025-05-11 11:31:14] [command] train weight_iter_3200.pkl 1 17
[2025-05-11 11:31:23] nn step 3250, lr: 0.1.
	loss_policy_0: 0.26815
	accuracy_policy_0: 0.75023
	loss_value_0: 0.30543
	loss_policy_1: 0.05332
	accuracy_policy_1: 0.75117
	loss_value_1: 0.0633
	loss_reward_1: 0.00826
	loss_policy_2: 0.0535
	accuracy_policy_2: 0.75188
	loss_value_2: 0.06503
	loss_reward_2: 0.01181
	loss_policy_3: 0.05336
	accuracy_policy_3: 0.75289
	loss_value_3: 0.06664
	loss_reward_3: 0.01151
	loss_policy_4: 0.05354
	accuracy_policy_4: 0.7518
	loss_value_4: 0.06808
	loss_reward_4: 0.01299
	loss_policy_5: 0.05363
	accuracy_policy_5: 0.75227
	loss_value_5: 0.06945
	loss_reward_5: 0.01408
	loss_policy: 0.53549
	loss_value: 0.63794
	loss_reward: 0.05865
[2025-05-11 11:31:32] nn step 3300, lr: 0.1.
	loss_policy_0: 0.2756
	accuracy_policy_0: 0.75086
	loss_value_0: 0.31018
	loss_policy_1: 0.05543
	accuracy_policy_1: 0.75652
	loss_value_1: 0.06415
	loss_reward_1: 0.0085
	loss_policy_2: 0.05484
	accuracy_policy_2: 0.75594
	loss_value_2: 0.06604
	loss_reward_2: 0.0126
	loss_policy_3: 0.05536
	accuracy_policy_3: 0.75332
	loss_value_3: 0.06794
	loss_reward_3: 0.01233
	loss_policy_4: 0.05569
	accuracy_policy_4: 0.74641
	loss_value_4: 0.06982
	loss_reward_4: 0.01366
	loss_policy_5: 0.05539
	accuracy_policy_5: 0.75461
	loss_value_5: 0.07159
	loss_reward_5: 0.0147
	loss_policy: 0.55231
	loss_value: 0.64974
	loss_reward: 0.06178
[2025-05-11 11:31:38] nn step 3350, lr: 0.1.
	loss_policy_0: 0.261
	accuracy_policy_0: 0.75195
	loss_value_0: 0.29391
	loss_policy_1: 0.05231
	accuracy_policy_1: 0.75492
	loss_value_1: 0.06044
	loss_reward_1: 0.00764
	loss_policy_2: 0.05228
	accuracy_policy_2: 0.75477
	loss_value_2: 0.06249
	loss_reward_2: 0.01152
	loss_policy_3: 0.05231
	accuracy_policy_3: 0.75297
	loss_value_3: 0.06443
	loss_reward_3: 0.01122
	loss_policy_4: 0.05236
	accuracy_policy_4: 0.7543
	loss_value_4: 0.06602
	loss_reward_4: 0.01295
	loss_policy_5: 0.05253
	accuracy_policy_5: 0.75402
	loss_value_5: 0.06773
	loss_reward_5: 0.0139
	loss_policy: 0.52279
	loss_value: 0.61503
	loss_reward: 0.05723
[2025-05-11 11:31:47] nn step 3400, lr: 0.1.
	loss_policy_0: 0.28133
	accuracy_policy_0: 0.75352
	loss_value_0: 0.31601
	loss_policy_1: 0.05624
	accuracy_policy_1: 0.75484
	loss_value_1: 0.06567
	loss_reward_1: 0.00845
	loss_policy_2: 0.05652
	accuracy_policy_2: 0.75496
	loss_value_2: 0.06768
	loss_reward_2: 0.01252
	loss_policy_3: 0.05644
	accuracy_policy_3: 0.75457
	loss_value_3: 0.06965
	loss_reward_3: 0.01186
	loss_policy_4: 0.05658
	accuracy_policy_4: 0.75508
	loss_value_4: 0.0715
	loss_reward_4: 0.01419
	loss_policy_5: 0.05679
	accuracy_policy_5: 0.75191
	loss_value_5: 0.07306
	loss_reward_5: 0.01488
	loss_policy: 0.56391
	loss_value: 0.66357
	loss_reward: 0.0619
Optimization_Done 3400
[2025-05-11 11:33:00] [command] train weight_iter_3400.pkl 1 18
[2025-05-11 11:33:09] nn step 3450, lr: 0.1.
	loss_policy_0: 0.2702
	accuracy_policy_0: 0.75465
	loss_value_0: 0.31862
	loss_policy_1: 0.05408
	accuracy_policy_1: 0.76004
	loss_value_1: 0.06547
	loss_reward_1: 0.00773
	loss_policy_2: 0.05411
	accuracy_policy_2: 0.75562
	loss_value_2: 0.06704
	loss_reward_2: 0.01178
	loss_policy_3: 0.05408
	accuracy_policy_3: 0.75707
	loss_value_3: 0.06864
	loss_reward_3: 0.01182
	loss_policy_4: 0.05408
	accuracy_policy_4: 0.75605
	loss_value_4: 0.07043
	loss_reward_4: 0.01296
	loss_policy_5: 0.05421
	accuracy_policy_5: 0.75598
	loss_value_5: 0.07222
	loss_reward_5: 0.0142
	loss_policy: 0.54076
	loss_value: 0.66242
	loss_reward: 0.05848
[2025-05-11 11:33:17] nn step 3500, lr: 0.1.
	loss_policy_0: 0.25534
	accuracy_policy_0: 0.75391
	loss_value_0: 0.29336
	loss_policy_1: 0.051
	accuracy_policy_1: 0.75809
	loss_value_1: 0.06007
	loss_reward_1: 0.00751
	loss_policy_2: 0.05116
	accuracy_policy_2: 0.75605
	loss_value_2: 0.06182
	loss_reward_2: 0.01134
	loss_policy_3: 0.05093
	accuracy_policy_3: 0.7532
	loss_value_3: 0.06364
	loss_reward_3: 0.01136
	loss_policy_4: 0.05108
	accuracy_policy_4: 0.75238
	loss_value_4: 0.06532
	loss_reward_4: 0.01234
	loss_policy_5: 0.05117
	accuracy_policy_5: 0.7532
	loss_value_5: 0.06725
	loss_reward_5: 0.01362
	loss_policy: 0.5107
	loss_value: 0.61147
	loss_reward: 0.05617
[2025-05-11 11:33:25] nn step 3550, lr: 0.1.
	loss_policy_0: 0.24806
	accuracy_policy_0: 0.75133
	loss_value_0: 0.27852
	loss_policy_1: 0.04942
	accuracy_policy_1: 0.75289
	loss_value_1: 0.05771
	loss_reward_1: 0.00676
	loss_policy_2: 0.04932
	accuracy_policy_2: 0.75395
	loss_value_2: 0.05962
	loss_reward_2: 0.01062
	loss_policy_3: 0.04967
	accuracy_policy_3: 0.75379
	loss_value_3: 0.06124
	loss_reward_3: 0.01073
	loss_policy_4: 0.04954
	accuracy_policy_4: 0.75328
	loss_value_4: 0.06275
	loss_reward_4: 0.01175
	loss_policy_5: 0.04957
	accuracy_policy_5: 0.75387
	loss_value_5: 0.06435
	loss_reward_5: 0.01278
	loss_policy: 0.49559
	loss_value: 0.58418
	loss_reward: 0.05264
[2025-05-11 11:33:32] nn step 3600, lr: 0.1.
	loss_policy_0: 0.27723
	accuracy_policy_0: 0.75477
	loss_value_0: 0.31416
	loss_policy_1: 0.05592
	accuracy_policy_1: 0.75676
	loss_value_1: 0.06513
	loss_reward_1: 0.0081
	loss_policy_2: 0.05589
	accuracy_policy_2: 0.75465
	loss_value_2: 0.06705
	loss_reward_2: 0.01179
	loss_policy_3: 0.05579
	accuracy_policy_3: 0.75438
	loss_value_3: 0.06888
	loss_reward_3: 0.01152
	loss_policy_4: 0.05578
	accuracy_policy_4: 0.75434
	loss_value_4: 0.07085
	loss_reward_4: 0.01319
	loss_policy_5: 0.05587
	accuracy_policy_5: 0.75121
	loss_value_5: 0.07253
	loss_reward_5: 0.0146
	loss_policy: 0.55649
	loss_value: 0.6586
	loss_reward: 0.0592
Optimization_Done 3600
[2025-05-11 11:34:48] [command] train weight_iter_3600.pkl 1 19
[2025-05-11 11:34:56] nn step 3650, lr: 0.1.
	loss_policy_0: 0.25287
	accuracy_policy_0: 0.75125
	loss_value_0: 0.29841
	loss_policy_1: 0.05054
	accuracy_policy_1: 0.75348
	loss_value_1: 0.06139
	loss_reward_1: 0.00682
	loss_policy_2: 0.0508
	accuracy_policy_2: 0.75008
	loss_value_2: 0.06318
	loss_reward_2: 0.01077
	loss_policy_3: 0.05074
	accuracy_policy_3: 0.75176
	loss_value_3: 0.06494
	loss_reward_3: 0.01102
	loss_policy_4: 0.05103
	accuracy_policy_4: 0.74613
	loss_value_4: 0.06625
	loss_reward_4: 0.012
	loss_policy_5: 0.05137
	accuracy_policy_5: 0.74523
	loss_value_5: 0.0678
	loss_reward_5: 0.01346
	loss_policy: 0.50736
	loss_value: 0.62196
	loss_reward: 0.05407
[2025-05-11 11:35:05] nn step 3700, lr: 0.1.
	loss_policy_0: 0.27644
	accuracy_policy_0: 0.75273
	loss_value_0: 0.31855
	loss_policy_1: 0.0557
	accuracy_policy_1: 0.74992
	loss_value_1: 0.06599
	loss_reward_1: 0.00769
	loss_policy_2: 0.05572
	accuracy_policy_2: 0.74855
	loss_value_2: 0.06785
	loss_reward_2: 0.01187
	loss_policy_3: 0.05565
	accuracy_policy_3: 0.75094
	loss_value_3: 0.06942
	loss_reward_3: 0.01207
	loss_policy_4: 0.05593
	accuracy_policy_4: 0.7475
	loss_value_4: 0.07143
	loss_reward_4: 0.01313
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.74871
	loss_value_5: 0.07292
	loss_reward_5: 0.01459
	loss_policy: 0.55532
	loss_value: 0.66615
	loss_reward: 0.05935
[2025-05-11 11:35:13] nn step 3750, lr: 0.1.
	loss_policy_0: 0.24977
	accuracy_policy_0: 0.75078
	loss_value_0: 0.28339
	loss_policy_1: 0.0501
	accuracy_policy_1: 0.75695
	loss_value_1: 0.05875
	loss_reward_1: 0.00679
	loss_policy_2: 0.05009
	accuracy_policy_2: 0.75629
	loss_value_2: 0.0604
	loss_reward_2: 0.01024
	loss_policy_3: 0.05007
	accuracy_policy_3: 0.75164
	loss_value_3: 0.06188
	loss_reward_3: 0.01084
	loss_policy_4: 0.05022
	accuracy_policy_4: 0.7527
	loss_value_4: 0.06365
	loss_reward_4: 0.01199
	loss_policy_5: 0.05021
	accuracy_policy_5: 0.75164
	loss_value_5: 0.06539
	loss_reward_5: 0.01321
	loss_policy: 0.50046
	loss_value: 0.59346
	loss_reward: 0.05308
[2025-05-11 11:35:22] nn step 3800, lr: 0.1.
	loss_policy_0: 0.28254
	accuracy_policy_0: 0.75395
	loss_value_0: 0.32796
	loss_policy_1: 0.05659
	accuracy_policy_1: 0.75535
	loss_value_1: 0.0671
	loss_reward_1: 0.00777
	loss_policy_2: 0.05681
	accuracy_policy_2: 0.75258
	loss_value_2: 0.06912
	loss_reward_2: 0.01238
	loss_policy_3: 0.05677
	accuracy_policy_3: 0.75254
	loss_value_3: 0.07109
	loss_reward_3: 0.01198
	loss_policy_4: 0.05698
	accuracy_policy_4: 0.75281
	loss_value_4: 0.0728
	loss_reward_4: 0.01404
	loss_policy_5: 0.05722
	accuracy_policy_5: 0.75082
	loss_value_5: 0.07506
	loss_reward_5: 0.01474
	loss_policy: 0.5669
	loss_value: 0.68314
	loss_reward: 0.06091
Optimization_Done 3800
[2025-05-11 11:36:35] [command] train weight_iter_3800.pkl 1 20
[2025-05-11 11:36:48] nn step 3850, lr: 0.1.
	loss_policy_0: 0.26136
	accuracy_policy_0: 0.75508
	loss_value_0: 0.30762
	loss_policy_1: 0.05244
	accuracy_policy_1: 0.7559
	loss_value_1: 0.06317
	loss_reward_1: 0.00713
	loss_policy_2: 0.0523
	accuracy_policy_2: 0.75488
	loss_value_2: 0.0649
	loss_reward_2: 0.01115
	loss_policy_3: 0.05284
	accuracy_policy_3: 0.75207
	loss_value_3: 0.06668
	loss_reward_3: 0.01136
	loss_policy_4: 0.05254
	accuracy_policy_4: 0.7502
	loss_value_4: 0.06824
	loss_reward_4: 0.01299
	loss_policy_5: 0.05287
	accuracy_policy_5: 0.75055
	loss_value_5: 0.06982
	loss_reward_5: 0.01352
	loss_policy: 0.52435
	loss_value: 0.64042
	loss_reward: 0.05615
[2025-05-11 11:36:55] nn step 3900, lr: 0.1.
	loss_policy_0: 0.27726
	accuracy_policy_0: 0.75062
	loss_value_0: 0.31977
	loss_policy_1: 0.05556
	accuracy_policy_1: 0.75066
	loss_value_1: 0.06586
	loss_reward_1: 0.00764
	loss_policy_2: 0.05546
	accuracy_policy_2: 0.74965
	loss_value_2: 0.06792
	loss_reward_2: 0.01189
	loss_policy_3: 0.05565
	accuracy_policy_3: 0.75156
	loss_value_3: 0.0699
	loss_reward_3: 0.01186
	loss_policy_4: 0.05576
	accuracy_policy_4: 0.74496
	loss_value_4: 0.07191
	loss_reward_4: 0.0133
	loss_policy_5: 0.0558
	accuracy_policy_5: 0.75199
	loss_value_5: 0.07379
	loss_reward_5: 0.01417
	loss_policy: 0.5555
	loss_value: 0.66915
	loss_reward: 0.05885
[2025-05-11 11:37:03] nn step 3950, lr: 0.1.
	loss_policy_0: 0.28083
	accuracy_policy_0: 0.74672
	loss_value_0: 0.31804
	loss_policy_1: 0.05612
	accuracy_policy_1: 0.74672
	loss_value_1: 0.06593
	loss_reward_1: 0.00787
	loss_policy_2: 0.05639
	accuracy_policy_2: 0.75184
	loss_value_2: 0.06808
	loss_reward_2: 0.01161
	loss_policy_3: 0.05605
	accuracy_policy_3: 0.75074
	loss_value_3: 0.07019
	loss_reward_3: 0.01205
	loss_policy_4: 0.05629
	accuracy_policy_4: 0.74766
	loss_value_4: 0.07148
	loss_reward_4: 0.01356
	loss_policy_5: 0.05657
	accuracy_policy_5: 0.74867
	loss_value_5: 0.07336
	loss_reward_5: 0.01454
	loss_policy: 0.56224
	loss_value: 0.66708
	loss_reward: 0.05963
[2025-05-11 11:37:12] nn step 4000, lr: 0.1.
	loss_policy_0: 0.27013
	accuracy_policy_0: 0.74723
	loss_value_0: 0.30895
	loss_policy_1: 0.05404
	accuracy_policy_1: 0.74648
	loss_value_1: 0.06341
	loss_reward_1: 0.00734
	loss_policy_2: 0.0543
	accuracy_policy_2: 0.75191
	loss_value_2: 0.06541
	loss_reward_2: 0.0114
	loss_policy_3: 0.05428
	accuracy_policy_3: 0.75191
	loss_value_3: 0.06719
	loss_reward_3: 0.01163
	loss_policy_4: 0.05413
	accuracy_policy_4: 0.74617
	loss_value_4: 0.06886
	loss_reward_4: 0.01326
	loss_policy_5: 0.05448
	accuracy_policy_5: 0.745
	loss_value_5: 0.07088
	loss_reward_5: 0.01413
	loss_policy: 0.54136
	loss_value: 0.64471
	loss_reward: 0.05776
Optimization_Done 4000
[2025-05-11 11:38:24] [command] train weight_iter_4000.pkl 2 21
[2025-05-11 11:38:33] nn step 4050, lr: 0.1.
	loss_policy_0: 0.26563
	accuracy_policy_0: 0.77457
	loss_value_0: 0.31858
	loss_policy_1: 0.05319
	accuracy_policy_1: 0.77828
	loss_value_1: 0.06587
	loss_reward_1: 0.00715
	loss_policy_2: 0.05311
	accuracy_policy_2: 0.77754
	loss_value_2: 0.06742
	loss_reward_2: 0.01102
	loss_policy_3: 0.05314
	accuracy_policy_3: 0.77871
	loss_value_3: 0.06903
	loss_reward_3: 0.01118
	loss_policy_4: 0.05301
	accuracy_policy_4: 0.77793
	loss_value_4: 0.07092
	loss_reward_4: 0.01291
	loss_policy_5: 0.05359
	accuracy_policy_5: 0.77301
	loss_value_5: 0.07267
	loss_reward_5: 0.01401
	loss_policy: 0.53167
	loss_value: 0.66449
	loss_reward: 0.05627
[2025-05-11 11:38:42] nn step 4100, lr: 0.1.
	loss_policy_0: 0.27783
	accuracy_policy_0: 0.77402
	loss_value_0: 0.32785
	loss_policy_1: 0.05532
	accuracy_policy_1: 0.77949
	loss_value_1: 0.06763
	loss_reward_1: 0.00755
	loss_policy_2: 0.05569
	accuracy_policy_2: 0.77582
	loss_value_2: 0.06963
	loss_reward_2: 0.0117
	loss_policy_3: 0.05587
	accuracy_policy_3: 0.77137
	loss_value_3: 0.07144
	loss_reward_3: 0.01188
	loss_policy_4: 0.056
	accuracy_policy_4: 0.77047
	loss_value_4: 0.07362
	loss_reward_4: 0.0133
	loss_policy_5: 0.05634
	accuracy_policy_5: 0.76844
	loss_value_5: 0.07531
	loss_reward_5: 0.01444
	loss_policy: 0.55705
	loss_value: 0.6855
	loss_reward: 0.05886
[2025-05-11 11:38:49] nn step 4150, lr: 0.1.
	loss_policy_0: 0.26926
	accuracy_policy_0: 0.77707
	loss_value_0: 0.31097
	loss_policy_1: 0.05374
	accuracy_policy_1: 0.77949
	loss_value_1: 0.06408
	loss_reward_1: 0.00716
	loss_policy_2: 0.05407
	accuracy_policy_2: 0.77691
	loss_value_2: 0.06606
	loss_reward_2: 0.0112
	loss_policy_3: 0.05381
	accuracy_policy_3: 0.77395
	loss_value_3: 0.06785
	loss_reward_3: 0.01152
	loss_policy_4: 0.05417
	accuracy_policy_4: 0.77395
	loss_value_4: 0.06982
	loss_reward_4: 0.0126
	loss_policy_5: 0.05422
	accuracy_policy_5: 0.77688
	loss_value_5: 0.07178
	loss_reward_5: 0.01395
	loss_policy: 0.53926
	loss_value: 0.65056
	loss_reward: 0.05643
[2025-05-11 11:38:57] nn step 4200, lr: 0.1.
	loss_policy_0: 0.28836
	accuracy_policy_0: 0.77449
	loss_value_0: 0.33426
	loss_policy_1: 0.05764
	accuracy_policy_1: 0.77719
	loss_value_1: 0.06918
	loss_reward_1: 0.00785
	loss_policy_2: 0.05771
	accuracy_policy_2: 0.77891
	loss_value_2: 0.07107
	loss_reward_2: 0.01222
	loss_policy_3: 0.05785
	accuracy_policy_3: 0.77648
	loss_value_3: 0.07323
	loss_reward_3: 0.01233
	loss_policy_4: 0.05796
	accuracy_policy_4: 0.77262
	loss_value_4: 0.07525
	loss_reward_4: 0.01383
	loss_policy_5: 0.05817
	accuracy_policy_5: 0.77633
	loss_value_5: 0.07715
	loss_reward_5: 0.015
	loss_policy: 0.5777
	loss_value: 0.70015
	loss_reward: 0.06123
Optimization_Done 4200
[2025-05-11 11:40:12] [command] train weight_iter_4200.pkl 3 22
[2025-05-11 11:40:22] nn step 4250, lr: 0.1.
	loss_policy_0: 0.26619
	accuracy_policy_0: 0.77938
	loss_value_0: 0.31592
	loss_policy_1: 0.05358
	accuracy_policy_1: 0.77871
	loss_value_1: 0.06513
	loss_reward_1: 0.00688
	loss_policy_2: 0.05352
	accuracy_policy_2: 0.77969
	loss_value_2: 0.06739
	loss_reward_2: 0.01108
	loss_policy_3: 0.05348
	accuracy_policy_3: 0.77832
	loss_value_3: 0.06948
	loss_reward_3: 0.0115
	loss_policy_4: 0.05361
	accuracy_policy_4: 0.77844
	loss_value_4: 0.07115
	loss_reward_4: 0.01259
	loss_policy_5: 0.05366
	accuracy_policy_5: 0.77922
	loss_value_5: 0.07288
	loss_reward_5: 0.01381
	loss_policy: 0.53403
	loss_value: 0.66193
	loss_reward: 0.05586
[2025-05-11 11:40:30] nn step 4300, lr: 0.1.
	loss_policy_0: 0.27323
	accuracy_policy_0: 0.77926
	loss_value_0: 0.3229
	loss_policy_1: 0.05482
	accuracy_policy_1: 0.78055
	loss_value_1: 0.06683
	loss_reward_1: 0.00743
	loss_policy_2: 0.05521
	accuracy_policy_2: 0.7777
	loss_value_2: 0.06915
	loss_reward_2: 0.01142
	loss_policy_3: 0.05475
	accuracy_policy_3: 0.77914
	loss_value_3: 0.07119
	loss_reward_3: 0.01194
	loss_policy_4: 0.05501
	accuracy_policy_4: 0.77668
	loss_value_4: 0.07303
	loss_reward_4: 0.01324
	loss_policy_5: 0.0549
	accuracy_policy_5: 0.78184
	loss_value_5: 0.07483
	loss_reward_5: 0.01449
	loss_policy: 0.54791
	loss_value: 0.67794
	loss_reward: 0.05852
[2025-05-11 11:40:37] nn step 4350, lr: 0.1.
	loss_policy_0: 0.26215
	accuracy_policy_0: 0.77816
	loss_value_0: 0.30743
	loss_policy_1: 0.05251
	accuracy_policy_1: 0.78066
	loss_value_1: 0.0632
	loss_reward_1: 0.00694
	loss_policy_2: 0.05257
	accuracy_policy_2: 0.78109
	loss_value_2: 0.06512
	loss_reward_2: 0.01105
	loss_policy_3: 0.05267
	accuracy_policy_3: 0.77961
	loss_value_3: 0.06709
	loss_reward_3: 0.01103
	loss_policy_4: 0.0528
	accuracy_policy_4: 0.78055
	loss_value_4: 0.06886
	loss_reward_4: 0.01267
	loss_policy_5: 0.0528
	accuracy_policy_5: 0.78051
	loss_value_5: 0.07069
	loss_reward_5: 0.0137
	loss_policy: 0.52551
	loss_value: 0.64239
	loss_reward: 0.05539
[2025-05-11 11:40:46] nn step 4400, lr: 0.1.
	loss_policy_0: 0.28234
	accuracy_policy_0: 0.77332
	loss_value_0: 0.33085
	loss_policy_1: 0.05663
	accuracy_policy_1: 0.77801
	loss_value_1: 0.06825
	loss_reward_1: 0.00752
	loss_policy_2: 0.05666
	accuracy_policy_2: 0.78066
	loss_value_2: 0.07006
	loss_reward_2: 0.01182
	loss_policy_3: 0.05656
	accuracy_policy_3: 0.7768
	loss_value_3: 0.07229
	loss_reward_3: 0.01223
	loss_policy_4: 0.05689
	accuracy_policy_4: 0.77633
	loss_value_4: 0.0741
	loss_reward_4: 0.0136
	loss_policy_5: 0.05682
	accuracy_policy_5: 0.77809
	loss_value_5: 0.07611
	loss_reward_5: 0.01465
	loss_policy: 0.56591
	loss_value: 0.69166
	loss_reward: 0.05983
Optimization_Done 4400
[2025-05-11 11:42:00] [command] train weight_iter_4400.pkl 4 23
[2025-05-11 11:42:08] nn step 4450, lr: 0.1.
	loss_policy_0: 0.27612
	accuracy_policy_0: 0.76469
	loss_value_0: 0.33455
	loss_policy_1: 0.05535
	accuracy_policy_1: 0.76863
	loss_value_1: 0.06863
	loss_reward_1: 0.0075
	loss_policy_2: 0.05533
	accuracy_policy_2: 0.7673
	loss_value_2: 0.07086
	loss_reward_2: 0.01118
	loss_policy_3: 0.05585
	accuracy_policy_3: 0.76602
	loss_value_3: 0.0729
	loss_reward_3: 0.01173
	loss_policy_4: 0.05594
	accuracy_policy_4: 0.76633
	loss_value_4: 0.07512
	loss_reward_4: 0.01398
	loss_policy_5: 0.05568
	accuracy_policy_5: 0.77258
	loss_value_5: 0.07704
	loss_reward_5: 0.01425
	loss_policy: 0.55426
	loss_value: 0.69909
	loss_reward: 0.05864
[2025-05-11 11:42:16] nn step 4500, lr: 0.1.
	loss_policy_0: 0.28597
	accuracy_policy_0: 0.76668
	loss_value_0: 0.34054
	loss_policy_1: 0.05715
	accuracy_policy_1: 0.76953
	loss_value_1: 0.07015
	loss_reward_1: 0.0075
	loss_policy_2: 0.05745
	accuracy_policy_2: 0.77094
	loss_value_2: 0.07202
	loss_reward_2: 0.01189
	loss_policy_3: 0.05738
	accuracy_policy_3: 0.76977
	loss_value_3: 0.07442
	loss_reward_3: 0.01218
	loss_policy_4: 0.05746
	accuracy_policy_4: 0.76703
	loss_value_4: 0.07681
	loss_reward_4: 0.01403
	loss_policy_5: 0.0572
	accuracy_policy_5: 0.7732
	loss_value_5: 0.07861
	loss_reward_5: 0.01503
	loss_policy: 0.5726
	loss_value: 0.71255
	loss_reward: 0.06063
[2025-05-11 11:42:25] nn step 4550, lr: 0.1.
	loss_policy_0: 0.27724
	accuracy_policy_0: 0.77098
	loss_value_0: 0.32669
	loss_policy_1: 0.05536
	accuracy_policy_1: 0.77195
	loss_value_1: 0.06721
	loss_reward_1: 0.00671
	loss_policy_2: 0.05572
	accuracy_policy_2: 0.76766
	loss_value_2: 0.06945
	loss_reward_2: 0.01144
	loss_policy_3: 0.05567
	accuracy_policy_3: 0.77195
	loss_value_3: 0.07177
	loss_reward_3: 0.01181
	loss_policy_4: 0.05555
	accuracy_policy_4: 0.77109
	loss_value_4: 0.0741
	loss_reward_4: 0.01322
	loss_policy_5: 0.05572
	accuracy_policy_5: 0.76863
	loss_value_5: 0.0761
	loss_reward_5: 0.0145
	loss_policy: 0.55527
	loss_value: 0.68532
	loss_reward: 0.05768
[2025-05-11 11:42:32] nn step 4600, lr: 0.1.
	loss_policy_0: 0.27123
	accuracy_policy_0: 0.7741
	loss_value_0: 0.32095
	loss_policy_1: 0.05458
	accuracy_policy_1: 0.77219
	loss_value_1: 0.06604
	loss_reward_1: 0.00702
	loss_policy_2: 0.05464
	accuracy_policy_2: 0.77258
	loss_value_2: 0.06835
	loss_reward_2: 0.01096
	loss_policy_3: 0.05502
	accuracy_policy_3: 0.76945
	loss_value_3: 0.07032
	loss_reward_3: 0.01163
	loss_policy_4: 0.05494
	accuracy_policy_4: 0.77027
	loss_value_4: 0.07236
	loss_reward_4: 0.01291
	loss_policy_5: 0.05472
	accuracy_policy_5: 0.77023
	loss_value_5: 0.07436
	loss_reward_5: 0.0136
	loss_policy: 0.54513
	loss_value: 0.67236
	loss_reward: 0.05612
Optimization_Done 4600
[2025-05-11 11:43:48] [command] train weight_iter_4600.pkl 5 24
[2025-05-11 11:43:56] nn step 4650, lr: 0.1.
	loss_policy_0: 0.26378
	accuracy_policy_0: 0.76551
	loss_value_0: 0.3187
	loss_policy_1: 0.05284
	accuracy_policy_1: 0.76895
	loss_value_1: 0.06569
	loss_reward_1: 0.00664
	loss_policy_2: 0.05274
	accuracy_policy_2: 0.77168
	loss_value_2: 0.06776
	loss_reward_2: 0.01067
	loss_policy_3: 0.05295
	accuracy_policy_3: 0.77016
	loss_value_3: 0.06994
	loss_reward_3: 0.0113
	loss_policy_4: 0.05331
	accuracy_policy_4: 0.76734
	loss_value_4: 0.07169
	loss_reward_4: 0.01276
	loss_policy_5: 0.05319
	accuracy_policy_5: 0.76871
	loss_value_5: 0.07336
	loss_reward_5: 0.01368
	loss_policy: 0.52881
	loss_value: 0.66713
	loss_reward: 0.05505
[2025-05-11 11:44:04] nn step 4700, lr: 0.1.
	loss_policy_0: 0.27455
	accuracy_policy_0: 0.77043
	loss_value_0: 0.32917
	loss_policy_1: 0.0548
	accuracy_policy_1: 0.77094
	loss_value_1: 0.06769
	loss_reward_1: 0.007
	loss_policy_2: 0.05496
	accuracy_policy_2: 0.77301
	loss_value_2: 0.06967
	loss_reward_2: 0.01105
	loss_policy_3: 0.05516
	accuracy_policy_3: 0.76812
	loss_value_3: 0.0716
	loss_reward_3: 0.01127
	loss_policy_4: 0.05508
	accuracy_policy_4: 0.76887
	loss_value_4: 0.07362
	loss_reward_4: 0.01322
	loss_policy_5: 0.0549
	accuracy_policy_5: 0.77297
	loss_value_5: 0.07544
	loss_reward_5: 0.01357
	loss_policy: 0.54946
	loss_value: 0.68718
	loss_reward: 0.05611
[2025-05-11 11:44:13] nn step 4750, lr: 0.1.
	loss_policy_0: 0.26413
	accuracy_policy_0: 0.76926
	loss_value_0: 0.31248
	loss_policy_1: 0.05307
	accuracy_policy_1: 0.76367
	loss_value_1: 0.06471
	loss_reward_1: 0.00666
	loss_policy_2: 0.05281
	accuracy_policy_2: 0.7716
	loss_value_2: 0.06674
	loss_reward_2: 0.01046
	loss_policy_3: 0.05313
	accuracy_policy_3: 0.76574
	loss_value_3: 0.06862
	loss_reward_3: 0.01131
	loss_policy_4: 0.05324
	accuracy_policy_4: 0.76699
	loss_value_4: 0.07042
	loss_reward_4: 0.01256
	loss_policy_5: 0.0533
	accuracy_policy_5: 0.76754
	loss_value_5: 0.07217
	loss_reward_5: 0.01344
	loss_policy: 0.52968
	loss_value: 0.65514
	loss_reward: 0.05443
[2025-05-11 11:44:21] nn step 4800, lr: 0.1.
	loss_policy_0: 0.26696
	accuracy_policy_0: 0.77145
	loss_value_0: 0.3164
	loss_policy_1: 0.05331
	accuracy_policy_1: 0.77348
	loss_value_1: 0.0654
	loss_reward_1: 0.00653
	loss_policy_2: 0.05382
	accuracy_policy_2: 0.77008
	loss_value_2: 0.06755
	loss_reward_2: 0.01085
	loss_policy_3: 0.05368
	accuracy_policy_3: 0.77051
	loss_value_3: 0.06975
	loss_reward_3: 0.01127
	loss_policy_4: 0.05395
	accuracy_policy_4: 0.7673
	loss_value_4: 0.07169
	loss_reward_4: 0.01259
	loss_policy_5: 0.05399
	accuracy_policy_5: 0.76711
	loss_value_5: 0.07365
	loss_reward_5: 0.01362
	loss_policy: 0.53571
	loss_value: 0.66445
	loss_reward: 0.05486
Optimization_Done 4800
[2025-05-11 11:45:33] [command] train weight_iter_4800.pkl 6 25
[2025-05-11 11:45:42] nn step 4850, lr: 0.1.
	loss_policy_0: 0.27349
	accuracy_policy_0: 0.78863
	loss_value_0: 0.34356
	loss_policy_1: 0.05443
	accuracy_policy_1: 0.79031
	loss_value_1: 0.07068
	loss_reward_1: 0.00684
	loss_policy_2: 0.05451
	accuracy_policy_2: 0.79652
	loss_value_2: 0.07275
	loss_reward_2: 0.01178
	loss_policy_3: 0.05503
	accuracy_policy_3: 0.78879
	loss_value_3: 0.07496
	loss_reward_3: 0.01181
	loss_policy_4: 0.05486
	accuracy_policy_4: 0.79105
	loss_value_4: 0.07676
	loss_reward_4: 0.01374
	loss_policy_5: 0.05485
	accuracy_policy_5: 0.79328
	loss_value_5: 0.07859
	loss_reward_5: 0.01463
	loss_policy: 0.54717
	loss_value: 0.7173
	loss_reward: 0.05879
[2025-05-11 11:45:49] nn step 4900, lr: 0.1.
	loss_policy_0: 0.28572
	accuracy_policy_0: 0.78625
	loss_value_0: 0.352
	loss_policy_1: 0.05727
	accuracy_policy_1: 0.78883
	loss_value_1: 0.07244
	loss_reward_1: 0.00741
	loss_policy_2: 0.0578
	accuracy_policy_2: 0.7848
	loss_value_2: 0.07496
	loss_reward_2: 0.01168
	loss_policy_3: 0.05736
	accuracy_policy_3: 0.7875
	loss_value_3: 0.07691
	loss_reward_3: 0.01219
	loss_policy_4: 0.0574
	accuracy_policy_4: 0.78344
	loss_value_4: 0.07912
	loss_reward_4: 0.014
	loss_policy_5: 0.0576
	accuracy_policy_5: 0.79051
	loss_value_5: 0.0814
	loss_reward_5: 0.01489
	loss_policy: 0.57314
	loss_value: 0.73682
	loss_reward: 0.06016
[2025-05-11 11:45:57] nn step 4950, lr: 0.1.
	loss_policy_0: 0.26941
	accuracy_policy_0: 0.78941
	loss_value_0: 0.32783
	loss_policy_1: 0.05436
	accuracy_policy_1: 0.78645
	loss_value_1: 0.06742
	loss_reward_1: 0.00678
	loss_policy_2: 0.05439
	accuracy_policy_2: 0.78719
	loss_value_2: 0.07001
	loss_reward_2: 0.01101
	loss_policy_3: 0.05423
	accuracy_policy_3: 0.79004
	loss_value_3: 0.07232
	loss_reward_3: 0.01175
	loss_policy_4: 0.05425
	accuracy_policy_4: 0.78504
	loss_value_4: 0.07432
	loss_reward_4: 0.01336
	loss_policy_5: 0.05426
	accuracy_policy_5: 0.78445
	loss_value_5: 0.07615
	loss_reward_5: 0.01414
	loss_policy: 0.54089
	loss_value: 0.68804
	loss_reward: 0.05703
[2025-05-11 11:46:06] nn step 5000, lr: 0.1.
	loss_policy_0: 0.26833
	accuracy_policy_0: 0.78051
	loss_value_0: 0.32528
	loss_policy_1: 0.05359
	accuracy_policy_1: 0.78586
	loss_value_1: 0.06747
	loss_reward_1: 0.00672
	loss_policy_2: 0.05385
	accuracy_policy_2: 0.78145
	loss_value_2: 0.06952
	loss_reward_2: 0.01079
	loss_policy_3: 0.05402
	accuracy_policy_3: 0.78082
	loss_value_3: 0.07153
	loss_reward_3: 0.01123
	loss_policy_4: 0.05392
	accuracy_policy_4: 0.77777
	loss_value_4: 0.07343
	loss_reward_4: 0.01281
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.78086
	loss_value_5: 0.07546
	loss_reward_5: 0.01423
	loss_policy: 0.53792
	loss_value: 0.6827
	loss_reward: 0.05578
Optimization_Done 5000
[2025-05-11 11:47:22] [command] train weight_iter_5000.pkl 7 26
[2025-05-11 11:47:31] nn step 5050, lr: 0.1.
	loss_policy_0: 0.2582
	accuracy_policy_0: 0.79652
	loss_value_0: 0.33034
	loss_policy_1: 0.05183
	accuracy_policy_1: 0.79391
	loss_value_1: 0.06822
	loss_reward_1: 0.0066
	loss_policy_2: 0.05158
	accuracy_policy_2: 0.79723
	loss_value_2: 0.06999
	loss_reward_2: 0.01075
	loss_policy_3: 0.05181
	accuracy_policy_3: 0.79746
	loss_value_3: 0.07206
	loss_reward_3: 0.01128
	loss_policy_4: 0.05185
	accuracy_policy_4: 0.79941
	loss_value_4: 0.0738
	loss_reward_4: 0.01272
	loss_policy_5: 0.05197
	accuracy_policy_5: 0.79848
	loss_value_5: 0.07573
	loss_reward_5: 0.01373
	loss_policy: 0.51723
	loss_value: 0.69014
	loss_reward: 0.05508
[2025-05-11 11:47:39] nn step 5100, lr: 0.1.
	loss_policy_0: 0.25611
	accuracy_policy_0: 0.7966
	loss_value_0: 0.32204
	loss_policy_1: 0.05149
	accuracy_policy_1: 0.79508
	loss_value_1: 0.06612
	loss_reward_1: 0.00643
	loss_policy_2: 0.05158
	accuracy_policy_2: 0.79781
	loss_value_2: 0.06835
	loss_reward_2: 0.01053
	loss_policy_3: 0.0514
	accuracy_policy_3: 0.79941
	loss_value_3: 0.07056
	loss_reward_3: 0.01124
	loss_policy_4: 0.05178
	accuracy_policy_4: 0.79773
	loss_value_4: 0.07292
	loss_reward_4: 0.01291
	loss_policy_5: 0.05164
	accuracy_policy_5: 0.79773
	loss_value_5: 0.07471
	loss_reward_5: 0.01338
	loss_policy: 0.514
	loss_value: 0.6747
	loss_reward: 0.0545
[2025-05-11 11:47:46] nn step 5150, lr: 0.1.
	loss_policy_0: 0.26302
	accuracy_policy_0: 0.79395
	loss_value_0: 0.32977
	loss_policy_1: 0.05254
	accuracy_policy_1: 0.79887
	loss_value_1: 0.068
	loss_reward_1: 0.00632
	loss_policy_2: 0.05269
	accuracy_policy_2: 0.7948
	loss_value_2: 0.0704
	loss_reward_2: 0.01075
	loss_policy_3: 0.05265
	accuracy_policy_3: 0.80031
	loss_value_3: 0.07251
	loss_reward_3: 0.01127
	loss_policy_4: 0.05269
	accuracy_policy_4: 0.79918
	loss_value_4: 0.07455
	loss_reward_4: 0.01295
	loss_policy_5: 0.05263
	accuracy_policy_5: 0.79941
	loss_value_5: 0.0766
	loss_reward_5: 0.01406
	loss_policy: 0.52622
	loss_value: 0.69182
	loss_reward: 0.05535
[2025-05-11 11:47:55] nn step 5200, lr: 0.1.
	loss_policy_0: 0.25868
	accuracy_policy_0: 0.79078
	loss_value_0: 0.32334
	loss_policy_1: 0.05186
	accuracy_policy_1: 0.7907
	loss_value_1: 0.06654
	loss_reward_1: 0.00636
	loss_policy_2: 0.05165
	accuracy_policy_2: 0.79535
	loss_value_2: 0.06876
	loss_reward_2: 0.01043
	loss_policy_3: 0.05173
	accuracy_policy_3: 0.79691
	loss_value_3: 0.07089
	loss_reward_3: 0.01111
	loss_policy_4: 0.05171
	accuracy_policy_4: 0.79637
	loss_value_4: 0.07299
	loss_reward_4: 0.01342
	loss_policy_5: 0.05176
	accuracy_policy_5: 0.79793
	loss_value_5: 0.07483
	loss_reward_5: 0.01392
	loss_policy: 0.51739
	loss_value: 0.67734
	loss_reward: 0.05523
Optimization_Done 5200
[2025-05-11 11:49:09] [command] train weight_iter_5200.pkl 8 27
[2025-05-11 11:49:16] nn step 5250, lr: 0.1.
	loss_policy_0: 0.24823
	accuracy_policy_0: 0.79727
	loss_value_0: 0.32479
	loss_policy_1: 0.04978
	accuracy_policy_1: 0.7998
	loss_value_1: 0.06681
	loss_reward_1: 0.00638
	loss_policy_2: 0.04957
	accuracy_policy_2: 0.79965
	loss_value_2: 0.06874
	loss_reward_2: 0.0103
	loss_policy_3: 0.04994
	accuracy_policy_3: 0.79977
	loss_value_3: 0.07071
	loss_reward_3: 0.01077
	loss_policy_4: 0.05005
	accuracy_policy_4: 0.80004
	loss_value_4: 0.07243
	loss_reward_4: 0.01281
	loss_policy_5: 0.05014
	accuracy_policy_5: 0.79645
	loss_value_5: 0.07434
	loss_reward_5: 0.01355
	loss_policy: 0.49771
	loss_value: 0.67782
	loss_reward: 0.05381
[2025-05-11 11:49:25] nn step 5300, lr: 0.1.
	loss_policy_0: 0.26457
	accuracy_policy_0: 0.79391
	loss_value_0: 0.33482
	loss_policy_1: 0.05287
	accuracy_policy_1: 0.79516
	loss_value_1: 0.06914
	loss_reward_1: 0.00668
	loss_policy_2: 0.05319
	accuracy_policy_2: 0.79426
	loss_value_2: 0.07155
	loss_reward_2: 0.01059
	loss_policy_3: 0.05289
	accuracy_policy_3: 0.79414
	loss_value_3: 0.07384
	loss_reward_3: 0.01126
	loss_policy_4: 0.05332
	accuracy_policy_4: 0.79059
	loss_value_4: 0.07594
	loss_reward_4: 0.01318
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.7927
	loss_value_5: 0.07754
	loss_reward_5: 0.01385
	loss_policy: 0.53011
	loss_value: 0.70283
	loss_reward: 0.05557
[2025-05-11 11:49:33] nn step 5350, lr: 0.1.
	loss_policy_0: 0.25097
	accuracy_policy_0: 0.79691
	loss_value_0: 0.31776
	loss_policy_1: 0.05026
	accuracy_policy_1: 0.7977
	loss_value_1: 0.06586
	loss_reward_1: 0.00654
	loss_policy_2: 0.05038
	accuracy_policy_2: 0.79379
	loss_value_2: 0.06764
	loss_reward_2: 0.01019
	loss_policy_3: 0.04995
	accuracy_policy_3: 0.8018
	loss_value_3: 0.06981
	loss_reward_3: 0.01103
	loss_policy_4: 0.05028
	accuracy_policy_4: 0.7982
	loss_value_4: 0.07179
	loss_reward_4: 0.01273
	loss_policy_5: 0.05022
	accuracy_policy_5: 0.79574
	loss_value_5: 0.07379
	loss_reward_5: 0.0137
	loss_policy: 0.50206
	loss_value: 0.66664
	loss_reward: 0.05418
[2025-05-11 11:49:40] nn step 5400, lr: 0.1.
	loss_policy_0: 0.28414
	accuracy_policy_0: 0.79609
	loss_value_0: 0.36235
	loss_policy_1: 0.05698
	accuracy_policy_1: 0.79324
	loss_value_1: 0.07438
	loss_reward_1: 0.00714
	loss_policy_2: 0.05692
	accuracy_policy_2: 0.79648
	loss_value_2: 0.07683
	loss_reward_2: 0.01131
	loss_policy_3: 0.05707
	accuracy_policy_3: 0.80098
	loss_value_3: 0.0795
	loss_reward_3: 0.01221
	loss_policy_4: 0.05708
	accuracy_policy_4: 0.7975
	loss_value_4: 0.08139
	loss_reward_4: 0.01414
	loss_policy_5: 0.05726
	accuracy_policy_5: 0.79348
	loss_value_5: 0.08388
	loss_reward_5: 0.01483
	loss_policy: 0.56945
	loss_value: 0.75832
	loss_reward: 0.05963
Optimization_Done 5400
[2025-05-11 11:50:57] [command] train weight_iter_5400.pkl 9 28
[2025-05-11 11:51:05] nn step 5450, lr: 0.1.
	loss_policy_0: 0.27092
	accuracy_policy_0: 0.8052
	loss_value_0: 0.35495
	loss_policy_1: 0.05452
	accuracy_policy_1: 0.80027
	loss_value_1: 0.07302
	loss_reward_1: 0.00676
	loss_policy_2: 0.05464
	accuracy_policy_2: 0.80328
	loss_value_2: 0.0753
	loss_reward_2: 0.01104
	loss_policy_3: 0.05484
	accuracy_policy_3: 0.80309
	loss_value_3: 0.07735
	loss_reward_3: 0.01191
	loss_policy_4: 0.0548
	accuracy_policy_4: 0.80117
	loss_value_4: 0.07889
	loss_reward_4: 0.01348
	loss_policy_5: 0.05495
	accuracy_policy_5: 0.80051
	loss_value_5: 0.08098
	loss_reward_5: 0.01427
	loss_policy: 0.54467
	loss_value: 0.74048
	loss_reward: 0.05746
[2025-05-11 11:51:13] nn step 5500, lr: 0.1.
	loss_policy_0: 0.25883
	accuracy_policy_0: 0.80293
	loss_value_0: 0.33641
	loss_policy_1: 0.05222
	accuracy_policy_1: 0.79906
	loss_value_1: 0.06953
	loss_reward_1: 0.00642
	loss_policy_2: 0.05209
	accuracy_policy_2: 0.80086
	loss_value_2: 0.0718
	loss_reward_2: 0.01092
	loss_policy_3: 0.05192
	accuracy_policy_3: 0.80406
	loss_value_3: 0.07387
	loss_reward_3: 0.01128
	loss_policy_4: 0.0522
	accuracy_policy_4: 0.80246
	loss_value_4: 0.07571
	loss_reward_4: 0.01295
	loss_policy_5: 0.0523
	accuracy_policy_5: 0.8025
	loss_value_5: 0.07795
	loss_reward_5: 0.01416
	loss_policy: 0.51957
	loss_value: 0.70526
	loss_reward: 0.05573
[2025-05-11 11:51:22] nn step 5550, lr: 0.1.
	loss_policy_0: 0.24986
	accuracy_policy_0: 0.79633
	loss_value_0: 0.32034
	loss_policy_1: 0.04985
	accuracy_policy_1: 0.79828
	loss_value_1: 0.06596
	loss_reward_1: 0.00624
	loss_policy_2: 0.05018
	accuracy_policy_2: 0.79875
	loss_value_2: 0.0681
	loss_reward_2: 0.00999
	loss_policy_3: 0.05014
	accuracy_policy_3: 0.79363
	loss_value_3: 0.07018
	loss_reward_3: 0.01071
	loss_policy_4: 0.05009
	accuracy_policy_4: 0.7991
	loss_value_4: 0.07201
	loss_reward_4: 0.01246
	loss_policy_5: 0.04999
	accuracy_policy_5: 0.79836
	loss_value_5: 0.07394
	loss_reward_5: 0.01361
	loss_policy: 0.50012
	loss_value: 0.67053
	loss_reward: 0.05301
[2025-05-11 11:51:30] nn step 5600, lr: 0.1.
	loss_policy_0: 0.24367
	accuracy_policy_0: 0.79637
	loss_value_0: 0.30514
	loss_policy_1: 0.04874
	accuracy_policy_1: 0.79445
	loss_value_1: 0.0631
	loss_reward_1: 0.0059
	loss_policy_2: 0.04877
	accuracy_policy_2: 0.7957
	loss_value_2: 0.06518
	loss_reward_2: 0.00978
	loss_policy_3: 0.04884
	accuracy_policy_3: 0.79762
	loss_value_3: 0.06737
	loss_reward_3: 0.0102
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.79891
	loss_value_4: 0.06934
	loss_reward_4: 0.01197
	loss_policy_5: 0.04898
	accuracy_policy_5: 0.79801
	loss_value_5: 0.07127
	loss_reward_5: 0.01282
	loss_policy: 0.4876
	loss_value: 0.6414
	loss_reward: 0.05068
Optimization_Done 5600
[2025-05-11 11:52:44] [command] train weight_iter_5600.pkl 10 29
[2025-05-11 11:52:54] nn step 5650, lr: 0.1.
	loss_policy_0: 0.26561
	accuracy_policy_0: 0.79984
	loss_value_0: 0.34455
	loss_policy_1: 0.05351
	accuracy_policy_1: 0.79543
	loss_value_1: 0.07081
	loss_reward_1: 0.00647
	loss_policy_2: 0.05283
	accuracy_policy_2: 0.80141
	loss_value_2: 0.07297
	loss_reward_2: 0.01067
	loss_policy_3: 0.0533
	accuracy_policy_3: 0.79848
	loss_value_3: 0.07519
	loss_reward_3: 0.01144
	loss_policy_4: 0.05306
	accuracy_policy_4: 0.80051
	loss_value_4: 0.07712
	loss_reward_4: 0.01322
	loss_policy_5: 0.05319
	accuracy_policy_5: 0.79973
	loss_value_5: 0.07928
	loss_reward_5: 0.01414
	loss_policy: 0.5315
	loss_value: 0.71993
	loss_reward: 0.05594
[2025-05-11 11:53:00] nn step 5700, lr: 0.1.
	loss_policy_0: 0.28437
	accuracy_policy_0: 0.80039
	loss_value_0: 0.36925
	loss_policy_1: 0.05683
	accuracy_policy_1: 0.79855
	loss_value_1: 0.07603
	loss_reward_1: 0.00726
	loss_policy_2: 0.05659
	accuracy_policy_2: 0.79875
	loss_value_2: 0.07845
	loss_reward_2: 0.01184
	loss_policy_3: 0.05707
	accuracy_policy_3: 0.79941
	loss_value_3: 0.0807
	loss_reward_3: 0.01227
	loss_policy_4: 0.05741
	accuracy_policy_4: 0.79887
	loss_value_4: 0.08279
	loss_reward_4: 0.01425
	loss_policy_5: 0.05708
	accuracy_policy_5: 0.79883
	loss_value_5: 0.08511
	loss_reward_5: 0.0155
	loss_policy: 0.56936
	loss_value: 0.77233
	loss_reward: 0.06112
[2025-05-11 11:53:09] nn step 5750, lr: 0.1.
	loss_policy_0: 0.25504
	accuracy_policy_0: 0.80266
	loss_value_0: 0.32821
	loss_policy_1: 0.05149
	accuracy_policy_1: 0.79383
	loss_value_1: 0.06768
	loss_reward_1: 0.0063
	loss_policy_2: 0.05146
	accuracy_policy_2: 0.79711
	loss_value_2: 0.07023
	loss_reward_2: 0.01018
	loss_policy_3: 0.05141
	accuracy_policy_3: 0.7993
	loss_value_3: 0.0722
	loss_reward_3: 0.01104
	loss_policy_4: 0.05176
	accuracy_policy_4: 0.7977
	loss_value_4: 0.07421
	loss_reward_4: 0.01312
	loss_policy_5: 0.05152
	accuracy_policy_5: 0.7998
	loss_value_5: 0.07618
	loss_reward_5: 0.01363
	loss_policy: 0.51268
	loss_value: 0.68873
	loss_reward: 0.05426
[2025-05-11 11:53:17] nn step 5800, lr: 0.1.
	loss_policy_0: 0.25862
	accuracy_policy_0: 0.8002
	loss_value_0: 0.33032
	loss_policy_1: 0.05205
	accuracy_policy_1: 0.79316
	loss_value_1: 0.068
	loss_reward_1: 0.0064
	loss_policy_2: 0.05186
	accuracy_policy_2: 0.79625
	loss_value_2: 0.0702
	loss_reward_2: 0.01039
	loss_policy_3: 0.05186
	accuracy_policy_3: 0.80004
	loss_value_3: 0.07235
	loss_reward_3: 0.01119
	loss_policy_4: 0.05198
	accuracy_policy_4: 0.7993
	loss_value_4: 0.07435
	loss_reward_4: 0.01294
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.79664
	loss_value_5: 0.07638
	loss_reward_5: 0.01369
	loss_policy: 0.51818
	loss_value: 0.69159
	loss_reward: 0.05461
Optimization_Done 5800
[2025-05-11 11:54:31] [command] train weight_iter_5800.pkl 11 30
[2025-05-11 11:54:40] nn step 5850, lr: 0.1.
	loss_policy_0: 0.26642
	accuracy_policy_0: 0.80066
	loss_value_0: 0.34616
	loss_policy_1: 0.05352
	accuracy_policy_1: 0.80012
	loss_value_1: 0.07096
	loss_reward_1: 0.00647
	loss_policy_2: 0.0534
	accuracy_policy_2: 0.79844
	loss_value_2: 0.07355
	loss_reward_2: 0.01072
	loss_policy_3: 0.05353
	accuracy_policy_3: 0.79992
	loss_value_3: 0.07597
	loss_reward_3: 0.01124
	loss_policy_4: 0.05354
	accuracy_policy_4: 0.79934
	loss_value_4: 0.07775
	loss_reward_4: 0.01321
	loss_policy_5: 0.0536
	accuracy_policy_5: 0.7991
	loss_value_5: 0.07975
	loss_reward_5: 0.01392
	loss_policy: 0.534
	loss_value: 0.72414
	loss_reward: 0.05557
[2025-05-11 11:54:48] nn step 5900, lr: 0.1.
	loss_policy_0: 0.26798
	accuracy_policy_0: 0.79254
	loss_value_0: 0.35005
	loss_policy_1: 0.05362
	accuracy_policy_1: 0.79379
	loss_value_1: 0.07208
	loss_reward_1: 0.00658
	loss_policy_2: 0.05383
	accuracy_policy_2: 0.79516
	loss_value_2: 0.07406
	loss_reward_2: 0.01041
	loss_policy_3: 0.05383
	accuracy_policy_3: 0.79711
	loss_value_3: 0.07629
	loss_reward_3: 0.01161
	loss_policy_4: 0.05397
	accuracy_policy_4: 0.79523
	loss_value_4: 0.07857
	loss_reward_4: 0.01318
	loss_policy_5: 0.05401
	accuracy_policy_5: 0.79609
	loss_value_5: 0.0804
	loss_reward_5: 0.01442
	loss_policy: 0.53724
	loss_value: 0.73144
	loss_reward: 0.05619
[2025-05-11 11:54:55] nn step 5950, lr: 0.1.
	loss_policy_0: 0.2637
	accuracy_policy_0: 0.79969
	loss_value_0: 0.34073
	loss_policy_1: 0.05341
	accuracy_policy_1: 0.79664
	loss_value_1: 0.0703
	loss_reward_1: 0.00661
	loss_policy_2: 0.0533
	accuracy_policy_2: 0.79336
	loss_value_2: 0.07282
	loss_reward_2: 0.01037
	loss_policy_3: 0.05336
	accuracy_policy_3: 0.79617
	loss_value_3: 0.07496
	loss_reward_3: 0.01117
	loss_policy_4: 0.05339
	accuracy_policy_4: 0.79594
	loss_value_4: 0.07662
	loss_reward_4: 0.01326
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.79961
	loss_value_5: 0.07866
	loss_reward_5: 0.01427
	loss_policy: 0.53043
	loss_value: 0.7141
	loss_reward: 0.05568
[2025-05-11 11:55:04] nn step 6000, lr: 0.1.
	loss_policy_0: 0.26982
	accuracy_policy_0: 0.79777
	loss_value_0: 0.34922
	loss_policy_1: 0.05414
	accuracy_policy_1: 0.795
	loss_value_1: 0.07182
	loss_reward_1: 0.00675
	loss_policy_2: 0.05426
	accuracy_policy_2: 0.79824
	loss_value_2: 0.07387
	loss_reward_2: 0.01062
	loss_policy_3: 0.05405
	accuracy_policy_3: 0.80125
	loss_value_3: 0.07652
	loss_reward_3: 0.01133
	loss_policy_4: 0.05422
	accuracy_policy_4: 0.80133
	loss_value_4: 0.07853
	loss_reward_4: 0.0132
	loss_policy_5: 0.05445
	accuracy_policy_5: 0.79594
	loss_value_5: 0.08023
	loss_reward_5: 0.01438
	loss_policy: 0.54095
	loss_value: 0.73019
	loss_reward: 0.05629
Optimization_Done 6000
[2025-05-11 11:56:19] [command] train weight_iter_6000.pkl 12 31
[2025-05-11 11:56:28] nn step 6050, lr: 0.1.
	loss_policy_0: 0.25064
	accuracy_policy_0: 0.80086
	loss_value_0: 0.33107
	loss_policy_1: 0.05045
	accuracy_policy_1: 0.79988
	loss_value_1: 0.06796
	loss_reward_1: 0.00633
	loss_policy_2: 0.05005
	accuracy_policy_2: 0.80188
	loss_value_2: 0.07028
	loss_reward_2: 0.01018
	loss_policy_3: 0.05051
	accuracy_policy_3: 0.80359
	loss_value_3: 0.0725
	loss_reward_3: 0.01092
	loss_policy_4: 0.05017
	accuracy_policy_4: 0.80621
	loss_value_4: 0.07469
	loss_reward_4: 0.01268
	loss_policy_5: 0.05045
	accuracy_policy_5: 0.79992
	loss_value_5: 0.07672
	loss_reward_5: 0.01329
	loss_policy: 0.50226
	loss_value: 0.69323
	loss_reward: 0.0534
[2025-05-11 11:56:36] nn step 6100, lr: 0.1.
	loss_policy_0: 0.2424
	accuracy_policy_0: 0.79742
	loss_value_0: 0.31895
	loss_policy_1: 0.04857
	accuracy_policy_1: 0.79715
	loss_value_1: 0.06587
	loss_reward_1: 0.00587
	loss_policy_2: 0.04851
	accuracy_policy_2: 0.80027
	loss_value_2: 0.06765
	loss_reward_2: 0.00951
	loss_policy_3: 0.04848
	accuracy_policy_3: 0.80285
	loss_value_3: 0.06951
	loss_reward_3: 0.01022
	loss_policy_4: 0.04864
	accuracy_policy_4: 0.80551
	loss_value_4: 0.0712
	loss_reward_4: 0.01207
	loss_policy_5: 0.04865
	accuracy_policy_5: 0.80023
	loss_value_5: 0.07318
	loss_reward_5: 0.01281
	loss_policy: 0.48525
	loss_value: 0.66637
	loss_reward: 0.05048
[2025-05-11 11:56:45] nn step 6150, lr: 0.1.
	loss_policy_0: 0.26426
	accuracy_policy_0: 0.79559
	loss_value_0: 0.3401
	loss_policy_1: 0.05312
	accuracy_policy_1: 0.7966
	loss_value_1: 0.07025
	loss_reward_1: 0.0063
	loss_policy_2: 0.05305
	accuracy_policy_2: 0.80105
	loss_value_2: 0.07264
	loss_reward_2: 0.01006
	loss_policy_3: 0.05302
	accuracy_policy_3: 0.80211
	loss_value_3: 0.075
	loss_reward_3: 0.01062
	loss_policy_4: 0.05334
	accuracy_policy_4: 0.79852
	loss_value_4: 0.07721
	loss_reward_4: 0.01292
	loss_policy_5: 0.05328
	accuracy_policy_5: 0.79918
	loss_value_5: 0.07952
	loss_reward_5: 0.01389
	loss_policy: 0.53008
	loss_value: 0.7147
	loss_reward: 0.0538
[2025-05-11 11:56:52] nn step 6200, lr: 0.1.
	loss_policy_0: 0.25586
	accuracy_policy_0: 0.80285
	loss_value_0: 0.33343
	loss_policy_1: 0.05116
	accuracy_policy_1: 0.79461
	loss_value_1: 0.06882
	loss_reward_1: 0.00629
	loss_policy_2: 0.05111
	accuracy_policy_2: 0.80027
	loss_value_2: 0.07097
	loss_reward_2: 0.00991
	loss_policy_3: 0.05118
	accuracy_policy_3: 0.8018
	loss_value_3: 0.07339
	loss_reward_3: 0.01105
	loss_policy_4: 0.05135
	accuracy_policy_4: 0.80098
	loss_value_4: 0.07543
	loss_reward_4: 0.01259
	loss_policy_5: 0.05131
	accuracy_policy_5: 0.79988
	loss_value_5: 0.07746
	loss_reward_5: 0.01366
	loss_policy: 0.51196
	loss_value: 0.6995
	loss_reward: 0.05351
Optimization_Done 6200
[2025-05-11 11:58:11] [command] train weight_iter_6200.pkl 13 32
[2025-05-11 11:58:18] nn step 6250, lr: 0.1.
	loss_policy_0: 0.25543
	accuracy_policy_0: 0.79316
	loss_value_0: 0.34554
	loss_policy_1: 0.05112
	accuracy_policy_1: 0.795
	loss_value_1: 0.07107
	loss_reward_1: 0.00612
	loss_policy_2: 0.05099
	accuracy_policy_2: 0.79258
	loss_value_2: 0.07343
	loss_reward_2: 0.00968
	loss_policy_3: 0.05108
	accuracy_policy_3: 0.79602
	loss_value_3: 0.07525
	loss_reward_3: 0.01075
	loss_policy_4: 0.05101
	accuracy_policy_4: 0.7975
	loss_value_4: 0.07746
	loss_reward_4: 0.01254
	loss_policy_5: 0.05101
	accuracy_policy_5: 0.79551
	loss_value_5: 0.07914
	loss_reward_5: 0.01371
	loss_policy: 0.51064
	loss_value: 0.72189
	loss_reward: 0.0528
[2025-05-11 11:58:27] nn step 6300, lr: 0.1.
	loss_policy_0: 0.27844
	accuracy_policy_0: 0.80262
	loss_value_0: 0.37165
	loss_policy_1: 0.05567
	accuracy_policy_1: 0.79516
	loss_value_1: 0.07647
	loss_reward_1: 0.00719
	loss_policy_2: 0.05593
	accuracy_policy_2: 0.79684
	loss_value_2: 0.07899
	loss_reward_2: 0.01132
	loss_policy_3: 0.05576
	accuracy_policy_3: 0.79906
	loss_value_3: 0.08102
	loss_reward_3: 0.01218
	loss_policy_4: 0.05557
	accuracy_policy_4: 0.80145
	loss_value_4: 0.08314
	loss_reward_4: 0.01383
	loss_policy_5: 0.05567
	accuracy_policy_5: 0.79902
	loss_value_5: 0.08533
	loss_reward_5: 0.01502
	loss_policy: 0.55704
	loss_value: 0.77659
	loss_reward: 0.05954
[2025-05-11 11:58:35] nn step 6350, lr: 0.1.
	loss_policy_0: 0.26292
	accuracy_policy_0: 0.80504
	loss_value_0: 0.34846
	loss_policy_1: 0.0531
	accuracy_policy_1: 0.80039
	loss_value_1: 0.07183
	loss_reward_1: 0.00624
	loss_policy_2: 0.05297
	accuracy_policy_2: 0.80086
	loss_value_2: 0.07391
	loss_reward_2: 0.0101
	loss_policy_3: 0.0529
	accuracy_policy_3: 0.80363
	loss_value_3: 0.07618
	loss_reward_3: 0.01121
	loss_policy_4: 0.05327
	accuracy_policy_4: 0.80227
	loss_value_4: 0.07847
	loss_reward_4: 0.0127
	loss_policy_5: 0.05304
	accuracy_policy_5: 0.80281
	loss_value_5: 0.08061
	loss_reward_5: 0.01382
	loss_policy: 0.5282
	loss_value: 0.72946
	loss_reward: 0.05407
[2025-05-11 11:58:42] nn step 6400, lr: 0.1.
	loss_policy_0: 0.27478
	accuracy_policy_0: 0.80078
	loss_value_0: 0.36507
	loss_policy_1: 0.05505
	accuracy_policy_1: 0.79957
	loss_value_1: 0.07517
	loss_reward_1: 0.00685
	loss_policy_2: 0.0556
	accuracy_policy_2: 0.80047
	loss_value_2: 0.07791
	loss_reward_2: 0.01105
	loss_policy_3: 0.05504
	accuracy_policy_3: 0.80254
	loss_value_3: 0.08042
	loss_reward_3: 0.0114
	loss_policy_4: 0.05552
	accuracy_policy_4: 0.80273
	loss_value_4: 0.08253
	loss_reward_4: 0.01401
	loss_policy_5: 0.05595
	accuracy_policy_5: 0.7973
	loss_value_5: 0.08454
	loss_reward_5: 0.01485
	loss_policy: 0.55195
	loss_value: 0.76563
	loss_reward: 0.05815
Optimization_Done 6400
[2025-05-11 11:59:57] [command] train weight_iter_6400.pkl 14 33
[2025-05-11 12:00:04] nn step 6450, lr: 0.1.
	loss_policy_0: 0.26972
	accuracy_policy_0: 0.80121
	loss_value_0: 0.37045
	loss_policy_1: 0.05398
	accuracy_policy_1: 0.80078
	loss_value_1: 0.0759
	loss_reward_1: 0.00677
	loss_policy_2: 0.05377
	accuracy_policy_2: 0.80062
	loss_value_2: 0.07796
	loss_reward_2: 0.0103
	loss_policy_3: 0.05399
	accuracy_policy_3: 0.80555
	loss_value_3: 0.07994
	loss_reward_3: 0.01147
	loss_policy_4: 0.05412
	accuracy_policy_4: 0.80234
	loss_value_4: 0.08208
	loss_reward_4: 0.01353
	loss_policy_5: 0.05348
	accuracy_policy_5: 0.80375
	loss_value_5: 0.08419
	loss_reward_5: 0.01407
	loss_policy: 0.53907
	loss_value: 0.77052
	loss_reward: 0.05613
[2025-05-11 12:00:13] nn step 6500, lr: 0.1.
	loss_policy_0: 0.24567
	accuracy_policy_0: 0.79711
	loss_value_0: 0.32993
	loss_policy_1: 0.0491
	accuracy_policy_1: 0.79883
	loss_value_1: 0.06749
	loss_reward_1: 0.00598
	loss_policy_2: 0.04917
	accuracy_policy_2: 0.79551
	loss_value_2: 0.0697
	loss_reward_2: 0.00942
	loss_policy_3: 0.0489
	accuracy_policy_3: 0.80457
	loss_value_3: 0.07196
	loss_reward_3: 0.00996
	loss_policy_4: 0.04925
	accuracy_policy_4: 0.79863
	loss_value_4: 0.07399
	loss_reward_4: 0.01206
	loss_policy_5: 0.04932
	accuracy_policy_5: 0.79816
	loss_value_5: 0.07607
	loss_reward_5: 0.01276
	loss_policy: 0.49141
	loss_value: 0.68914
	loss_reward: 0.05018
[2025-05-11 12:00:21] nn step 6550, lr: 0.1.
	loss_policy_0: 0.26166
	accuracy_policy_0: 0.8016
	loss_value_0: 0.35145
	loss_policy_1: 0.05265
	accuracy_policy_1: 0.80074
	loss_value_1: 0.07249
	loss_reward_1: 0.00662
	loss_policy_2: 0.05267
	accuracy_policy_2: 0.80285
	loss_value_2: 0.07498
	loss_reward_2: 0.0103
	loss_policy_3: 0.05306
	accuracy_policy_3: 0.80273
	loss_value_3: 0.07722
	loss_reward_3: 0.01106
	loss_policy_4: 0.05286
	accuracy_policy_4: 0.8043
	loss_value_4: 0.07962
	loss_reward_4: 0.01288
	loss_policy_5: 0.05294
	accuracy_policy_5: 0.80172
	loss_value_5: 0.08176
	loss_reward_5: 0.01385
	loss_policy: 0.52585
	loss_value: 0.73752
	loss_reward: 0.05471
[2025-05-11 12:00:30] nn step 6600, lr: 0.1.
	loss_policy_0: 0.24712
	accuracy_policy_0: 0.80195
	loss_value_0: 0.32668
	loss_policy_1: 0.04931
	accuracy_policy_1: 0.80012
	loss_value_1: 0.06701
	loss_reward_1: 0.00589
	loss_policy_2: 0.04932
	accuracy_policy_2: 0.80207
	loss_value_2: 0.0696
	loss_reward_2: 0.0091
	loss_policy_3: 0.0494
	accuracy_policy_3: 0.80453
	loss_value_3: 0.07202
	loss_reward_3: 0.01048
	loss_policy_4: 0.04936
	accuracy_policy_4: 0.80074
	loss_value_4: 0.07398
	loss_reward_4: 0.01203
	loss_policy_5: 0.04967
	accuracy_policy_5: 0.80016
	loss_value_5: 0.07599
	loss_reward_5: 0.0132
	loss_policy: 0.49419
	loss_value: 0.68527
	loss_reward: 0.05069
Optimization_Done 6600
[2025-05-11 12:01:46] [command] train weight_iter_6600.pkl 15 34
[2025-05-11 12:01:55] nn step 6650, lr: 0.1.
	loss_policy_0: 0.24507
	accuracy_policy_0: 0.80531
	loss_value_0: 0.33139
	loss_policy_1: 0.04907
	accuracy_policy_1: 0.80707
	loss_value_1: 0.06853
	loss_reward_1: 0.00604
	loss_policy_2: 0.04923
	accuracy_policy_2: 0.80477
	loss_value_2: 0.07072
	loss_reward_2: 0.00951
	loss_policy_3: 0.04898
	accuracy_policy_3: 0.80605
	loss_value_3: 0.07293
	loss_reward_3: 0.01038
	loss_policy_4: 0.04921
	accuracy_policy_4: 0.80438
	loss_value_4: 0.07443
	loss_reward_4: 0.01203
	loss_policy_5: 0.04939
	accuracy_policy_5: 0.8007
	loss_value_5: 0.07612
	loss_reward_5: 0.01266
	loss_policy: 0.49095
	loss_value: 0.69411
	loss_reward: 0.05062
[2025-05-11 12:02:03] nn step 6700, lr: 0.1.
	loss_policy_0: 0.25573
	accuracy_policy_0: 0.80828
	loss_value_0: 0.34197
	loss_policy_1: 0.05112
	accuracy_policy_1: 0.80332
	loss_value_1: 0.07029
	loss_reward_1: 0.00628
	loss_policy_2: 0.05092
	accuracy_policy_2: 0.80309
	loss_value_2: 0.07282
	loss_reward_2: 0.0097
	loss_policy_3: 0.05092
	accuracy_policy_3: 0.8068
	loss_value_3: 0.07507
	loss_reward_3: 0.01084
	loss_policy_4: 0.05115
	accuracy_policy_4: 0.80762
	loss_value_4: 0.07747
	loss_reward_4: 0.01298
	loss_policy_5: 0.0513
	accuracy_policy_5: 0.80531
	loss_value_5: 0.07959
	loss_reward_5: 0.01366
	loss_policy: 0.51115
	loss_value: 0.71721
	loss_reward: 0.05346
[2025-05-11 12:02:11] nn step 6750, lr: 0.1.
	loss_policy_0: 0.25804
	accuracy_policy_0: 0.80566
	loss_value_0: 0.34365
	loss_policy_1: 0.05163
	accuracy_policy_1: 0.80418
	loss_value_1: 0.07078
	loss_reward_1: 0.00603
	loss_policy_2: 0.05154
	accuracy_policy_2: 0.80559
	loss_value_2: 0.07318
	loss_reward_2: 0.00959
	loss_policy_3: 0.05157
	accuracy_policy_3: 0.80621
	loss_value_3: 0.07567
	loss_reward_3: 0.01051
	loss_policy_4: 0.05183
	accuracy_policy_4: 0.80613
	loss_value_4: 0.07732
	loss_reward_4: 0.01249
	loss_policy_5: 0.05179
	accuracy_policy_5: 0.80543
	loss_value_5: 0.07966
	loss_reward_5: 0.01319
	loss_policy: 0.51639
	loss_value: 0.72026
	loss_reward: 0.05183
[2025-05-11 12:02:18] nn step 6800, lr: 0.1.
	loss_policy_0: 0.26218
	accuracy_policy_0: 0.80836
	loss_value_0: 0.34753
	loss_policy_1: 0.05275
	accuracy_policy_1: 0.80129
	loss_value_1: 0.07171
	loss_reward_1: 0.00634
	loss_policy_2: 0.05312
	accuracy_policy_2: 0.79887
	loss_value_2: 0.07406
	loss_reward_2: 0.01003
	loss_policy_3: 0.05256
	accuracy_policy_3: 0.80602
	loss_value_3: 0.07665
	loss_reward_3: 0.01083
	loss_policy_4: 0.05275
	accuracy_policy_4: 0.8034
	loss_value_4: 0.07859
	loss_reward_4: 0.0128
	loss_policy_5: 0.05304
	accuracy_policy_5: 0.80199
	loss_value_5: 0.08081
	loss_reward_5: 0.0141
	loss_policy: 0.5264
	loss_value: 0.72936
	loss_reward: 0.0541
Optimization_Done 6800
[2025-05-11 12:03:32] [command] train weight_iter_6800.pkl 16 35
[2025-05-11 12:03:41] nn step 6850, lr: 0.1.
	loss_policy_0: 0.25982
	accuracy_policy_0: 0.80039
	loss_value_0: 0.3556
	loss_policy_1: 0.0526
	accuracy_policy_1: 0.80023
	loss_value_1: 0.07316
	loss_reward_1: 0.00614
	loss_policy_2: 0.05257
	accuracy_policy_2: 0.79902
	loss_value_2: 0.07546
	loss_reward_2: 0.00995
	loss_policy_3: 0.05231
	accuracy_policy_3: 0.8041
	loss_value_3: 0.07746
	loss_reward_3: 0.0109
	loss_policy_4: 0.0523
	accuracy_policy_4: 0.80062
	loss_value_4: 0.07974
	loss_reward_4: 0.01258
	loss_policy_5: 0.05289
	accuracy_policy_5: 0.79574
	loss_value_5: 0.08188
	loss_reward_5: 0.01342
	loss_policy: 0.52248
	loss_value: 0.7433
	loss_reward: 0.05299
[2025-05-11 12:03:49] nn step 6900, lr: 0.1.
	loss_policy_0: 0.26053
	accuracy_policy_0: 0.79797
	loss_value_0: 0.34986
	loss_policy_1: 0.05213
	accuracy_policy_1: 0.79891
	loss_value_1: 0.07223
	loss_reward_1: 0.00621
	loss_policy_2: 0.05196
	accuracy_policy_2: 0.80156
	loss_value_2: 0.07409
	loss_reward_2: 0.00984
	loss_policy_3: 0.0521
	accuracy_policy_3: 0.80422
	loss_value_3: 0.07636
	loss_reward_3: 0.01082
	loss_policy_4: 0.05223
	accuracy_policy_4: 0.79996
	loss_value_4: 0.07834
	loss_reward_4: 0.0127
	loss_policy_5: 0.05262
	accuracy_policy_5: 0.79703
	loss_value_5: 0.08032
	loss_reward_5: 0.01392
	loss_policy: 0.52157
	loss_value: 0.73121
	loss_reward: 0.05349
[2025-05-11 12:03:57] nn step 6950, lr: 0.1.
	loss_policy_0: 0.26809
	accuracy_policy_0: 0.79727
	loss_value_0: 0.35543
	loss_policy_1: 0.05346
	accuracy_policy_1: 0.79578
	loss_value_1: 0.07299
	loss_reward_1: 0.00636
	loss_policy_2: 0.0535
	accuracy_policy_2: 0.80105
	loss_value_2: 0.07558
	loss_reward_2: 0.00999
	loss_policy_3: 0.05386
	accuracy_policy_3: 0.80211
	loss_value_3: 0.07773
	loss_reward_3: 0.0112
	loss_policy_4: 0.05405
	accuracy_policy_4: 0.80012
	loss_value_4: 0.08008
	loss_reward_4: 0.01273
	loss_policy_5: 0.0537
	accuracy_policy_5: 0.8027
	loss_value_5: 0.08249
	loss_reward_5: 0.01364
	loss_policy: 0.53666
	loss_value: 0.74431
	loss_reward: 0.05391
[2025-05-11 12:04:05] nn step 7000, lr: 0.1.
	loss_policy_0: 0.2586
	accuracy_policy_0: 0.80332
	loss_value_0: 0.34338
	loss_policy_1: 0.05227
	accuracy_policy_1: 0.8002
	loss_value_1: 0.07054
	loss_reward_1: 0.00602
	loss_policy_2: 0.05217
	accuracy_policy_2: 0.7984
	loss_value_2: 0.07312
	loss_reward_2: 0.00952
	loss_policy_3: 0.05222
	accuracy_policy_3: 0.80152
	loss_value_3: 0.07554
	loss_reward_3: 0.0108
	loss_policy_4: 0.05206
	accuracy_policy_4: 0.79797
	loss_value_4: 0.07761
	loss_reward_4: 0.01243
	loss_policy_5: 0.05232
	accuracy_policy_5: 0.79969
	loss_value_5: 0.08016
	loss_reward_5: 0.0136
	loss_policy: 0.51964
	loss_value: 0.72036
	loss_reward: 0.05237
Optimization_Done 7000
[2025-05-11 12:05:21] [command] train weight_iter_7000.pkl 17 36
[2025-05-11 12:05:30] nn step 7050, lr: 0.1.
	loss_policy_0: 0.25293
	accuracy_policy_0: 0.80133
	loss_value_0: 0.34242
	loss_policy_1: 0.05083
	accuracy_policy_1: 0.80422
	loss_value_1: 0.07046
	loss_reward_1: 0.00592
	loss_policy_2: 0.05107
	accuracy_policy_2: 0.80105
	loss_value_2: 0.07276
	loss_reward_2: 0.00949
	loss_policy_3: 0.05093
	accuracy_policy_3: 0.80363
	loss_value_3: 0.07488
	loss_reward_3: 0.01085
	loss_policy_4: 0.05114
	accuracy_policy_4: 0.8025
	loss_value_4: 0.0769
	loss_reward_4: 0.01207
	loss_policy_5: 0.05099
	accuracy_policy_5: 0.8016
	loss_value_5: 0.07914
	loss_reward_5: 0.0132
	loss_policy: 0.50788
	loss_value: 0.71657
	loss_reward: 0.05153
[2025-05-11 12:05:39] nn step 7100, lr: 0.1.
	loss_policy_0: 0.24427
	accuracy_policy_0: 0.80961
	loss_value_0: 0.32802
	loss_policy_1: 0.04882
	accuracy_policy_1: 0.80672
	loss_value_1: 0.06719
	loss_reward_1: 0.00556
	loss_policy_2: 0.04868
	accuracy_policy_2: 0.80629
	loss_value_2: 0.06927
	loss_reward_2: 0.00894
	loss_policy_3: 0.04867
	accuracy_policy_3: 0.80984
	loss_value_3: 0.07133
	loss_reward_3: 0.01004
	loss_policy_4: 0.04871
	accuracy_policy_4: 0.80977
	loss_value_4: 0.07335
	loss_reward_4: 0.01182
	loss_policy_5: 0.04887
	accuracy_policy_5: 0.8075
	loss_value_5: 0.07514
	loss_reward_5: 0.01218
	loss_policy: 0.48801
	loss_value: 0.6843
	loss_reward: 0.04854
[2025-05-11 12:05:46] nn step 7150, lr: 0.1.
	loss_policy_0: 0.25101
	accuracy_policy_0: 0.80359
	loss_value_0: 0.33218
	loss_policy_1: 0.05005
	accuracy_policy_1: 0.80418
	loss_value_1: 0.06805
	loss_reward_1: 0.00571
	loss_policy_2: 0.05008
	accuracy_policy_2: 0.80723
	loss_value_2: 0.07042
	loss_reward_2: 0.00914
	loss_policy_3: 0.05008
	accuracy_policy_3: 0.80891
	loss_value_3: 0.07284
	loss_reward_3: 0.01012
	loss_policy_4: 0.0504
	accuracy_policy_4: 0.80676
	loss_value_4: 0.0749
	loss_reward_4: 0.01211
	loss_policy_5: 0.05022
	accuracy_policy_5: 0.80746
	loss_value_5: 0.0771
	loss_reward_5: 0.01343
	loss_policy: 0.50184
	loss_value: 0.6955
	loss_reward: 0.05051
[2025-05-11 12:05:54] nn step 7200, lr: 0.1.
	loss_policy_0: 0.25852
	accuracy_policy_0: 0.80363
	loss_value_0: 0.34645
	loss_policy_1: 0.05185
	accuracy_policy_1: 0.79984
	loss_value_1: 0.07148
	loss_reward_1: 0.00641
	loss_policy_2: 0.05215
	accuracy_policy_2: 0.80508
	loss_value_2: 0.07376
	loss_reward_2: 0.00965
	loss_policy_3: 0.05257
	accuracy_policy_3: 0.80262
	loss_value_3: 0.07597
	loss_reward_3: 0.01054
	loss_policy_4: 0.05247
	accuracy_policy_4: 0.80172
	loss_value_4: 0.07833
	loss_reward_4: 0.01291
	loss_policy_5: 0.05222
	accuracy_policy_5: 0.80348
	loss_value_5: 0.0805
	loss_reward_5: 0.01434
	loss_policy: 0.51977
	loss_value: 0.72649
	loss_reward: 0.05384
Optimization_Done 7200
[2025-05-11 12:07:10] [command] train weight_iter_7200.pkl 18 37
[2025-05-11 12:07:18] nn step 7250, lr: 0.1.
	loss_policy_0: 0.25114
	accuracy_policy_0: 0.8107
	loss_value_0: 0.34755
	loss_policy_1: 0.05044
	accuracy_policy_1: 0.81168
	loss_value_1: 0.07144
	loss_reward_1: 0.00607
	loss_policy_2: 0.05026
	accuracy_policy_2: 0.81426
	loss_value_2: 0.07363
	loss_reward_2: 0.00938
	loss_policy_3: 0.05058
	accuracy_policy_3: 0.81277
	loss_value_3: 0.07571
	loss_reward_3: 0.01082
	loss_policy_4: 0.05042
	accuracy_policy_4: 0.81398
	loss_value_4: 0.07767
	loss_reward_4: 0.0125
	loss_policy_5: 0.05079
	accuracy_policy_5: 0.81359
	loss_value_5: 0.07961
	loss_reward_5: 0.01357
	loss_policy: 0.50361
	loss_value: 0.7256
	loss_reward: 0.05233
[2025-05-11 12:07:26] nn step 7300, lr: 0.1.
	loss_policy_0: 0.24553
	accuracy_policy_0: 0.81012
	loss_value_0: 0.33623
	loss_policy_1: 0.04934
	accuracy_policy_1: 0.80973
	loss_value_1: 0.06916
	loss_reward_1: 0.00583
	loss_policy_2: 0.04934
	accuracy_policy_2: 0.81062
	loss_value_2: 0.07159
	loss_reward_2: 0.00913
	loss_policy_3: 0.04916
	accuracy_policy_3: 0.81523
	loss_value_3: 0.07338
	loss_reward_3: 0.0106
	loss_policy_4: 0.04932
	accuracy_policy_4: 0.81652
	loss_value_4: 0.07566
	loss_reward_4: 0.01282
	loss_policy_5: 0.04941
	accuracy_policy_5: 0.81617
	loss_value_5: 0.07782
	loss_reward_5: 0.01329
	loss_policy: 0.49211
	loss_value: 0.70385
	loss_reward: 0.05168
[2025-05-11 12:07:34] nn step 7350, lr: 0.1.
	loss_policy_0: 0.24372
	accuracy_policy_0: 0.81168
	loss_value_0: 0.32482
	loss_policy_1: 0.04879
	accuracy_policy_1: 0.81016
	loss_value_1: 0.06697
	loss_reward_1: 0.00571
	loss_policy_2: 0.04885
	accuracy_policy_2: 0.81652
	loss_value_2: 0.06915
	loss_reward_2: 0.00867
	loss_policy_3: 0.04873
	accuracy_policy_3: 0.8118
	loss_value_3: 0.07171
	loss_reward_3: 0.00987
	loss_policy_4: 0.04896
	accuracy_policy_4: 0.81277
	loss_value_4: 0.07379
	loss_reward_4: 0.0121
	loss_policy_5: 0.04893
	accuracy_policy_5: 0.81246
	loss_value_5: 0.0757
	loss_reward_5: 0.0128
	loss_policy: 0.48798
	loss_value: 0.68214
	loss_reward: 0.04915
[2025-05-11 12:07:41] nn step 7400, lr: 0.1.
	loss_policy_0: 0.2405
	accuracy_policy_0: 0.80727
	loss_value_0: 0.32527
	loss_policy_1: 0.04813
	accuracy_policy_1: 0.8091
	loss_value_1: 0.06718
	loss_reward_1: 0.00561
	loss_policy_2: 0.04836
	accuracy_policy_2: 0.80934
	loss_value_2: 0.06908
	loss_reward_2: 0.00899
	loss_policy_3: 0.04818
	accuracy_policy_3: 0.81367
	loss_value_3: 0.07114
	loss_reward_3: 0.01033
	loss_policy_4: 0.04814
	accuracy_policy_4: 0.81539
	loss_value_4: 0.07347
	loss_reward_4: 0.01199
	loss_policy_5: 0.04827
	accuracy_policy_5: 0.81324
	loss_value_5: 0.07527
	loss_reward_5: 0.01304
	loss_policy: 0.48158
	loss_value: 0.6814
	loss_reward: 0.04994
Optimization_Done 7400
[2025-05-11 12:08:58] [command] train weight_iter_7400.pkl 19 38
[2025-05-11 12:09:05] nn step 7450, lr: 0.1.
	loss_policy_0: 0.23255
	accuracy_policy_0: 0.81379
	loss_value_0: 0.33356
	loss_policy_1: 0.04683
	accuracy_policy_1: 0.8127
	loss_value_1: 0.06828
	loss_reward_1: 0.00547
	loss_policy_2: 0.04698
	accuracy_policy_2: 0.81172
	loss_value_2: 0.07044
	loss_reward_2: 0.00872
	loss_policy_3: 0.04703
	accuracy_policy_3: 0.81559
	loss_value_3: 0.07235
	loss_reward_3: 0.00972
	loss_policy_4: 0.04703
	accuracy_policy_4: 0.81574
	loss_value_4: 0.07399
	loss_reward_4: 0.01144
	loss_policy_5: 0.04696
	accuracy_policy_5: 0.81199
	loss_value_5: 0.07599
	loss_reward_5: 0.01238
	loss_policy: 0.46738
	loss_value: 0.6946
	loss_reward: 0.04772
[2025-05-11 12:09:14] nn step 7500, lr: 0.1.
	loss_policy_0: 0.22716
	accuracy_policy_0: 0.81559
	loss_value_0: 0.31605
	loss_policy_1: 0.04541
	accuracy_policy_1: 0.8118
	loss_value_1: 0.06476
	loss_reward_1: 0.00524
	loss_policy_2: 0.04584
	accuracy_policy_2: 0.81387
	loss_value_2: 0.06699
	loss_reward_2: 0.00841
	loss_policy_3: 0.04563
	accuracy_policy_3: 0.81363
	loss_value_3: 0.06903
	loss_reward_3: 0.00953
	loss_policy_4: 0.04584
	accuracy_policy_4: 0.80918
	loss_value_4: 0.07101
	loss_reward_4: 0.01092
	loss_policy_5: 0.04612
	accuracy_policy_5: 0.81133
	loss_value_5: 0.07283
	loss_reward_5: 0.01205
	loss_policy: 0.456
	loss_value: 0.66067
	loss_reward: 0.04616
[2025-05-11 12:09:22] nn step 7550, lr: 0.1.
	loss_policy_0: 0.26534
	accuracy_policy_0: 0.80992
	loss_value_0: 0.36514
	loss_policy_1: 0.05289
	accuracy_policy_1: 0.81352
	loss_value_1: 0.07478
	loss_reward_1: 0.0061
	loss_policy_2: 0.05283
	accuracy_policy_2: 0.81332
	loss_value_2: 0.07705
	loss_reward_2: 0.00977
	loss_policy_3: 0.05296
	accuracy_policy_3: 0.8168
	loss_value_3: 0.07949
	loss_reward_3: 0.01071
	loss_policy_4: 0.05296
	accuracy_policy_4: 0.81672
	loss_value_4: 0.08203
	loss_reward_4: 0.01332
	loss_policy_5: 0.053
	accuracy_policy_5: 0.81531
	loss_value_5: 0.08465
	loss_reward_5: 0.01408
	loss_policy: 0.52997
	loss_value: 0.76314
	loss_reward: 0.05399
[2025-05-11 12:09:31] nn step 7600, lr: 0.1.
	loss_policy_0: 0.23602
	accuracy_policy_0: 0.81148
	loss_value_0: 0.32526
	loss_policy_1: 0.04751
	accuracy_policy_1: 0.81059
	loss_value_1: 0.06683
	loss_reward_1: 0.00567
	loss_policy_2: 0.04763
	accuracy_policy_2: 0.81379
	loss_value_2: 0.06879
	loss_reward_2: 0.00876
	loss_policy_3: 0.04734
	accuracy_policy_3: 0.81785
	loss_value_3: 0.07101
	loss_reward_3: 0.01013
	loss_policy_4: 0.04738
	accuracy_policy_4: 0.81902
	loss_value_4: 0.07316
	loss_reward_4: 0.01172
	loss_policy_5: 0.04767
	accuracy_policy_5: 0.81746
	loss_value_5: 0.07546
	loss_reward_5: 0.01305
	loss_policy: 0.47357
	loss_value: 0.68051
	loss_reward: 0.04933
Optimization_Done 7600
[2025-05-11 12:10:47] [command] train weight_iter_7600.pkl 20 39
[2025-05-11 12:10:56] nn step 7650, lr: 0.1.
	loss_policy_0: 0.24465
	accuracy_policy_0: 0.81719
	loss_value_0: 0.34462
	loss_policy_1: 0.04888
	accuracy_policy_1: 0.81602
	loss_value_1: 0.07098
	loss_reward_1: 0.0056
	loss_policy_2: 0.04908
	accuracy_policy_2: 0.81625
	loss_value_2: 0.07324
	loss_reward_2: 0.00862
	loss_policy_3: 0.04892
	accuracy_policy_3: 0.81934
	loss_value_3: 0.07513
	loss_reward_3: 0.01012
	loss_policy_4: 0.04907
	accuracy_policy_4: 0.82152
	loss_value_4: 0.07752
	loss_reward_4: 0.01186
	loss_policy_5: 0.04904
	accuracy_policy_5: 0.82164
	loss_value_5: 0.07985
	loss_reward_5: 0.01286
	loss_policy: 0.48964
	loss_value: 0.72133
	loss_reward: 0.04907
[2025-05-11 12:11:03] nn step 7700, lr: 0.1.
	loss_policy_0: 0.22998
	accuracy_policy_0: 0.81773
	loss_value_0: 0.31995
	loss_policy_1: 0.04631
	accuracy_policy_1: 0.81906
	loss_value_1: 0.06607
	loss_reward_1: 0.00521
	loss_policy_2: 0.04616
	accuracy_policy_2: 0.82312
	loss_value_2: 0.06822
	loss_reward_2: 0.00852
	loss_policy_3: 0.04621
	accuracy_policy_3: 0.81898
	loss_value_3: 0.07057
	loss_reward_3: 0.00922
	loss_policy_4: 0.04614
	accuracy_policy_4: 0.82297
	loss_value_4: 0.07278
	loss_reward_4: 0.01102
	loss_policy_5: 0.04637
	accuracy_policy_5: 0.82445
	loss_value_5: 0.07489
	loss_reward_5: 0.01222
	loss_policy: 0.46118
	loss_value: 0.67248
	loss_reward: 0.04619
[2025-05-11 12:11:12] nn step 7750, lr: 0.1.
	loss_policy_0: 0.23879
	accuracy_policy_0: 0.82094
	loss_value_0: 0.33247
	loss_policy_1: 0.04793
	accuracy_policy_1: 0.8225
	loss_value_1: 0.06852
	loss_reward_1: 0.00549
	loss_policy_2: 0.04771
	accuracy_policy_2: 0.82273
	loss_value_2: 0.07058
	loss_reward_2: 0.00853
	loss_policy_3: 0.04797
	accuracy_policy_3: 0.82531
	loss_value_3: 0.07273
	loss_reward_3: 0.00972
	loss_policy_4: 0.04779
	accuracy_policy_4: 0.82355
	loss_value_4: 0.07509
	loss_reward_4: 0.01161
	loss_policy_5: 0.04797
	accuracy_policy_5: 0.82602
	loss_value_5: 0.07738
	loss_reward_5: 0.01283
	loss_policy: 0.47816
	loss_value: 0.69677
	loss_reward: 0.04818
[2025-05-11 12:11:20] nn step 7800, lr: 0.1.
	loss_policy_0: 0.25839
	accuracy_policy_0: 0.81785
	loss_value_0: 0.36125
	loss_policy_1: 0.05209
	accuracy_policy_1: 0.81641
	loss_value_1: 0.07439
	loss_reward_1: 0.00606
	loss_policy_2: 0.05138
	accuracy_policy_2: 0.82367
	loss_value_2: 0.07679
	loss_reward_2: 0.00911
	loss_policy_3: 0.052
	accuracy_policy_3: 0.8257
	loss_value_3: 0.07895
	loss_reward_3: 0.01062
	loss_policy_4: 0.05197
	accuracy_policy_4: 0.82523
	loss_value_4: 0.08115
	loss_reward_4: 0.01262
	loss_policy_5: 0.05212
	accuracy_policy_5: 0.82359
	loss_value_5: 0.0835
	loss_reward_5: 0.01426
	loss_policy: 0.51795
	loss_value: 0.75604
	loss_reward: 0.05266
Optimization_Done 7800
[2025-05-11 12:12:34] [command] train weight_iter_7800.pkl 21 40
[2025-05-11 12:12:43] nn step 7850, lr: 0.1.
	loss_policy_0: 0.24664
	accuracy_policy_0: 0.8323
	loss_value_0: 0.35059
	loss_policy_1: 0.04952
	accuracy_policy_1: 0.83016
	loss_value_1: 0.07185
	loss_reward_1: 0.0057
	loss_policy_2: 0.04953
	accuracy_policy_2: 0.83062
	loss_value_2: 0.07432
	loss_reward_2: 0.00852
	loss_policy_3: 0.04972
	accuracy_policy_3: 0.83082
	loss_value_3: 0.07628
	loss_reward_3: 0.01028
	loss_policy_4: 0.04937
	accuracy_policy_4: 0.83645
	loss_value_4: 0.07846
	loss_reward_4: 0.01225
	loss_policy_5: 0.04957
	accuracy_policy_5: 0.83641
	loss_value_5: 0.08058
	loss_reward_5: 0.01314
	loss_policy: 0.49436
	loss_value: 0.73208
	loss_reward: 0.0499
[2025-05-11 12:12:51] nn step 7900, lr: 0.1.
	loss_policy_0: 0.2425
	accuracy_policy_0: 0.83016
	loss_value_0: 0.34206
	loss_policy_1: 0.0488
	accuracy_policy_1: 0.82809
	loss_value_1: 0.07024
	loss_reward_1: 0.00547
	loss_policy_2: 0.04845
	accuracy_policy_2: 0.83273
	loss_value_2: 0.07275
	loss_reward_2: 0.00845
	loss_policy_3: 0.04852
	accuracy_policy_3: 0.83387
	loss_value_3: 0.07479
	loss_reward_3: 0.00991
	loss_policy_4: 0.04895
	accuracy_policy_4: 0.83656
	loss_value_4: 0.07708
	loss_reward_4: 0.0116
	loss_policy_5: 0.04865
	accuracy_policy_5: 0.83492
	loss_value_5: 0.07906
	loss_reward_5: 0.01283
	loss_policy: 0.48587
	loss_value: 0.71598
	loss_reward: 0.04826
[2025-05-11 12:12:58] nn step 7950, lr: 0.1.
	loss_policy_0: 0.25152
	accuracy_policy_0: 0.83012
	loss_value_0: 0.35294
	loss_policy_1: 0.05015
	accuracy_policy_1: 0.82875
	loss_value_1: 0.07238
	loss_reward_1: 0.00562
	loss_policy_2: 0.05061
	accuracy_policy_2: 0.82934
	loss_value_2: 0.075
	loss_reward_2: 0.00876
	loss_policy_3: 0.05023
	accuracy_policy_3: 0.83562
	loss_value_3: 0.07757
	loss_reward_3: 0.01004
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.83652
	loss_value_4: 0.07934
	loss_reward_4: 0.01231
	loss_policy_5: 0.05009
	accuracy_policy_5: 0.83656
	loss_value_5: 0.08168
	loss_reward_5: 0.01322
	loss_policy: 0.50244
	loss_value: 0.73891
	loss_reward: 0.04995
[2025-05-11 12:13:07] nn step 8000, lr: 0.1.
	loss_policy_0: 0.2362
	accuracy_policy_0: 0.82648
	loss_value_0: 0.33279
	loss_policy_1: 0.04706
	accuracy_policy_1: 0.82703
	loss_value_1: 0.068
	loss_reward_1: 0.00542
	loss_policy_2: 0.0472
	accuracy_policy_2: 0.83027
	loss_value_2: 0.07028
	loss_reward_2: 0.00811
	loss_policy_3: 0.04682
	accuracy_policy_3: 0.83445
	loss_value_3: 0.07235
	loss_reward_3: 0.00983
	loss_policy_4: 0.04726
	accuracy_policy_4: 0.83438
	loss_value_4: 0.07418
	loss_reward_4: 0.01162
	loss_policy_5: 0.04734
	accuracy_policy_5: 0.83387
	loss_value_5: 0.0765
	loss_reward_5: 0.01292
	loss_policy: 0.47186
	loss_value: 0.69409
	loss_reward: 0.0479
Optimization_Done 8000
[2025-05-11 12:14:22] [command] train weight_iter_8000.pkl 22 41
[2025-05-11 12:14:31] nn step 8050, lr: 0.1.
	loss_policy_0: 0.24145
	accuracy_policy_0: 0.83055
	loss_value_0: 0.34687
	loss_policy_1: 0.0482
	accuracy_policy_1: 0.83328
	loss_value_1: 0.07103
	loss_reward_1: 0.00562
	loss_policy_2: 0.04838
	accuracy_policy_2: 0.83547
	loss_value_2: 0.07348
	loss_reward_2: 0.00871
	loss_policy_3: 0.04815
	accuracy_policy_3: 0.83598
	loss_value_3: 0.0758
	loss_reward_3: 0.01025
	loss_policy_4: 0.04848
	accuracy_policy_4: 0.83594
	loss_value_4: 0.07751
	loss_reward_4: 0.0119
	loss_policy_5: 0.04845
	accuracy_policy_5: 0.83723
	loss_value_5: 0.07984
	loss_reward_5: 0.01299
	loss_policy: 0.48311
	loss_value: 0.72452
	loss_reward: 0.04947
[2025-05-11 12:14:40] nn step 8100, lr: 0.1.
	loss_policy_0: 0.2432
	accuracy_policy_0: 0.82852
	loss_value_0: 0.34093
	loss_policy_1: 0.04835
	accuracy_policy_1: 0.83188
	loss_value_1: 0.06991
	loss_reward_1: 0.00574
	loss_policy_2: 0.04854
	accuracy_policy_2: 0.8325
	loss_value_2: 0.07252
	loss_reward_2: 0.00872
	loss_policy_3: 0.04849
	accuracy_policy_3: 0.83078
	loss_value_3: 0.07491
	loss_reward_3: 0.01026
	loss_policy_4: 0.04863
	accuracy_policy_4: 0.83395
	loss_value_4: 0.07721
	loss_reward_4: 0.01214
	loss_policy_5: 0.04856
	accuracy_policy_5: 0.83551
	loss_value_5: 0.07944
	loss_reward_5: 0.01319
	loss_policy: 0.48577
	loss_value: 0.71492
	loss_reward: 0.05004
[2025-05-11 12:14:47] nn step 8150, lr: 0.1.
	loss_policy_0: 0.23837
	accuracy_policy_0: 0.83133
	loss_value_0: 0.33377
	loss_policy_1: 0.04775
	accuracy_policy_1: 0.83297
	loss_value_1: 0.06893
	loss_reward_1: 0.00565
	loss_policy_2: 0.04804
	accuracy_policy_2: 0.8309
	loss_value_2: 0.07124
	loss_reward_2: 0.00845
	loss_policy_3: 0.04776
	accuracy_policy_3: 0.83152
	loss_value_3: 0.07334
	loss_reward_3: 0.00992
	loss_policy_4: 0.04777
	accuracy_policy_4: 0.8341
	loss_value_4: 0.07548
	loss_reward_4: 0.01172
	loss_policy_5: 0.04778
	accuracy_policy_5: 0.83527
	loss_value_5: 0.07766
	loss_reward_5: 0.01308
	loss_policy: 0.47747
	loss_value: 0.70041
	loss_reward: 0.04882
[2025-05-11 12:14:55] nn step 8200, lr: 0.1.
	loss_policy_0: 0.24616
	accuracy_policy_0: 0.83082
	loss_value_0: 0.34242
	loss_policy_1: 0.04941
	accuracy_policy_1: 0.8275
	loss_value_1: 0.07083
	loss_reward_1: 0.00563
	loss_policy_2: 0.04989
	accuracy_policy_2: 0.83168
	loss_value_2: 0.07328
	loss_reward_2: 0.00871
	loss_policy_3: 0.04958
	accuracy_policy_3: 0.83387
	loss_value_3: 0.07561
	loss_reward_3: 0.0103
	loss_policy_4: 0.04959
	accuracy_policy_4: 0.83535
	loss_value_4: 0.07803
	loss_reward_4: 0.01208
	loss_policy_5: 0.04963
	accuracy_policy_5: 0.83309
	loss_value_5: 0.08034
	loss_reward_5: 0.01303
	loss_policy: 0.49426
	loss_value: 0.7205
	loss_reward: 0.04975
Optimization_Done 8200
[2025-05-11 12:16:11] [command] train weight_iter_8200.pkl 23 42
[2025-05-11 12:16:19] nn step 8250, lr: 0.1.
	loss_policy_0: 0.24082
	accuracy_policy_0: 0.82297
	loss_value_0: 0.34439
	loss_policy_1: 0.04765
	accuracy_policy_1: 0.82727
	loss_value_1: 0.07042
	loss_reward_1: 0.00541
	loss_policy_2: 0.04744
	accuracy_policy_2: 0.82898
	loss_value_2: 0.07256
	loss_reward_2: 0.00813
	loss_policy_3: 0.04778
	accuracy_policy_3: 0.82918
	loss_value_3: 0.07457
	loss_reward_3: 0.00983
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.83055
	loss_value_4: 0.07674
	loss_reward_4: 0.0118
	loss_policy_5: 0.04803
	accuracy_policy_5: 0.82945
	loss_value_5: 0.07906
	loss_reward_5: 0.01252
	loss_policy: 0.47931
	loss_value: 0.71775
	loss_reward: 0.04769
[2025-05-11 12:16:27] nn step 8300, lr: 0.1.
	loss_policy_0: 0.24117
	accuracy_policy_0: 0.82699
	loss_value_0: 0.34372
	loss_policy_1: 0.04819
	accuracy_policy_1: 0.82477
	loss_value_1: 0.07047
	loss_reward_1: 0.00538
	loss_policy_2: 0.04822
	accuracy_policy_2: 0.82777
	loss_value_2: 0.07267
	loss_reward_2: 0.00859
	loss_policy_3: 0.0482
	accuracy_policy_3: 0.82355
	loss_value_3: 0.07502
	loss_reward_3: 0.01005
	loss_policy_4: 0.04808
	accuracy_policy_4: 0.83281
	loss_value_4: 0.07729
	loss_reward_4: 0.01145
	loss_policy_5: 0.0481
	accuracy_policy_5: 0.83703
	loss_value_5: 0.07972
	loss_reward_5: 0.01305
	loss_policy: 0.48195
	loss_value: 0.71889
	loss_reward: 0.04852
[2025-05-11 12:16:36] nn step 8350, lr: 0.1.
	loss_policy_0: 0.23632
	accuracy_policy_0: 0.82773
	loss_value_0: 0.33431
	loss_policy_1: 0.04727
	accuracy_policy_1: 0.82754
	loss_value_1: 0.06864
	loss_reward_1: 0.00502
	loss_policy_2: 0.04723
	accuracy_policy_2: 0.83066
	loss_value_2: 0.07101
	loss_reward_2: 0.00811
	loss_policy_3: 0.04726
	accuracy_policy_3: 0.83344
	loss_value_3: 0.073
	loss_reward_3: 0.00997
	loss_policy_4: 0.04742
	accuracy_policy_4: 0.83707
	loss_value_4: 0.07519
	loss_reward_4: 0.01087
	loss_policy_5: 0.04759
	accuracy_policy_5: 0.83438
	loss_value_5: 0.07736
	loss_reward_5: 0.01221
	loss_policy: 0.47308
	loss_value: 0.6995
	loss_reward: 0.04618
[2025-05-11 12:16:43] nn step 8400, lr: 0.1.
	loss_policy_0: 0.24182
	accuracy_policy_0: 0.83023
	loss_value_0: 0.3412
	loss_policy_1: 0.04873
	accuracy_policy_1: 0.82969
	loss_value_1: 0.07025
	loss_reward_1: 0.00554
	loss_policy_2: 0.04877
	accuracy_policy_2: 0.82844
	loss_value_2: 0.07237
	loss_reward_2: 0.00811
	loss_policy_3: 0.04854
	accuracy_policy_3: 0.82816
	loss_value_3: 0.07397
	loss_reward_3: 0.00973
	loss_policy_4: 0.04852
	accuracy_policy_4: 0.83336
	loss_value_4: 0.07648
	loss_reward_4: 0.0119
	loss_policy_5: 0.04886
	accuracy_policy_5: 0.83105
	loss_value_5: 0.07869
	loss_reward_5: 0.01289
	loss_policy: 0.48525
	loss_value: 0.71297
	loss_reward: 0.04817
Optimization_Done 8400
[2025-05-11 12:17:59] [command] train weight_iter_8400.pkl 24 43
[2025-05-11 12:18:06] nn step 8450, lr: 0.1.
	loss_policy_0: 0.22036
	accuracy_policy_0: 0.83012
	loss_value_0: 0.31484
	loss_policy_1: 0.04403
	accuracy_policy_1: 0.83109
	loss_value_1: 0.06439
	loss_reward_1: 0.00488
	loss_policy_2: 0.04434
	accuracy_policy_2: 0.83051
	loss_value_2: 0.06644
	loss_reward_2: 0.00755
	loss_policy_3: 0.04432
	accuracy_policy_3: 0.83816
	loss_value_3: 0.06892
	loss_reward_3: 0.00866
	loss_policy_4: 0.04465
	accuracy_policy_4: 0.83441
	loss_value_4: 0.07078
	loss_reward_4: 0.01061
	loss_policy_5: 0.04412
	accuracy_policy_5: 0.83445
	loss_value_5: 0.07275
	loss_reward_5: 0.01146
	loss_policy: 0.44182
	loss_value: 0.65812
	loss_reward: 0.04317
[2025-05-11 12:18:15] nn step 8500, lr: 0.1.
	loss_policy_0: 0.23007
	accuracy_policy_0: 0.83086
	loss_value_0: 0.32477
	loss_policy_1: 0.04581
	accuracy_policy_1: 0.8343
	loss_value_1: 0.06686
	loss_reward_1: 0.00503
	loss_policy_2: 0.04592
	accuracy_policy_2: 0.83309
	loss_value_2: 0.06924
	loss_reward_2: 0.00802
	loss_policy_3: 0.04593
	accuracy_policy_3: 0.8348
	loss_value_3: 0.07134
	loss_reward_3: 0.00968
	loss_policy_4: 0.04568
	accuracy_policy_4: 0.83645
	loss_value_4: 0.07342
	loss_reward_4: 0.01091
	loss_policy_5: 0.04618
	accuracy_policy_5: 0.83684
	loss_value_5: 0.07562
	loss_reward_5: 0.0119
	loss_policy: 0.45959
	loss_value: 0.68125
	loss_reward: 0.04553
[2025-05-11 12:18:23] nn step 8550, lr: 0.1.
	loss_policy_0: 0.23597
	accuracy_policy_0: 0.83215
	loss_value_0: 0.33358
	loss_policy_1: 0.04749
	accuracy_policy_1: 0.82637
	loss_value_1: 0.0688
	loss_reward_1: 0.00516
	loss_policy_2: 0.04718
	accuracy_policy_2: 0.83125
	loss_value_2: 0.07108
	loss_reward_2: 0.00824
	loss_policy_3: 0.04705
	accuracy_policy_3: 0.83445
	loss_value_3: 0.07301
	loss_reward_3: 0.00959
	loss_policy_4: 0.04714
	accuracy_policy_4: 0.83422
	loss_value_4: 0.07518
	loss_reward_4: 0.0114
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.83703
	loss_value_5: 0.07723
	loss_reward_5: 0.01235
	loss_policy: 0.47195
	loss_value: 0.69888
	loss_reward: 0.04673
[2025-05-11 12:18:32] nn step 8600, lr: 0.1.
	loss_policy_0: 0.23241
	accuracy_policy_0: 0.83234
	loss_value_0: 0.33148
	loss_policy_1: 0.04652
	accuracy_policy_1: 0.83168
	loss_value_1: 0.06796
	loss_reward_1: 0.00538
	loss_policy_2: 0.04668
	accuracy_policy_2: 0.83129
	loss_value_2: 0.07
	loss_reward_2: 0.00803
	loss_policy_3: 0.04654
	accuracy_policy_3: 0.83633
	loss_value_3: 0.0723
	loss_reward_3: 0.00966
	loss_policy_4: 0.04679
	accuracy_policy_4: 0.83
	loss_value_4: 0.07458
	loss_reward_4: 0.01122
	loss_policy_5: 0.04651
	accuracy_policy_5: 0.83641
	loss_value_5: 0.07725
	loss_reward_5: 0.01269
	loss_policy: 0.46543
	loss_value: 0.69359
	loss_reward: 0.04697
Optimization_Done 8600
[2025-05-11 12:19:48] [command] train weight_iter_8600.pkl 25 44
[2025-05-11 12:19:57] nn step 8650, lr: 0.1.
	loss_policy_0: 0.22618
	accuracy_policy_0: 0.84121
	loss_value_0: 0.32388
	loss_policy_1: 0.04528
	accuracy_policy_1: 0.83863
	loss_value_1: 0.06644
	loss_reward_1: 0.00491
	loss_policy_2: 0.04562
	accuracy_policy_2: 0.84043
	loss_value_2: 0.06857
	loss_reward_2: 0.00766
	loss_policy_3: 0.04514
	accuracy_policy_3: 0.84117
	loss_value_3: 0.07097
	loss_reward_3: 0.00904
	loss_policy_4: 0.04537
	accuracy_policy_4: 0.84387
	loss_value_4: 0.07307
	loss_reward_4: 0.01105
	loss_policy_5: 0.04524
	accuracy_policy_5: 0.84473
	loss_value_5: 0.07474
	loss_reward_5: 0.01189
	loss_policy: 0.45283
	loss_value: 0.67765
	loss_reward: 0.04455
[2025-05-11 12:20:04] nn step 8700, lr: 0.1.
	loss_policy_0: 0.21732
	accuracy_policy_0: 0.83797
	loss_value_0: 0.30559
	loss_policy_1: 0.04324
	accuracy_policy_1: 0.8377
	loss_value_1: 0.06306
	loss_reward_1: 0.00461
	loss_policy_2: 0.04369
	accuracy_policy_2: 0.835
	loss_value_2: 0.06517
	loss_reward_2: 0.00697
	loss_policy_3: 0.04336
	accuracy_policy_3: 0.83906
	loss_value_3: 0.06693
	loss_reward_3: 0.00859
	loss_policy_4: 0.04357
	accuracy_policy_4: 0.84105
	loss_value_4: 0.06895
	loss_reward_4: 0.01021
	loss_policy_5: 0.04319
	accuracy_policy_5: 0.84309
	loss_value_5: 0.07114
	loss_reward_5: 0.01084
	loss_policy: 0.43437
	loss_value: 0.64085
	loss_reward: 0.04122
[2025-05-11 12:20:13] nn step 8750, lr: 0.1.
	loss_policy_0: 0.22506
	accuracy_policy_0: 0.83695
	loss_value_0: 0.31801
	loss_policy_1: 0.04515
	accuracy_policy_1: 0.83625
	loss_value_1: 0.06538
	loss_reward_1: 0.00491
	loss_policy_2: 0.0449
	accuracy_policy_2: 0.8398
	loss_value_2: 0.06721
	loss_reward_2: 0.00705
	loss_policy_3: 0.04475
	accuracy_policy_3: 0.84117
	loss_value_3: 0.06963
	loss_reward_3: 0.00911
	loss_policy_4: 0.04506
	accuracy_policy_4: 0.84312
	loss_value_4: 0.07196
	loss_reward_4: 0.01048
	loss_policy_5: 0.04522
	accuracy_policy_5: 0.84426
	loss_value_5: 0.07389
	loss_reward_5: 0.01185
	loss_policy: 0.45014
	loss_value: 0.66607
	loss_reward: 0.0434
[2025-05-11 12:20:21] nn step 8800, lr: 0.1.
	loss_policy_0: 0.22923
	accuracy_policy_0: 0.83684
	loss_value_0: 0.32614
	loss_policy_1: 0.04572
	accuracy_policy_1: 0.83828
	loss_value_1: 0.06732
	loss_reward_1: 0.00507
	loss_policy_2: 0.0457
	accuracy_policy_2: 0.84121
	loss_value_2: 0.06928
	loss_reward_2: 0.00796
	loss_policy_3: 0.04566
	accuracy_policy_3: 0.84281
	loss_value_3: 0.07115
	loss_reward_3: 0.00951
	loss_policy_4: 0.04558
	accuracy_policy_4: 0.84375
	loss_value_4: 0.07334
	loss_reward_4: 0.01092
	loss_policy_5: 0.04583
	accuracy_policy_5: 0.84227
	loss_value_5: 0.07564
	loss_reward_5: 0.01209
	loss_policy: 0.45772
	loss_value: 0.68287
	loss_reward: 0.04556
Optimization_Done 8800
[2025-05-11 12:21:38] [command] train weight_iter_8800.pkl 26 45
[2025-05-11 12:21:47] nn step 8850, lr: 0.1.
	loss_policy_0: 0.23574
	accuracy_policy_0: 0.83723
	loss_value_0: 0.33531
	loss_policy_1: 0.04693
	accuracy_policy_1: 0.83676
	loss_value_1: 0.06911
	loss_reward_1: 0.0051
	loss_policy_2: 0.04699
	accuracy_policy_2: 0.83707
	loss_value_2: 0.07153
	loss_reward_2: 0.0079
	loss_policy_3: 0.04712
	accuracy_policy_3: 0.83789
	loss_value_3: 0.07331
	loss_reward_3: 0.0091
	loss_policy_4: 0.04711
	accuracy_policy_4: 0.83965
	loss_value_4: 0.07539
	loss_reward_4: 0.01109
	loss_policy_5: 0.047
	accuracy_policy_5: 0.84234
	loss_value_5: 0.0775
	loss_reward_5: 0.01259
	loss_policy: 0.47089
	loss_value: 0.70216
	loss_reward: 0.04578
[2025-05-11 12:21:54] nn step 8900, lr: 0.1.
	loss_policy_0: 0.23887
	accuracy_policy_0: 0.83738
	loss_value_0: 0.34454
	loss_policy_1: 0.04778
	accuracy_policy_1: 0.83984
	loss_value_1: 0.0707
	loss_reward_1: 0.0053
	loss_policy_2: 0.0482
	accuracy_policy_2: 0.8393
	loss_value_2: 0.07287
	loss_reward_2: 0.00807
	loss_policy_3: 0.04781
	accuracy_policy_3: 0.84148
	loss_value_3: 0.07519
	loss_reward_3: 0.00962
	loss_policy_4: 0.04817
	accuracy_policy_4: 0.84227
	loss_value_4: 0.07734
	loss_reward_4: 0.01121
	loss_policy_5: 0.04821
	accuracy_policy_5: 0.84168
	loss_value_5: 0.07955
	loss_reward_5: 0.01241
	loss_policy: 0.47903
	loss_value: 0.72018
	loss_reward: 0.0466
[2025-05-11 12:22:03] nn step 8950, lr: 0.1.
	loss_policy_0: 0.22548
	accuracy_policy_0: 0.84074
	loss_value_0: 0.32065
	loss_policy_1: 0.04516
	accuracy_policy_1: 0.84098
	loss_value_1: 0.06585
	loss_reward_1: 0.00491
	loss_policy_2: 0.04514
	accuracy_policy_2: 0.8402
	loss_value_2: 0.06797
	loss_reward_2: 0.00752
	loss_policy_3: 0.04508
	accuracy_policy_3: 0.84488
	loss_value_3: 0.06993
	loss_reward_3: 0.00906
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.8427
	loss_value_4: 0.07219
	loss_reward_4: 0.01066
	loss_policy_5: 0.04524
	accuracy_policy_5: 0.84555
	loss_value_5: 0.07422
	loss_reward_5: 0.01186
	loss_policy: 0.45103
	loss_value: 0.6708
	loss_reward: 0.04401
[2025-05-11 12:22:11] nn step 9000, lr: 0.1.
	loss_policy_0: 0.22919
	accuracy_policy_0: 0.83523
	loss_value_0: 0.3258
	loss_policy_1: 0.04611
	accuracy_policy_1: 0.83922
	loss_value_1: 0.0672
	loss_reward_1: 0.00521
	loss_policy_2: 0.04633
	accuracy_policy_2: 0.83621
	loss_value_2: 0.06901
	loss_reward_2: 0.00728
	loss_policy_3: 0.0461
	accuracy_policy_3: 0.83988
	loss_value_3: 0.07089
	loss_reward_3: 0.0092
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.83809
	loss_value_4: 0.07325
	loss_reward_4: 0.01089
	loss_policy_5: 0.04606
	accuracy_policy_5: 0.84414
	loss_value_5: 0.07578
	loss_reward_5: 0.01223
	loss_policy: 0.46
	loss_value: 0.68193
	loss_reward: 0.04481
Optimization_Done 9000
[2025-05-11 12:23:26] [command] train weight_iter_9000.pkl 27 46
[2025-05-11 12:23:35] nn step 9050, lr: 0.1.
	loss_policy_0: 0.22743
	accuracy_policy_0: 0.83758
	loss_value_0: 0.32912
	loss_policy_1: 0.04561
	accuracy_policy_1: 0.84027
	loss_value_1: 0.0676
	loss_reward_1: 0.00504
	loss_policy_2: 0.04562
	accuracy_policy_2: 0.83695
	loss_value_2: 0.06937
	loss_reward_2: 0.0073
	loss_policy_3: 0.04578
	accuracy_policy_3: 0.83895
	loss_value_3: 0.07099
	loss_reward_3: 0.00897
	loss_policy_4: 0.04586
	accuracy_policy_4: 0.84355
	loss_value_4: 0.07345
	loss_reward_4: 0.01037
	loss_policy_5: 0.04583
	accuracy_policy_5: 0.84289
	loss_value_5: 0.07579
	loss_reward_5: 0.01157
	loss_policy: 0.45613
	loss_value: 0.68631
	loss_reward: 0.04324
[2025-05-11 12:23:44] nn step 9100, lr: 0.1.
	loss_policy_0: 0.22647
	accuracy_policy_0: 0.83219
	loss_value_0: 0.3197
	loss_policy_1: 0.04515
	accuracy_policy_1: 0.83203
	loss_value_1: 0.06578
	loss_reward_1: 0.00479
	loss_policy_2: 0.04541
	accuracy_policy_2: 0.83277
	loss_value_2: 0.06793
	loss_reward_2: 0.00725
	loss_policy_3: 0.04491
	accuracy_policy_3: 0.83949
	loss_value_3: 0.07
	loss_reward_3: 0.00873
	loss_policy_4: 0.04511
	accuracy_policy_4: 0.83688
	loss_value_4: 0.07207
	loss_reward_4: 0.01029
	loss_policy_5: 0.04493
	accuracy_policy_5: 0.84004
	loss_value_5: 0.07396
	loss_reward_5: 0.01138
	loss_policy: 0.45199
	loss_value: 0.66945
	loss_reward: 0.04244
[2025-05-11 12:23:51] nn step 9150, lr: 0.1.
	loss_policy_0: 0.22957
	accuracy_policy_0: 0.84137
	loss_value_0: 0.32737
	loss_policy_1: 0.04591
	accuracy_policy_1: 0.83914
	loss_value_1: 0.06694
	loss_reward_1: 0.00522
	loss_policy_2: 0.0462
	accuracy_policy_2: 0.83691
	loss_value_2: 0.0691
	loss_reward_2: 0.00728
	loss_policy_3: 0.04607
	accuracy_policy_3: 0.83863
	loss_value_3: 0.07106
	loss_reward_3: 0.0085
	loss_policy_4: 0.04618
	accuracy_policy_4: 0.83543
	loss_value_4: 0.07347
	loss_reward_4: 0.01051
	loss_policy_5: 0.04634
	accuracy_policy_5: 0.83801
	loss_value_5: 0.07537
	loss_reward_5: 0.01166
	loss_policy: 0.46028
	loss_value: 0.68332
	loss_reward: 0.04318
[2025-05-11 12:24:00] nn step 9200, lr: 0.1.
	loss_policy_0: 0.22606
	accuracy_policy_0: 0.83602
	loss_value_0: 0.31523
	loss_policy_1: 0.04495
	accuracy_policy_1: 0.83199
	loss_value_1: 0.06473
	loss_reward_1: 0.00477
	loss_policy_2: 0.04476
	accuracy_policy_2: 0.83754
	loss_value_2: 0.06679
	loss_reward_2: 0.00722
	loss_policy_3: 0.04525
	accuracy_policy_3: 0.83566
	loss_value_3: 0.06867
	loss_reward_3: 0.00869
	loss_policy_4: 0.04501
	accuracy_policy_4: 0.83855
	loss_value_4: 0.07088
	loss_reward_4: 0.01041
	loss_policy_5: 0.04502
	accuracy_policy_5: 0.8384
	loss_value_5: 0.0727
	loss_reward_5: 0.01122
	loss_policy: 0.45105
	loss_value: 0.659
	loss_reward: 0.04231
Optimization_Done 9200
[2025-05-11 12:25:16] [command] train weight_iter_9200.pkl 28 47
[2025-05-11 12:25:25] nn step 9250, lr: 0.1.
	loss_policy_0: 0.23681
	accuracy_policy_0: 0.84105
	loss_value_0: 0.33841
	loss_policy_1: 0.04736
	accuracy_policy_1: 0.83375
	loss_value_1: 0.06939
	loss_reward_1: 0.00509
	loss_policy_2: 0.04715
	accuracy_policy_2: 0.83922
	loss_value_2: 0.07166
	loss_reward_2: 0.00743
	loss_policy_3: 0.04729
	accuracy_policy_3: 0.83988
	loss_value_3: 0.07372
	loss_reward_3: 0.00946
	loss_policy_4: 0.04726
	accuracy_policy_4: 0.84309
	loss_value_4: 0.07583
	loss_reward_4: 0.01064
	loss_policy_5: 0.04703
	accuracy_policy_5: 0.84254
	loss_value_5: 0.07811
	loss_reward_5: 0.01185
	loss_policy: 0.47289
	loss_value: 0.70712
	loss_reward: 0.04446
[2025-05-11 12:25:34] nn step 9300, lr: 0.1.
	loss_policy_0: 0.22269
	accuracy_policy_0: 0.83414
	loss_value_0: 0.31678
	loss_policy_1: 0.04447
	accuracy_policy_1: 0.83695
	loss_value_1: 0.06489
	loss_reward_1: 0.00477
	loss_policy_2: 0.0445
	accuracy_policy_2: 0.83695
	loss_value_2: 0.06739
	loss_reward_2: 0.00722
	loss_policy_3: 0.04428
	accuracy_policy_3: 0.84402
	loss_value_3: 0.06913
	loss_reward_3: 0.0086
	loss_policy_4: 0.04452
	accuracy_policy_4: 0.84113
	loss_value_4: 0.07109
	loss_reward_4: 0.01017
	loss_policy_5: 0.04483
	accuracy_policy_5: 0.84027
	loss_value_5: 0.07327
	loss_reward_5: 0.01152
	loss_policy: 0.44529
	loss_value: 0.66255
	loss_reward: 0.04227
[2025-05-11 12:25:42] nn step 9350, lr: 0.1.
	loss_policy_0: 0.23764
	accuracy_policy_0: 0.8343
	loss_value_0: 0.33051
	loss_policy_1: 0.04739
	accuracy_policy_1: 0.83523
	loss_value_1: 0.06753
	loss_reward_1: 0.00477
	loss_policy_2: 0.04753
	accuracy_policy_2: 0.83516
	loss_value_2: 0.06995
	loss_reward_2: 0.00728
	loss_policy_3: 0.04736
	accuracy_policy_3: 0.83512
	loss_value_3: 0.07208
	loss_reward_3: 0.00881
	loss_policy_4: 0.04738
	accuracy_policy_4: 0.83836
	loss_value_4: 0.07442
	loss_reward_4: 0.01057
	loss_policy_5: 0.04733
	accuracy_policy_5: 0.83816
	loss_value_5: 0.07663
	loss_reward_5: 0.01173
	loss_policy: 0.47464
	loss_value: 0.69111
	loss_reward: 0.04315
[2025-05-11 12:25:49] nn step 9400, lr: 0.1.
	loss_policy_0: 0.23086
	accuracy_policy_0: 0.83332
	loss_value_0: 0.32341
	loss_policy_1: 0.04638
	accuracy_policy_1: 0.83105
	loss_value_1: 0.06636
	loss_reward_1: 0.00499
	loss_policy_2: 0.04596
	accuracy_policy_2: 0.83836
	loss_value_2: 0.06853
	loss_reward_2: 0.00729
	loss_policy_3: 0.04628
	accuracy_policy_3: 0.83871
	loss_value_3: 0.07033
	loss_reward_3: 0.00856
	loss_policy_4: 0.04645
	accuracy_policy_4: 0.83883
	loss_value_4: 0.07276
	loss_reward_4: 0.01079
	loss_policy_5: 0.04618
	accuracy_policy_5: 0.84
	loss_value_5: 0.0752
	loss_reward_5: 0.01159
	loss_policy: 0.4621
	loss_value: 0.67659
	loss_reward: 0.04322
Optimization_Done 9400
[2025-05-11 12:27:06] [command] train weight_iter_9400.pkl 29 48
[2025-05-11 12:27:13] nn step 9450, lr: 0.1.
	loss_policy_0: 0.20924
	accuracy_policy_0: 0.84457
	loss_value_0: 0.29874
	loss_policy_1: 0.04172
	accuracy_policy_1: 0.84207
	loss_value_1: 0.06145
	loss_reward_1: 0.00425
	loss_policy_2: 0.04163
	accuracy_policy_2: 0.8427
	loss_value_2: 0.06353
	loss_reward_2: 0.00635
	loss_policy_3: 0.04176
	accuracy_policy_3: 0.84523
	loss_value_3: 0.06536
	loss_reward_3: 0.00797
	loss_policy_4: 0.04149
	accuracy_policy_4: 0.84902
	loss_value_4: 0.06725
	loss_reward_4: 0.00927
	loss_policy_5: 0.0417
	accuracy_policy_5: 0.84859
	loss_value_5: 0.06897
	loss_reward_5: 0.0103
	loss_policy: 0.41753
	loss_value: 0.62529
	loss_reward: 0.03815
[2025-05-11 12:27:22] nn step 9500, lr: 0.1.
	loss_policy_0: 0.23737
	accuracy_policy_0: 0.84551
	loss_value_0: 0.33542
	loss_policy_1: 0.04719
	accuracy_policy_1: 0.84766
	loss_value_1: 0.06846
	loss_reward_1: 0.00503
	loss_policy_2: 0.04736
	accuracy_policy_2: 0.85004
	loss_value_2: 0.07073
	loss_reward_2: 0.00745
	loss_policy_3: 0.04732
	accuracy_policy_3: 0.84836
	loss_value_3: 0.07261
	loss_reward_3: 0.00902
	loss_policy_4: 0.04704
	accuracy_policy_4: 0.85043
	loss_value_4: 0.07508
	loss_reward_4: 0.01057
	loss_policy_5: 0.04752
	accuracy_policy_5: 0.85031
	loss_value_5: 0.07735
	loss_reward_5: 0.01162
	loss_policy: 0.47379
	loss_value: 0.69965
	loss_reward: 0.04368
[2025-05-11 12:27:30] nn step 9550, lr: 0.1.
	loss_policy_0: 0.21681
	accuracy_policy_0: 0.84789
	loss_value_0: 0.30657
	loss_policy_1: 0.04357
	accuracy_policy_1: 0.84637
	loss_value_1: 0.06298
	loss_reward_1: 0.00488
	loss_policy_2: 0.04353
	accuracy_policy_2: 0.84582
	loss_value_2: 0.06529
	loss_reward_2: 0.0069
	loss_policy_3: 0.04374
	accuracy_policy_3: 0.84781
	loss_value_3: 0.06713
	loss_reward_3: 0.00826
	loss_policy_4: 0.04385
	accuracy_policy_4: 0.84406
	loss_value_4: 0.06895
	loss_reward_4: 0.00983
	loss_policy_5: 0.04365
	accuracy_policy_5: 0.84738
	loss_value_5: 0.07095
	loss_reward_5: 0.01072
	loss_policy: 0.43515
	loss_value: 0.64188
	loss_reward: 0.0406
[2025-05-11 12:27:37] nn step 9600, lr: 0.1.
	loss_policy_0: 0.22396
	accuracy_policy_0: 0.84219
	loss_value_0: 0.30916
	loss_policy_1: 0.04447
	accuracy_policy_1: 0.84281
	loss_value_1: 0.06349
	loss_reward_1: 0.00476
	loss_policy_2: 0.04461
	accuracy_policy_2: 0.84547
	loss_value_2: 0.06568
	loss_reward_2: 0.00679
	loss_policy_3: 0.04447
	accuracy_policy_3: 0.84727
	loss_value_3: 0.06774
	loss_reward_3: 0.00854
	loss_policy_4: 0.04462
	accuracy_policy_4: 0.84578
	loss_value_4: 0.06953
	loss_reward_4: 0.00991
	loss_policy_5: 0.0445
	accuracy_policy_5: 0.85094
	loss_value_5: 0.07174
	loss_reward_5: 0.01105
	loss_policy: 0.44663
	loss_value: 0.64734
	loss_reward: 0.04105
Optimization_Done 9600
[2025-05-11 12:28:53] [command] train weight_iter_9600.pkl 30 49
[2025-05-11 12:29:02] nn step 9650, lr: 0.1.
	loss_policy_0: 0.23641
	accuracy_policy_0: 0.84812
	loss_value_0: 0.33405
	loss_policy_1: 0.04704
	accuracy_policy_1: 0.84852
	loss_value_1: 0.06872
	loss_reward_1: 0.00516
	loss_policy_2: 0.04719
	accuracy_policy_2: 0.84562
	loss_value_2: 0.07089
	loss_reward_2: 0.00726
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.85008
	loss_value_3: 0.07333
	loss_reward_3: 0.00914
	loss_policy_4: 0.04698
	accuracy_policy_4: 0.85465
	loss_value_4: 0.07494
	loss_reward_4: 0.01065
	loss_policy_5: 0.0469
	accuracy_policy_5: 0.85516
	loss_value_5: 0.07692
	loss_reward_5: 0.01166
	loss_policy: 0.47172
	loss_value: 0.69886
	loss_reward: 0.04388
[2025-05-11 12:29:09] nn step 9700, lr: 0.1.
	loss_policy_0: 0.22974
	accuracy_policy_0: 0.84883
	loss_value_0: 0.32177
	loss_policy_1: 0.04624
	accuracy_policy_1: 0.84348
	loss_value_1: 0.06651
	loss_reward_1: 0.00502
	loss_policy_2: 0.04603
	accuracy_policy_2: 0.84809
	loss_value_2: 0.06853
	loss_reward_2: 0.00722
	loss_policy_3: 0.04586
	accuracy_policy_3: 0.85219
	loss_value_3: 0.07077
	loss_reward_3: 0.00852
	loss_policy_4: 0.0461
	accuracy_policy_4: 0.84996
	loss_value_4: 0.07278
	loss_reward_4: 0.01038
	loss_policy_5: 0.04576
	accuracy_policy_5: 0.85395
	loss_value_5: 0.07495
	loss_reward_5: 0.01139
	loss_policy: 0.45973
	loss_value: 0.67531
	loss_reward: 0.04253
[2025-05-11 12:29:18] nn step 9750, lr: 0.1.
	loss_policy_0: 0.21125
	accuracy_policy_0: 0.84965
	loss_value_0: 0.29848
	loss_policy_1: 0.04271
	accuracy_policy_1: 0.84453
	loss_value_1: 0.06164
	loss_reward_1: 0.00452
	loss_policy_2: 0.04271
	accuracy_policy_2: 0.84809
	loss_value_2: 0.06366
	loss_reward_2: 0.00677
	loss_policy_3: 0.0426
	accuracy_policy_3: 0.84633
	loss_value_3: 0.06569
	loss_reward_3: 0.00839
	loss_policy_4: 0.04261
	accuracy_policy_4: 0.84559
	loss_value_4: 0.06765
	loss_reward_4: 0.00954
	loss_policy_5: 0.04211
	accuracy_policy_5: 0.85215
	loss_value_5: 0.06928
	loss_reward_5: 0.01109
	loss_policy: 0.424
	loss_value: 0.62641
	loss_reward: 0.04032
[2025-05-11 12:29:27] nn step 9800, lr: 0.1.
	loss_policy_0: 0.22712
	accuracy_policy_0: 0.84969
	loss_value_0: 0.32044
	loss_policy_1: 0.04578
	accuracy_policy_1: 0.84336
	loss_value_1: 0.06589
	loss_reward_1: 0.00479
	loss_policy_2: 0.04566
	accuracy_policy_2: 0.84492
	loss_value_2: 0.06799
	loss_reward_2: 0.00704
	loss_policy_3: 0.04626
	accuracy_policy_3: 0.84262
	loss_value_3: 0.07029
	loss_reward_3: 0.00829
	loss_policy_4: 0.04573
	accuracy_policy_4: 0.84801
	loss_value_4: 0.07196
	loss_reward_4: 0.01029
	loss_policy_5: 0.04553
	accuracy_policy_5: 0.84801
	loss_value_5: 0.07411
	loss_reward_5: 0.01125
	loss_policy: 0.45608
	loss_value: 0.67068
	loss_reward: 0.04166
Optimization_Done 9800
[2025-05-11 12:30:43] [command] train weight_iter_9800.pkl 31 50
[2025-05-11 12:30:52] nn step 9850, lr: 0.1.
	loss_policy_0: 0.22054
	accuracy_policy_0: 0.85363
	loss_value_0: 0.31542
	loss_policy_1: 0.04422
	accuracy_policy_1: 0.85191
	loss_value_1: 0.06477
	loss_reward_1: 0.00459
	loss_policy_2: 0.04455
	accuracy_policy_2: 0.85184
	loss_value_2: 0.06682
	loss_reward_2: 0.00663
	loss_policy_3: 0.04453
	accuracy_policy_3: 0.85184
	loss_value_3: 0.06879
	loss_reward_3: 0.00827
	loss_policy_4: 0.04442
	accuracy_policy_4: 0.8523
	loss_value_4: 0.0708
	loss_reward_4: 0.00953
	loss_policy_5: 0.04444
	accuracy_policy_5: 0.85562
	loss_value_5: 0.07262
	loss_reward_5: 0.01085
	loss_policy: 0.44271
	loss_value: 0.65921
	loss_reward: 0.03987
[2025-05-11 12:30:59] nn step 9900, lr: 0.1.
	loss_policy_0: 0.21768
	accuracy_policy_0: 0.85387
	loss_value_0: 0.30691
	loss_policy_1: 0.04386
	accuracy_policy_1: 0.85137
	loss_value_1: 0.06315
	loss_reward_1: 0.00474
	loss_policy_2: 0.04363
	accuracy_policy_2: 0.85031
	loss_value_2: 0.06498
	loss_reward_2: 0.00704
	loss_policy_3: 0.04368
	accuracy_policy_3: 0.8507
	loss_value_3: 0.06705
	loss_reward_3: 0.0081
	loss_policy_4: 0.04361
	accuracy_policy_4: 0.85469
	loss_value_4: 0.06897
	loss_reward_4: 0.00988
	loss_policy_5: 0.04336
	accuracy_policy_5: 0.85375
	loss_value_5: 0.07085
	loss_reward_5: 0.01102
	loss_policy: 0.43583
	loss_value: 0.6419
	loss_reward: 0.04077
[2025-05-11 12:31:08] nn step 9950, lr: 0.1.
	loss_policy_0: 0.22439
	accuracy_policy_0: 0.84832
	loss_value_0: 0.31626
	loss_policy_1: 0.04482
	accuracy_policy_1: 0.8468
	loss_value_1: 0.06499
	loss_reward_1: 0.00467
	loss_policy_2: 0.04514
	accuracy_policy_2: 0.84684
	loss_value_2: 0.06717
	loss_reward_2: 0.00657
	loss_policy_3: 0.04483
	accuracy_policy_3: 0.84934
	loss_value_3: 0.06891
	loss_reward_3: 0.00828
	loss_policy_4: 0.04462
	accuracy_policy_4: 0.85223
	loss_value_4: 0.07095
	loss_reward_4: 0.00984
	loss_policy_5: 0.04496
	accuracy_policy_5: 0.85148
	loss_value_5: 0.07305
	loss_reward_5: 0.01099
	loss_policy: 0.44876
	loss_value: 0.66133
	loss_reward: 0.04034
[2025-05-11 12:31:17] nn step 10000, lr: 0.1.
	loss_policy_0: 0.23132
	accuracy_policy_0: 0.84898
	loss_value_0: 0.3279
	loss_policy_1: 0.04637
	accuracy_policy_1: 0.84805
	loss_value_1: 0.06747
	loss_reward_1: 0.00477
	loss_policy_2: 0.04676
	accuracy_policy_2: 0.84934
	loss_value_2: 0.06942
	loss_reward_2: 0.00716
	loss_policy_3: 0.04651
	accuracy_policy_3: 0.85195
	loss_value_3: 0.07116
	loss_reward_3: 0.00873
	loss_policy_4: 0.04669
	accuracy_policy_4: 0.85375
	loss_value_4: 0.07322
	loss_reward_4: 0.01018
	loss_policy_5: 0.0463
	accuracy_policy_5: 0.85746
	loss_value_5: 0.07554
	loss_reward_5: 0.0115
	loss_policy: 0.46395
	loss_value: 0.6847
	loss_reward: 0.04232
Optimization_Done 10000
[2025-05-11 12:32:33] [command] train weight_iter_10000.pkl 32 51
[2025-05-11 12:32:42] nn step 10050, lr: 0.1.
	loss_policy_0: 0.2091
	accuracy_policy_0: 0.85121
	loss_value_0: 0.29684
	loss_policy_1: 0.04166
	accuracy_policy_1: 0.85574
	loss_value_1: 0.06085
	loss_reward_1: 0.00428
	loss_policy_2: 0.04168
	accuracy_policy_2: 0.85176
	loss_value_2: 0.06272
	loss_reward_2: 0.00637
	loss_policy_3: 0.04202
	accuracy_policy_3: 0.85344
	loss_value_3: 0.06465
	loss_reward_3: 0.00749
	loss_policy_4: 0.04184
	accuracy_policy_4: 0.8543
	loss_value_4: 0.06672
	loss_reward_4: 0.00903
	loss_policy_5: 0.04187
	accuracy_policy_5: 0.85172
	loss_value_5: 0.06878
	loss_reward_5: 0.0105
	loss_policy: 0.41817
	loss_value: 0.62056
	loss_reward: 0.03766
[2025-05-11 12:32:49] nn step 10100, lr: 0.1.
	loss_policy_0: 0.22997
	accuracy_policy_0: 0.85117
	loss_value_0: 0.31917
	loss_policy_1: 0.04565
	accuracy_policy_1: 0.85293
	loss_value_1: 0.06567
	loss_reward_1: 0.0048
	loss_policy_2: 0.04558
	accuracy_policy_2: 0.8552
	loss_value_2: 0.068
	loss_reward_2: 0.00689
	loss_policy_3: 0.04561
	accuracy_policy_3: 0.8566
	loss_value_3: 0.07003
	loss_reward_3: 0.00838
	loss_policy_4: 0.04571
	accuracy_policy_4: 0.8573
	loss_value_4: 0.07239
	loss_reward_4: 0.00976
	loss_policy_5: 0.0456
	accuracy_policy_5: 0.85668
	loss_value_5: 0.07411
	loss_reward_5: 0.01184
	loss_policy: 0.45811
	loss_value: 0.66937
	loss_reward: 0.04167
[2025-05-11 12:32:58] nn step 10150, lr: 0.1.
	loss_policy_0: 0.2112
	accuracy_policy_0: 0.85211
	loss_value_0: 0.29594
	loss_policy_1: 0.04204
	accuracy_policy_1: 0.8559
	loss_value_1: 0.06088
	loss_reward_1: 0.00444
	loss_policy_2: 0.04267
	accuracy_policy_2: 0.85012
	loss_value_2: 0.06295
	loss_reward_2: 0.00626
	loss_policy_3: 0.04271
	accuracy_policy_3: 0.85086
	loss_value_3: 0.06476
	loss_reward_3: 0.00788
	loss_policy_4: 0.04244
	accuracy_policy_4: 0.85449
	loss_value_4: 0.06671
	loss_reward_4: 0.00908
	loss_policy_5: 0.0421
	accuracy_policy_5: 0.85703
	loss_value_5: 0.06865
	loss_reward_5: 0.01028
	loss_policy: 0.42317
	loss_value: 0.6199
	loss_reward: 0.03794
[2025-05-11 12:33:07] nn step 10200, lr: 0.1.
	loss_policy_0: 0.22125
	accuracy_policy_0: 0.85625
	loss_value_0: 0.31027
	loss_policy_1: 0.04426
	accuracy_policy_1: 0.84902
	loss_value_1: 0.06371
	loss_reward_1: 0.0047
	loss_policy_2: 0.04456
	accuracy_policy_2: 0.85066
	loss_value_2: 0.06558
	loss_reward_2: 0.00675
	loss_policy_3: 0.04441
	accuracy_policy_3: 0.85074
	loss_value_3: 0.06747
	loss_reward_3: 0.0081
	loss_policy_4: 0.04453
	accuracy_policy_4: 0.85434
	loss_value_4: 0.06925
	loss_reward_4: 0.00968
	loss_policy_5: 0.04444
	accuracy_policy_5: 0.85543
	loss_value_5: 0.07144
	loss_reward_5: 0.01084
	loss_policy: 0.44344
	loss_value: 0.64772
	loss_reward: 0.04006
Optimization_Done 10200
[2025-05-11 12:34:20] [command] train weight_iter_10200.pkl 33 52
[2025-05-11 12:34:30] nn step 10250, lr: 0.1.
	loss_policy_0: 0.21765
	accuracy_policy_0: 0.84988
	loss_value_0: 0.30771
	loss_policy_1: 0.04343
	accuracy_policy_1: 0.85055
	loss_value_1: 0.06302
	loss_reward_1: 0.00466
	loss_policy_2: 0.0431
	accuracy_policy_2: 0.85688
	loss_value_2: 0.06496
	loss_reward_2: 0.00627
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.85512
	loss_value_3: 0.06693
	loss_reward_3: 0.0078
	loss_policy_4: 0.04329
	accuracy_policy_4: 0.85277
	loss_value_4: 0.06858
	loss_reward_4: 0.00971
	loss_policy_5: 0.04298
	accuracy_policy_5: 0.85785
	loss_value_5: 0.07069
	loss_reward_5: 0.01071
	loss_policy: 0.43359
	loss_value: 0.64189
	loss_reward: 0.03914
[2025-05-11 12:34:38] nn step 10300, lr: 0.1.
	loss_policy_0: 0.22771
	accuracy_policy_0: 0.84934
	loss_value_0: 0.32044
	loss_policy_1: 0.04555
	accuracy_policy_1: 0.85184
	loss_value_1: 0.06592
	loss_reward_1: 0.00477
	loss_policy_2: 0.04544
	accuracy_policy_2: 0.85238
	loss_value_2: 0.06784
	loss_reward_2: 0.00653
	loss_policy_3: 0.0451
	accuracy_policy_3: 0.85734
	loss_value_3: 0.06957
	loss_reward_3: 0.00819
	loss_policy_4: 0.04555
	accuracy_policy_4: 0.85352
	loss_value_4: 0.07151
	loss_reward_4: 0.01001
	loss_policy_5: 0.04553
	accuracy_policy_5: 0.85621
	loss_value_5: 0.07367
	loss_reward_5: 0.0113
	loss_policy: 0.45487
	loss_value: 0.66896
	loss_reward: 0.04079
[2025-05-11 12:34:45] nn step 10350, lr: 0.1.
	loss_policy_0: 0.21825
	accuracy_policy_0: 0.85449
	loss_value_0: 0.30686
	loss_policy_1: 0.04377
	accuracy_policy_1: 0.85246
	loss_value_1: 0.06291
	loss_reward_1: 0.00454
	loss_policy_2: 0.04376
	accuracy_policy_2: 0.85207
	loss_value_2: 0.06484
	loss_reward_2: 0.0063
	loss_policy_3: 0.04427
	accuracy_policy_3: 0.85113
	loss_value_3: 0.06694
	loss_reward_3: 0.00777
	loss_policy_4: 0.04362
	accuracy_policy_4: 0.85715
	loss_value_4: 0.06879
	loss_reward_4: 0.0096
	loss_policy_5: 0.04423
	accuracy_policy_5: 0.85352
	loss_value_5: 0.07105
	loss_reward_5: 0.01091
	loss_policy: 0.4379
	loss_value: 0.64139
	loss_reward: 0.03911
[2025-05-11 12:34:54] nn step 10400, lr: 0.1.
	loss_policy_0: 0.20937
	accuracy_policy_0: 0.85246
	loss_value_0: 0.29352
	loss_policy_1: 0.042
	accuracy_policy_1: 0.85199
	loss_value_1: 0.06021
	loss_reward_1: 0.0044
	loss_policy_2: 0.04189
	accuracy_policy_2: 0.85398
	loss_value_2: 0.062
	loss_reward_2: 0.00641
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.85254
	loss_value_3: 0.06361
	loss_reward_3: 0.00768
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.8568
	loss_value_4: 0.0655
	loss_reward_4: 0.00887
	loss_policy_5: 0.04138
	accuracy_policy_5: 0.85875
	loss_value_5: 0.06743
	loss_reward_5: 0.01019
	loss_policy: 0.41824
	loss_value: 0.61226
	loss_reward: 0.03755
Optimization_Done 10400
[2025-05-11 12:36:10] [command] train weight_iter_10400.pkl 34 53
[2025-05-11 12:36:17] nn step 10450, lr: 0.1.
	loss_policy_0: 0.21149
	accuracy_policy_0: 0.85426
	loss_value_0: 0.29737
	loss_policy_1: 0.04181
	accuracy_policy_1: 0.8548
	loss_value_1: 0.06099
	loss_reward_1: 0.00447
	loss_policy_2: 0.04199
	accuracy_policy_2: 0.85363
	loss_value_2: 0.06292
	loss_reward_2: 0.00634
	loss_policy_3: 0.04217
	accuracy_policy_3: 0.85277
	loss_value_3: 0.06462
	loss_reward_3: 0.00784
	loss_policy_4: 0.04197
	accuracy_policy_4: 0.85441
	loss_value_4: 0.06629
	loss_reward_4: 0.00911
	loss_policy_5: 0.04214
	accuracy_policy_5: 0.85734
	loss_value_5: 0.06783
	loss_reward_5: 0.01047
	loss_policy: 0.42157
	loss_value: 0.62002
	loss_reward: 0.03823
[2025-05-11 12:36:26] nn step 10500, lr: 0.1.
	loss_policy_0: 0.22192
	accuracy_policy_0: 0.85383
	loss_value_0: 0.30777
	loss_policy_1: 0.04419
	accuracy_policy_1: 0.85555
	loss_value_1: 0.06311
	loss_reward_1: 0.0046
	loss_policy_2: 0.04423
	accuracy_policy_2: 0.85801
	loss_value_2: 0.0651
	loss_reward_2: 0.00637
	loss_policy_3: 0.04405
	accuracy_policy_3: 0.85934
	loss_value_3: 0.0669
	loss_reward_3: 0.0081
	loss_policy_4: 0.0441
	accuracy_policy_4: 0.85695
	loss_value_4: 0.06865
	loss_reward_4: 0.00958
	loss_policy_5: 0.04425
	accuracy_policy_5: 0.85645
	loss_value_5: 0.07089
	loss_reward_5: 0.01087
	loss_policy: 0.44275
	loss_value: 0.64241
	loss_reward: 0.03951
[2025-05-11 12:36:34] nn step 10550, lr: 0.1.
	loss_policy_0: 0.21924
	accuracy_policy_0: 0.85238
	loss_value_0: 0.306
	loss_policy_1: 0.04414
	accuracy_policy_1: 0.85164
	loss_value_1: 0.06313
	loss_reward_1: 0.00457
	loss_policy_2: 0.04397
	accuracy_policy_2: 0.85793
	loss_value_2: 0.06491
	loss_reward_2: 0.00617
	loss_policy_3: 0.04367
	accuracy_policy_3: 0.86039
	loss_value_3: 0.06722
	loss_reward_3: 0.00758
	loss_policy_4: 0.04382
	accuracy_policy_4: 0.85688
	loss_value_4: 0.06881
	loss_reward_4: 0.00937
	loss_policy_5: 0.04333
	accuracy_policy_5: 0.86117
	loss_value_5: 0.0709
	loss_reward_5: 0.01085
	loss_policy: 0.43818
	loss_value: 0.64098
	loss_reward: 0.03854
[2025-05-11 12:36:41] nn step 10600, lr: 0.1.
	loss_policy_0: 0.22602
	accuracy_policy_0: 0.85656
	loss_value_0: 0.31605
	loss_policy_1: 0.04555
	accuracy_policy_1: 0.85461
	loss_value_1: 0.06495
	loss_reward_1: 0.00469
	loss_policy_2: 0.04532
	accuracy_policy_2: 0.85527
	loss_value_2: 0.06695
	loss_reward_2: 0.00658
	loss_policy_3: 0.04535
	accuracy_policy_3: 0.85543
	loss_value_3: 0.06895
	loss_reward_3: 0.00801
	loss_policy_4: 0.04572
	accuracy_policy_4: 0.85719
	loss_value_4: 0.07113
	loss_reward_4: 0.00941
	loss_policy_5: 0.04567
	accuracy_policy_5: 0.85949
	loss_value_5: 0.07341
	loss_reward_5: 0.01116
	loss_policy: 0.45362
	loss_value: 0.66143
	loss_reward: 0.03985
Optimization_Done 10600
[2025-05-11 12:37:59] [command] train weight_iter_10600.pkl 35 54
[2025-05-11 12:38:07] nn step 10650, lr: 0.1.
	loss_policy_0: 0.21269
	accuracy_policy_0: 0.85984
	loss_value_0: 0.30411
	loss_policy_1: 0.04293
	accuracy_policy_1: 0.86402
	loss_value_1: 0.06201
	loss_reward_1: 0.00453
	loss_policy_2: 0.04314
	accuracy_policy_2: 0.86082
	loss_value_2: 0.06377
	loss_reward_2: 0.00633
	loss_policy_3: 0.04286
	accuracy_policy_3: 0.86043
	loss_value_3: 0.06566
	loss_reward_3: 0.00766
	loss_policy_4: 0.04295
	accuracy_policy_4: 0.8643
	loss_value_4: 0.06744
	loss_reward_4: 0.0095
	loss_policy_5: 0.04302
	accuracy_policy_5: 0.86176
	loss_value_5: 0.06983
	loss_reward_5: 0.01057
	loss_policy: 0.42759
	loss_value: 0.63282
	loss_reward: 0.03859
[2025-05-11 12:38:15] nn step 10700, lr: 0.1.
	loss_policy_0: 0.21305
	accuracy_policy_0: 0.86242
	loss_value_0: 0.29894
	loss_policy_1: 0.04284
	accuracy_policy_1: 0.85664
	loss_value_1: 0.06128
	loss_reward_1: 0.00439
	loss_policy_2: 0.04279
	accuracy_policy_2: 0.86301
	loss_value_2: 0.0631
	loss_reward_2: 0.00598
	loss_policy_3: 0.0427
	accuracy_policy_3: 0.86328
	loss_value_3: 0.0651
	loss_reward_3: 0.00743
	loss_policy_4: 0.04279
	accuracy_policy_4: 0.86387
	loss_value_4: 0.06713
	loss_reward_4: 0.0091
	loss_policy_5: 0.04297
	accuracy_policy_5: 0.86621
	loss_value_5: 0.06916
	loss_reward_5: 0.01051
	loss_policy: 0.42714
	loss_value: 0.62472
	loss_reward: 0.03741
[2025-05-11 12:38:24] nn step 10750, lr: 0.1.
	loss_policy_0: 0.20969
	accuracy_policy_0: 0.85961
	loss_value_0: 0.29277
	loss_policy_1: 0.04193
	accuracy_policy_1: 0.85656
	loss_value_1: 0.06004
	loss_reward_1: 0.00451
	loss_policy_2: 0.04207
	accuracy_policy_2: 0.8573
	loss_value_2: 0.06151
	loss_reward_2: 0.00605
	loss_policy_3: 0.0419
	accuracy_policy_3: 0.86305
	loss_value_3: 0.06326
	loss_reward_3: 0.00764
	loss_policy_4: 0.04207
	accuracy_policy_4: 0.86023
	loss_value_4: 0.0652
	loss_reward_4: 0.00888
	loss_policy_5: 0.0422
	accuracy_policy_5: 0.86465
	loss_value_5: 0.06724
	loss_reward_5: 0.01008
	loss_policy: 0.41988
	loss_value: 0.61002
	loss_reward: 0.03717
[2025-05-11 12:38:32] nn step 10800, lr: 0.1.
	loss_policy_0: 0.21785
	accuracy_policy_0: 0.85742
	loss_value_0: 0.29758
	loss_policy_1: 0.04338
	accuracy_policy_1: 0.85895
	loss_value_1: 0.06122
	loss_reward_1: 0.0045
	loss_policy_2: 0.04351
	accuracy_policy_2: 0.85695
	loss_value_2: 0.06317
	loss_reward_2: 0.00633
	loss_policy_3: 0.04354
	accuracy_policy_3: 0.85844
	loss_value_3: 0.06512
	loss_reward_3: 0.00749
	loss_policy_4: 0.04325
	accuracy_policy_4: 0.86324
	loss_value_4: 0.06704
	loss_reward_4: 0.0092
	loss_policy_5: 0.04357
	accuracy_policy_5: 0.86246
	loss_value_5: 0.06913
	loss_reward_5: 0.01051
	loss_policy: 0.43511
	loss_value: 0.62326
	loss_reward: 0.03804
Optimization_Done 10800
[2025-05-11 12:39:47] [command] train weight_iter_10800.pkl 36 55
[2025-05-11 12:39:56] nn step 10850, lr: 0.1.
	loss_policy_0: 0.2072
	accuracy_policy_0: 0.86492
	loss_value_0: 0.298
	loss_policy_1: 0.04098
	accuracy_policy_1: 0.86859
	loss_value_1: 0.06057
	loss_reward_1: 0.00413
	loss_policy_2: 0.04115
	accuracy_policy_2: 0.86375
	loss_value_2: 0.06211
	loss_reward_2: 0.00608
	loss_policy_3: 0.0412
	accuracy_policy_3: 0.86812
	loss_value_3: 0.06349
	loss_reward_3: 0.00779
	loss_policy_4: 0.04106
	accuracy_policy_4: 0.87027
	loss_value_4: 0.06533
	loss_reward_4: 0.00903
	loss_policy_5: 0.04129
	accuracy_policy_5: 0.87059
	loss_value_5: 0.06723
	loss_reward_5: 0.01025
	loss_policy: 0.41288
	loss_value: 0.61673
	loss_reward: 0.03728
[2025-05-11 12:40:03] nn step 10900, lr: 0.1.
	loss_policy_0: 0.1882
	accuracy_policy_0: 0.85977
	loss_value_0: 0.25805
	loss_policy_1: 0.03739
	accuracy_policy_1: 0.86156
	loss_value_1: 0.05296
	loss_reward_1: 0.00382
	loss_policy_2: 0.03748
	accuracy_policy_2: 0.8652
	loss_value_2: 0.05444
	loss_reward_2: 0.00517
	loss_policy_3: 0.03717
	accuracy_policy_3: 0.86547
	loss_value_3: 0.05614
	loss_reward_3: 0.00642
	loss_policy_4: 0.03708
	accuracy_policy_4: 0.86508
	loss_value_4: 0.05779
	loss_reward_4: 0.00781
	loss_policy_5: 0.03746
	accuracy_policy_5: 0.8693
	loss_value_5: 0.05953
	loss_reward_5: 0.00903
	loss_policy: 0.37478
	loss_value: 0.53891
	loss_reward: 0.03224
[2025-05-11 12:40:12] nn step 10950, lr: 0.1.
	loss_policy_0: 0.20883
	accuracy_policy_0: 0.86477
	loss_value_0: 0.29246
	loss_policy_1: 0.04167
	accuracy_policy_1: 0.86305
	loss_value_1: 0.06011
	loss_reward_1: 0.0043
	loss_policy_2: 0.04185
	accuracy_policy_2: 0.86402
	loss_value_2: 0.06172
	loss_reward_2: 0.00595
	loss_policy_3: 0.04164
	accuracy_policy_3: 0.8625
	loss_value_3: 0.06362
	loss_reward_3: 0.00733
	loss_policy_4: 0.04164
	accuracy_policy_4: 0.86441
	loss_value_4: 0.06553
	loss_reward_4: 0.00886
	loss_policy_5: 0.04175
	accuracy_policy_5: 0.86742
	loss_value_5: 0.06733
	loss_reward_5: 0.0103
	loss_policy: 0.41739
	loss_value: 0.61076
	loss_reward: 0.03675
[2025-05-11 12:40:21] nn step 11000, lr: 0.1.
	loss_policy_0: 0.22383
	accuracy_policy_0: 0.86156
	loss_value_0: 0.30825
	loss_policy_1: 0.04421
	accuracy_policy_1: 0.86301
	loss_value_1: 0.06319
	loss_reward_1: 0.00475
	loss_policy_2: 0.04461
	accuracy_policy_2: 0.86426
	loss_value_2: 0.06514
	loss_reward_2: 0.00626
	loss_policy_3: 0.04445
	accuracy_policy_3: 0.86539
	loss_value_3: 0.06729
	loss_reward_3: 0.00804
	loss_policy_4: 0.04469
	accuracy_policy_4: 0.86574
	loss_value_4: 0.06927
	loss_reward_4: 0.00947
	loss_policy_5: 0.0444
	accuracy_policy_5: 0.86898
	loss_value_5: 0.07114
	loss_reward_5: 0.01093
	loss_policy: 0.44619
	loss_value: 0.64428
	loss_reward: 0.03946
Optimization_Done 11000
[2025-05-11 12:41:36] [command] train weight_iter_11000.pkl 37 56
[2025-05-11 12:41:46] nn step 11050, lr: 0.1.
	loss_policy_0: 0.20974
	accuracy_policy_0: 0.86141
	loss_value_0: 0.29844
	loss_policy_1: 0.04193
	accuracy_policy_1: 0.86488
	loss_value_1: 0.06079
	loss_reward_1: 0.00439
	loss_policy_2: 0.04204
	accuracy_policy_2: 0.85949
	loss_value_2: 0.06292
	loss_reward_2: 0.00607
	loss_policy_3: 0.0418
	accuracy_policy_3: 0.86371
	loss_value_3: 0.06478
	loss_reward_3: 0.00765
	loss_policy_4: 0.0422
	accuracy_policy_4: 0.86332
	loss_value_4: 0.06629
	loss_reward_4: 0.00881
	loss_policy_5: 0.04155
	accuracy_policy_5: 0.86688
	loss_value_5: 0.068
	loss_reward_5: 0.01017
	loss_policy: 0.41926
	loss_value: 0.62123
	loss_reward: 0.0371
[2025-05-11 12:41:53] nn step 11100, lr: 0.1.
	loss_policy_0: 0.21098
	accuracy_policy_0: 0.86285
	loss_value_0: 0.30025
	loss_policy_1: 0.04235
	accuracy_policy_1: 0.86262
	loss_value_1: 0.06169
	loss_reward_1: 0.0045
	loss_policy_2: 0.04219
	accuracy_policy_2: 0.85809
	loss_value_2: 0.06358
	loss_reward_2: 0.00607
	loss_policy_3: 0.04196
	accuracy_policy_3: 0.865
	loss_value_3: 0.06547
	loss_reward_3: 0.00732
	loss_policy_4: 0.04198
	accuracy_policy_4: 0.86707
	loss_value_4: 0.06732
	loss_reward_4: 0.00942
	loss_policy_5: 0.04207
	accuracy_policy_5: 0.87109
	loss_value_5: 0.06933
	loss_reward_5: 0.01045
	loss_policy: 0.42153
	loss_value: 0.62763
	loss_reward: 0.03775
[2025-05-11 12:42:01] nn step 11150, lr: 0.1.
	loss_policy_0: 0.21674
	accuracy_policy_0: 0.86605
	loss_value_0: 0.3008
	loss_policy_1: 0.04293
	accuracy_policy_1: 0.865
	loss_value_1: 0.06174
	loss_reward_1: 0.00446
	loss_policy_2: 0.04297
	accuracy_policy_2: 0.86227
	loss_value_2: 0.0636
	loss_reward_2: 0.00612
	loss_policy_3: 0.04305
	accuracy_policy_3: 0.86383
	loss_value_3: 0.06581
	loss_reward_3: 0.00751
	loss_policy_4: 0.0431
	accuracy_policy_4: 0.86281
	loss_value_4: 0.06767
	loss_reward_4: 0.00902
	loss_policy_5: 0.04293
	accuracy_policy_5: 0.86742
	loss_value_5: 0.06961
	loss_reward_5: 0.01043
	loss_policy: 0.43172
	loss_value: 0.62923
	loss_reward: 0.03754
[2025-05-11 12:42:10] nn step 11200, lr: 0.1.
	loss_policy_0: 0.20955
	accuracy_policy_0: 0.86254
	loss_value_0: 0.2893
	loss_policy_1: 0.04218
	accuracy_policy_1: 0.86129
	loss_value_1: 0.05914
	loss_reward_1: 0.00442
	loss_policy_2: 0.04207
	accuracy_policy_2: 0.86117
	loss_value_2: 0.06104
	loss_reward_2: 0.00599
	loss_policy_3: 0.04177
	accuracy_policy_3: 0.86258
	loss_value_3: 0.06313
	loss_reward_3: 0.00749
	loss_policy_4: 0.04213
	accuracy_policy_4: 0.86379
	loss_value_4: 0.06499
	loss_reward_4: 0.00881
	loss_policy_5: 0.04193
	accuracy_policy_5: 0.86816
	loss_value_5: 0.06695
	loss_reward_5: 0.01044
	loss_policy: 0.41963
	loss_value: 0.60455
	loss_reward: 0.03715
Optimization_Done 11200
[2025-05-11 12:43:28] [command] train weight_iter_11200.pkl 38 57
[2025-05-11 12:43:38] nn step 11250, lr: 0.1.
	loss_policy_0: 0.20769
	accuracy_policy_0: 0.86805
	loss_value_0: 0.29808
	loss_policy_1: 0.04151
	accuracy_policy_1: 0.86582
	loss_value_1: 0.0612
	loss_reward_1: 0.00428
	loss_policy_2: 0.04151
	accuracy_policy_2: 0.86641
	loss_value_2: 0.06279
	loss_reward_2: 0.0062
	loss_policy_3: 0.04135
	accuracy_policy_3: 0.86848
	loss_value_3: 0.06443
	loss_reward_3: 0.00753
	loss_policy_4: 0.04116
	accuracy_policy_4: 0.87016
	loss_value_4: 0.06607
	loss_reward_4: 0.00909
	loss_policy_5: 0.04106
	accuracy_policy_5: 0.87297
	loss_value_5: 0.06803
	loss_reward_5: 0.01001
	loss_policy: 0.41428
	loss_value: 0.6206
	loss_reward: 0.03712
[2025-05-11 12:43:45] nn step 11300, lr: 0.1.
	loss_policy_0: 0.21125
	accuracy_policy_0: 0.86766
	loss_value_0: 0.30068
	loss_policy_1: 0.04229
	accuracy_policy_1: 0.86801
	loss_value_1: 0.06162
	loss_reward_1: 0.0043
	loss_policy_2: 0.04262
	accuracy_policy_2: 0.86656
	loss_value_2: 0.06335
	loss_reward_2: 0.00605
	loss_policy_3: 0.04223
	accuracy_policy_3: 0.87512
	loss_value_3: 0.06516
	loss_reward_3: 0.00747
	loss_policy_4: 0.04232
	accuracy_policy_4: 0.8707
	loss_value_4: 0.0667
	loss_reward_4: 0.00892
	loss_policy_5: 0.04198
	accuracy_policy_5: 0.8743
	loss_value_5: 0.06855
	loss_reward_5: 0.01036
	loss_policy: 0.42269
	loss_value: 0.62605
	loss_reward: 0.03711
[2025-05-11 12:43:53] nn step 11350, lr: 0.1.
	loss_policy_0: 0.21389
	accuracy_policy_0: 0.86332
	loss_value_0: 0.30144
	loss_policy_1: 0.04327
	accuracy_policy_1: 0.86453
	loss_value_1: 0.06192
	loss_reward_1: 0.00435
	loss_policy_2: 0.04345
	accuracy_policy_2: 0.86672
	loss_value_2: 0.06389
	loss_reward_2: 0.00615
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.86508
	loss_value_3: 0.06556
	loss_reward_3: 0.00793
	loss_policy_4: 0.04305
	accuracy_policy_4: 0.87012
	loss_value_4: 0.06728
	loss_reward_4: 0.00919
	loss_policy_5: 0.04291
	accuracy_policy_5: 0.87203
	loss_value_5: 0.06974
	loss_reward_5: 0.01056
	loss_policy: 0.42994
	loss_value: 0.62982
	loss_reward: 0.03817
[2025-05-11 12:44:02] nn step 11400, lr: 0.1.
	loss_policy_0: 0.20683
	accuracy_policy_0: 0.8625
	loss_value_0: 0.28582
	loss_policy_1: 0.04107
	accuracy_policy_1: 0.86754
	loss_value_1: 0.05882
	loss_reward_1: 0.00397
	loss_policy_2: 0.0408
	accuracy_policy_2: 0.86523
	loss_value_2: 0.06059
	loss_reward_2: 0.00582
	loss_policy_3: 0.04089
	accuracy_policy_3: 0.86762
	loss_value_3: 0.06257
	loss_reward_3: 0.00723
	loss_policy_4: 0.04097
	accuracy_policy_4: 0.87035
	loss_value_4: 0.06466
	loss_reward_4: 0.00913
	loss_policy_5: 0.04032
	accuracy_policy_5: 0.87164
	loss_value_5: 0.0666
	loss_reward_5: 0.01016
	loss_policy: 0.41089
	loss_value: 0.59906
	loss_reward: 0.0363
Optimization_Done 11400
[2025-05-11 12:45:17] [command] train weight_iter_11400.pkl 39 58
[2025-05-11 12:45:26] nn step 11450, lr: 0.1.
	loss_policy_0: 0.19956
	accuracy_policy_0: 0.8675
	loss_value_0: 0.282
	loss_policy_1: 0.03975
	accuracy_policy_1: 0.86793
	loss_value_1: 0.05776
	loss_reward_1: 0.00414
	loss_policy_2: 0.03999
	accuracy_policy_2: 0.8666
	loss_value_2: 0.05939
	loss_reward_2: 0.00562
	loss_policy_3: 0.04001
	accuracy_policy_3: 0.87012
	loss_value_3: 0.06138
	loss_reward_3: 0.00719
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.87309
	loss_value_4: 0.06314
	loss_reward_4: 0.00871
	loss_policy_5: 0.03972
	accuracy_policy_5: 0.87371
	loss_value_5: 0.06492
	loss_reward_5: 0.00972
	loss_policy: 0.39869
	loss_value: 0.58859
	loss_reward: 0.03538
[2025-05-11 12:45:35] nn step 11500, lr: 0.1.
	loss_policy_0: 0.22352
	accuracy_policy_0: 0.86891
	loss_value_0: 0.31837
	loss_policy_1: 0.04488
	accuracy_policy_1: 0.8718
	loss_value_1: 0.06484
	loss_reward_1: 0.00472
	loss_policy_2: 0.04478
	accuracy_policy_2: 0.86742
	loss_value_2: 0.06675
	loss_reward_2: 0.00633
	loss_policy_3: 0.04483
	accuracy_policy_3: 0.8693
	loss_value_3: 0.06867
	loss_reward_3: 0.00786
	loss_policy_4: 0.04463
	accuracy_policy_4: 0.87434
	loss_value_4: 0.07061
	loss_reward_4: 0.00972
	loss_policy_5: 0.04492
	accuracy_policy_5: 0.87188
	loss_value_5: 0.0732
	loss_reward_5: 0.01091
	loss_policy: 0.44754
	loss_value: 0.66244
	loss_reward: 0.03953
[2025-05-11 12:45:42] nn step 11550, lr: 0.1.
	loss_policy_0: 0.20431
	accuracy_policy_0: 0.86875
	loss_value_0: 0.28636
	loss_policy_1: 0.0408
	accuracy_policy_1: 0.87105
	loss_value_1: 0.05857
	loss_reward_1: 0.00439
	loss_policy_2: 0.04119
	accuracy_policy_2: 0.86625
	loss_value_2: 0.06065
	loss_reward_2: 0.00567
	loss_policy_3: 0.04122
	accuracy_policy_3: 0.86836
	loss_value_3: 0.0625
	loss_reward_3: 0.00739
	loss_policy_4: 0.04121
	accuracy_policy_4: 0.87184
	loss_value_4: 0.0643
	loss_reward_4: 0.00862
	loss_policy_5: 0.04088
	accuracy_policy_5: 0.8748
	loss_value_5: 0.06634
	loss_reward_5: 0.00988
	loss_policy: 0.40961
	loss_value: 0.59871
	loss_reward: 0.03596
[2025-05-11 12:45:50] nn step 11600, lr: 0.1.
	loss_policy_0: 0.19622
	accuracy_policy_0: 0.87285
	loss_value_0: 0.27311
	loss_policy_1: 0.03954
	accuracy_policy_1: 0.86766
	loss_value_1: 0.05637
	loss_reward_1: 0.00403
	loss_policy_2: 0.03972
	accuracy_policy_2: 0.86559
	loss_value_2: 0.05831
	loss_reward_2: 0.00588
	loss_policy_3: 0.03931
	accuracy_policy_3: 0.87121
	loss_value_3: 0.06015
	loss_reward_3: 0.00708
	loss_policy_4: 0.03955
	accuracy_policy_4: 0.86969
	loss_value_4: 0.06216
	loss_reward_4: 0.00872
	loss_policy_5: 0.03908
	accuracy_policy_5: 0.87414
	loss_value_5: 0.06432
	loss_reward_5: 0.00962
	loss_policy: 0.39341
	loss_value: 0.57443
	loss_reward: 0.03533
Optimization_Done 11600
[2025-05-11 12:47:07] [command] train weight_iter_11600.pkl 40 59
[2025-05-11 12:47:17] nn step 11650, lr: 0.1.
	loss_policy_0: 0.20829
	accuracy_policy_0: 0.8632
	loss_value_0: 0.29365
	loss_policy_1: 0.04154
	accuracy_policy_1: 0.86918
	loss_value_1: 0.06004
	loss_reward_1: 0.00434
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.86465
	loss_value_2: 0.06215
	loss_reward_2: 0.00593
	loss_policy_3: 0.04176
	accuracy_policy_3: 0.86766
	loss_value_3: 0.06404
	loss_reward_3: 0.00719
	loss_policy_4: 0.04164
	accuracy_policy_4: 0.87137
	loss_value_4: 0.06583
	loss_reward_4: 0.00888
	loss_policy_5: 0.0415
	accuracy_policy_5: 0.87152
	loss_value_5: 0.06777
	loss_reward_5: 0.01034
	loss_policy: 0.41666
	loss_value: 0.61349
	loss_reward: 0.03668
[2025-05-11 12:47:25] nn step 11700, lr: 0.1.
	loss_policy_0: 0.19997
	accuracy_policy_0: 0.86637
	loss_value_0: 0.27817
	loss_policy_1: 0.04027
	accuracy_policy_1: 0.86301
	loss_value_1: 0.05682
	loss_reward_1: 0.0042
	loss_policy_2: 0.03999
	accuracy_policy_2: 0.86703
	loss_value_2: 0.05894
	loss_reward_2: 0.00576
	loss_policy_3: 0.04016
	accuracy_policy_3: 0.8652
	loss_value_3: 0.06057
	loss_reward_3: 0.00679
	loss_policy_4: 0.03963
	accuracy_policy_4: 0.87152
	loss_value_4: 0.06215
	loss_reward_4: 0.00853
	loss_policy_5: 0.03961
	accuracy_policy_5: 0.87348
	loss_value_5: 0.0642
	loss_reward_5: 0.01
	loss_policy: 0.39963
	loss_value: 0.58085
	loss_reward: 0.03528
[2025-05-11 12:47:32] nn step 11750, lr: 0.1.
	loss_policy_0: 0.20586
	accuracy_policy_0: 0.86645
	loss_value_0: 0.28522
	loss_policy_1: 0.0407
	accuracy_policy_1: 0.86559
	loss_value_1: 0.05848
	loss_reward_1: 0.00423
	loss_policy_2: 0.04132
	accuracy_policy_2: 0.86836
	loss_value_2: 0.06027
	loss_reward_2: 0.00583
	loss_policy_3: 0.04119
	accuracy_policy_3: 0.86789
	loss_value_3: 0.06221
	loss_reward_3: 0.00691
	loss_policy_4: 0.041
	accuracy_policy_4: 0.87051
	loss_value_4: 0.06381
	loss_reward_4: 0.00875
	loss_policy_5: 0.04139
	accuracy_policy_5: 0.87273
	loss_value_5: 0.06578
	loss_reward_5: 0.01001
	loss_policy: 0.41147
	loss_value: 0.59577
	loss_reward: 0.03572
[2025-05-11 12:47:41] nn step 11800, lr: 0.1.
	loss_policy_0: 0.20172
	accuracy_policy_0: 0.86957
	loss_value_0: 0.2807
	loss_policy_1: 0.04071
	accuracy_policy_1: 0.8652
	loss_value_1: 0.05747
	loss_reward_1: 0.00414
	loss_policy_2: 0.04065
	accuracy_policy_2: 0.86988
	loss_value_2: 0.05954
	loss_reward_2: 0.00578
	loss_policy_3: 0.04072
	accuracy_policy_3: 0.86895
	loss_value_3: 0.0613
	loss_reward_3: 0.00733
	loss_policy_4: 0.04068
	accuracy_policy_4: 0.86891
	loss_value_4: 0.06312
	loss_reward_4: 0.00891
	loss_policy_5: 0.04036
	accuracy_policy_5: 0.87398
	loss_value_5: 0.06488
	loss_reward_5: 0.01008
	loss_policy: 0.40484
	loss_value: 0.58701
	loss_reward: 0.03624
Optimization_Done 11800
[2025-05-11 12:48:56] [command] train weight_iter_11800.pkl 41 60
[2025-05-11 12:49:04] nn step 11850, lr: 0.1.
	loss_policy_0: 0.21413
	accuracy_policy_0: 0.8652
	loss_value_0: 0.30523
	loss_policy_1: 0.04267
	accuracy_policy_1: 0.86883
	loss_value_1: 0.06247
	loss_reward_1: 0.00491
	loss_policy_2: 0.04301
	accuracy_policy_2: 0.86762
	loss_value_2: 0.06459
	loss_reward_2: 0.00641
	loss_policy_3: 0.04268
	accuracy_policy_3: 0.87016
	loss_value_3: 0.06655
	loss_reward_3: 0.00752
	loss_policy_4: 0.043
	accuracy_policy_4: 0.87125
	loss_value_4: 0.06839
	loss_reward_4: 0.00959
	loss_policy_5: 0.04265
	accuracy_policy_5: 0.87566
	loss_value_5: 0.07035
	loss_reward_5: 0.01096
	loss_policy: 0.42814
	loss_value: 0.63757
	loss_reward: 0.03939
[2025-05-11 12:49:13] nn step 11900, lr: 0.1.
	loss_policy_0: 0.20687
	accuracy_policy_0: 0.87172
	loss_value_0: 0.28971
	loss_policy_1: 0.0414
	accuracy_policy_1: 0.86832
	loss_value_1: 0.05909
	loss_reward_1: 0.00428
	loss_policy_2: 0.04153
	accuracy_policy_2: 0.86762
	loss_value_2: 0.06114
	loss_reward_2: 0.00618
	loss_policy_3: 0.04137
	accuracy_policy_3: 0.86879
	loss_value_3: 0.0629
	loss_reward_3: 0.00762
	loss_policy_4: 0.04133
	accuracy_policy_4: 0.87246
	loss_value_4: 0.06501
	loss_reward_4: 0.00913
	loss_policy_5: 0.0412
	accuracy_policy_5: 0.87449
	loss_value_5: 0.06728
	loss_reward_5: 0.01005
	loss_policy: 0.41369
	loss_value: 0.60513
	loss_reward: 0.03727
[2025-05-11 12:49:21] nn step 11950, lr: 0.1.
	loss_policy_0: 0.20554
	accuracy_policy_0: 0.86668
	loss_value_0: 0.2877
	loss_policy_1: 0.0409
	accuracy_policy_1: 0.86875
	loss_value_1: 0.0588
	loss_reward_1: 0.00437
	loss_policy_2: 0.0409
	accuracy_policy_2: 0.86934
	loss_value_2: 0.06053
	loss_reward_2: 0.00573
	loss_policy_3: 0.04073
	accuracy_policy_3: 0.87035
	loss_value_3: 0.06229
	loss_reward_3: 0.00698
	loss_policy_4: 0.04086
	accuracy_policy_4: 0.87047
	loss_value_4: 0.06395
	loss_reward_4: 0.00898
	loss_policy_5: 0.0407
	accuracy_policy_5: 0.87215
	loss_value_5: 0.06612
	loss_reward_5: 0.01018
	loss_policy: 0.40961
	loss_value: 0.59938
	loss_reward: 0.03624
[2025-05-11 12:49:28] nn step 12000, lr: 0.1.
	loss_policy_0: 0.20718
	accuracy_policy_0: 0.8691
	loss_value_0: 0.2897
	loss_policy_1: 0.04145
	accuracy_policy_1: 0.86551
	loss_value_1: 0.05924
	loss_reward_1: 0.00445
	loss_policy_2: 0.04165
	accuracy_policy_2: 0.86617
	loss_value_2: 0.06131
	loss_reward_2: 0.00598
	loss_policy_3: 0.04136
	accuracy_policy_3: 0.87094
	loss_value_3: 0.06315
	loss_reward_3: 0.00752
	loss_policy_4: 0.04125
	accuracy_policy_4: 0.87504
	loss_value_4: 0.06499
	loss_reward_4: 0.00889
	loss_policy_5: 0.04076
	accuracy_policy_5: 0.87227
	loss_value_5: 0.06678
	loss_reward_5: 0.01032
	loss_policy: 0.41365
	loss_value: 0.60518
	loss_reward: 0.03717
Optimization_Done 12000
[2025-05-11 12:50:45] [command] train weight_iter_12000.pkl 42 61
[2025-05-11 12:50:53] nn step 12050, lr: 0.1.
	loss_policy_0: 0.20823
	accuracy_policy_0: 0.8707
	loss_value_0: 0.30329
	loss_policy_1: 0.04184
	accuracy_policy_1: 0.87043
	loss_value_1: 0.0618
	loss_reward_1: 0.00439
	loss_policy_2: 0.04198
	accuracy_policy_2: 0.87379
	loss_value_2: 0.0637
	loss_reward_2: 0.00611
	loss_policy_3: 0.04182
	accuracy_policy_3: 0.87344
	loss_value_3: 0.06544
	loss_reward_3: 0.00749
	loss_policy_4: 0.04166
	accuracy_policy_4: 0.87496
	loss_value_4: 0.06706
	loss_reward_4: 0.00906
	loss_policy_5: 0.0418
	accuracy_policy_5: 0.87602
	loss_value_5: 0.06911
	loss_reward_5: 0.01033
	loss_policy: 0.41733
	loss_value: 0.63039
	loss_reward: 0.03738
[2025-05-11 12:51:02] nn step 12100, lr: 0.1.
	loss_policy_0: 0.19437
	accuracy_policy_0: 0.87137
	loss_value_0: 0.27787
	loss_policy_1: 0.03871
	accuracy_policy_1: 0.87109
	loss_value_1: 0.05693
	loss_reward_1: 0.00405
	loss_policy_2: 0.03933
	accuracy_policy_2: 0.87047
	loss_value_2: 0.05858
	loss_reward_2: 0.00537
	loss_policy_3: 0.03898
	accuracy_policy_3: 0.87055
	loss_value_3: 0.06016
	loss_reward_3: 0.00698
	loss_policy_4: 0.03919
	accuracy_policy_4: 0.87512
	loss_value_4: 0.06186
	loss_reward_4: 0.00853
	loss_policy_5: 0.03918
	accuracy_policy_5: 0.87379
	loss_value_5: 0.06352
	loss_reward_5: 0.00928
	loss_policy: 0.38976
	loss_value: 0.57892
	loss_reward: 0.03422
[2025-05-11 12:51:10] nn step 12150, lr: 0.1.
	loss_policy_0: 0.20251
	accuracy_policy_0: 0.86891
	loss_value_0: 0.28407
	loss_policy_1: 0.04062
	accuracy_policy_1: 0.87004
	loss_value_1: 0.05806
	loss_reward_1: 0.00422
	loss_policy_2: 0.04054
	accuracy_policy_2: 0.86988
	loss_value_2: 0.06012
	loss_reward_2: 0.00584
	loss_policy_3: 0.04031
	accuracy_policy_3: 0.87363
	loss_value_3: 0.06187
	loss_reward_3: 0.00728
	loss_policy_4: 0.04063
	accuracy_policy_4: 0.87562
	loss_value_4: 0.0635
	loss_reward_4: 0.00872
	loss_policy_5: 0.04027
	accuracy_policy_5: 0.87523
	loss_value_5: 0.06549
	loss_reward_5: 0.00979
	loss_policy: 0.40487
	loss_value: 0.5931
	loss_reward: 0.03585
[2025-05-11 12:51:19] nn step 12200, lr: 0.1.
	loss_policy_0: 0.20449
	accuracy_policy_0: 0.86855
	loss_value_0: 0.28512
	loss_policy_1: 0.04083
	accuracy_policy_1: 0.86816
	loss_value_1: 0.05843
	loss_reward_1: 0.00432
	loss_policy_2: 0.04091
	accuracy_policy_2: 0.86832
	loss_value_2: 0.06016
	loss_reward_2: 0.00599
	loss_policy_3: 0.04089
	accuracy_policy_3: 0.87113
	loss_value_3: 0.0622
	loss_reward_3: 0.0073
	loss_policy_4: 0.04095
	accuracy_policy_4: 0.86672
	loss_value_4: 0.06401
	loss_reward_4: 0.00866
	loss_policy_5: 0.04059
	accuracy_policy_5: 0.87508
	loss_value_5: 0.06605
	loss_reward_5: 0.01005
	loss_policy: 0.40865
	loss_value: 0.59596
	loss_reward: 0.03632
Optimization_Done 12200
[2025-05-11 12:52:34] [command] train weight_iter_12200.pkl 43 62
[2025-05-11 12:52:43] nn step 12250, lr: 0.1.
	loss_policy_0: 0.18998
	accuracy_policy_0: 0.87707
	loss_value_0: 0.27387
	loss_policy_1: 0.03816
	accuracy_policy_1: 0.87832
	loss_value_1: 0.05568
	loss_reward_1: 0.00405
	loss_policy_2: 0.03835
	accuracy_policy_2: 0.87984
	loss_value_2: 0.05728
	loss_reward_2: 0.00569
	loss_policy_3: 0.03822
	accuracy_policy_3: 0.88328
	loss_value_3: 0.05899
	loss_reward_3: 0.00675
	loss_policy_4: 0.03828
	accuracy_policy_4: 0.88
	loss_value_4: 0.06071
	loss_reward_4: 0.00845
	loss_policy_5: 0.03813
	accuracy_policy_5: 0.8852
	loss_value_5: 0.06249
	loss_reward_5: 0.00955
	loss_policy: 0.38112
	loss_value: 0.56902
	loss_reward: 0.03449
[2025-05-11 12:52:50] nn step 12300, lr: 0.1.
	loss_policy_0: 0.20568
	accuracy_policy_0: 0.87602
	loss_value_0: 0.28946
	loss_policy_1: 0.04121
	accuracy_policy_1: 0.87441
	loss_value_1: 0.05924
	loss_reward_1: 0.00463
	loss_policy_2: 0.0411
	accuracy_policy_2: 0.87535
	loss_value_2: 0.06099
	loss_reward_2: 0.00619
	loss_policy_3: 0.04105
	accuracy_policy_3: 0.87672
	loss_value_3: 0.06268
	loss_reward_3: 0.00724
	loss_policy_4: 0.04079
	accuracy_policy_4: 0.87738
	loss_value_4: 0.06469
	loss_reward_4: 0.00911
	loss_policy_5: 0.04052
	accuracy_policy_5: 0.88254
	loss_value_5: 0.06692
	loss_reward_5: 0.01083
	loss_policy: 0.41035
	loss_value: 0.60398
	loss_reward: 0.038
[2025-05-11 12:52:58] nn step 12350, lr: 0.1.
	loss_policy_0: 0.20466
	accuracy_policy_0: 0.87758
	loss_value_0: 0.28991
	loss_policy_1: 0.04065
	accuracy_policy_1: 0.87656
	loss_value_1: 0.05925
	loss_reward_1: 0.00446
	loss_policy_2: 0.04074
	accuracy_policy_2: 0.87812
	loss_value_2: 0.06122
	loss_reward_2: 0.00633
	loss_policy_3: 0.04062
	accuracy_policy_3: 0.88164
	loss_value_3: 0.0628
	loss_reward_3: 0.00767
	loss_policy_4: 0.04095
	accuracy_policy_4: 0.87844
	loss_value_4: 0.06498
	loss_reward_4: 0.00961
	loss_policy_5: 0.04077
	accuracy_policy_5: 0.8832
	loss_value_5: 0.06705
	loss_reward_5: 0.01059
	loss_policy: 0.40839
	loss_value: 0.60521
	loss_reward: 0.03865
[2025-05-11 12:53:07] nn step 12400, lr: 0.1.
	loss_policy_0: 0.20713
	accuracy_policy_0: 0.87277
	loss_value_0: 0.28888
	loss_policy_1: 0.04135
	accuracy_policy_1: 0.87617
	loss_value_1: 0.0593
	loss_reward_1: 0.00424
	loss_policy_2: 0.04126
	accuracy_policy_2: 0.8759
	loss_value_2: 0.06137
	loss_reward_2: 0.00574
	loss_policy_3: 0.04117
	accuracy_policy_3: 0.87867
	loss_value_3: 0.06294
	loss_reward_3: 0.00723
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.87848
	loss_value_4: 0.06508
	loss_reward_4: 0.00879
	loss_policy_5: 0.04121
	accuracy_policy_5: 0.87984
	loss_value_5: 0.06708
	loss_reward_5: 0.01004
	loss_policy: 0.41348
	loss_value: 0.60466
	loss_reward: 0.03604
Optimization_Done 12400
[2025-05-11 12:54:23] [command] train weight_iter_12400.pkl 44 63
[2025-05-11 12:54:33] nn step 12450, lr: 0.1.
	loss_policy_0: 0.1981
	accuracy_policy_0: 0.87672
	loss_value_0: 0.28482
	loss_policy_1: 0.03934
	accuracy_policy_1: 0.88004
	loss_value_1: 0.05797
	loss_reward_1: 0.0042
	loss_policy_2: 0.03957
	accuracy_policy_2: 0.87629
	loss_value_2: 0.06004
	loss_reward_2: 0.00585
	loss_policy_3: 0.03927
	accuracy_policy_3: 0.87941
	loss_value_3: 0.06138
	loss_reward_3: 0.00708
	loss_policy_4: 0.03917
	accuracy_policy_4: 0.88277
	loss_value_4: 0.06328
	loss_reward_4: 0.00881
	loss_policy_5: 0.03906
	accuracy_policy_5: 0.88172
	loss_value_5: 0.06512
	loss_reward_5: 0.01005
	loss_policy: 0.39451
	loss_value: 0.59261
	loss_reward: 0.03599
[2025-05-11 12:54:40] nn step 12500, lr: 0.1.
	loss_policy_0: 0.19603
	accuracy_policy_0: 0.87727
	loss_value_0: 0.27895
	loss_policy_1: 0.03913
	accuracy_policy_1: 0.87543
	loss_value_1: 0.05723
	loss_reward_1: 0.00412
	loss_policy_2: 0.03902
	accuracy_policy_2: 0.87613
	loss_value_2: 0.05933
	loss_reward_2: 0.00573
	loss_policy_3: 0.03914
	accuracy_policy_3: 0.87781
	loss_value_3: 0.061
	loss_reward_3: 0.00704
	loss_policy_4: 0.03932
	accuracy_policy_4: 0.87699
	loss_value_4: 0.06259
	loss_reward_4: 0.00855
	loss_policy_5: 0.03911
	accuracy_policy_5: 0.88012
	loss_value_5: 0.06423
	loss_reward_5: 0.00966
	loss_policy: 0.39175
	loss_value: 0.58334
	loss_reward: 0.03511
[2025-05-11 12:54:48] nn step 12550, lr: 0.1.
	loss_policy_0: 0.20176
	accuracy_policy_0: 0.87906
	loss_value_0: 0.28622
	loss_policy_1: 0.04044
	accuracy_policy_1: 0.88242
	loss_value_1: 0.0586
	loss_reward_1: 0.00426
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.87961
	loss_value_2: 0.06052
	loss_reward_2: 0.00587
	loss_policy_3: 0.04034
	accuracy_policy_3: 0.87945
	loss_value_3: 0.06278
	loss_reward_3: 0.00743
	loss_policy_4: 0.04045
	accuracy_policy_4: 0.88312
	loss_value_4: 0.06432
	loss_reward_4: 0.00899
	loss_policy_5: 0.03998
	accuracy_policy_5: 0.88543
	loss_value_5: 0.0661
	loss_reward_5: 0.01038
	loss_policy: 0.40302
	loss_value: 0.59854
	loss_reward: 0.03693
[2025-05-11 12:54:57] nn step 12600, lr: 0.1.
	loss_policy_0: 0.20854
	accuracy_policy_0: 0.87812
	loss_value_0: 0.29575
	loss_policy_1: 0.04176
	accuracy_policy_1: 0.87508
	loss_value_1: 0.06092
	loss_reward_1: 0.00478
	loss_policy_2: 0.04167
	accuracy_policy_2: 0.87605
	loss_value_2: 0.06244
	loss_reward_2: 0.00624
	loss_policy_3: 0.04159
	accuracy_policy_3: 0.8773
	loss_value_3: 0.06428
	loss_reward_3: 0.00766
	loss_policy_4: 0.04168
	accuracy_policy_4: 0.87961
	loss_value_4: 0.06597
	loss_reward_4: 0.00941
	loss_policy_5: 0.0416
	accuracy_policy_5: 0.88156
	loss_value_5: 0.06798
	loss_reward_5: 0.01065
	loss_policy: 0.41683
	loss_value: 0.61735
	loss_reward: 0.03874
Optimization_Done 12600
[2025-05-11 12:56:13] [command] train weight_iter_12600.pkl 45 64
[2025-05-11 12:56:22] nn step 12650, lr: 0.1.
	loss_policy_0: 0.1955
	accuracy_policy_0: 0.87711
	loss_value_0: 0.28499
	loss_policy_1: 0.03902
	accuracy_policy_1: 0.87617
	loss_value_1: 0.05811
	loss_reward_1: 0.00429
	loss_policy_2: 0.03916
	accuracy_policy_2: 0.87957
	loss_value_2: 0.05978
	loss_reward_2: 0.00588
	loss_policy_3: 0.03917
	accuracy_policy_3: 0.87754
	loss_value_3: 0.06148
	loss_reward_3: 0.00722
	loss_policy_4: 0.03914
	accuracy_policy_4: 0.88195
	loss_value_4: 0.063
	loss_reward_4: 0.00873
	loss_policy_5: 0.03887
	accuracy_policy_5: 0.88477
	loss_value_5: 0.06497
	loss_reward_5: 0.01011
	loss_policy: 0.39087
	loss_value: 0.59233
	loss_reward: 0.03624
[2025-05-11 12:56:30] nn step 12700, lr: 0.1.
	loss_policy_0: 0.20517
	accuracy_policy_0: 0.8768
	loss_value_0: 0.2944
	loss_policy_1: 0.0409
	accuracy_policy_1: 0.87848
	loss_value_1: 0.06027
	loss_reward_1: 0.00429
	loss_policy_2: 0.04126
	accuracy_policy_2: 0.87703
	loss_value_2: 0.06228
	loss_reward_2: 0.00583
	loss_policy_3: 0.04102
	accuracy_policy_3: 0.87723
	loss_value_3: 0.06405
	loss_reward_3: 0.00735
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.87852
	loss_value_4: 0.06564
	loss_reward_4: 0.00945
	loss_policy_5: 0.04082
	accuracy_policy_5: 0.8809
	loss_value_5: 0.06773
	loss_reward_5: 0.01075
	loss_policy: 0.40997
	loss_value: 0.61436
	loss_reward: 0.03767
[2025-05-11 12:56:37] nn step 12750, lr: 0.1.
	loss_policy_0: 0.19602
	accuracy_policy_0: 0.87848
	loss_value_0: 0.27782
	loss_policy_1: 0.03979
	accuracy_policy_1: 0.8741
	loss_value_1: 0.05683
	loss_reward_1: 0.00407
	loss_policy_2: 0.03976
	accuracy_policy_2: 0.87773
	loss_value_2: 0.05885
	loss_reward_2: 0.00574
	loss_policy_3: 0.03948
	accuracy_policy_3: 0.87969
	loss_value_3: 0.06069
	loss_reward_3: 0.00703
	loss_policy_4: 0.03927
	accuracy_policy_4: 0.88133
	loss_value_4: 0.06279
	loss_reward_4: 0.00905
	loss_policy_5: 0.0399
	accuracy_policy_5: 0.87727
	loss_value_5: 0.06469
	loss_reward_5: 0.00999
	loss_policy: 0.39422
	loss_value: 0.58169
	loss_reward: 0.03588
[2025-05-11 12:56:46] nn step 12800, lr: 0.1.
	loss_policy_0: 0.19702
	accuracy_policy_0: 0.87656
	loss_value_0: 0.27491
	loss_policy_1: 0.03959
	accuracy_policy_1: 0.87562
	loss_value_1: 0.0564
	loss_reward_1: 0.00434
	loss_policy_2: 0.03938
	accuracy_policy_2: 0.87449
	loss_value_2: 0.05824
	loss_reward_2: 0.00567
	loss_policy_3: 0.03939
	accuracy_policy_3: 0.87383
	loss_value_3: 0.06028
	loss_reward_3: 0.00713
	loss_policy_4: 0.03922
	accuracy_policy_4: 0.88
	loss_value_4: 0.06213
	loss_reward_4: 0.00892
	loss_policy_5: 0.03935
	accuracy_policy_5: 0.87789
	loss_value_5: 0.06417
	loss_reward_5: 0.01013
	loss_policy: 0.39394
	loss_value: 0.57611
	loss_reward: 0.03619
Optimization_Done 12800
[2025-05-11 12:58:01] [command] train weight_iter_12800.pkl 46 65
[2025-05-11 12:58:10] nn step 12850, lr: 0.1.
	loss_policy_0: 0.19286
	accuracy_policy_0: 0.88684
	loss_value_0: 0.28604
	loss_policy_1: 0.03877
	accuracy_policy_1: 0.88969
	loss_value_1: 0.05872
	loss_reward_1: 0.00422
	loss_policy_2: 0.03872
	accuracy_policy_2: 0.88566
	loss_value_2: 0.06071
	loss_reward_2: 0.00578
	loss_policy_3: 0.03894
	accuracy_policy_3: 0.88641
	loss_value_3: 0.06231
	loss_reward_3: 0.0071
	loss_policy_4: 0.03886
	accuracy_policy_4: 0.8857
	loss_value_4: 0.06422
	loss_reward_4: 0.0089
	loss_policy_5: 0.03843
	accuracy_policy_5: 0.88867
	loss_value_5: 0.06613
	loss_reward_5: 0.01001
	loss_policy: 0.38658
	loss_value: 0.59814
	loss_reward: 0.03602
[2025-05-11 12:58:19] nn step 12900, lr: 0.1.
	loss_policy_0: 0.20561
	accuracy_policy_0: 0.88688
	loss_value_0: 0.30297
	loss_policy_1: 0.04103
	accuracy_policy_1: 0.88875
	loss_value_1: 0.06176
	loss_reward_1: 0.00455
	loss_policy_2: 0.04111
	accuracy_policy_2: 0.8884
	loss_value_2: 0.06396
	loss_reward_2: 0.00619
	loss_policy_3: 0.04113
	accuracy_policy_3: 0.88957
	loss_value_3: 0.06569
	loss_reward_3: 0.00763
	loss_policy_4: 0.04126
	accuracy_policy_4: 0.88875
	loss_value_4: 0.06771
	loss_reward_4: 0.00933
	loss_policy_5: 0.04095
	accuracy_policy_5: 0.88734
	loss_value_5: 0.06994
	loss_reward_5: 0.01069
	loss_policy: 0.41109
	loss_value: 0.63202
	loss_reward: 0.03838
[2025-05-11 12:58:27] nn step 12950, lr: 0.1.
	loss_policy_0: 0.19833
	accuracy_policy_0: 0.88465
	loss_value_0: 0.28436
	loss_policy_1: 0.0395
	accuracy_policy_1: 0.88391
	loss_value_1: 0.05841
	loss_reward_1: 0.0043
	loss_policy_2: 0.03948
	accuracy_policy_2: 0.88766
	loss_value_2: 0.06015
	loss_reward_2: 0.006
	loss_policy_3: 0.03924
	accuracy_policy_3: 0.88492
	loss_value_3: 0.06194
	loss_reward_3: 0.00711
	loss_policy_4: 0.03908
	accuracy_policy_4: 0.88809
	loss_value_4: 0.06379
	loss_reward_4: 0.00902
	loss_policy_5: 0.03944
	accuracy_policy_5: 0.88328
	loss_value_5: 0.06572
	loss_reward_5: 0.0101
	loss_policy: 0.39508
	loss_value: 0.59438
	loss_reward: 0.03653
[2025-05-11 12:58:34] nn step 13000, lr: 0.1.
	loss_policy_0: 0.20154
	accuracy_policy_0: 0.88242
	loss_value_0: 0.28963
	loss_policy_1: 0.04047
	accuracy_policy_1: 0.87898
	loss_value_1: 0.05959
	loss_reward_1: 0.0043
	loss_policy_2: 0.04006
	accuracy_policy_2: 0.88152
	loss_value_2: 0.06124
	loss_reward_2: 0.00597
	loss_policy_3: 0.04037
	accuracy_policy_3: 0.88309
	loss_value_3: 0.06298
	loss_reward_3: 0.00719
	loss_policy_4: 0.04002
	accuracy_policy_4: 0.88543
	loss_value_4: 0.06479
	loss_reward_4: 0.00893
	loss_policy_5: 0.03994
	accuracy_policy_5: 0.88777
	loss_value_5: 0.06688
	loss_reward_5: 0.0104
	loss_policy: 0.4024
	loss_value: 0.60512
	loss_reward: 0.03679
Optimization_Done 13000
[2025-05-11 12:59:51] [command] train weight_iter_13000.pkl 47 66
[2025-05-11 12:59:59] nn step 13050, lr: 0.1.
	loss_policy_0: 0.19548
	accuracy_policy_0: 0.88309
	loss_value_0: 0.28958
	loss_policy_1: 0.03914
	accuracy_policy_1: 0.88473
	loss_value_1: 0.05927
	loss_reward_1: 0.00429
	loss_policy_2: 0.03893
	accuracy_policy_2: 0.88504
	loss_value_2: 0.06081
	loss_reward_2: 0.00622
	loss_policy_3: 0.03842
	accuracy_policy_3: 0.88762
	loss_value_3: 0.06238
	loss_reward_3: 0.00756
	loss_policy_4: 0.03857
	accuracy_policy_4: 0.88602
	loss_value_4: 0.06385
	loss_reward_4: 0.00901
	loss_policy_5: 0.03849
	accuracy_policy_5: 0.88938
	loss_value_5: 0.06563
	loss_reward_5: 0.01043
	loss_policy: 0.38903
	loss_value: 0.60152
	loss_reward: 0.03752
[2025-05-11 13:00:08] nn step 13100, lr: 0.1.
	loss_policy_0: 0.18749
	accuracy_policy_0: 0.88562
	loss_value_0: 0.27609
	loss_policy_1: 0.03763
	accuracy_policy_1: 0.88211
	loss_value_1: 0.0563
	loss_reward_1: 0.00426
	loss_policy_2: 0.03741
	accuracy_policy_2: 0.88176
	loss_value_2: 0.05818
	loss_reward_2: 0.00558
	loss_policy_3: 0.03744
	accuracy_policy_3: 0.88516
	loss_value_3: 0.05975
	loss_reward_3: 0.00689
	loss_policy_4: 0.03748
	accuracy_policy_4: 0.88441
	loss_value_4: 0.06156
	loss_reward_4: 0.00863
	loss_policy_5: 0.03735
	accuracy_policy_5: 0.88832
	loss_value_5: 0.0635
	loss_reward_5: 0.00994
	loss_policy: 0.37481
	loss_value: 0.57539
	loss_reward: 0.03529
[2025-05-11 13:00:17] nn step 13150, lr: 0.1.
	loss_policy_0: 0.198
	accuracy_policy_0: 0.88418
	loss_value_0: 0.28827
	loss_policy_1: 0.03958
	accuracy_policy_1: 0.88285
	loss_value_1: 0.05923
	loss_reward_1: 0.00466
	loss_policy_2: 0.03956
	accuracy_policy_2: 0.88633
	loss_value_2: 0.06107
	loss_reward_2: 0.0061
	loss_policy_3: 0.03948
	accuracy_policy_3: 0.8875
	loss_value_3: 0.06281
	loss_reward_3: 0.00752
	loss_policy_4: 0.03958
	accuracy_policy_4: 0.88566
	loss_value_4: 0.06469
	loss_reward_4: 0.00916
	loss_policy_5: 0.03941
	accuracy_policy_5: 0.88777
	loss_value_5: 0.06662
	loss_reward_5: 0.01054
	loss_policy: 0.3956
	loss_value: 0.60269
	loss_reward: 0.03797
[2025-05-11 13:00:24] nn step 13200, lr: 0.1.
	loss_policy_0: 0.20029
	accuracy_policy_0: 0.88723
	loss_value_0: 0.29444
	loss_policy_1: 0.04068
	accuracy_policy_1: 0.88367
	loss_value_1: 0.06005
	loss_reward_1: 0.00452
	loss_policy_2: 0.0404
	accuracy_policy_2: 0.88605
	loss_value_2: 0.0617
	loss_reward_2: 0.00637
	loss_policy_3: 0.04013
	accuracy_policy_3: 0.88477
	loss_value_3: 0.06318
	loss_reward_3: 0.00755
	loss_policy_4: 0.04042
	accuracy_policy_4: 0.88195
	loss_value_4: 0.06512
	loss_reward_4: 0.00922
	loss_policy_5: 0.03997
	accuracy_policy_5: 0.88789
	loss_value_5: 0.06727
	loss_reward_5: 0.011
	loss_policy: 0.40189
	loss_value: 0.61176
	loss_reward: 0.03866
Optimization_Done 13200
[2025-05-11 13:01:42] [command] train weight_iter_13200.pkl 48 67
[2025-05-11 13:01:50] nn step 13250, lr: 0.1.
	loss_policy_0: 0.19834
	accuracy_policy_0: 0.89098
	loss_value_0: 0.2986
	loss_policy_1: 0.03921
	accuracy_policy_1: 0.89211
	loss_value_1: 0.0607
	loss_reward_1: 0.00462
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.89078
	loss_value_2: 0.06232
	loss_reward_2: 0.00607
	loss_policy_3: 0.03924
	accuracy_policy_3: 0.88926
	loss_value_3: 0.06454
	loss_reward_3: 0.00744
	loss_policy_4: 0.03949
	accuracy_policy_4: 0.89141
	loss_value_4: 0.06634
	loss_reward_4: 0.00951
	loss_policy_5: 0.03923
	accuracy_policy_5: 0.89336
	loss_value_5: 0.068
	loss_reward_5: 0.01036
	loss_policy: 0.39482
	loss_value: 0.62049
	loss_reward: 0.03799
[2025-05-11 13:01:58] nn step 13300, lr: 0.1.
	loss_policy_0: 0.1932
	accuracy_policy_0: 0.88711
	loss_value_0: 0.28559
	loss_policy_1: 0.03878
	accuracy_policy_1: 0.88355
	loss_value_1: 0.05796
	loss_reward_1: 0.00433
	loss_policy_2: 0.03837
	accuracy_policy_2: 0.88773
	loss_value_2: 0.05977
	loss_reward_2: 0.00595
	loss_policy_3: 0.03865
	accuracy_policy_3: 0.89109
	loss_value_3: 0.06144
	loss_reward_3: 0.00716
	loss_policy_4: 0.03876
	accuracy_policy_4: 0.89297
	loss_value_4: 0.06343
	loss_reward_4: 0.00882
	loss_policy_5: 0.03838
	accuracy_policy_5: 0.89516
	loss_value_5: 0.06556
	loss_reward_5: 0.01021
	loss_policy: 0.38614
	loss_value: 0.59374
	loss_reward: 0.03647
[2025-05-11 13:02:07] nn step 13350, lr: 0.1.
	loss_policy_0: 0.19993
	accuracy_policy_0: 0.88676
	loss_value_0: 0.29226
	loss_policy_1: 0.04047
	accuracy_policy_1: 0.88594
	loss_value_1: 0.05979
	loss_reward_1: 0.00453
	loss_policy_2: 0.04009
	accuracy_policy_2: 0.8882
	loss_value_2: 0.06152
	loss_reward_2: 0.00593
	loss_policy_3: 0.03972
	accuracy_policy_3: 0.89461
	loss_value_3: 0.06331
	loss_reward_3: 0.00727
	loss_policy_4: 0.03968
	accuracy_policy_4: 0.89141
	loss_value_4: 0.06557
	loss_reward_4: 0.00933
	loss_policy_5: 0.03979
	accuracy_policy_5: 0.89285
	loss_value_5: 0.06734
	loss_reward_5: 0.01027
	loss_policy: 0.39968
	loss_value: 0.60979
	loss_reward: 0.03733
[2025-05-11 13:02:15] nn step 13400, lr: 0.1.
	loss_policy_0: 0.19402
	accuracy_policy_0: 0.88953
	loss_value_0: 0.28073
	loss_policy_1: 0.03906
	accuracy_policy_1: 0.88668
	loss_value_1: 0.05759
	loss_reward_1: 0.0042
	loss_policy_2: 0.03888
	accuracy_policy_2: 0.88902
	loss_value_2: 0.05969
	loss_reward_2: 0.00566
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.88723
	loss_value_3: 0.06139
	loss_reward_3: 0.00697
	loss_policy_4: 0.03858
	accuracy_policy_4: 0.89199
	loss_value_4: 0.06304
	loss_reward_4: 0.00875
	loss_policy_5: 0.03891
	accuracy_policy_5: 0.89156
	loss_value_5: 0.06498
	loss_reward_5: 0.00987
	loss_policy: 0.38839
	loss_value: 0.58741
	loss_reward: 0.03545
Optimization_Done 13400
[2025-05-11 13:03:29] [command] train weight_iter_13400.pkl 49 68
[2025-05-11 13:03:39] nn step 13450, lr: 0.1.
	loss_policy_0: 0.19739
	accuracy_policy_0: 0.88293
	loss_value_0: 0.291
	loss_policy_1: 0.03893
	accuracy_policy_1: 0.88617
	loss_value_1: 0.05929
	loss_reward_1: 0.00432
	loss_policy_2: 0.03888
	accuracy_policy_2: 0.88609
	loss_value_2: 0.06077
	loss_reward_2: 0.00575
	loss_policy_3: 0.03864
	accuracy_policy_3: 0.88969
	loss_value_3: 0.06245
	loss_reward_3: 0.00705
	loss_policy_4: 0.03857
	accuracy_policy_4: 0.89336
	loss_value_4: 0.06429
	loss_reward_4: 0.0088
	loss_policy_5: 0.03877
	accuracy_policy_5: 0.89652
	loss_value_5: 0.06614
	loss_reward_5: 0.01025
	loss_policy: 0.39118
	loss_value: 0.60394
	loss_reward: 0.03617
[2025-05-11 13:03:46] nn step 13500, lr: 0.1.
	loss_policy_0: 0.19052
	accuracy_policy_0: 0.88402
	loss_value_0: 0.2788
	loss_policy_1: 0.03826
	accuracy_policy_1: 0.8857
	loss_value_1: 0.05696
	loss_reward_1: 0.00435
	loss_policy_2: 0.0385
	accuracy_policy_2: 0.88703
	loss_value_2: 0.05926
	loss_reward_2: 0.00578
	loss_policy_3: 0.03791
	accuracy_policy_3: 0.88582
	loss_value_3: 0.06083
	loss_reward_3: 0.00702
	loss_policy_4: 0.03797
	accuracy_policy_4: 0.89191
	loss_value_4: 0.06255
	loss_reward_4: 0.00872
	loss_policy_5: 0.03789
	accuracy_policy_5: 0.88883
	loss_value_5: 0.06422
	loss_reward_5: 0.00989
	loss_policy: 0.38104
	loss_value: 0.58261
	loss_reward: 0.03576
[2025-05-11 13:03:54] nn step 13550, lr: 0.1.
	loss_policy_0: 0.18919
	accuracy_policy_0: 0.88184
	loss_value_0: 0.27565
	loss_policy_1: 0.03774
	accuracy_policy_1: 0.88367
	loss_value_1: 0.05644
	loss_reward_1: 0.0042
	loss_policy_2: 0.03773
	accuracy_policy_2: 0.8827
	loss_value_2: 0.05775
	loss_reward_2: 0.00592
	loss_policy_3: 0.03769
	accuracy_policy_3: 0.88438
	loss_value_3: 0.05932
	loss_reward_3: 0.00672
	loss_policy_4: 0.03755
	accuracy_policy_4: 0.88754
	loss_value_4: 0.06093
	loss_reward_4: 0.00857
	loss_policy_5: 0.03742
	accuracy_policy_5: 0.89293
	loss_value_5: 0.06261
	loss_reward_5: 0.01028
	loss_policy: 0.37732
	loss_value: 0.5727
	loss_reward: 0.03569
[2025-05-11 13:04:03] nn step 13600, lr: 0.1.
	loss_policy_0: 0.19284
	accuracy_policy_0: 0.88492
	loss_value_0: 0.27732
	loss_policy_1: 0.03845
	accuracy_policy_1: 0.88402
	loss_value_1: 0.05688
	loss_reward_1: 0.00415
	loss_policy_2: 0.03834
	accuracy_policy_2: 0.8875
	loss_value_2: 0.0585
	loss_reward_2: 0.00581
	loss_policy_3: 0.03829
	accuracy_policy_3: 0.88637
	loss_value_3: 0.06027
	loss_reward_3: 0.00731
	loss_policy_4: 0.03816
	accuracy_policy_4: 0.88727
	loss_value_4: 0.06194
	loss_reward_4: 0.00893
	loss_policy_5: 0.03808
	accuracy_policy_5: 0.89254
	loss_value_5: 0.06406
	loss_reward_5: 0.00992
	loss_policy: 0.38415
	loss_value: 0.57897
	loss_reward: 0.03612
Optimization_Done 13600
[2025-05-11 13:05:20] [command] train weight_iter_13600.pkl 50 69
[2025-05-11 13:05:29] nn step 13650, lr: 0.1.
	loss_policy_0: 0.18253
	accuracy_policy_0: 0.88465
	loss_value_0: 0.27463
	loss_policy_1: 0.03677
	accuracy_policy_1: 0.88516
	loss_value_1: 0.05613
	loss_reward_1: 0.00408
	loss_policy_2: 0.03698
	accuracy_policy_2: 0.87875
	loss_value_2: 0.05757
	loss_reward_2: 0.00553
	loss_policy_3: 0.03683
	accuracy_policy_3: 0.88523
	loss_value_3: 0.05892
	loss_reward_3: 0.00685
	loss_policy_4: 0.03659
	accuracy_policy_4: 0.88594
	loss_value_4: 0.06059
	loss_reward_4: 0.00857
	loss_policy_5: 0.03641
	accuracy_policy_5: 0.89453
	loss_value_5: 0.06226
	loss_reward_5: 0.00965
	loss_policy: 0.3661
	loss_value: 0.5701
	loss_reward: 0.03468
[2025-05-11 13:05:36] nn step 13700, lr: 0.1.
	loss_policy_0: 0.1826
	accuracy_policy_0: 0.88191
	loss_value_0: 0.26594
	loss_policy_1: 0.03653
	accuracy_policy_1: 0.87797
	loss_value_1: 0.05426
	loss_reward_1: 0.00434
	loss_policy_2: 0.03657
	accuracy_policy_2: 0.88012
	loss_value_2: 0.05548
	loss_reward_2: 0.00562
	loss_policy_3: 0.03635
	accuracy_policy_3: 0.8827
	loss_value_3: 0.05718
	loss_reward_3: 0.00708
	loss_policy_4: 0.03626
	accuracy_policy_4: 0.88812
	loss_value_4: 0.05892
	loss_reward_4: 0.0083
	loss_policy_5: 0.03618
	accuracy_policy_5: 0.88914
	loss_value_5: 0.06066
	loss_reward_5: 0.00961
	loss_policy: 0.36449
	loss_value: 0.55244
	loss_reward: 0.03495
[2025-05-11 13:05:45] nn step 13750, lr: 0.1.
	loss_policy_0: 0.1911
	accuracy_policy_0: 0.88379
	loss_value_0: 0.27808
	loss_policy_1: 0.0382
	accuracy_policy_1: 0.8791
	loss_value_1: 0.05713
	loss_reward_1: 0.00419
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.88051
	loss_value_2: 0.0588
	loss_reward_2: 0.00573
	loss_policy_3: 0.03806
	accuracy_policy_3: 0.88402
	loss_value_3: 0.06057
	loss_reward_3: 0.0073
	loss_policy_4: 0.038
	accuracy_policy_4: 0.88352
	loss_value_4: 0.0622
	loss_reward_4: 0.00859
	loss_policy_5: 0.03791
	accuracy_policy_5: 0.88898
	loss_value_5: 0.06422
	loss_reward_5: 0.00992
	loss_policy: 0.38155
	loss_value: 0.581
	loss_reward: 0.03574
[2025-05-11 13:05:54] nn step 13800, lr: 0.1.
	loss_policy_0: 0.18954
	accuracy_policy_0: 0.87953
	loss_value_0: 0.27526
	loss_policy_1: 0.03736
	accuracy_policy_1: 0.88195
	loss_value_1: 0.05633
	loss_reward_1: 0.00422
	loss_policy_2: 0.03731
	accuracy_policy_2: 0.88551
	loss_value_2: 0.05767
	loss_reward_2: 0.00558
	loss_policy_3: 0.03735
	accuracy_policy_3: 0.88141
	loss_value_3: 0.05947
	loss_reward_3: 0.00679
	loss_policy_4: 0.03735
	accuracy_policy_4: 0.88578
	loss_value_4: 0.06125
	loss_reward_4: 0.00861
	loss_policy_5: 0.037
	accuracy_policy_5: 0.88938
	loss_value_5: 0.0631
	loss_reward_5: 0.00989
	loss_policy: 0.37591
	loss_value: 0.57309
	loss_reward: 0.03509
Optimization_Done 13800
[2025-05-11 13:07:07] [command] train weight_iter_13800.pkl 51 70
[2025-05-11 13:07:16] nn step 13850, lr: 0.1.
	loss_policy_0: 0.1816
	accuracy_policy_0: 0.88188
	loss_value_0: 0.27098
	loss_policy_1: 0.03641
	accuracy_policy_1: 0.88203
	loss_value_1: 0.05535
	loss_reward_1: 0.00412
	loss_policy_2: 0.03651
	accuracy_policy_2: 0.88559
	loss_value_2: 0.05684
	loss_reward_2: 0.00567
	loss_policy_3: 0.03643
	accuracy_policy_3: 0.88027
	loss_value_3: 0.05821
	loss_reward_3: 0.00704
	loss_policy_4: 0.03679
	accuracy_policy_4: 0.88164
	loss_value_4: 0.05996
	loss_reward_4: 0.00859
	loss_policy_5: 0.03613
	accuracy_policy_5: 0.88617
	loss_value_5: 0.0615
	loss_reward_5: 0.00967
	loss_policy: 0.36388
	loss_value: 0.56283
	loss_reward: 0.03509
[2025-05-11 13:07:25] nn step 13900, lr: 0.1.
	loss_policy_0: 0.20429
	accuracy_policy_0: 0.88266
	loss_value_0: 0.30031
	loss_policy_1: 0.04086
	accuracy_policy_1: 0.87988
	loss_value_1: 0.06139
	loss_reward_1: 0.00468
	loss_policy_2: 0.04038
	accuracy_policy_2: 0.87898
	loss_value_2: 0.06324
	loss_reward_2: 0.00629
	loss_policy_3: 0.04067
	accuracy_policy_3: 0.88316
	loss_value_3: 0.06524
	loss_reward_3: 0.00792
	loss_policy_4: 0.04084
	accuracy_policy_4: 0.885
	loss_value_4: 0.06718
	loss_reward_4: 0.00952
	loss_policy_5: 0.04047
	accuracy_policy_5: 0.88938
	loss_value_5: 0.06895
	loss_reward_5: 0.01137
	loss_policy: 0.40752
	loss_value: 0.62631
	loss_reward: 0.03976
[2025-05-11 13:07:32] nn step 13950, lr: 0.1.
	loss_policy_0: 0.19154
	accuracy_policy_0: 0.88023
	loss_value_0: 0.2814
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.87922
	loss_value_1: 0.0574
	loss_reward_1: 0.00409
	loss_policy_2: 0.038
	accuracy_policy_2: 0.88082
	loss_value_2: 0.05916
	loss_reward_2: 0.00574
	loss_policy_3: 0.03779
	accuracy_policy_3: 0.88484
	loss_value_3: 0.06061
	loss_reward_3: 0.00744
	loss_policy_4: 0.03802
	accuracy_policy_4: 0.8841
	loss_value_4: 0.06228
	loss_reward_4: 0.00867
	loss_policy_5: 0.03795
	accuracy_policy_5: 0.89031
	loss_value_5: 0.06421
	loss_reward_5: 0.00992
	loss_policy: 0.38115
	loss_value: 0.58506
	loss_reward: 0.03587
[2025-05-11 13:07:41] nn step 14000, lr: 0.1.
	loss_policy_0: 0.19603
	accuracy_policy_0: 0.87664
	loss_value_0: 0.28616
	loss_policy_1: 0.03897
	accuracy_policy_1: 0.88395
	loss_value_1: 0.05821
	loss_reward_1: 0.00446
	loss_policy_2: 0.03904
	accuracy_policy_2: 0.87844
	loss_value_2: 0.0599
	loss_reward_2: 0.00591
	loss_policy_3: 0.03872
	accuracy_policy_3: 0.88129
	loss_value_3: 0.06178
	loss_reward_3: 0.00725
	loss_policy_4: 0.03912
	accuracy_policy_4: 0.88
	loss_value_4: 0.06345
	loss_reward_4: 0.00884
	loss_policy_5: 0.03906
	accuracy_policy_5: 0.88148
	loss_value_5: 0.0652
	loss_reward_5: 0.01032
	loss_policy: 0.39095
	loss_value: 0.5947
	loss_reward: 0.03677
Optimization_Done 14000
[2025-05-11 13:08:58] [command] train weight_iter_14000.pkl 52 71
[2025-05-11 13:09:07] nn step 14050, lr: 0.1.
	loss_policy_0: 0.18142
	accuracy_policy_0: 0.87875
	loss_value_0: 0.27477
	loss_policy_1: 0.0362
	accuracy_policy_1: 0.88293
	loss_value_1: 0.05624
	loss_reward_1: 0.00403
	loss_policy_2: 0.03596
	accuracy_policy_2: 0.88742
	loss_value_2: 0.05755
	loss_reward_2: 0.00551
	loss_policy_3: 0.03639
	accuracy_policy_3: 0.88324
	loss_value_3: 0.05914
	loss_reward_3: 0.00688
	loss_policy_4: 0.03626
	accuracy_policy_4: 0.88707
	loss_value_4: 0.06076
	loss_reward_4: 0.00877
	loss_policy_5: 0.03604
	accuracy_policy_5: 0.88543
	loss_value_5: 0.06221
	loss_reward_5: 0.0097
	loss_policy: 0.36227
	loss_value: 0.57066
	loss_reward: 0.03489
[2025-05-11 13:09:16] nn step 14100, lr: 0.1.
	loss_policy_0: 0.18977
	accuracy_policy_0: 0.88449
	loss_value_0: 0.27779
	loss_policy_1: 0.0379
	accuracy_policy_1: 0.88473
	loss_value_1: 0.05698
	loss_reward_1: 0.00429
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.88391
	loss_value_2: 0.05873
	loss_reward_2: 0.00559
	loss_policy_3: 0.03767
	accuracy_policy_3: 0.88688
	loss_value_3: 0.06034
	loss_reward_3: 0.00705
	loss_policy_4: 0.03794
	accuracy_policy_4: 0.88617
	loss_value_4: 0.06211
	loss_reward_4: 0.00871
	loss_policy_5: 0.03774
	accuracy_policy_5: 0.8866
	loss_value_5: 0.06397
	loss_reward_5: 0.00988
	loss_policy: 0.37899
	loss_value: 0.57994
	loss_reward: 0.03551
[2025-05-11 13:09:23] nn step 14150, lr: 0.1.
	loss_policy_0: 0.18964
	accuracy_policy_0: 0.88523
	loss_value_0: 0.2782
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.88219
	loss_value_1: 0.05668
	loss_reward_1: 0.00439
	loss_policy_2: 0.03784
	accuracy_policy_2: 0.88203
	loss_value_2: 0.05886
	loss_reward_2: 0.00576
	loss_policy_3: 0.03789
	accuracy_policy_3: 0.88582
	loss_value_3: 0.06042
	loss_reward_3: 0.00714
	loss_policy_4: 0.03748
	accuracy_policy_4: 0.88586
	loss_value_4: 0.06233
	loss_reward_4: 0.00907
	loss_policy_5: 0.03728
	accuracy_policy_5: 0.89098
	loss_value_5: 0.06427
	loss_reward_5: 0.01023
	loss_policy: 0.37788
	loss_value: 0.58075
	loss_reward: 0.03658
[2025-05-11 13:09:31] nn step 14200, lr: 0.1.
	loss_policy_0: 0.18846
	accuracy_policy_0: 0.88207
	loss_value_0: 0.2786
	loss_policy_1: 0.03825
	accuracy_policy_1: 0.88098
	loss_value_1: 0.05685
	loss_reward_1: 0.00431
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.87824
	loss_value_2: 0.05849
	loss_reward_2: 0.0055
	loss_policy_3: 0.03794
	accuracy_policy_3: 0.88094
	loss_value_3: 0.06021
	loss_reward_3: 0.007
	loss_policy_4: 0.03783
	accuracy_policy_4: 0.88352
	loss_value_4: 0.0621
	loss_reward_4: 0.00904
	loss_policy_5: 0.03764
	accuracy_policy_5: 0.88531
	loss_value_5: 0.06386
	loss_reward_5: 0.01009
	loss_policy: 0.37807
	loss_value: 0.58012
	loss_reward: 0.03594
Optimization_Done 14200
[2025-05-11 13:10:47] [command] train weight_iter_14200.pkl 53 72
[2025-05-11 13:10:55] nn step 14250, lr: 0.1.
	loss_policy_0: 0.18336
	accuracy_policy_0: 0.88828
	loss_value_0: 0.27958
	loss_policy_1: 0.03678
	accuracy_policy_1: 0.88531
	loss_value_1: 0.05687
	loss_reward_1: 0.00434
	loss_policy_2: 0.0368
	accuracy_policy_2: 0.88812
	loss_value_2: 0.05857
	loss_reward_2: 0.00571
	loss_policy_3: 0.03682
	accuracy_policy_3: 0.88934
	loss_value_3: 0.06023
	loss_reward_3: 0.00701
	loss_policy_4: 0.03666
	accuracy_policy_4: 0.88816
	loss_value_4: 0.06182
	loss_reward_4: 0.00867
	loss_policy_5: 0.0365
	accuracy_policy_5: 0.8943
	loss_value_5: 0.06357
	loss_reward_5: 0.00986
	loss_policy: 0.36691
	loss_value: 0.58064
	loss_reward: 0.0356
[2025-05-11 13:11:04] nn step 14300, lr: 0.1.
	loss_policy_0: 0.19046
	accuracy_policy_0: 0.88504
	loss_value_0: 0.28776
	loss_policy_1: 0.03815
	accuracy_policy_1: 0.8907
	loss_value_1: 0.05857
	loss_reward_1: 0.00458
	loss_policy_2: 0.03788
	accuracy_policy_2: 0.88969
	loss_value_2: 0.06054
	loss_reward_2: 0.00589
	loss_policy_3: 0.03821
	accuracy_policy_3: 0.88945
	loss_value_3: 0.06233
	loss_reward_3: 0.00752
	loss_policy_4: 0.03806
	accuracy_policy_4: 0.8916
	loss_value_4: 0.06409
	loss_reward_4: 0.00901
	loss_policy_5: 0.0381
	accuracy_policy_5: 0.89051
	loss_value_5: 0.06574
	loss_reward_5: 0.01044
	loss_policy: 0.38086
	loss_value: 0.59904
	loss_reward: 0.03742
[2025-05-11 13:11:12] nn step 14350, lr: 0.1.
	loss_policy_0: 0.18019
	accuracy_policy_0: 0.88898
	loss_value_0: 0.26542
	loss_policy_1: 0.0359
	accuracy_policy_1: 0.88723
	loss_value_1: 0.05459
	loss_reward_1: 0.00423
	loss_policy_2: 0.03596
	accuracy_policy_2: 0.88684
	loss_value_2: 0.05627
	loss_reward_2: 0.00541
	loss_policy_3: 0.03609
	accuracy_policy_3: 0.88824
	loss_value_3: 0.05787
	loss_reward_3: 0.00657
	loss_policy_4: 0.03603
	accuracy_policy_4: 0.88621
	loss_value_4: 0.05943
	loss_reward_4: 0.00835
	loss_policy_5: 0.03594
	accuracy_policy_5: 0.89016
	loss_value_5: 0.06126
	loss_reward_5: 0.00986
	loss_policy: 0.36011
	loss_value: 0.55484
	loss_reward: 0.03442
[2025-05-11 13:11:20] nn step 14400, lr: 0.1.
	loss_policy_0: 0.18871
	accuracy_policy_0: 0.88645
	loss_value_0: 0.2763
	loss_policy_1: 0.03763
	accuracy_policy_1: 0.88766
	loss_value_1: 0.05645
	loss_reward_1: 0.00454
	loss_policy_2: 0.03792
	accuracy_policy_2: 0.88723
	loss_value_2: 0.05827
	loss_reward_2: 0.00568
	loss_policy_3: 0.03762
	accuracy_policy_3: 0.88875
	loss_value_3: 0.06001
	loss_reward_3: 0.00722
	loss_policy_4: 0.0374
	accuracy_policy_4: 0.88957
	loss_value_4: 0.06197
	loss_reward_4: 0.00898
	loss_policy_5: 0.03734
	accuracy_policy_5: 0.89371
	loss_value_5: 0.06376
	loss_reward_5: 0.01014
	loss_policy: 0.37662
	loss_value: 0.57675
	loss_reward: 0.03656
Optimization_Done 14400
[2025-05-11 13:12:37] [command] train weight_iter_14400.pkl 54 73
[2025-05-11 13:12:45] nn step 14450, lr: 0.1.
	loss_policy_0: 0.18996
	accuracy_policy_0: 0.88594
	loss_value_0: 0.28899
	loss_policy_1: 0.03786
	accuracy_policy_1: 0.88672
	loss_value_1: 0.05904
	loss_reward_1: 0.00454
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.88906
	loss_value_2: 0.06071
	loss_reward_2: 0.00586
	loss_policy_3: 0.03768
	accuracy_policy_3: 0.88809
	loss_value_3: 0.06223
	loss_reward_3: 0.00724
	loss_policy_4: 0.03801
	accuracy_policy_4: 0.89078
	loss_value_4: 0.06392
	loss_reward_4: 0.00915
	loss_policy_5: 0.03772
	accuracy_policy_5: 0.89488
	loss_value_5: 0.06563
	loss_reward_5: 0.01045
	loss_policy: 0.37953
	loss_value: 0.60052
	loss_reward: 0.03723
[2025-05-11 13:12:54] nn step 14500, lr: 0.1.
	loss_policy_0: 0.18575
	accuracy_policy_0: 0.88824
	loss_value_0: 0.2805
	loss_policy_1: 0.03769
	accuracy_policy_1: 0.88336
	loss_value_1: 0.05733
	loss_reward_1: 0.00458
	loss_policy_2: 0.03738
	accuracy_policy_2: 0.8868
	loss_value_2: 0.05886
	loss_reward_2: 0.00585
	loss_policy_3: 0.03742
	accuracy_policy_3: 0.89016
	loss_value_3: 0.06103
	loss_reward_3: 0.00716
	loss_policy_4: 0.03764
	accuracy_policy_4: 0.89273
	loss_value_4: 0.06263
	loss_reward_4: 0.00896
	loss_policy_5: 0.03746
	accuracy_policy_5: 0.89359
	loss_value_5: 0.06449
	loss_reward_5: 0.01033
	loss_policy: 0.37333
	loss_value: 0.58485
	loss_reward: 0.03688
[2025-05-11 13:13:03] nn step 14550, lr: 0.1.
	loss_policy_0: 0.19584
	accuracy_policy_0: 0.88496
	loss_value_0: 0.29139
	loss_policy_1: 0.039
	accuracy_policy_1: 0.88672
	loss_value_1: 0.05977
	loss_reward_1: 0.00461
	loss_policy_2: 0.03935
	accuracy_policy_2: 0.8882
	loss_value_2: 0.06148
	loss_reward_2: 0.0059
	loss_policy_3: 0.03907
	accuracy_policy_3: 0.89164
	loss_value_3: 0.06311
	loss_reward_3: 0.00745
	loss_policy_4: 0.03898
	accuracy_policy_4: 0.89102
	loss_value_4: 0.06494
	loss_reward_4: 0.00953
	loss_policy_5: 0.03871
	accuracy_policy_5: 0.89438
	loss_value_5: 0.06708
	loss_reward_5: 0.01063
	loss_policy: 0.39095
	loss_value: 0.60776
	loss_reward: 0.03812
[2025-05-11 13:13:11] nn step 14600, lr: 0.1.
	loss_policy_0: 0.18922
	accuracy_policy_0: 0.88754
	loss_value_0: 0.28518
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.8866
	loss_value_1: 0.05824
	loss_reward_1: 0.00455
	loss_policy_2: 0.03744
	accuracy_policy_2: 0.88723
	loss_value_2: 0.06018
	loss_reward_2: 0.00587
	loss_policy_3: 0.03749
	accuracy_policy_3: 0.89062
	loss_value_3: 0.0616
	loss_reward_3: 0.00728
	loss_policy_4: 0.03768
	accuracy_policy_4: 0.89031
	loss_value_4: 0.06359
	loss_reward_4: 0.00902
	loss_policy_5: 0.03753
	accuracy_policy_5: 0.89691
	loss_value_5: 0.06535
	loss_reward_5: 0.01042
	loss_policy: 0.3772
	loss_value: 0.59415
	loss_reward: 0.03713
Optimization_Done 14600
[2025-05-11 13:14:27] [command] train weight_iter_14600.pkl 55 74
[2025-05-11 13:14:36] nn step 14650, lr: 0.1.
	loss_policy_0: 0.19012
	accuracy_policy_0: 0.8868
	loss_value_0: 0.29295
	loss_policy_1: 0.03812
	accuracy_policy_1: 0.88648
	loss_value_1: 0.05974
	loss_reward_1: 0.00463
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.88898
	loss_value_2: 0.06164
	loss_reward_2: 0.00589
	loss_policy_3: 0.03805
	accuracy_policy_3: 0.88906
	loss_value_3: 0.06312
	loss_reward_3: 0.00738
	loss_policy_4: 0.03809
	accuracy_policy_4: 0.89004
	loss_value_4: 0.0646
	loss_reward_4: 0.00896
	loss_policy_5: 0.03799
	accuracy_policy_5: 0.89473
	loss_value_5: 0.06649
	loss_reward_5: 0.0102
	loss_policy: 0.38038
	loss_value: 0.60854
	loss_reward: 0.03705
[2025-05-11 13:14:43] nn step 14700, lr: 0.1.
	loss_policy_0: 0.18756
	accuracy_policy_0: 0.88855
	loss_value_0: 0.28749
	loss_policy_1: 0.03789
	accuracy_policy_1: 0.88672
	loss_value_1: 0.05844
	loss_reward_1: 0.00438
	loss_policy_2: 0.03757
	accuracy_policy_2: 0.88762
	loss_value_2: 0.05988
	loss_reward_2: 0.00581
	loss_policy_3: 0.03769
	accuracy_policy_3: 0.88832
	loss_value_3: 0.06167
	loss_reward_3: 0.00712
	loss_policy_4: 0.03729
	accuracy_policy_4: 0.89016
	loss_value_4: 0.06362
	loss_reward_4: 0.00919
	loss_policy_5: 0.03758
	accuracy_policy_5: 0.89266
	loss_value_5: 0.06569
	loss_reward_5: 0.01035
	loss_policy: 0.37559
	loss_value: 0.5968
	loss_reward: 0.03685
[2025-05-11 13:14:51] nn step 14750, lr: 0.1.
	loss_policy_0: 0.19771
	accuracy_policy_0: 0.88863
	loss_value_0: 0.30076
	loss_policy_1: 0.03954
	accuracy_policy_1: 0.88906
	loss_value_1: 0.06137
	loss_reward_1: 0.00466
	loss_policy_2: 0.04
	accuracy_policy_2: 0.8875
	loss_value_2: 0.06314
	loss_reward_2: 0.00618
	loss_policy_3: 0.03961
	accuracy_policy_3: 0.89117
	loss_value_3: 0.06473
	loss_reward_3: 0.00787
	loss_policy_4: 0.03961
	accuracy_policy_4: 0.89109
	loss_value_4: 0.06657
	loss_reward_4: 0.00966
	loss_policy_5: 0.03973
	accuracy_policy_5: 0.89449
	loss_value_5: 0.06833
	loss_reward_5: 0.01139
	loss_policy: 0.3962
	loss_value: 0.62491
	loss_reward: 0.03977
[2025-05-11 13:15:00] nn step 14800, lr: 0.1.
	loss_policy_0: 0.18954
	accuracy_policy_0: 0.88754
	loss_value_0: 0.28175
	loss_policy_1: 0.03794
	accuracy_policy_1: 0.88656
	loss_value_1: 0.0574
	loss_reward_1: 0.0045
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.88945
	loss_value_2: 0.0593
	loss_reward_2: 0.00599
	loss_policy_3: 0.03781
	accuracy_policy_3: 0.89086
	loss_value_3: 0.06104
	loss_reward_3: 0.00738
	loss_policy_4: 0.03782
	accuracy_policy_4: 0.89348
	loss_value_4: 0.0627
	loss_reward_4: 0.00894
	loss_policy_5: 0.03729
	accuracy_policy_5: 0.89734
	loss_value_5: 0.06466
	loss_reward_5: 0.01036
	loss_policy: 0.3782
	loss_value: 0.58684
	loss_reward: 0.03716
Optimization_Done 14800
[2025-05-11 13:16:16] [command] train weight_iter_14800.pkl 56 75
[2025-05-11 13:16:25] nn step 14850, lr: 0.1.
	loss_policy_0: 0.18321
	accuracy_policy_0: 0.89082
	loss_value_0: 0.28786
	loss_policy_1: 0.03687
	accuracy_policy_1: 0.8875
	loss_value_1: 0.05873
	loss_reward_1: 0.00432
	loss_policy_2: 0.03678
	accuracy_policy_2: 0.88688
	loss_value_2: 0.06049
	loss_reward_2: 0.00581
	loss_policy_3: 0.03689
	accuracy_policy_3: 0.88957
	loss_value_3: 0.06183
	loss_reward_3: 0.00728
	loss_policy_4: 0.03677
	accuracy_policy_4: 0.88816
	loss_value_4: 0.06316
	loss_reward_4: 0.00889
	loss_policy_5: 0.03649
	accuracy_policy_5: 0.89629
	loss_value_5: 0.06519
	loss_reward_5: 0.01047
	loss_policy: 0.36702
	loss_value: 0.59725
	loss_reward: 0.03677
[2025-05-11 13:16:32] nn step 14900, lr: 0.1.
	loss_policy_0: 0.19174
	accuracy_policy_0: 0.8857
	loss_value_0: 0.29436
	loss_policy_1: 0.03846
	accuracy_policy_1: 0.89004
	loss_value_1: 0.06007
	loss_reward_1: 0.00451
	loss_policy_2: 0.03856
	accuracy_policy_2: 0.88871
	loss_value_2: 0.06197
	loss_reward_2: 0.0059
	loss_policy_3: 0.03849
	accuracy_policy_3: 0.88871
	loss_value_3: 0.06347
	loss_reward_3: 0.00734
	loss_policy_4: 0.03838
	accuracy_policy_4: 0.89031
	loss_value_4: 0.06524
	loss_reward_4: 0.00944
	loss_policy_5: 0.03821
	accuracy_policy_5: 0.8973
	loss_value_5: 0.0671
	loss_reward_5: 0.01039
	loss_policy: 0.38385
	loss_value: 0.6122
	loss_reward: 0.03758
[2025-05-11 13:16:41] nn step 14950, lr: 0.1.
	loss_policy_0: 0.18743
	accuracy_policy_0: 0.8852
	loss_value_0: 0.28406
	loss_policy_1: 0.03725
	accuracy_policy_1: 0.88941
	loss_value_1: 0.05793
	loss_reward_1: 0.00435
	loss_policy_2: 0.03757
	accuracy_policy_2: 0.88926
	loss_value_2: 0.05953
	loss_reward_2: 0.00562
	loss_policy_3: 0.03749
	accuracy_policy_3: 0.89285
	loss_value_3: 0.06126
	loss_reward_3: 0.00676
	loss_policy_4: 0.03731
	accuracy_policy_4: 0.89383
	loss_value_4: 0.06318
	loss_reward_4: 0.00885
	loss_policy_5: 0.03749
	accuracy_policy_5: 0.89805
	loss_value_5: 0.0648
	loss_reward_5: 0.01032
	loss_policy: 0.37455
	loss_value: 0.59076
	loss_reward: 0.03589
[2025-05-11 13:16:49] nn step 15000, lr: 0.1.
	loss_policy_0: 0.19318
	accuracy_policy_0: 0.88707
	loss_value_0: 0.29036
	loss_policy_1: 0.03832
	accuracy_policy_1: 0.88656
	loss_value_1: 0.0591
	loss_reward_1: 0.00439
	loss_policy_2: 0.03839
	accuracy_policy_2: 0.88938
	loss_value_2: 0.06097
	loss_reward_2: 0.00568
	loss_policy_3: 0.03865
	accuracy_policy_3: 0.89031
	loss_value_3: 0.06258
	loss_reward_3: 0.00706
	loss_policy_4: 0.03848
	accuracy_policy_4: 0.89406
	loss_value_4: 0.06443
	loss_reward_4: 0.00906
	loss_policy_5: 0.03828
	accuracy_policy_5: 0.89613
	loss_value_5: 0.06638
	loss_reward_5: 0.01058
	loss_policy: 0.38531
	loss_value: 0.60382
	loss_reward: 0.03677
Optimization_Done 15000
[2025-05-11 13:18:07] [command] train weight_iter_15000.pkl 57 76
[2025-05-11 13:18:15] nn step 15050, lr: 0.1.
	loss_policy_0: 0.18363
	accuracy_policy_0: 0.88516
	loss_value_0: 0.28642
	loss_policy_1: 0.03694
	accuracy_policy_1: 0.885
	loss_value_1: 0.05836
	loss_reward_1: 0.0045
	loss_policy_2: 0.03701
	accuracy_policy_2: 0.88574
	loss_value_2: 0.05949
	loss_reward_2: 0.00589
	loss_policy_3: 0.03665
	accuracy_policy_3: 0.88668
	loss_value_3: 0.06111
	loss_reward_3: 0.00705
	loss_policy_4: 0.03675
	accuracy_policy_4: 0.8882
	loss_value_4: 0.06282
	loss_reward_4: 0.00874
	loss_policy_5: 0.03682
	accuracy_policy_5: 0.88777
	loss_value_5: 0.06483
	loss_reward_5: 0.01026
	loss_policy: 0.3678
	loss_value: 0.59303
	loss_reward: 0.03644
[2025-05-11 13:18:24] nn step 15100, lr: 0.1.
	loss_policy_0: 0.18011
	accuracy_policy_0: 0.88762
	loss_value_0: 0.27198
	loss_policy_1: 0.03594
	accuracy_policy_1: 0.88684
	loss_value_1: 0.05551
	loss_reward_1: 0.00446
	loss_policy_2: 0.03608
	accuracy_policy_2: 0.8859
	loss_value_2: 0.0575
	loss_reward_2: 0.00563
	loss_policy_3: 0.03565
	accuracy_policy_3: 0.88926
	loss_value_3: 0.05957
	loss_reward_3: 0.00702
	loss_policy_4: 0.03594
	accuracy_policy_4: 0.88969
	loss_value_4: 0.06103
	loss_reward_4: 0.00883
	loss_policy_5: 0.03581
	accuracy_policy_5: 0.89277
	loss_value_5: 0.06263
	loss_reward_5: 0.01003
	loss_policy: 0.35953
	loss_value: 0.56822
	loss_reward: 0.03597
[2025-05-11 13:18:31] nn step 15150, lr: 0.1.
	loss_policy_0: 0.19184
	accuracy_policy_0: 0.8891
	loss_value_0: 0.29102
	loss_policy_1: 0.03849
	accuracy_policy_1: 0.8866
	loss_value_1: 0.05914
	loss_reward_1: 0.00444
	loss_policy_2: 0.03869
	accuracy_policy_2: 0.88578
	loss_value_2: 0.06101
	loss_reward_2: 0.00595
	loss_policy_3: 0.03845
	accuracy_policy_3: 0.88746
	loss_value_3: 0.06236
	loss_reward_3: 0.00743
	loss_policy_4: 0.03841
	accuracy_policy_4: 0.89066
	loss_value_4: 0.06443
	loss_reward_4: 0.00918
	loss_policy_5: 0.03805
	accuracy_policy_5: 0.89168
	loss_value_5: 0.06643
	loss_reward_5: 0.01053
	loss_policy: 0.38394
	loss_value: 0.60439
	loss_reward: 0.03752
[2025-05-11 13:18:40] nn step 15200, lr: 0.1.
	loss_policy_0: 0.19807
	accuracy_policy_0: 0.88336
	loss_value_0: 0.30136
	loss_policy_1: 0.03931
	accuracy_policy_1: 0.88664
	loss_value_1: 0.06153
	loss_reward_1: 0.00469
	loss_policy_2: 0.03933
	accuracy_policy_2: 0.88633
	loss_value_2: 0.06337
	loss_reward_2: 0.00622
	loss_policy_3: 0.03933
	accuracy_policy_3: 0.88938
	loss_value_3: 0.0652
	loss_reward_3: 0.00764
	loss_policy_4: 0.03928
	accuracy_policy_4: 0.88922
	loss_value_4: 0.06719
	loss_reward_4: 0.00958
	loss_policy_5: 0.03884
	accuracy_policy_5: 0.8925
	loss_value_5: 0.06918
	loss_reward_5: 0.01105
	loss_policy: 0.39416
	loss_value: 0.62783
	loss_reward: 0.03918
Optimization_Done 15200
[2025-05-11 13:19:54] [command] train weight_iter_15200.pkl 58 77
[2025-05-11 13:20:04] nn step 15250, lr: 0.1.
	loss_policy_0: 0.17942
	accuracy_policy_0: 0.8818
	loss_value_0: 0.28837
	loss_policy_1: 0.03584
	accuracy_policy_1: 0.88234
	loss_value_1: 0.05825
	loss_reward_1: 0.0042
	loss_policy_2: 0.03625
	accuracy_policy_2: 0.8777
	loss_value_2: 0.05998
	loss_reward_2: 0.00566
	loss_policy_3: 0.03596
	accuracy_policy_3: 0.88406
	loss_value_3: 0.06158
	loss_reward_3: 0.00731
	loss_policy_4: 0.03581
	accuracy_policy_4: 0.88902
	loss_value_4: 0.06328
	loss_reward_4: 0.00898
	loss_policy_5: 0.03594
	accuracy_policy_5: 0.8877
	loss_value_5: 0.06482
	loss_reward_5: 0.01024
	loss_policy: 0.35921
	loss_value: 0.59628
	loss_reward: 0.03639
[2025-05-11 13:20:12] nn step 15300, lr: 0.1.
	loss_policy_0: 0.18336
	accuracy_policy_0: 0.88461
	loss_value_0: 0.2799
	loss_policy_1: 0.03622
	accuracy_policy_1: 0.88582
	loss_value_1: 0.057
	loss_reward_1: 0.00441
	loss_policy_2: 0.03655
	accuracy_policy_2: 0.88211
	loss_value_2: 0.05859
	loss_reward_2: 0.00585
	loss_policy_3: 0.03617
	accuracy_policy_3: 0.8859
	loss_value_3: 0.06019
	loss_reward_3: 0.00713
	loss_policy_4: 0.03579
	accuracy_policy_4: 0.89133
	loss_value_4: 0.06199
	loss_reward_4: 0.00884
	loss_policy_5: 0.03599
	accuracy_policy_5: 0.89129
	loss_value_5: 0.0638
	loss_reward_5: 0.01029
	loss_policy: 0.36407
	loss_value: 0.58148
	loss_reward: 0.03653
[2025-05-11 13:20:19] nn step 15350, lr: 0.1.
	loss_policy_0: 0.18274
	accuracy_policy_0: 0.88559
	loss_value_0: 0.28132
	loss_policy_1: 0.03663
	accuracy_policy_1: 0.88762
	loss_value_1: 0.0572
	loss_reward_1: 0.00471
	loss_policy_2: 0.03658
	accuracy_policy_2: 0.88793
	loss_value_2: 0.05866
	loss_reward_2: 0.00577
	loss_policy_3: 0.03665
	accuracy_policy_3: 0.88758
	loss_value_3: 0.06019
	loss_reward_3: 0.00711
	loss_policy_4: 0.03659
	accuracy_policy_4: 0.89223
	loss_value_4: 0.06208
	loss_reward_4: 0.00904
	loss_policy_5: 0.03696
	accuracy_policy_5: 0.88777
	loss_value_5: 0.06406
	loss_reward_5: 0.01044
	loss_policy: 0.36615
	loss_value: 0.58352
	loss_reward: 0.03707
[2025-05-11 13:20:27] nn step 15400, lr: 0.1.
	loss_policy_0: 0.19317
	accuracy_policy_0: 0.88086
	loss_value_0: 0.29279
	loss_policy_1: 0.03837
	accuracy_policy_1: 0.88742
	loss_value_1: 0.05988
	loss_reward_1: 0.0047
	loss_policy_2: 0.03844
	accuracy_policy_2: 0.88836
	loss_value_2: 0.06142
	loss_reward_2: 0.00595
	loss_policy_3: 0.0384
	accuracy_policy_3: 0.88676
	loss_value_3: 0.06341
	loss_reward_3: 0.00726
	loss_policy_4: 0.03791
	accuracy_policy_4: 0.89191
	loss_value_4: 0.06521
	loss_reward_4: 0.00929
	loss_policy_5: 0.03841
	accuracy_policy_5: 0.89324
	loss_value_5: 0.06714
	loss_reward_5: 0.01057
	loss_policy: 0.3847
	loss_value: 0.60985
	loss_reward: 0.03778
Optimization_Done 15400
[2025-05-11 13:21:44] [command] train weight_iter_15400.pkl 59 78
[2025-05-11 13:21:53] nn step 15450, lr: 0.1.
	loss_policy_0: 0.17597
	accuracy_policy_0: 0.88387
	loss_value_0: 0.28304
	loss_policy_1: 0.03511
	accuracy_policy_1: 0.88625
	loss_value_1: 0.05732
	loss_reward_1: 0.00438
	loss_policy_2: 0.0352
	accuracy_policy_2: 0.88496
	loss_value_2: 0.05879
	loss_reward_2: 0.00584
	loss_policy_3: 0.03515
	accuracy_policy_3: 0.88688
	loss_value_3: 0.06074
	loss_reward_3: 0.00704
	loss_policy_4: 0.03523
	accuracy_policy_4: 0.8877
	loss_value_4: 0.06258
	loss_reward_4: 0.00887
	loss_policy_5: 0.03519
	accuracy_policy_5: 0.89086
	loss_value_5: 0.06417
	loss_reward_5: 0.00999
	loss_policy: 0.35185
	loss_value: 0.58664
	loss_reward: 0.03612
[2025-05-11 13:22:02] nn step 15500, lr: 0.1.
	loss_policy_0: 0.18167
	accuracy_policy_0: 0.88266
	loss_value_0: 0.28674
	loss_policy_1: 0.03596
	accuracy_policy_1: 0.88723
	loss_value_1: 0.05827
	loss_reward_1: 0.00438
	loss_policy_2: 0.03671
	accuracy_policy_2: 0.88484
	loss_value_2: 0.06022
	loss_reward_2: 0.00587
	loss_policy_3: 0.03652
	accuracy_policy_3: 0.88566
	loss_value_3: 0.0618
	loss_reward_3: 0.00716
	loss_policy_4: 0.03648
	accuracy_policy_4: 0.88801
	loss_value_4: 0.06374
	loss_reward_4: 0.00883
	loss_policy_5: 0.03637
	accuracy_policy_5: 0.8941
	loss_value_5: 0.06559
	loss_reward_5: 0.01046
	loss_policy: 0.36371
	loss_value: 0.59638
	loss_reward: 0.03671
[2025-05-11 13:22:10] nn step 15550, lr: 0.1.
	loss_policy_0: 0.17728
	accuracy_policy_0: 0.88664
	loss_value_0: 0.2773
	loss_policy_1: 0.03541
	accuracy_policy_1: 0.88758
	loss_value_1: 0.0564
	loss_reward_1: 0.0042
	loss_policy_2: 0.03557
	accuracy_policy_2: 0.88734
	loss_value_2: 0.05807
	loss_reward_2: 0.00529
	loss_policy_3: 0.03539
	accuracy_policy_3: 0.8902
	loss_value_3: 0.05983
	loss_reward_3: 0.00688
	loss_policy_4: 0.03528
	accuracy_policy_4: 0.88949
	loss_value_4: 0.06154
	loss_reward_4: 0.00882
	loss_policy_5: 0.0354
	accuracy_policy_5: 0.89328
	loss_value_5: 0.06359
	loss_reward_5: 0.01015
	loss_policy: 0.35434
	loss_value: 0.57672
	loss_reward: 0.03534
[2025-05-11 13:22:18] nn step 15600, lr: 0.1.
	loss_policy_0: 0.17167
	accuracy_policy_0: 0.88578
	loss_value_0: 0.26292
	loss_policy_1: 0.03456
	accuracy_policy_1: 0.88609
	loss_value_1: 0.05395
	loss_reward_1: 0.00429
	loss_policy_2: 0.03448
	accuracy_policy_2: 0.89105
	loss_value_2: 0.05544
	loss_reward_2: 0.00515
	loss_policy_3: 0.03433
	accuracy_policy_3: 0.88941
	loss_value_3: 0.057
	loss_reward_3: 0.00656
	loss_policy_4: 0.0341
	accuracy_policy_4: 0.89293
	loss_value_4: 0.05878
	loss_reward_4: 0.00845
	loss_policy_5: 0.03422
	accuracy_policy_5: 0.89246
	loss_value_5: 0.06081
	loss_reward_5: 0.00942
	loss_policy: 0.34337
	loss_value: 0.54891
	loss_reward: 0.03387
Optimization_Done 15600
[2025-05-11 13:23:34] [command] train weight_iter_15600.pkl 60 79
[2025-05-11 13:23:42] nn step 15650, lr: 0.1.
	loss_policy_0: 0.18318
	accuracy_policy_0: 0.88801
	loss_value_0: 0.29684
	loss_policy_1: 0.03655
	accuracy_policy_1: 0.89133
	loss_value_1: 0.06063
	loss_reward_1: 0.00482
	loss_policy_2: 0.03644
	accuracy_policy_2: 0.89285
	loss_value_2: 0.06185
	loss_reward_2: 0.00609
	loss_policy_3: 0.03652
	accuracy_policy_3: 0.89156
	loss_value_3: 0.0637
	loss_reward_3: 0.00756
	loss_policy_4: 0.03639
	accuracy_policy_4: 0.8952
	loss_value_4: 0.06534
	loss_reward_4: 0.00954
	loss_policy_5: 0.03641
	accuracy_policy_5: 0.89598
	loss_value_5: 0.06703
	loss_reward_5: 0.01046
	loss_policy: 0.36548
	loss_value: 0.61539
	loss_reward: 0.03847
[2025-05-11 13:23:51] nn step 15700, lr: 0.1.
	loss_policy_0: 0.17964
	accuracy_policy_0: 0.88977
	loss_value_0: 0.28887
	loss_policy_1: 0.03634
	accuracy_policy_1: 0.89254
	loss_value_1: 0.05883
	loss_reward_1: 0.00447
	loss_policy_2: 0.0362
	accuracy_policy_2: 0.88879
	loss_value_2: 0.06042
	loss_reward_2: 0.00565
	loss_policy_3: 0.0361
	accuracy_policy_3: 0.89246
	loss_value_3: 0.06254
	loss_reward_3: 0.00748
	loss_policy_4: 0.03595
	accuracy_policy_4: 0.89031
	loss_value_4: 0.06404
	loss_reward_4: 0.00929
	loss_policy_5: 0.03612
	accuracy_policy_5: 0.89477
	loss_value_5: 0.06607
	loss_reward_5: 0.01037
	loss_policy: 0.36036
	loss_value: 0.60077
	loss_reward: 0.03727
[2025-05-11 13:23:59] nn step 15750, lr: 0.1.
	loss_policy_0: 0.19063
	accuracy_policy_0: 0.89238
	loss_value_0: 0.30304
	loss_policy_1: 0.03809
	accuracy_policy_1: 0.89223
	loss_value_1: 0.06155
	loss_reward_1: 0.00486
	loss_policy_2: 0.03841
	accuracy_policy_2: 0.89223
	loss_value_2: 0.06342
	loss_reward_2: 0.00634
	loss_policy_3: 0.03807
	accuracy_policy_3: 0.89348
	loss_value_3: 0.06532
	loss_reward_3: 0.00784
	loss_policy_4: 0.03844
	accuracy_policy_4: 0.89426
	loss_value_4: 0.06756
	loss_reward_4: 0.00976
	loss_policy_5: 0.03853
	accuracy_policy_5: 0.89746
	loss_value_5: 0.06934
	loss_reward_5: 0.0111
	loss_policy: 0.38217
	loss_value: 0.63023
	loss_reward: 0.03991
[2025-05-11 13:24:06] nn step 15800, lr: 0.1.
	loss_policy_0: 0.1722
	accuracy_policy_0: 0.89359
	loss_value_0: 0.26908
	loss_policy_1: 0.03434
	accuracy_policy_1: 0.88816
	loss_value_1: 0.05442
	loss_reward_1: 0.00413
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.88891
	loss_value_2: 0.05627
	loss_reward_2: 0.00551
	loss_policy_3: 0.03427
	accuracy_policy_3: 0.89254
	loss_value_3: 0.05825
	loss_reward_3: 0.00677
	loss_policy_4: 0.03437
	accuracy_policy_4: 0.89094
	loss_value_4: 0.05977
	loss_reward_4: 0.00863
	loss_policy_5: 0.034
	accuracy_policy_5: 0.89832
	loss_value_5: 0.0616
	loss_reward_5: 0.01002
	loss_policy: 0.34352
	loss_value: 0.55939
	loss_reward: 0.03506
Optimization_Done 15800
[2025-05-11 13:25:24] [command] train weight_iter_15800.pkl 61 80
[2025-05-11 13:25:32] nn step 15850, lr: 0.1.
	loss_policy_0: 0.18818
	accuracy_policy_0: 0.88922
	loss_value_0: 0.30251
	loss_policy_1: 0.0377
	accuracy_policy_1: 0.88621
	loss_value_1: 0.06165
	loss_reward_1: 0.00478
	loss_policy_2: 0.03786
	accuracy_policy_2: 0.88598
	loss_value_2: 0.06345
	loss_reward_2: 0.00607
	loss_policy_3: 0.03795
	accuracy_policy_3: 0.88629
	loss_value_3: 0.0653
	loss_reward_3: 0.00775
	loss_policy_4: 0.03791
	accuracy_policy_4: 0.8859
	loss_value_4: 0.06736
	loss_reward_4: 0.00958
	loss_policy_5: 0.03785
	accuracy_policy_5: 0.89102
	loss_value_5: 0.06903
	loss_reward_5: 0.01073
	loss_policy: 0.37745
	loss_value: 0.6293
	loss_reward: 0.03891
[2025-05-11 13:25:40] nn step 15900, lr: 0.1.
	loss_policy_0: 0.17933
	accuracy_policy_0: 0.88301
	loss_value_0: 0.28004
	loss_policy_1: 0.03514
	accuracy_policy_1: 0.88473
	loss_value_1: 0.05714
	loss_reward_1: 0.00455
	loss_policy_2: 0.03546
	accuracy_policy_2: 0.88516
	loss_value_2: 0.0587
	loss_reward_2: 0.00557
	loss_policy_3: 0.03561
	accuracy_policy_3: 0.88438
	loss_value_3: 0.06046
	loss_reward_3: 0.00711
	loss_policy_4: 0.03569
	accuracy_policy_4: 0.88664
	loss_value_4: 0.06225
	loss_reward_4: 0.00874
	loss_policy_5: 0.03577
	accuracy_policy_5: 0.88945
	loss_value_5: 0.06407
	loss_reward_5: 0.01009
	loss_policy: 0.35701
	loss_value: 0.58265
	loss_reward: 0.03606
[2025-05-11 13:25:49] nn step 15950, lr: 0.1.
	loss_policy_0: 0.18662
	accuracy_policy_0: 0.88477
	loss_value_0: 0.29771
	loss_policy_1: 0.03759
	accuracy_policy_1: 0.88633
	loss_value_1: 0.06067
	loss_reward_1: 0.00478
	loss_policy_2: 0.03755
	accuracy_policy_2: 0.8891
	loss_value_2: 0.06257
	loss_reward_2: 0.00596
	loss_policy_3: 0.03768
	accuracy_policy_3: 0.88496
	loss_value_3: 0.06448
	loss_reward_3: 0.00752
	loss_policy_4: 0.03754
	accuracy_policy_4: 0.88836
	loss_value_4: 0.06621
	loss_reward_4: 0.00977
	loss_policy_5: 0.03717
	accuracy_policy_5: 0.89281
	loss_value_5: 0.06822
	loss_reward_5: 0.01086
	loss_policy: 0.37414
	loss_value: 0.61986
	loss_reward: 0.03889
[2025-05-11 13:25:57] nn step 16000, lr: 0.1.
	loss_policy_0: 0.17925
	accuracy_policy_0: 0.88855
	loss_value_0: 0.28108
	loss_policy_1: 0.03605
	accuracy_policy_1: 0.88324
	loss_value_1: 0.05747
	loss_reward_1: 0.00449
	loss_policy_2: 0.03586
	accuracy_policy_2: 0.8882
	loss_value_2: 0.05906
	loss_reward_2: 0.00599
	loss_policy_3: 0.03582
	accuracy_policy_3: 0.88969
	loss_value_3: 0.06076
	loss_reward_3: 0.007
	loss_policy_4: 0.03561
	accuracy_policy_4: 0.88895
	loss_value_4: 0.06268
	loss_reward_4: 0.00914
	loss_policy_5: 0.03555
	accuracy_policy_5: 0.8943
	loss_value_5: 0.06436
	loss_reward_5: 0.01025
	loss_policy: 0.35813
	loss_value: 0.5854
	loss_reward: 0.03688
Optimization_Done 16000
[2025-05-11 13:27:15] [command] train weight_iter_16000.pkl 62 81
[2025-05-11 13:27:24] nn step 16050, lr: 0.1.
	loss_policy_0: 0.18222
	accuracy_policy_0: 0.89109
	loss_value_0: 0.29595
	loss_policy_1: 0.03664
	accuracy_policy_1: 0.88746
	loss_value_1: 0.06033
	loss_reward_1: 0.0049
	loss_policy_2: 0.03652
	accuracy_policy_2: 0.88715
	loss_value_2: 0.06225
	loss_reward_2: 0.00594
	loss_policy_3: 0.03681
	accuracy_policy_3: 0.89184
	loss_value_3: 0.06393
	loss_reward_3: 0.00707
	loss_policy_4: 0.03634
	accuracy_policy_4: 0.89246
	loss_value_4: 0.06583
	loss_reward_4: 0.00928
	loss_policy_5: 0.0366
	accuracy_policy_5: 0.89551
	loss_value_5: 0.06772
	loss_reward_5: 0.01082
	loss_policy: 0.36513
	loss_value: 0.61601
	loss_reward: 0.038
[2025-05-11 13:27:31] nn step 16100, lr: 0.1.
	loss_policy_0: 0.18277
	accuracy_policy_0: 0.88637
	loss_value_0: 0.29398
	loss_policy_1: 0.03658
	accuracy_policy_1: 0.88715
	loss_value_1: 0.06004
	loss_reward_1: 0.00474
	loss_policy_2: 0.03669
	accuracy_policy_2: 0.88715
	loss_value_2: 0.06216
	loss_reward_2: 0.006
	loss_policy_3: 0.03698
	accuracy_policy_3: 0.88801
	loss_value_3: 0.06382
	loss_reward_3: 0.00746
	loss_policy_4: 0.03692
	accuracy_policy_4: 0.89004
	loss_value_4: 0.06562
	loss_reward_4: 0.0093
	loss_policy_5: 0.0366
	accuracy_policy_5: 0.89531
	loss_value_5: 0.06736
	loss_reward_5: 0.01099
	loss_policy: 0.36655
	loss_value: 0.61298
	loss_reward: 0.03848
[2025-05-11 13:27:40] nn step 16150, lr: 0.1.
	loss_policy_0: 0.17687
	accuracy_policy_0: 0.88945
	loss_value_0: 0.27928
	loss_policy_1: 0.03562
	accuracy_policy_1: 0.88551
	loss_value_1: 0.05704
	loss_reward_1: 0.00452
	loss_policy_2: 0.03565
	accuracy_policy_2: 0.88844
	loss_value_2: 0.05898
	loss_reward_2: 0.0055
	loss_policy_3: 0.03588
	accuracy_policy_3: 0.88758
	loss_value_3: 0.06061
	loss_reward_3: 0.00731
	loss_policy_4: 0.03542
	accuracy_policy_4: 0.89094
	loss_value_4: 0.06257
	loss_reward_4: 0.00891
	loss_policy_5: 0.03539
	accuracy_policy_5: 0.89238
	loss_value_5: 0.06438
	loss_reward_5: 0.0103
	loss_policy: 0.35482
	loss_value: 0.58286
	loss_reward: 0.03655
[2025-05-11 13:27:48] nn step 16200, lr: 0.1.
	loss_policy_0: 0.19402
	accuracy_policy_0: 0.88191
	loss_value_0: 0.30645
	loss_policy_1: 0.03874
	accuracy_policy_1: 0.88176
	loss_value_1: 0.06272
	loss_reward_1: 0.00512
	loss_policy_2: 0.03904
	accuracy_policy_2: 0.88246
	loss_value_2: 0.06472
	loss_reward_2: 0.00626
	loss_policy_3: 0.0388
	accuracy_policy_3: 0.88676
	loss_value_3: 0.06686
	loss_reward_3: 0.00792
	loss_policy_4: 0.03883
	accuracy_policy_4: 0.88766
	loss_value_4: 0.06894
	loss_reward_4: 0.01033
	loss_policy_5: 0.03841
	accuracy_policy_5: 0.89297
	loss_value_5: 0.07088
	loss_reward_5: 0.0112
	loss_policy: 0.38784
	loss_value: 0.64057
	loss_reward: 0.04082
Optimization_Done 16200
[2025-05-11 13:29:07] [command] train weight_iter_16200.pkl 63 82
[2025-05-11 13:29:16] nn step 16250, lr: 0.1.
	loss_policy_0: 0.18388
	accuracy_policy_0: 0.87305
	loss_value_0: 0.30687
	loss_policy_1: 0.03658
	accuracy_policy_1: 0.87496
	loss_value_1: 0.06222
	loss_reward_1: 0.00462
	loss_policy_2: 0.03673
	accuracy_policy_2: 0.87391
	loss_value_2: 0.06394
	loss_reward_2: 0.00571
	loss_policy_3: 0.03688
	accuracy_policy_3: 0.87723
	loss_value_3: 0.06531
	loss_reward_3: 0.00713
	loss_policy_4: 0.03679
	accuracy_policy_4: 0.87766
	loss_value_4: 0.06721
	loss_reward_4: 0.0094
	loss_policy_5: 0.03661
	accuracy_policy_5: 0.87957
	loss_value_5: 0.06879
	loss_reward_5: 0.01082
	loss_policy: 0.36746
	loss_value: 0.63435
	loss_reward: 0.03769
[2025-05-11 13:29:23] nn step 16300, lr: 0.1.
	loss_policy_0: 0.16782
	accuracy_policy_0: 0.8734
	loss_value_0: 0.27228
	loss_policy_1: 0.03359
	accuracy_policy_1: 0.87172
	loss_value_1: 0.05529
	loss_reward_1: 0.00429
	loss_policy_2: 0.03336
	accuracy_policy_2: 0.87367
	loss_value_2: 0.05701
	loss_reward_2: 0.00552
	loss_policy_3: 0.03351
	accuracy_policy_3: 0.87414
	loss_value_3: 0.05856
	loss_reward_3: 0.0068
	loss_policy_4: 0.03375
	accuracy_policy_4: 0.875
	loss_value_4: 0.05993
	loss_reward_4: 0.00846
	loss_policy_5: 0.03338
	accuracy_policy_5: 0.87738
	loss_value_5: 0.06156
	loss_reward_5: 0.00976
	loss_policy: 0.33543
	loss_value: 0.56462
	loss_reward: 0.03484
[2025-05-11 13:29:32] nn step 16350, lr: 0.1.
	loss_policy_0: 0.18148
	accuracy_policy_0: 0.87453
	loss_value_0: 0.28946
	loss_policy_1: 0.03629
	accuracy_policy_1: 0.87484
	loss_value_1: 0.05892
	loss_reward_1: 0.00445
	loss_policy_2: 0.03629
	accuracy_policy_2: 0.87305
	loss_value_2: 0.06073
	loss_reward_2: 0.00586
	loss_policy_3: 0.03658
	accuracy_policy_3: 0.87938
	loss_value_3: 0.06253
	loss_reward_3: 0.00689
	loss_policy_4: 0.03645
	accuracy_policy_4: 0.87684
	loss_value_4: 0.06429
	loss_reward_4: 0.00907
	loss_policy_5: 0.03627
	accuracy_policy_5: 0.87828
	loss_value_5: 0.06634
	loss_reward_5: 0.0104
	loss_policy: 0.36336
	loss_value: 0.60227
	loss_reward: 0.03667
[2025-05-11 13:29:40] nn step 16400, lr: 0.1.
	loss_policy_0: 0.18556
	accuracy_policy_0: 0.87375
	loss_value_0: 0.29424
	loss_policy_1: 0.03712
	accuracy_policy_1: 0.87578
	loss_value_1: 0.05955
	loss_reward_1: 0.00456
	loss_policy_2: 0.0373
	accuracy_policy_2: 0.87758
	loss_value_2: 0.0613
	loss_reward_2: 0.00565
	loss_policy_3: 0.03727
	accuracy_policy_3: 0.87641
	loss_value_3: 0.06339
	loss_reward_3: 0.00729
	loss_policy_4: 0.03724
	accuracy_policy_4: 0.87965
	loss_value_4: 0.06517
	loss_reward_4: 0.00925
	loss_policy_5: 0.03713
	accuracy_policy_5: 0.87883
	loss_value_5: 0.06686
	loss_reward_5: 0.01034
	loss_policy: 0.37161
	loss_value: 0.61051
	loss_reward: 0.0371
Optimization_Done 16400
[2025-05-11 13:30:55] [command] train weight_iter_16400.pkl 64 83
[2025-05-11 13:31:04] nn step 16450, lr: 0.1.
	loss_policy_0: 0.18096
	accuracy_policy_0: 0.87273
	loss_value_0: 0.29306
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.87332
	loss_value_1: 0.05979
	loss_reward_1: 0.00474
	loss_policy_2: 0.03617
	accuracy_policy_2: 0.87258
	loss_value_2: 0.06145
	loss_reward_2: 0.00599
	loss_policy_3: 0.03595
	accuracy_policy_3: 0.87707
	loss_value_3: 0.06297
	loss_reward_3: 0.00724
	loss_policy_4: 0.03591
	accuracy_policy_4: 0.87621
	loss_value_4: 0.06464
	loss_reward_4: 0.00918
	loss_policy_5: 0.03599
	accuracy_policy_5: 0.87875
	loss_value_5: 0.0664
	loss_reward_5: 0.01099
	loss_policy: 0.36103
	loss_value: 0.60831
	loss_reward: 0.03814
[2025-05-11 13:31:13] nn step 16500, lr: 0.1.
	loss_policy_0: 0.19067
	accuracy_policy_0: 0.87863
	loss_value_0: 0.30482
	loss_policy_1: 0.03812
	accuracy_policy_1: 0.87828
	loss_value_1: 0.06185
	loss_reward_1: 0.00491
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.87836
	loss_value_2: 0.06369
	loss_reward_2: 0.00625
	loss_policy_3: 0.03838
	accuracy_policy_3: 0.87801
	loss_value_3: 0.06567
	loss_reward_3: 0.00764
	loss_policy_4: 0.0386
	accuracy_policy_4: 0.87871
	loss_value_4: 0.06725
	loss_reward_4: 0.00976
	loss_policy_5: 0.03842
	accuracy_policy_5: 0.88094
	loss_value_5: 0.06886
	loss_reward_5: 0.01094
	loss_policy: 0.38214
	loss_value: 0.63215
	loss_reward: 0.0395
[2025-05-11 13:31:20] nn step 16550, lr: 0.1.
	loss_policy_0: 0.18854
	accuracy_policy_0: 0.87691
	loss_value_0: 0.29889
	loss_policy_1: 0.03776
	accuracy_policy_1: 0.87961
	loss_value_1: 0.06106
	loss_reward_1: 0.00475
	loss_policy_2: 0.03768
	accuracy_policy_2: 0.87848
	loss_value_2: 0.06286
	loss_reward_2: 0.00625
	loss_policy_3: 0.03803
	accuracy_policy_3: 0.87621
	loss_value_3: 0.06478
	loss_reward_3: 0.00757
	loss_policy_4: 0.03775
	accuracy_policy_4: 0.88078
	loss_value_4: 0.06652
	loss_reward_4: 0.00939
	loss_policy_5: 0.03754
	accuracy_policy_5: 0.88562
	loss_value_5: 0.0682
	loss_reward_5: 0.01074
	loss_policy: 0.37731
	loss_value: 0.62231
	loss_reward: 0.0387
[2025-05-11 13:31:29] nn step 16600, lr: 0.1.
	loss_policy_0: 0.18892
	accuracy_policy_0: 0.87547
	loss_value_0: 0.29542
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.87777
	loss_value_1: 0.0604
	loss_reward_1: 0.0049
	loss_policy_2: 0.03775
	accuracy_policy_2: 0.88023
	loss_value_2: 0.06195
	loss_reward_2: 0.00631
	loss_policy_3: 0.03751
	accuracy_policy_3: 0.88012
	loss_value_3: 0.06396
	loss_reward_3: 0.00772
	loss_policy_4: 0.03809
	accuracy_policy_4: 0.88129
	loss_value_4: 0.06599
	loss_reward_4: 0.00973
	loss_policy_5: 0.03793
	accuracy_policy_5: 0.88293
	loss_value_5: 0.06775
	loss_reward_5: 0.01108
	loss_policy: 0.37795
	loss_value: 0.61548
	loss_reward: 0.03974
Optimization_Done 16600
[2025-05-11 13:32:47] [command] train weight_iter_16600.pkl 65 84
[2025-05-11 13:32:56] nn step 16650, lr: 0.1.
	loss_policy_0: 0.18274
	accuracy_policy_0: 0.87605
	loss_value_0: 0.29818
	loss_policy_1: 0.03679
	accuracy_policy_1: 0.87562
	loss_value_1: 0.06064
	loss_reward_1: 0.00483
	loss_policy_2: 0.03683
	accuracy_policy_2: 0.87617
	loss_value_2: 0.06259
	loss_reward_2: 0.00606
	loss_policy_3: 0.0368
	accuracy_policy_3: 0.87934
	loss_value_3: 0.06415
	loss_reward_3: 0.0074
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.88121
	loss_value_4: 0.06606
	loss_reward_4: 0.00946
	loss_policy_5: 0.03654
	accuracy_policy_5: 0.88145
	loss_value_5: 0.06799
	loss_reward_5: 0.01061
	loss_policy: 0.36652
	loss_value: 0.61961
	loss_reward: 0.03835
[2025-05-11 13:33:04] nn step 16700, lr: 0.1.
	loss_policy_0: 0.17164
	accuracy_policy_0: 0.88129
	loss_value_0: 0.27353
	loss_policy_1: 0.03427
	accuracy_policy_1: 0.87711
	loss_value_1: 0.05523
	loss_reward_1: 0.0046
	loss_policy_2: 0.03439
	accuracy_policy_2: 0.87703
	loss_value_2: 0.05731
	loss_reward_2: 0.00567
	loss_policy_3: 0.03419
	accuracy_policy_3: 0.88445
	loss_value_3: 0.05898
	loss_reward_3: 0.00714
	loss_policy_4: 0.03407
	accuracy_policy_4: 0.88176
	loss_value_4: 0.06049
	loss_reward_4: 0.00875
	loss_policy_5: 0.03439
	accuracy_policy_5: 0.88262
	loss_value_5: 0.06261
	loss_reward_5: 0.0102
	loss_policy: 0.34295
	loss_value: 0.56816
	loss_reward: 0.03636
[2025-05-11 13:33:11] nn step 16750, lr: 0.1.
	loss_policy_0: 0.18755
	accuracy_policy_0: 0.87719
	loss_value_0: 0.29712
	loss_policy_1: 0.03758
	accuracy_policy_1: 0.87766
	loss_value_1: 0.06083
	loss_reward_1: 0.00505
	loss_policy_2: 0.03742
	accuracy_policy_2: 0.8791
	loss_value_2: 0.06285
	loss_reward_2: 0.00596
	loss_policy_3: 0.03741
	accuracy_policy_3: 0.88141
	loss_value_3: 0.06461
	loss_reward_3: 0.00772
	loss_policy_4: 0.03765
	accuracy_policy_4: 0.88254
	loss_value_4: 0.06653
	loss_reward_4: 0.0099
	loss_policy_5: 0.03759
	accuracy_policy_5: 0.8857
	loss_value_5: 0.06828
	loss_reward_5: 0.01115
	loss_policy: 0.3752
	loss_value: 0.62022
	loss_reward: 0.03977
[2025-05-11 13:33:20] nn step 16800, lr: 0.1.
	loss_policy_0: 0.19918
	accuracy_policy_0: 0.87875
	loss_value_0: 0.31453
	loss_policy_1: 0.03961
	accuracy_policy_1: 0.87984
	loss_value_1: 0.0644
	loss_reward_1: 0.00502
	loss_policy_2: 0.03968
	accuracy_policy_2: 0.88129
	loss_value_2: 0.0662
	loss_reward_2: 0.00656
	loss_policy_3: 0.03976
	accuracy_policy_3: 0.87805
	loss_value_3: 0.06828
	loss_reward_3: 0.00782
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.8825
	loss_value_4: 0.07058
	loss_reward_4: 0.00995
	loss_policy_5: 0.03956
	accuracy_policy_5: 0.88277
	loss_value_5: 0.07305
	loss_reward_5: 0.01177
	loss_policy: 0.39747
	loss_value: 0.65704
	loss_reward: 0.04112
Optimization_Done 16800
[2025-05-11 13:34:35] [command] train weight_iter_16800.pkl 66 85
[2025-05-11 13:34:44] nn step 16850, lr: 0.1.
	loss_policy_0: 0.18577
	accuracy_policy_0: 0.86129
	loss_value_0: 0.33057
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.86297
	loss_value_1: 0.06729
	loss_reward_1: 0.00476
	loss_policy_2: 0.03725
	accuracy_policy_2: 0.8616
	loss_value_2: 0.069
	loss_reward_2: 0.00615
	loss_policy_3: 0.03718
	accuracy_policy_3: 0.8627
	loss_value_3: 0.07052
	loss_reward_3: 0.0076
	loss_policy_4: 0.03756
	accuracy_policy_4: 0.86699
	loss_value_4: 0.07273
	loss_reward_4: 0.01001
	loss_policy_5: 0.03746
	accuracy_policy_5: 0.8641
	loss_value_5: 0.0744
	loss_reward_5: 0.01142
	loss_policy: 0.37263
	loss_value: 0.68451
	loss_reward: 0.03994
[2025-05-11 13:34:53] nn step 16900, lr: 0.1.
	loss_policy_0: 0.18025
	accuracy_policy_0: 0.86312
	loss_value_0: 0.3034
	loss_policy_1: 0.03626
	accuracy_policy_1: 0.86066
	loss_value_1: 0.06165
	loss_reward_1: 0.00463
	loss_policy_2: 0.03648
	accuracy_policy_2: 0.86902
	loss_value_2: 0.06342
	loss_reward_2: 0.00563
	loss_policy_3: 0.03651
	accuracy_policy_3: 0.86824
	loss_value_3: 0.06532
	loss_reward_3: 0.00736
	loss_policy_4: 0.03641
	accuracy_policy_4: 0.86988
	loss_value_4: 0.06705
	loss_reward_4: 0.00914
	loss_policy_5: 0.03629
	accuracy_policy_5: 0.87441
	loss_value_5: 0.06889
	loss_reward_5: 0.01037
	loss_policy: 0.36221
	loss_value: 0.62973
	loss_reward: 0.03713
[2025-05-11 13:35:01] nn step 16950, lr: 0.1.
	loss_policy_0: 0.18604
	accuracy_policy_0: 0.86137
	loss_value_0: 0.30922
	loss_policy_1: 0.03729
	accuracy_policy_1: 0.86605
	loss_value_1: 0.06282
	loss_reward_1: 0.00449
	loss_policy_2: 0.0372
	accuracy_policy_2: 0.86625
	loss_value_2: 0.06446
	loss_reward_2: 0.00612
	loss_policy_3: 0.03742
	accuracy_policy_3: 0.86914
	loss_value_3: 0.06627
	loss_reward_3: 0.00731
	loss_policy_4: 0.03735
	accuracy_policy_4: 0.87477
	loss_value_4: 0.06849
	loss_reward_4: 0.00943
	loss_policy_5: 0.03696
	accuracy_policy_5: 0.87379
	loss_value_5: 0.07049
	loss_reward_5: 0.01125
	loss_policy: 0.37225
	loss_value: 0.64175
	loss_reward: 0.0386
[2025-05-11 13:35:08] nn step 17000, lr: 0.1.
	loss_policy_0: 0.18448
	accuracy_policy_0: 0.86488
	loss_value_0: 0.30391
	loss_policy_1: 0.03729
	accuracy_policy_1: 0.86805
	loss_value_1: 0.06193
	loss_reward_1: 0.00487
	loss_policy_2: 0.03714
	accuracy_policy_2: 0.86781
	loss_value_2: 0.06338
	loss_reward_2: 0.00589
	loss_policy_3: 0.03704
	accuracy_policy_3: 0.86984
	loss_value_3: 0.0649
	loss_reward_3: 0.00769
	loss_policy_4: 0.03705
	accuracy_policy_4: 0.87078
	loss_value_4: 0.06675
	loss_reward_4: 0.00929
	loss_policy_5: 0.03706
	accuracy_policy_5: 0.87113
	loss_value_5: 0.06873
	loss_reward_5: 0.01092
	loss_policy: 0.37005
	loss_value: 0.6296
	loss_reward: 0.03867
Optimization_Done 17000
[2025-05-11 13:36:26] [command] train weight_iter_17000.pkl 67 86
[2025-05-11 13:36:33] nn step 17050, lr: 0.1.
	loss_policy_0: 0.18034
	accuracy_policy_0: 0.86758
	loss_value_0: 0.30865
	loss_policy_1: 0.03628
	accuracy_policy_1: 0.8682
	loss_value_1: 0.06247
	loss_reward_1: 0.00465
	loss_policy_2: 0.03626
	accuracy_policy_2: 0.86832
	loss_value_2: 0.06383
	loss_reward_2: 0.00576
	loss_policy_3: 0.03628
	accuracy_policy_3: 0.87023
	loss_value_3: 0.06524
	loss_reward_3: 0.00721
	loss_policy_4: 0.0366
	accuracy_policy_4: 0.86727
	loss_value_4: 0.06701
	loss_reward_4: 0.00925
	loss_policy_5: 0.03643
	accuracy_policy_5: 0.8725
	loss_value_5: 0.06889
	loss_reward_5: 0.01072
	loss_policy: 0.36218
	loss_value: 0.63608
	loss_reward: 0.03757
[2025-05-11 13:36:42] nn step 17100, lr: 0.1.
	loss_policy_0: 0.1823
	accuracy_policy_0: 0.86965
	loss_value_0: 0.30136
	loss_policy_1: 0.03662
	accuracy_policy_1: 0.86441
	loss_value_1: 0.06145
	loss_reward_1: 0.00463
	loss_policy_2: 0.03642
	accuracy_policy_2: 0.86949
	loss_value_2: 0.06299
	loss_reward_2: 0.0059
	loss_policy_3: 0.03646
	accuracy_policy_3: 0.87141
	loss_value_3: 0.06474
	loss_reward_3: 0.00777
	loss_policy_4: 0.03632
	accuracy_policy_4: 0.8702
	loss_value_4: 0.06678
	loss_reward_4: 0.00933
	loss_policy_5: 0.0364
	accuracy_policy_5: 0.87516
	loss_value_5: 0.06859
	loss_reward_5: 0.01063
	loss_policy: 0.36452
	loss_value: 0.62591
	loss_reward: 0.03826
[2025-05-11 13:36:50] nn step 17150, lr: 0.1.
	loss_policy_0: 0.18323
	accuracy_policy_0: 0.86793
	loss_value_0: 0.3023
	loss_policy_1: 0.03653
	accuracy_policy_1: 0.86555
	loss_value_1: 0.06147
	loss_reward_1: 0.0045
	loss_policy_2: 0.03645
	accuracy_policy_2: 0.86852
	loss_value_2: 0.06287
	loss_reward_2: 0.00582
	loss_policy_3: 0.03645
	accuracy_policy_3: 0.87094
	loss_value_3: 0.06456
	loss_reward_3: 0.0075
	loss_policy_4: 0.03618
	accuracy_policy_4: 0.87508
	loss_value_4: 0.06621
	loss_reward_4: 0.00899
	loss_policy_5: 0.03648
	accuracy_policy_5: 0.87371
	loss_value_5: 0.06792
	loss_reward_5: 0.01051
	loss_policy: 0.36531
	loss_value: 0.62532
	loss_reward: 0.03732
[2025-05-11 13:36:58] nn step 17200, lr: 0.1.
	loss_policy_0: 0.18064
	accuracy_policy_0: 0.86809
	loss_value_0: 0.29356
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.86754
	loss_value_1: 0.05968
	loss_reward_1: 0.00433
	loss_policy_2: 0.03607
	accuracy_policy_2: 0.86941
	loss_value_2: 0.06121
	loss_reward_2: 0.00558
	loss_policy_3: 0.03652
	accuracy_policy_3: 0.87047
	loss_value_3: 0.06304
	loss_reward_3: 0.00718
	loss_policy_4: 0.03603
	accuracy_policy_4: 0.87125
	loss_value_4: 0.06473
	loss_reward_4: 0.009
	loss_policy_5: 0.0361
	accuracy_policy_5: 0.87379
	loss_value_5: 0.06668
	loss_reward_5: 0.01031
	loss_policy: 0.36134
	loss_value: 0.60889
	loss_reward: 0.0364
Optimization_Done 17200
[2025-05-11 13:38:14] [command] train weight_iter_17200.pkl 68 87
[2025-05-11 13:38:21] nn step 17250, lr: 0.1.
	loss_policy_0: 0.18543
	accuracy_policy_0: 0.87172
	loss_value_0: 0.30963
	loss_policy_1: 0.03689
	accuracy_policy_1: 0.87383
	loss_value_1: 0.0628
	loss_reward_1: 0.00475
	loss_policy_2: 0.03701
	accuracy_policy_2: 0.8734
	loss_value_2: 0.06445
	loss_reward_2: 0.00582
	loss_policy_3: 0.03691
	accuracy_policy_3: 0.87359
	loss_value_3: 0.06585
	loss_reward_3: 0.00757
	loss_policy_4: 0.03712
	accuracy_policy_4: 0.87512
	loss_value_4: 0.06742
	loss_reward_4: 0.00927
	loss_policy_5: 0.03704
	accuracy_policy_5: 0.8752
	loss_value_5: 0.06925
	loss_reward_5: 0.01079
	loss_policy: 0.37039
	loss_value: 0.6394
	loss_reward: 0.0382
[2025-05-11 13:38:30] nn step 17300, lr: 0.1.
	loss_policy_0: 0.18338
	accuracy_policy_0: 0.87316
	loss_value_0: 0.30282
	loss_policy_1: 0.03686
	accuracy_policy_1: 0.87316
	loss_value_1: 0.06138
	loss_reward_1: 0.00472
	loss_policy_2: 0.03693
	accuracy_policy_2: 0.87387
	loss_value_2: 0.06298
	loss_reward_2: 0.0061
	loss_policy_3: 0.03655
	accuracy_policy_3: 0.87902
	loss_value_3: 0.06456
	loss_reward_3: 0.00758
	loss_policy_4: 0.03675
	accuracy_policy_4: 0.87852
	loss_value_4: 0.06678
	loss_reward_4: 0.00907
	loss_policy_5: 0.03645
	accuracy_policy_5: 0.88184
	loss_value_5: 0.06848
	loss_reward_5: 0.01063
	loss_policy: 0.36693
	loss_value: 0.62701
	loss_reward: 0.0381
[2025-05-11 13:38:38] nn step 17350, lr: 0.1.
	loss_policy_0: 0.1903
	accuracy_policy_0: 0.86809
	loss_value_0: 0.30951
	loss_policy_1: 0.03826
	accuracy_policy_1: 0.86742
	loss_value_1: 0.0631
	loss_reward_1: 0.0048
	loss_policy_2: 0.03827
	accuracy_policy_2: 0.87363
	loss_value_2: 0.06476
	loss_reward_2: 0.00609
	loss_policy_3: 0.03798
	accuracy_policy_3: 0.87539
	loss_value_3: 0.06659
	loss_reward_3: 0.0077
	loss_policy_4: 0.03806
	accuracy_policy_4: 0.87746
	loss_value_4: 0.06842
	loss_reward_4: 0.00978
	loss_policy_5: 0.0379
	accuracy_policy_5: 0.87699
	loss_value_5: 0.07071
	loss_reward_5: 0.01101
	loss_policy: 0.38077
	loss_value: 0.64309
	loss_reward: 0.03939
[2025-05-11 13:38:47] nn step 17400, lr: 0.1.
	loss_policy_0: 0.1793
	accuracy_policy_0: 0.87148
	loss_value_0: 0.28522
	loss_policy_1: 0.03545
	accuracy_policy_1: 0.87465
	loss_value_1: 0.05859
	loss_reward_1: 0.00459
	loss_policy_2: 0.03571
	accuracy_policy_2: 0.87297
	loss_value_2: 0.05999
	loss_reward_2: 0.00563
	loss_policy_3: 0.03557
	accuracy_policy_3: 0.87586
	loss_value_3: 0.06176
	loss_reward_3: 0.00703
	loss_policy_4: 0.03567
	accuracy_policy_4: 0.88273
	loss_value_4: 0.06334
	loss_reward_4: 0.00886
	loss_policy_5: 0.03556
	accuracy_policy_5: 0.88352
	loss_value_5: 0.06536
	loss_reward_5: 0.01038
	loss_policy: 0.35726
	loss_value: 0.59427
	loss_reward: 0.03649
Optimization_Done 17400
[2025-05-11 13:40:05] [command] train weight_iter_17400.pkl 69 88
[2025-05-11 13:40:13] nn step 17450, lr: 0.1.
	loss_policy_0: 0.16898
	accuracy_policy_0: 0.87324
	loss_value_0: 0.27802
	loss_policy_1: 0.03399
	accuracy_policy_1: 0.8766
	loss_value_1: 0.05652
	loss_reward_1: 0.00454
	loss_policy_2: 0.03403
	accuracy_policy_2: 0.87637
	loss_value_2: 0.05788
	loss_reward_2: 0.00544
	loss_policy_3: 0.03405
	accuracy_policy_3: 0.87559
	loss_value_3: 0.05948
	loss_reward_3: 0.007
	loss_policy_4: 0.03382
	accuracy_policy_4: 0.8759
	loss_value_4: 0.06113
	loss_reward_4: 0.00882
	loss_policy_5: 0.03421
	accuracy_policy_5: 0.87648
	loss_value_5: 0.06276
	loss_reward_5: 0.01015
	loss_policy: 0.33908
	loss_value: 0.57579
	loss_reward: 0.03596
[2025-05-11 13:40:22] nn step 17500, lr: 0.1.
	loss_policy_0: 0.19692
	accuracy_policy_0: 0.87332
	loss_value_0: 0.31782
	loss_policy_1: 0.03929
	accuracy_policy_1: 0.87344
	loss_value_1: 0.06517
	loss_reward_1: 0.00497
	loss_policy_2: 0.03898
	accuracy_policy_2: 0.87781
	loss_value_2: 0.06679
	loss_reward_2: 0.00624
	loss_policy_3: 0.03924
	accuracy_policy_3: 0.87949
	loss_value_3: 0.06855
	loss_reward_3: 0.00797
	loss_policy_4: 0.03941
	accuracy_policy_4: 0.8766
	loss_value_4: 0.07066
	loss_reward_4: 0.00996
	loss_policy_5: 0.03932
	accuracy_policy_5: 0.87801
	loss_value_5: 0.07298
	loss_reward_5: 0.01129
	loss_policy: 0.39316
	loss_value: 0.66199
	loss_reward: 0.04043
[2025-05-11 13:40:30] nn step 17550, lr: 0.1.
	loss_policy_0: 0.18776
	accuracy_policy_0: 0.87113
	loss_value_0: 0.30565
	loss_policy_1: 0.0377
	accuracy_policy_1: 0.86938
	loss_value_1: 0.06237
	loss_reward_1: 0.00494
	loss_policy_2: 0.03741
	accuracy_policy_2: 0.87402
	loss_value_2: 0.06413
	loss_reward_2: 0.00603
	loss_policy_3: 0.03725
	accuracy_policy_3: 0.87738
	loss_value_3: 0.06582
	loss_reward_3: 0.008
	loss_policy_4: 0.03737
	accuracy_policy_4: 0.87785
	loss_value_4: 0.06783
	loss_reward_4: 0.00994
	loss_policy_5: 0.03761
	accuracy_policy_5: 0.88031
	loss_value_5: 0.06979
	loss_reward_5: 0.01106
	loss_policy: 0.37509
	loss_value: 0.63559
	loss_reward: 0.03996
[2025-05-11 13:40:39] nn step 17600, lr: 0.1.
	loss_policy_0: 0.17879
	accuracy_policy_0: 0.87531
	loss_value_0: 0.28628
	loss_policy_1: 0.03596
	accuracy_policy_1: 0.87477
	loss_value_1: 0.05805
	loss_reward_1: 0.0045
	loss_policy_2: 0.03595
	accuracy_policy_2: 0.87543
	loss_value_2: 0.05974
	loss_reward_2: 0.00541
	loss_policy_3: 0.03579
	accuracy_policy_3: 0.87855
	loss_value_3: 0.06166
	loss_reward_3: 0.00689
	loss_policy_4: 0.03564
	accuracy_policy_4: 0.88105
	loss_value_4: 0.06381
	loss_reward_4: 0.00891
	loss_policy_5: 0.03573
	accuracy_policy_5: 0.88086
	loss_value_5: 0.06573
	loss_reward_5: 0.01031
	loss_policy: 0.35785
	loss_value: 0.59528
	loss_reward: 0.03601
Optimization_Done 17600
[2025-05-11 13:41:58] [command] train weight_iter_17600.pkl 70 89
[2025-05-11 13:42:07] nn step 17650, lr: 0.1.
	loss_policy_0: 0.18406
	accuracy_policy_0: 0.87227
	loss_value_0: 0.29759
	loss_policy_1: 0.03674
	accuracy_policy_1: 0.8707
	loss_value_1: 0.06055
	loss_reward_1: 0.00468
	loss_policy_2: 0.03634
	accuracy_policy_2: 0.87383
	loss_value_2: 0.06225
	loss_reward_2: 0.00602
	loss_policy_3: 0.03665
	accuracy_policy_3: 0.87004
	loss_value_3: 0.06402
	loss_reward_3: 0.00739
	loss_policy_4: 0.03651
	accuracy_policy_4: 0.87684
	loss_value_4: 0.06579
	loss_reward_4: 0.00914
	loss_policy_5: 0.03645
	accuracy_policy_5: 0.87652
	loss_value_5: 0.06777
	loss_reward_5: 0.0108
	loss_policy: 0.36675
	loss_value: 0.61796
	loss_reward: 0.03802
[2025-05-11 13:42:14] nn step 17700, lr: 0.1.
	loss_policy_0: 0.18749
	accuracy_policy_0: 0.875
	loss_value_0: 0.30414
	loss_policy_1: 0.03774
	accuracy_policy_1: 0.87211
	loss_value_1: 0.06205
	loss_reward_1: 0.00488
	loss_policy_2: 0.03786
	accuracy_policy_2: 0.87305
	loss_value_2: 0.06375
	loss_reward_2: 0.00589
	loss_policy_3: 0.03812
	accuracy_policy_3: 0.87852
	loss_value_3: 0.06554
	loss_reward_3: 0.00759
	loss_policy_4: 0.03783
	accuracy_policy_4: 0.87895
	loss_value_4: 0.06744
	loss_reward_4: 0.00947
	loss_policy_5: 0.03741
	accuracy_policy_5: 0.88211
	loss_value_5: 0.06943
	loss_reward_5: 0.01094
	loss_policy: 0.37644
	loss_value: 0.63235
	loss_reward: 0.03878
[2025-05-11 13:42:23] nn step 17750, lr: 0.1.
	loss_policy_0: 0.18521
	accuracy_policy_0: 0.87043
	loss_value_0: 0.2987
	loss_policy_1: 0.03721
	accuracy_policy_1: 0.87199
	loss_value_1: 0.06067
	loss_reward_1: 0.00496
	loss_policy_2: 0.03698
	accuracy_policy_2: 0.87617
	loss_value_2: 0.06248
	loss_reward_2: 0.00605
	loss_policy_3: 0.03742
	accuracy_policy_3: 0.87551
	loss_value_3: 0.06434
	loss_reward_3: 0.00753
	loss_policy_4: 0.03718
	accuracy_policy_4: 0.88086
	loss_value_4: 0.06616
	loss_reward_4: 0.00931
	loss_policy_5: 0.03682
	accuracy_policy_5: 0.88004
	loss_value_5: 0.06835
	loss_reward_5: 0.01124
	loss_policy: 0.37082
	loss_value: 0.6207
	loss_reward: 0.03909
[2025-05-11 13:42:31] nn step 17800, lr: 0.1.
	loss_policy_0: 0.17655
	accuracy_policy_0: 0.86969
	loss_value_0: 0.27867
	loss_policy_1: 0.03509
	accuracy_policy_1: 0.87031
	loss_value_1: 0.05677
	loss_reward_1: 0.00452
	loss_policy_2: 0.03471
	accuracy_policy_2: 0.8759
	loss_value_2: 0.05845
	loss_reward_2: 0.00563
	loss_policy_3: 0.03506
	accuracy_policy_3: 0.87938
	loss_value_3: 0.06029
	loss_reward_3: 0.00705
	loss_policy_4: 0.03499
	accuracy_policy_4: 0.88008
	loss_value_4: 0.06242
	loss_reward_4: 0.00902
	loss_policy_5: 0.0347
	accuracy_policy_5: 0.88152
	loss_value_5: 0.06407
	loss_reward_5: 0.01047
	loss_policy: 0.35111
	loss_value: 0.58067
	loss_reward: 0.03669
Optimization_Done 17800
[2025-05-11 13:43:47] [command] train weight_iter_17800.pkl 71 90
[2025-05-11 13:43:56] nn step 17850, lr: 0.1.
	loss_policy_0: 0.18168
	accuracy_policy_0: 0.87754
	loss_value_0: 0.30074
	loss_policy_1: 0.0364
	accuracy_policy_1: 0.87555
	loss_value_1: 0.06114
	loss_reward_1: 0.00479
	loss_policy_2: 0.03644
	accuracy_policy_2: 0.87852
	loss_value_2: 0.06289
	loss_reward_2: 0.00589
	loss_policy_3: 0.03642
	accuracy_policy_3: 0.88059
	loss_value_3: 0.06476
	loss_reward_3: 0.00769
	loss_policy_4: 0.03622
	accuracy_policy_4: 0.88363
	loss_value_4: 0.06659
	loss_reward_4: 0.00951
	loss_policy_5: 0.03617
	accuracy_policy_5: 0.88359
	loss_value_5: 0.06821
	loss_reward_5: 0.0108
	loss_policy: 0.36334
	loss_value: 0.62433
	loss_reward: 0.03868
[2025-05-11 13:44:03] nn step 17900, lr: 0.1.
	loss_policy_0: 0.19331
	accuracy_policy_0: 0.87789
	loss_value_0: 0.31392
	loss_policy_1: 0.0389
	accuracy_policy_1: 0.87371
	loss_value_1: 0.06414
	loss_reward_1: 0.00505
	loss_policy_2: 0.03904
	accuracy_policy_2: 0.87305
	loss_value_2: 0.06601
	loss_reward_2: 0.00621
	loss_policy_3: 0.03918
	accuracy_policy_3: 0.87746
	loss_value_3: 0.06793
	loss_reward_3: 0.00785
	loss_policy_4: 0.03916
	accuracy_policy_4: 0.87922
	loss_value_4: 0.06964
	loss_reward_4: 0.00993
	loss_policy_5: 0.03879
	accuracy_policy_5: 0.88375
	loss_value_5: 0.07192
	loss_reward_5: 0.01151
	loss_policy: 0.38837
	loss_value: 0.65355
	loss_reward: 0.04056
[2025-05-11 13:44:11] nn step 17950, lr: 0.1.
	loss_policy_0: 0.18337
	accuracy_policy_0: 0.87375
	loss_value_0: 0.29676
	loss_policy_1: 0.03663
	accuracy_policy_1: 0.87512
	loss_value_1: 0.06038
	loss_reward_1: 0.00472
	loss_policy_2: 0.03681
	accuracy_policy_2: 0.87453
	loss_value_2: 0.06223
	loss_reward_2: 0.00592
	loss_policy_3: 0.03669
	accuracy_policy_3: 0.87945
	loss_value_3: 0.06429
	loss_reward_3: 0.00751
	loss_policy_4: 0.03637
	accuracy_policy_4: 0.88363
	loss_value_4: 0.0663
	loss_reward_4: 0.00953
	loss_policy_5: 0.03646
	accuracy_policy_5: 0.88648
	loss_value_5: 0.06827
	loss_reward_5: 0.01081
	loss_policy: 0.36634
	loss_value: 0.61823
	loss_reward: 0.0385
[2025-05-11 13:44:20] nn step 18000, lr: 0.1.
	loss_policy_0: 0.18796
	accuracy_policy_0: 0.87473
	loss_value_0: 0.30478
	loss_policy_1: 0.03719
	accuracy_policy_1: 0.87625
	loss_value_1: 0.06165
	loss_reward_1: 0.00486
	loss_policy_2: 0.0373
	accuracy_policy_2: 0.87645
	loss_value_2: 0.06366
	loss_reward_2: 0.00605
	loss_policy_3: 0.03731
	accuracy_policy_3: 0.87879
	loss_value_3: 0.06536
	loss_reward_3: 0.00786
	loss_policy_4: 0.0374
	accuracy_policy_4: 0.88188
	loss_value_4: 0.06735
	loss_reward_4: 0.00959
	loss_policy_5: 0.03724
	accuracy_policy_5: 0.88117
	loss_value_5: 0.06946
	loss_reward_5: 0.01117
	loss_policy: 0.37439
	loss_value: 0.63226
	loss_reward: 0.03954
Optimization_Done 18000
[2025-05-11 13:45:35] [command] train weight_iter_18000.pkl 72 91
[2025-05-11 13:45:44] nn step 18050, lr: 0.1.
	loss_policy_0: 0.18194
	accuracy_policy_0: 0.87469
	loss_value_0: 0.29802
	loss_policy_1: 0.0363
	accuracy_policy_1: 0.87457
	loss_value_1: 0.06061
	loss_reward_1: 0.00487
	loss_policy_2: 0.03664
	accuracy_policy_2: 0.87867
	loss_value_2: 0.06219
	loss_reward_2: 0.00593
	loss_policy_3: 0.03634
	accuracy_policy_3: 0.87859
	loss_value_3: 0.06394
	loss_reward_3: 0.00742
	loss_policy_4: 0.03656
	accuracy_policy_4: 0.88105
	loss_value_4: 0.0656
	loss_reward_4: 0.00936
	loss_policy_5: 0.03614
	accuracy_policy_5: 0.88641
	loss_value_5: 0.06774
	loss_reward_5: 0.0105
	loss_policy: 0.36392
	loss_value: 0.6181
	loss_reward: 0.03807
[2025-05-11 13:45:53] nn step 18100, lr: 0.1.
	loss_policy_0: 0.18234
	accuracy_policy_0: 0.8709
	loss_value_0: 0.29971
	loss_policy_1: 0.03666
	accuracy_policy_1: 0.87395
	loss_value_1: 0.06099
	loss_reward_1: 0.00481
	loss_policy_2: 0.03681
	accuracy_policy_2: 0.87121
	loss_value_2: 0.06234
	loss_reward_2: 0.00581
	loss_policy_3: 0.03648
	accuracy_policy_3: 0.87934
	loss_value_3: 0.06406
	loss_reward_3: 0.00744
	loss_policy_4: 0.03666
	accuracy_policy_4: 0.88105
	loss_value_4: 0.06588
	loss_reward_4: 0.00938
	loss_policy_5: 0.03665
	accuracy_policy_5: 0.88348
	loss_value_5: 0.06829
	loss_reward_5: 0.01085
	loss_policy: 0.36559
	loss_value: 0.62127
	loss_reward: 0.03829
[2025-05-11 13:46:00] nn step 18150, lr: 0.1.
	loss_policy_0: 0.17929
	accuracy_policy_0: 0.87461
	loss_value_0: 0.2903
	loss_policy_1: 0.03596
	accuracy_policy_1: 0.87688
	loss_value_1: 0.05924
	loss_reward_1: 0.0046
	loss_policy_2: 0.03596
	accuracy_policy_2: 0.87562
	loss_value_2: 0.06091
	loss_reward_2: 0.00617
	loss_policy_3: 0.03596
	accuracy_policy_3: 0.88051
	loss_value_3: 0.06287
	loss_reward_3: 0.00725
	loss_policy_4: 0.03584
	accuracy_policy_4: 0.88234
	loss_value_4: 0.0646
	loss_reward_4: 0.00919
	loss_policy_5: 0.03553
	accuracy_policy_5: 0.88559
	loss_value_5: 0.06663
	loss_reward_5: 0.01072
	loss_policy: 0.35853
	loss_value: 0.60455
	loss_reward: 0.03794
[2025-05-11 13:46:09] nn step 18200, lr: 0.1.
	loss_policy_0: 0.17923
	accuracy_policy_0: 0.87508
	loss_value_0: 0.28989
	loss_policy_1: 0.03582
	accuracy_policy_1: 0.87129
	loss_value_1: 0.05927
	loss_reward_1: 0.00467
	loss_policy_2: 0.03597
	accuracy_policy_2: 0.87684
	loss_value_2: 0.0609
	loss_reward_2: 0.00559
	loss_policy_3: 0.03558
	accuracy_policy_3: 0.88109
	loss_value_3: 0.0626
	loss_reward_3: 0.00754
	loss_policy_4: 0.03606
	accuracy_policy_4: 0.87934
	loss_value_4: 0.0645
	loss_reward_4: 0.00933
	loss_policy_5: 0.03562
	accuracy_policy_5: 0.88164
	loss_value_5: 0.06634
	loss_reward_5: 0.01048
	loss_policy: 0.35828
	loss_value: 0.60351
	loss_reward: 0.03761
Optimization_Done 18200
[2025-05-11 13:47:26] [command] train weight_iter_18200.pkl 73 92
[2025-05-11 13:47:35] nn step 18250, lr: 0.1.
	loss_policy_0: 0.19219
	accuracy_policy_0: 0.87238
	loss_value_0: 0.31514
	loss_policy_1: 0.03815
	accuracy_policy_1: 0.8741
	loss_value_1: 0.06416
	loss_reward_1: 0.00501
	loss_policy_2: 0.03821
	accuracy_policy_2: 0.87301
	loss_value_2: 0.06579
	loss_reward_2: 0.00627
	loss_policy_3: 0.03843
	accuracy_policy_3: 0.87883
	loss_value_3: 0.06754
	loss_reward_3: 0.00808
	loss_policy_4: 0.03826
	accuracy_policy_4: 0.8823
	loss_value_4: 0.06948
	loss_reward_4: 0.01009
	loss_policy_5: 0.03817
	accuracy_policy_5: 0.88188
	loss_value_5: 0.07128
	loss_reward_5: 0.01129
	loss_policy: 0.38341
	loss_value: 0.6534
	loss_reward: 0.04074
[2025-05-11 13:47:44] nn step 18300, lr: 0.1.
	loss_policy_0: 0.19533
	accuracy_policy_0: 0.87539
	loss_value_0: 0.31815
	loss_policy_1: 0.03852
	accuracy_policy_1: 0.87723
	loss_value_1: 0.06455
	loss_reward_1: 0.00521
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.87715
	loss_value_2: 0.06619
	loss_reward_2: 0.00643
	loss_policy_3: 0.03862
	accuracy_policy_3: 0.88387
	loss_value_3: 0.06817
	loss_reward_3: 0.00811
	loss_policy_4: 0.03854
	accuracy_policy_4: 0.88148
	loss_value_4: 0.06997
	loss_reward_4: 0.00998
	loss_policy_5: 0.03866
	accuracy_policy_5: 0.8834
	loss_value_5: 0.07214
	loss_reward_5: 0.01193
	loss_policy: 0.38852
	loss_value: 0.65916
	loss_reward: 0.04166
[2025-05-11 13:47:51] nn step 18350, lr: 0.1.
	loss_policy_0: 0.17903
	accuracy_policy_0: 0.87211
	loss_value_0: 0.28759
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.87316
	loss_value_1: 0.05863
	loss_reward_1: 0.00461
	loss_policy_2: 0.03599
	accuracy_policy_2: 0.87398
	loss_value_2: 0.06036
	loss_reward_2: 0.00574
	loss_policy_3: 0.03566
	accuracy_policy_3: 0.88039
	loss_value_3: 0.06237
	loss_reward_3: 0.00701
	loss_policy_4: 0.03564
	accuracy_policy_4: 0.88164
	loss_value_4: 0.06407
	loss_reward_4: 0.00904
	loss_policy_5: 0.03609
	accuracy_policy_5: 0.88121
	loss_value_5: 0.06614
	loss_reward_5: 0.01086
	loss_policy: 0.35846
	loss_value: 0.59917
	loss_reward: 0.03726
[2025-05-11 13:47:59] nn step 18400, lr: 0.1.
	loss_policy_0: 0.19181
	accuracy_policy_0: 0.87383
	loss_value_0: 0.30855
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.87922
	loss_value_1: 0.06273
	loss_reward_1: 0.00501
	loss_policy_2: 0.03787
	accuracy_policy_2: 0.88184
	loss_value_2: 0.06437
	loss_reward_2: 0.00645
	loss_policy_3: 0.03789
	accuracy_policy_3: 0.88531
	loss_value_3: 0.06631
	loss_reward_3: 0.00785
	loss_policy_4: 0.03798
	accuracy_policy_4: 0.88195
	loss_value_4: 0.06836
	loss_reward_4: 0.00947
	loss_policy_5: 0.03783
	accuracy_policy_5: 0.88762
	loss_value_5: 0.07042
	loss_reward_5: 0.01147
	loss_policy: 0.38122
	loss_value: 0.64074
	loss_reward: 0.04023
Optimization_Done 18400
[2025-05-11 13:49:16] [command] train weight_iter_18400.pkl 74 93
[2025-05-11 13:49:25] nn step 18450, lr: 0.1.
	loss_policy_0: 0.18636
	accuracy_policy_0: 0.87672
	loss_value_0: 0.29968
	loss_policy_1: 0.0371
	accuracy_policy_1: 0.87715
	loss_value_1: 0.0613
	loss_reward_1: 0.00486
	loss_policy_2: 0.03712
	accuracy_policy_2: 0.87887
	loss_value_2: 0.06271
	loss_reward_2: 0.00605
	loss_policy_3: 0.03735
	accuracy_policy_3: 0.87957
	loss_value_3: 0.0647
	loss_reward_3: 0.00783
	loss_policy_4: 0.03701
	accuracy_policy_4: 0.88527
	loss_value_4: 0.06661
	loss_reward_4: 0.00975
	loss_policy_5: 0.03728
	accuracy_policy_5: 0.88602
	loss_value_5: 0.06864
	loss_reward_5: 0.01096
	loss_policy: 0.37223
	loss_value: 0.62365
	loss_reward: 0.03946
[2025-05-11 13:49:34] nn step 18500, lr: 0.1.
	loss_policy_0: 0.18014
	accuracy_policy_0: 0.87578
	loss_value_0: 0.28629
	loss_policy_1: 0.03593
	accuracy_policy_1: 0.8734
	loss_value_1: 0.05842
	loss_reward_1: 0.00476
	loss_policy_2: 0.03578
	accuracy_policy_2: 0.87887
	loss_value_2: 0.06021
	loss_reward_2: 0.00588
	loss_policy_3: 0.03582
	accuracy_policy_3: 0.87793
	loss_value_3: 0.06198
	loss_reward_3: 0.00716
	loss_policy_4: 0.03565
	accuracy_policy_4: 0.88207
	loss_value_4: 0.06415
	loss_reward_4: 0.00912
	loss_policy_5: 0.03553
	accuracy_policy_5: 0.88957
	loss_value_5: 0.0663
	loss_reward_5: 0.01086
	loss_policy: 0.35885
	loss_value: 0.59734
	loss_reward: 0.03778
[2025-05-11 13:49:42] nn step 18550, lr: 0.1.
	loss_policy_0: 0.19384
	accuracy_policy_0: 0.87602
	loss_value_0: 0.31538
	loss_policy_1: 0.03898
	accuracy_policy_1: 0.87531
	loss_value_1: 0.06427
	loss_reward_1: 0.0052
	loss_policy_2: 0.03913
	accuracy_policy_2: 0.88199
	loss_value_2: 0.06628
	loss_reward_2: 0.00635
	loss_policy_3: 0.03848
	accuracy_policy_3: 0.88352
	loss_value_3: 0.06795
	loss_reward_3: 0.00788
	loss_policy_4: 0.03863
	accuracy_policy_4: 0.8852
	loss_value_4: 0.07013
	loss_reward_4: 0.0101
	loss_policy_5: 0.03869
	accuracy_policy_5: 0.88703
	loss_value_5: 0.07231
	loss_reward_5: 0.0115
	loss_policy: 0.38775
	loss_value: 0.65633
	loss_reward: 0.04102
[2025-05-11 13:49:50] nn step 18600, lr: 0.1.
	loss_policy_0: 0.17492
	accuracy_policy_0: 0.87727
	loss_value_0: 0.28028
	loss_policy_1: 0.03523
	accuracy_policy_1: 0.87531
	loss_value_1: 0.05699
	loss_reward_1: 0.00435
	loss_policy_2: 0.03503
	accuracy_policy_2: 0.87836
	loss_value_2: 0.05859
	loss_reward_2: 0.00541
	loss_policy_3: 0.03501
	accuracy_policy_3: 0.8832
	loss_value_3: 0.06056
	loss_reward_3: 0.00697
	loss_policy_4: 0.03492
	accuracy_policy_4: 0.88609
	loss_value_4: 0.06224
	loss_reward_4: 0.00856
	loss_policy_5: 0.03479
	accuracy_policy_5: 0.89004
	loss_value_5: 0.06453
	loss_reward_5: 0.00999
	loss_policy: 0.3499
	loss_value: 0.58319
	loss_reward: 0.03529
Optimization_Done 18600
[2025-05-11 13:51:07] [command] train weight_iter_18600.pkl 75 94
[2025-05-11 13:51:15] nn step 18650, lr: 0.1.
	loss_policy_0: 0.17425
	accuracy_policy_0: 0.87523
	loss_value_0: 0.28513
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.87406
	loss_value_1: 0.05776
	loss_reward_1: 0.0047
	loss_policy_2: 0.03521
	accuracy_policy_2: 0.87648
	loss_value_2: 0.05957
	loss_reward_2: 0.00545
	loss_policy_3: 0.035
	accuracy_policy_3: 0.87973
	loss_value_3: 0.06149
	loss_reward_3: 0.00704
	loss_policy_4: 0.03514
	accuracy_policy_4: 0.88316
	loss_value_4: 0.06309
	loss_reward_4: 0.0091
	loss_policy_5: 0.03501
	accuracy_policy_5: 0.88391
	loss_value_5: 0.06491
	loss_reward_5: 0.01047
	loss_policy: 0.34992
	loss_value: 0.59194
	loss_reward: 0.03677
[2025-05-11 13:51:23] nn step 18700, lr: 0.1.
	loss_policy_0: 0.19468
	accuracy_policy_0: 0.87727
	loss_value_0: 0.31151
	loss_policy_1: 0.03905
	accuracy_policy_1: 0.87316
	loss_value_1: 0.06314
	loss_reward_1: 0.00506
	loss_policy_2: 0.03867
	accuracy_policy_2: 0.87918
	loss_value_2: 0.06501
	loss_reward_2: 0.0061
	loss_policy_3: 0.03879
	accuracy_policy_3: 0.8809
	loss_value_3: 0.06664
	loss_reward_3: 0.00773
	loss_policy_4: 0.03849
	accuracy_policy_4: 0.88508
	loss_value_4: 0.06863
	loss_reward_4: 0.00981
	loss_policy_5: 0.03872
	accuracy_policy_5: 0.88484
	loss_value_5: 0.07086
	loss_reward_5: 0.01166
	loss_policy: 0.38841
	loss_value: 0.64578
	loss_reward: 0.04035
[2025-05-11 13:51:32] nn step 18750, lr: 0.1.
	loss_policy_0: 0.18788
	accuracy_policy_0: 0.87727
	loss_value_0: 0.29807
	loss_policy_1: 0.03753
	accuracy_policy_1: 0.87691
	loss_value_1: 0.06067
	loss_reward_1: 0.00486
	loss_policy_2: 0.03749
	accuracy_policy_2: 0.87449
	loss_value_2: 0.06243
	loss_reward_2: 0.00613
	loss_policy_3: 0.03767
	accuracy_policy_3: 0.8782
	loss_value_3: 0.06434
	loss_reward_3: 0.00736
	loss_policy_4: 0.03744
	accuracy_policy_4: 0.88164
	loss_value_4: 0.06618
	loss_reward_4: 0.0094
	loss_policy_5: 0.03729
	accuracy_policy_5: 0.88441
	loss_value_5: 0.06832
	loss_reward_5: 0.01108
	loss_policy: 0.3753
	loss_value: 0.62001
	loss_reward: 0.03883
[2025-05-11 13:51:39] nn step 18800, lr: 0.1.
	loss_policy_0: 0.18462
	accuracy_policy_0: 0.87855
	loss_value_0: 0.29305
	loss_policy_1: 0.03677
	accuracy_policy_1: 0.87902
	loss_value_1: 0.05969
	loss_reward_1: 0.00502
	loss_policy_2: 0.03668
	accuracy_policy_2: 0.88125
	loss_value_2: 0.061
	loss_reward_2: 0.00604
	loss_policy_3: 0.03682
	accuracy_policy_3: 0.87953
	loss_value_3: 0.06298
	loss_reward_3: 0.0073
	loss_policy_4: 0.03675
	accuracy_policy_4: 0.8852
	loss_value_4: 0.06535
	loss_reward_4: 0.00925
	loss_policy_5: 0.03679
	accuracy_policy_5: 0.88539
	loss_value_5: 0.06715
	loss_reward_5: 0.01089
	loss_policy: 0.36843
	loss_value: 0.60921
	loss_reward: 0.0385
Optimization_Done 18800
[2025-05-11 13:52:56] [command] train weight_iter_18800.pkl 76 95
[2025-05-11 13:53:04] nn step 18850, lr: 0.1.
	loss_policy_0: 0.18546
	accuracy_policy_0: 0.87352
	loss_value_0: 0.30311
	loss_policy_1: 0.03746
	accuracy_policy_1: 0.8748
	loss_value_1: 0.0619
	loss_reward_1: 0.00488
	loss_policy_2: 0.03718
	accuracy_policy_2: 0.87656
	loss_value_2: 0.06385
	loss_reward_2: 0.00616
	loss_policy_3: 0.03726
	accuracy_policy_3: 0.88129
	loss_value_3: 0.06563
	loss_reward_3: 0.00769
	loss_policy_4: 0.03746
	accuracy_policy_4: 0.88445
	loss_value_4: 0.06737
	loss_reward_4: 0.00957
	loss_policy_5: 0.03716
	accuracy_policy_5: 0.88406
	loss_value_5: 0.06918
	loss_reward_5: 0.01116
	loss_policy: 0.37198
	loss_value: 0.63104
	loss_reward: 0.03947
[2025-05-11 13:53:12] nn step 18900, lr: 0.1.
	loss_policy_0: 0.1878
	accuracy_policy_0: 0.87547
	loss_value_0: 0.29754
	loss_policy_1: 0.03715
	accuracy_policy_1: 0.87812
	loss_value_1: 0.06053
	loss_reward_1: 0.00485
	loss_policy_2: 0.03708
	accuracy_policy_2: 0.87973
	loss_value_2: 0.06237
	loss_reward_2: 0.00595
	loss_policy_3: 0.03722
	accuracy_policy_3: 0.87863
	loss_value_3: 0.06394
	loss_reward_3: 0.00738
	loss_policy_4: 0.03742
	accuracy_policy_4: 0.88309
	loss_value_4: 0.06586
	loss_reward_4: 0.00963
	loss_policy_5: 0.037
	accuracy_policy_5: 0.88613
	loss_value_5: 0.06781
	loss_reward_5: 0.0112
	loss_policy: 0.37366
	loss_value: 0.61804
	loss_reward: 0.039
[2025-05-11 13:53:21] nn step 18950, lr: 0.1.
	loss_policy_0: 0.18082
	accuracy_policy_0: 0.87465
	loss_value_0: 0.28675
	loss_policy_1: 0.03609
	accuracy_policy_1: 0.87895
	loss_value_1: 0.05824
	loss_reward_1: 0.0051
	loss_policy_2: 0.03591
	accuracy_policy_2: 0.88348
	loss_value_2: 0.06
	loss_reward_2: 0.00599
	loss_policy_3: 0.03599
	accuracy_policy_3: 0.88387
	loss_value_3: 0.06143
	loss_reward_3: 0.0073
	loss_policy_4: 0.03601
	accuracy_policy_4: 0.88281
	loss_value_4: 0.06322
	loss_reward_4: 0.00928
	loss_policy_5: 0.03597
	accuracy_policy_5: 0.88352
	loss_value_5: 0.06527
	loss_reward_5: 0.01098
	loss_policy: 0.3608
	loss_value: 0.5949
	loss_reward: 0.03865
[2025-05-11 13:53:29] nn step 19000, lr: 0.1.
	loss_policy_0: 0.18994
	accuracy_policy_0: 0.87855
	loss_value_0: 0.30422
	loss_policy_1: 0.03825
	accuracy_policy_1: 0.87438
	loss_value_1: 0.06192
	loss_reward_1: 0.00498
	loss_policy_2: 0.03797
	accuracy_policy_2: 0.87641
	loss_value_2: 0.06379
	loss_reward_2: 0.00626
	loss_policy_3: 0.03788
	accuracy_policy_3: 0.87887
	loss_value_3: 0.06572
	loss_reward_3: 0.00759
	loss_policy_4: 0.03801
	accuracy_policy_4: 0.88016
	loss_value_4: 0.06756
	loss_reward_4: 0.00961
	loss_policy_5: 0.03803
	accuracy_policy_5: 0.88328
	loss_value_5: 0.06957
	loss_reward_5: 0.01124
	loss_policy: 0.38008
	loss_value: 0.63277
	loss_reward: 0.03968
Optimization_Done 19000
[2025-05-11 13:54:47] [command] train weight_iter_19000.pkl 77 96
[2025-05-11 13:54:56] nn step 19050, lr: 0.1.
	loss_policy_0: 0.1904
	accuracy_policy_0: 0.87535
	loss_value_0: 0.30768
	loss_policy_1: 0.03819
	accuracy_policy_1: 0.87316
	loss_value_1: 0.06294
	loss_reward_1: 0.00508
	loss_policy_2: 0.03813
	accuracy_policy_2: 0.87449
	loss_value_2: 0.06418
	loss_reward_2: 0.00639
	loss_policy_3: 0.03817
	accuracy_policy_3: 0.87879
	loss_value_3: 0.06599
	loss_reward_3: 0.00756
	loss_policy_4: 0.03811
	accuracy_policy_4: 0.88188
	loss_value_4: 0.06776
	loss_reward_4: 0.00984
	loss_policy_5: 0.03799
	accuracy_policy_5: 0.88367
	loss_value_5: 0.06947
	loss_reward_5: 0.01099
	loss_policy: 0.38098
	loss_value: 0.63802
	loss_reward: 0.03986
[2025-05-11 13:55:03] nn step 19100, lr: 0.1.
	loss_policy_0: 0.19391
	accuracy_policy_0: 0.87801
	loss_value_0: 0.31146
	loss_policy_1: 0.03857
	accuracy_policy_1: 0.87723
	loss_value_1: 0.06346
	loss_reward_1: 0.00508
	loss_policy_2: 0.03867
	accuracy_policy_2: 0.87883
	loss_value_2: 0.0647
	loss_reward_2: 0.00656
	loss_policy_3: 0.03849
	accuracy_policy_3: 0.87969
	loss_value_3: 0.06651
	loss_reward_3: 0.00795
	loss_policy_4: 0.03872
	accuracy_policy_4: 0.88117
	loss_value_4: 0.06831
	loss_reward_4: 0.00989
	loss_policy_5: 0.03856
	accuracy_policy_5: 0.88609
	loss_value_5: 0.07045
	loss_reward_5: 0.01136
	loss_policy: 0.38691
	loss_value: 0.6449
	loss_reward: 0.04084
[2025-05-11 13:55:11] nn step 19150, lr: 0.1.
	loss_policy_0: 0.19237
	accuracy_policy_0: 0.88008
	loss_value_0: 0.30563
	loss_policy_1: 0.03829
	accuracy_policy_1: 0.87629
	loss_value_1: 0.06239
	loss_reward_1: 0.00511
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.87852
	loss_value_2: 0.06427
	loss_reward_2: 0.00659
	loss_policy_3: 0.03853
	accuracy_policy_3: 0.88266
	loss_value_3: 0.06599
	loss_reward_3: 0.00778
	loss_policy_4: 0.03806
	accuracy_policy_4: 0.88367
	loss_value_4: 0.06842
	loss_reward_4: 0.00964
	loss_policy_5: 0.03847
	accuracy_policy_5: 0.88746
	loss_value_5: 0.07017
	loss_reward_5: 0.01131
	loss_policy: 0.38401
	loss_value: 0.63687
	loss_reward: 0.04043
[2025-05-11 13:55:20] nn step 19200, lr: 0.1.
	loss_policy_0: 0.18945
	accuracy_policy_0: 0.8818
	loss_value_0: 0.30154
	loss_policy_1: 0.03778
	accuracy_policy_1: 0.88105
	loss_value_1: 0.0614
	loss_reward_1: 0.00505
	loss_policy_2: 0.03794
	accuracy_policy_2: 0.87574
	loss_value_2: 0.06313
	loss_reward_2: 0.00601
	loss_policy_3: 0.03797
	accuracy_policy_3: 0.88293
	loss_value_3: 0.06496
	loss_reward_3: 0.00787
	loss_policy_4: 0.03759
	accuracy_policy_4: 0.88324
	loss_value_4: 0.06687
	loss_reward_4: 0.00997
	loss_policy_5: 0.03777
	accuracy_policy_5: 0.8868
	loss_value_5: 0.06892
	loss_reward_5: 0.01146
	loss_policy: 0.3785
	loss_value: 0.62682
	loss_reward: 0.04036
Optimization_Done 19200
[2025-05-11 13:56:36] [command] train weight_iter_19200.pkl 78 97
[2025-05-11 13:56:46] nn step 19250, lr: 0.1.
	loss_policy_0: 0.17968
	accuracy_policy_0: 0.87852
	loss_value_0: 0.28884
	loss_policy_1: 0.03615
	accuracy_policy_1: 0.87688
	loss_value_1: 0.05899
	loss_reward_1: 0.00493
	loss_policy_2: 0.03606
	accuracy_policy_2: 0.87957
	loss_value_2: 0.06069
	loss_reward_2: 0.00588
	loss_policy_3: 0.03593
	accuracy_policy_3: 0.88012
	loss_value_3: 0.06246
	loss_reward_3: 0.0071
	loss_policy_4: 0.03579
	accuracy_policy_4: 0.88617
	loss_value_4: 0.06427
	loss_reward_4: 0.00939
	loss_policy_5: 0.03597
	accuracy_policy_5: 0.88633
	loss_value_5: 0.06616
	loss_reward_5: 0.01006
	loss_policy: 0.35959
	loss_value: 0.60141
	loss_reward: 0.03736
[2025-05-11 13:56:53] nn step 19300, lr: 0.1.
	loss_policy_0: 0.17648
	accuracy_policy_0: 0.87836
	loss_value_0: 0.27772
	loss_policy_1: 0.03476
	accuracy_policy_1: 0.88027
	loss_value_1: 0.05654
	loss_reward_1: 0.00451
	loss_policy_2: 0.03485
	accuracy_policy_2: 0.88102
	loss_value_2: 0.05796
	loss_reward_2: 0.00567
	loss_policy_3: 0.03516
	accuracy_policy_3: 0.88215
	loss_value_3: 0.05913
	loss_reward_3: 0.00688
	loss_policy_4: 0.03467
	accuracy_policy_4: 0.88785
	loss_value_4: 0.06079
	loss_reward_4: 0.00862
	loss_policy_5: 0.03484
	accuracy_policy_5: 0.88934
	loss_value_5: 0.06289
	loss_reward_5: 0.01026
	loss_policy: 0.35075
	loss_value: 0.57502
	loss_reward: 0.03595
[2025-05-11 13:57:01] nn step 19350, lr: 0.1.
	loss_policy_0: 0.19326
	accuracy_policy_0: 0.87852
	loss_value_0: 0.30836
	loss_policy_1: 0.03887
	accuracy_policy_1: 0.87812
	loss_value_1: 0.0629
	loss_reward_1: 0.00502
	loss_policy_2: 0.03873
	accuracy_policy_2: 0.87973
	loss_value_2: 0.06467
	loss_reward_2: 0.00654
	loss_policy_3: 0.03863
	accuracy_policy_3: 0.88262
	loss_value_3: 0.06633
	loss_reward_3: 0.00763
	loss_policy_4: 0.0386
	accuracy_policy_4: 0.88781
	loss_value_4: 0.06797
	loss_reward_4: 0.01005
	loss_policy_5: 0.03873
	accuracy_policy_5: 0.88797
	loss_value_5: 0.06996
	loss_reward_5: 0.01147
	loss_policy: 0.38683
	loss_value: 0.64018
	loss_reward: 0.0407
[2025-05-11 13:57:10] nn step 19400, lr: 0.1.
	loss_policy_0: 0.18008
	accuracy_policy_0: 0.87715
	loss_value_0: 0.28244
	loss_policy_1: 0.03627
	accuracy_policy_1: 0.87938
	loss_value_1: 0.05733
	loss_reward_1: 0.00465
	loss_policy_2: 0.03599
	accuracy_policy_2: 0.88227
	loss_value_2: 0.05878
	loss_reward_2: 0.0059
	loss_policy_3: 0.03609
	accuracy_policy_3: 0.87918
	loss_value_3: 0.06031
	loss_reward_3: 0.00737
	loss_policy_4: 0.03624
	accuracy_policy_4: 0.88379
	loss_value_4: 0.06232
	loss_reward_4: 0.00843
	loss_policy_5: 0.03567
	accuracy_policy_5: 0.88473
	loss_value_5: 0.06424
	loss_reward_5: 0.01048
	loss_policy: 0.36034
	loss_value: 0.58542
	loss_reward: 0.03684
Optimization_Done 19400
[2025-05-11 13:58:24] [command] train weight_iter_19400.pkl 79 98
[2025-05-11 13:58:33] nn step 19450, lr: 0.1.
	loss_policy_0: 0.17158
	accuracy_policy_0: 0.88129
	loss_value_0: 0.2719
	loss_policy_1: 0.03459
	accuracy_policy_1: 0.87992
	loss_value_1: 0.05542
	loss_reward_1: 0.00455
	loss_policy_2: 0.0345
	accuracy_policy_2: 0.88531
	loss_value_2: 0.0569
	loss_reward_2: 0.00533
	loss_policy_3: 0.03457
	accuracy_policy_3: 0.885
	loss_value_3: 0.05884
	loss_reward_3: 0.00646
	loss_policy_4: 0.03439
	accuracy_policy_4: 0.88547
	loss_value_4: 0.06022
	loss_reward_4: 0.0081
	loss_policy_5: 0.03451
	accuracy_policy_5: 0.88969
	loss_value_5: 0.06199
	loss_reward_5: 0.0098
	loss_policy: 0.34415
	loss_value: 0.56526
	loss_reward: 0.03424
[2025-05-11 13:58:41] nn step 19500, lr: 0.1.
	loss_policy_0: 0.19295
	accuracy_policy_0: 0.87953
	loss_value_0: 0.30194
	loss_policy_1: 0.03862
	accuracy_policy_1: 0.88254
	loss_value_1: 0.06165
	loss_reward_1: 0.00509
	loss_policy_2: 0.03841
	accuracy_policy_2: 0.88367
	loss_value_2: 0.06346
	loss_reward_2: 0.00616
	loss_policy_3: 0.03845
	accuracy_policy_3: 0.88555
	loss_value_3: 0.0652
	loss_reward_3: 0.00772
	loss_policy_4: 0.03852
	accuracy_policy_4: 0.89148
	loss_value_4: 0.06698
	loss_reward_4: 0.00997
	loss_policy_5: 0.03821
	accuracy_policy_5: 0.89438
	loss_value_5: 0.06914
	loss_reward_5: 0.01177
	loss_policy: 0.38516
	loss_value: 0.62837
	loss_reward: 0.0407
[2025-05-11 13:58:48] nn step 19550, lr: 0.1.
	loss_policy_0: 0.18677
	accuracy_policy_0: 0.88555
	loss_value_0: 0.29462
	loss_policy_1: 0.03748
	accuracy_policy_1: 0.88031
	loss_value_1: 0.06015
	loss_reward_1: 0.00488
	loss_policy_2: 0.03695
	accuracy_policy_2: 0.88402
	loss_value_2: 0.06187
	loss_reward_2: 0.00584
	loss_policy_3: 0.03734
	accuracy_policy_3: 0.88453
	loss_value_3: 0.06372
	loss_reward_3: 0.00731
	loss_policy_4: 0.03748
	accuracy_policy_4: 0.88809
	loss_value_4: 0.06563
	loss_reward_4: 0.00933
	loss_policy_5: 0.03715
	accuracy_policy_5: 0.89098
	loss_value_5: 0.06748
	loss_reward_5: 0.01064
	loss_policy: 0.37318
	loss_value: 0.61347
	loss_reward: 0.03801
[2025-05-11 13:58:57] nn step 19600, lr: 0.1.
	loss_policy_0: 0.17875
	accuracy_policy_0: 0.88141
	loss_value_0: 0.28047
	loss_policy_1: 0.03536
	accuracy_policy_1: 0.88004
	loss_value_1: 0.05698
	loss_reward_1: 0.00462
	loss_policy_2: 0.03527
	accuracy_policy_2: 0.88562
	loss_value_2: 0.05845
	loss_reward_2: 0.0057
	loss_policy_3: 0.03528
	accuracy_policy_3: 0.88539
	loss_value_3: 0.06004
	loss_reward_3: 0.00688
	loss_policy_4: 0.03522
	accuracy_policy_4: 0.88562
	loss_value_4: 0.06161
	loss_reward_4: 0.00879
	loss_policy_5: 0.03506
	accuracy_policy_5: 0.89
	loss_value_5: 0.06326
	loss_reward_5: 0.01
	loss_policy: 0.35495
	loss_value: 0.58081
	loss_reward: 0.03598
Optimization_Done 19600
[2025-05-11 14:00:13] [command] train weight_iter_19600.pkl 80 99
[2025-05-11 14:00:22] nn step 19650, lr: 0.1.
	loss_policy_0: 0.19353
	accuracy_policy_0: 0.88613
	loss_value_0: 0.30819
	loss_policy_1: 0.03841
	accuracy_policy_1: 0.88227
	loss_value_1: 0.06226
	loss_reward_1: 0.00501
	loss_policy_2: 0.03841
	accuracy_policy_2: 0.88566
	loss_value_2: 0.06414
	loss_reward_2: 0.00602
	loss_policy_3: 0.03818
	accuracy_policy_3: 0.88938
	loss_value_3: 0.06533
	loss_reward_3: 0.00777
	loss_policy_4: 0.03813
	accuracy_policy_4: 0.89293
	loss_value_4: 0.06748
	loss_reward_4: 0.00921
	loss_policy_5: 0.03827
	accuracy_policy_5: 0.89246
	loss_value_5: 0.0692
	loss_reward_5: 0.01061
	loss_policy: 0.38493
	loss_value: 0.6366
	loss_reward: 0.03862
[2025-05-11 14:00:30] nn step 19700, lr: 0.1.
	loss_policy_0: 0.17928
	accuracy_policy_0: 0.8775
	loss_value_0: 0.28218
	loss_policy_1: 0.03574
	accuracy_policy_1: 0.88062
	loss_value_1: 0.05733
	loss_reward_1: 0.00451
	loss_policy_2: 0.03567
	accuracy_policy_2: 0.88406
	loss_value_2: 0.05889
	loss_reward_2: 0.0061
	loss_policy_3: 0.03561
	accuracy_policy_3: 0.88414
	loss_value_3: 0.06036
	loss_reward_3: 0.00735
	loss_policy_4: 0.03559
	accuracy_policy_4: 0.88945
	loss_value_4: 0.06221
	loss_reward_4: 0.00867
	loss_policy_5: 0.03517
	accuracy_policy_5: 0.88914
	loss_value_5: 0.06411
	loss_reward_5: 0.01034
	loss_policy: 0.35706
	loss_value: 0.58508
	loss_reward: 0.03697
[2025-05-11 14:00:37] nn step 19750, lr: 0.1.
	loss_policy_0: 0.17805
	accuracy_policy_0: 0.88555
	loss_value_0: 0.27935
	loss_policy_1: 0.0359
	accuracy_policy_1: 0.88082
	loss_value_1: 0.05678
	loss_reward_1: 0.00467
	loss_policy_2: 0.03561
	accuracy_policy_2: 0.88285
	loss_value_2: 0.05838
	loss_reward_2: 0.00559
	loss_policy_3: 0.03555
	accuracy_policy_3: 0.88754
	loss_value_3: 0.05988
	loss_reward_3: 0.00696
	loss_policy_4: 0.03553
	accuracy_policy_4: 0.88797
	loss_value_4: 0.06172
	loss_reward_4: 0.00886
	loss_policy_5: 0.03538
	accuracy_policy_5: 0.8925
	loss_value_5: 0.06387
	loss_reward_5: 0.01003
	loss_policy: 0.35602
	loss_value: 0.57998
	loss_reward: 0.03612
[2025-05-11 14:00:46] nn step 19800, lr: 0.1.
	loss_policy_0: 0.19013
	accuracy_policy_0: 0.8818
	loss_value_0: 0.29837
	loss_policy_1: 0.03797
	accuracy_policy_1: 0.88191
	loss_value_1: 0.06058
	loss_reward_1: 0.0051
	loss_policy_2: 0.0381
	accuracy_policy_2: 0.88625
	loss_value_2: 0.062
	loss_reward_2: 0.00634
	loss_policy_3: 0.03787
	accuracy_policy_3: 0.88496
	loss_value_3: 0.06393
	loss_reward_3: 0.00754
	loss_policy_4: 0.03825
	accuracy_policy_4: 0.88754
	loss_value_4: 0.06587
	loss_reward_4: 0.00913
	loss_policy_5: 0.03804
	accuracy_policy_5: 0.89004
	loss_value_5: 0.06802
	loss_reward_5: 0.01069
	loss_policy: 0.38036
	loss_value: 0.61878
	loss_reward: 0.03881
Optimization_Done 19800
[2025-05-11 14:02:04] [command] train weight_iter_19800.pkl 81 100
[2025-05-11 14:02:13] nn step 19850, lr: 0.1.
	loss_policy_0: 0.18569
	accuracy_policy_0: 0.89137
	loss_value_0: 0.29503
	loss_policy_1: 0.03704
	accuracy_policy_1: 0.88645
	loss_value_1: 0.05994
	loss_reward_1: 0.00491
	loss_policy_2: 0.03731
	accuracy_policy_2: 0.89027
	loss_value_2: 0.06185
	loss_reward_2: 0.00603
	loss_policy_3: 0.03698
	accuracy_policy_3: 0.89492
	loss_value_3: 0.06347
	loss_reward_3: 0.00735
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.89406
	loss_value_4: 0.06529
	loss_reward_4: 0.00902
	loss_policy_5: 0.03665
	accuracy_policy_5: 0.89727
	loss_value_5: 0.06704
	loss_reward_5: 0.01089
	loss_policy: 0.37049
	loss_value: 0.61261
	loss_reward: 0.03821
[2025-05-11 14:02:22] nn step 19900, lr: 0.1.
	loss_policy_0: 0.17754
	accuracy_policy_0: 0.88719
	loss_value_0: 0.28418
	loss_policy_1: 0.03577
	accuracy_policy_1: 0.88242
	loss_value_1: 0.0577
	loss_reward_1: 0.00481
	loss_policy_2: 0.03585
	accuracy_policy_2: 0.88461
	loss_value_2: 0.05909
	loss_reward_2: 0.0058
	loss_policy_3: 0.03604
	accuracy_policy_3: 0.88625
	loss_value_3: 0.06068
	loss_reward_3: 0.0071
	loss_policy_4: 0.03577
	accuracy_policy_4: 0.8916
	loss_value_4: 0.06249
	loss_reward_4: 0.00862
	loss_policy_5: 0.03563
	accuracy_policy_5: 0.8941
	loss_value_5: 0.06407
	loss_reward_5: 0.01034
	loss_policy: 0.3566
	loss_value: 0.58822
	loss_reward: 0.03667
[2025-05-11 14:02:30] nn step 19950, lr: 0.1.
	loss_policy_0: 0.17679
	accuracy_policy_0: 0.88859
	loss_value_0: 0.27904
	loss_policy_1: 0.03514
	accuracy_policy_1: 0.88707
	loss_value_1: 0.05685
	loss_reward_1: 0.00476
	loss_policy_2: 0.03524
	accuracy_policy_2: 0.88738
	loss_value_2: 0.05845
	loss_reward_2: 0.00597
	loss_policy_3: 0.03495
	accuracy_policy_3: 0.89312
	loss_value_3: 0.05987
	loss_reward_3: 0.00692
	loss_policy_4: 0.03545
	accuracy_policy_4: 0.89281
	loss_value_4: 0.06175
	loss_reward_4: 0.00838
	loss_policy_5: 0.03526
	accuracy_policy_5: 0.89402
	loss_value_5: 0.06335
	loss_reward_5: 0.0104
	loss_policy: 0.35283
	loss_value: 0.57931
	loss_reward: 0.03642
[2025-05-11 14:02:37] nn step 20000, lr: 0.1.
	loss_policy_0: 0.19524
	accuracy_policy_0: 0.89098
	loss_value_0: 0.30318
	loss_policy_1: 0.03913
	accuracy_policy_1: 0.88867
	loss_value_1: 0.06161
	loss_reward_1: 0.0052
	loss_policy_2: 0.03865
	accuracy_policy_2: 0.89059
	loss_value_2: 0.06364
	loss_reward_2: 0.00639
	loss_policy_3: 0.03909
	accuracy_policy_3: 0.8902
	loss_value_3: 0.06549
	loss_reward_3: 0.00775
	loss_policy_4: 0.03933
	accuracy_policy_4: 0.89316
	loss_value_4: 0.06712
	loss_reward_4: 0.00951
	loss_policy_5: 0.03855
	accuracy_policy_5: 0.89984
	loss_value_5: 0.06918
	loss_reward_5: 0.0112
	loss_policy: 0.38999
	loss_value: 0.63022
	loss_reward: 0.04006
Optimization_Done 20000
[2025-05-11 14:03:55] [command] train weight_iter_20000.pkl 82 101
[2025-05-11 14:04:02] nn step 20050, lr: 0.1.
	loss_policy_0: 0.19048
	accuracy_policy_0: 0.88633
	loss_value_0: 0.29603
	loss_policy_1: 0.03801
	accuracy_policy_1: 0.88406
	loss_value_1: 0.06025
	loss_reward_1: 0.00503
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.88258
	loss_value_2: 0.06196
	loss_reward_2: 0.00602
	loss_policy_3: 0.0379
	accuracy_policy_3: 0.88852
	loss_value_3: 0.06349
	loss_reward_3: 0.00742
	loss_policy_4: 0.03814
	accuracy_policy_4: 0.8902
	loss_value_4: 0.0651
	loss_reward_4: 0.00925
	loss_policy_5: 0.03767
	accuracy_policy_5: 0.89434
	loss_value_5: 0.06725
	loss_reward_5: 0.01065
	loss_policy: 0.38016
	loss_value: 0.61408
	loss_reward: 0.03837
[2025-05-11 14:04:11] nn step 20100, lr: 0.1.
	loss_policy_0: 0.18163
	accuracy_policy_0: 0.88523
	loss_value_0: 0.28464
	loss_policy_1: 0.03619
	accuracy_policy_1: 0.8859
	loss_value_1: 0.05778
	loss_reward_1: 0.00498
	loss_policy_2: 0.036
	accuracy_policy_2: 0.8868
	loss_value_2: 0.05938
	loss_reward_2: 0.00613
	loss_policy_3: 0.03603
	accuracy_policy_3: 0.88773
	loss_value_3: 0.06085
	loss_reward_3: 0.0073
	loss_policy_4: 0.03612
	accuracy_policy_4: 0.89168
	loss_value_4: 0.06253
	loss_reward_4: 0.00873
	loss_policy_5: 0.03603
	accuracy_policy_5: 0.89348
	loss_value_5: 0.06449
	loss_reward_5: 0.01055
	loss_policy: 0.36201
	loss_value: 0.58967
	loss_reward: 0.03769
[2025-05-11 14:04:20] nn step 20150, lr: 0.1.
	loss_policy_0: 0.17426
	accuracy_policy_0: 0.88727
	loss_value_0: 0.27278
	loss_policy_1: 0.03497
	accuracy_policy_1: 0.88273
	loss_value_1: 0.05554
	loss_reward_1: 0.00476
	loss_policy_2: 0.03474
	accuracy_policy_2: 0.88809
	loss_value_2: 0.05705
	loss_reward_2: 0.00565
	loss_policy_3: 0.03501
	accuracy_policy_3: 0.88855
	loss_value_3: 0.05866
	loss_reward_3: 0.00684
	loss_policy_4: 0.03456
	accuracy_policy_4: 0.89047
	loss_value_4: 0.06037
	loss_reward_4: 0.00843
	loss_policy_5: 0.03476
	accuracy_policy_5: 0.89492
	loss_value_5: 0.0621
	loss_reward_5: 0.00998
	loss_policy: 0.3483
	loss_value: 0.56651
	loss_reward: 0.03565
[2025-05-11 14:04:27] nn step 20200, lr: 0.1.
	loss_policy_0: 0.18892
	accuracy_policy_0: 0.88355
	loss_value_0: 0.29904
	loss_policy_1: 0.03786
	accuracy_policy_1: 0.88527
	loss_value_1: 0.06083
	loss_reward_1: 0.00507
	loss_policy_2: 0.03748
	accuracy_policy_2: 0.88832
	loss_value_2: 0.06234
	loss_reward_2: 0.00643
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.89352
	loss_value_3: 0.06402
	loss_reward_3: 0.00757
	loss_policy_4: 0.03773
	accuracy_policy_4: 0.8902
	loss_value_4: 0.06603
	loss_reward_4: 0.00916
	loss_policy_5: 0.03762
	accuracy_policy_5: 0.89652
	loss_value_5: 0.06754
	loss_reward_5: 0.01102
	loss_policy: 0.37725
	loss_value: 0.6198
	loss_reward: 0.03925
Optimization_Done 20200
[2025-05-11 14:05:43] [command] train weight_iter_20200.pkl 83 102
[2025-05-11 14:05:51] nn step 20250, lr: 0.1.
	loss_policy_0: 0.17174
	accuracy_policy_0: 0.89293
	loss_value_0: 0.27049
	loss_policy_1: 0.03452
	accuracy_policy_1: 0.89176
	loss_value_1: 0.05552
	loss_reward_1: 0.00467
	loss_policy_2: 0.03418
	accuracy_policy_2: 0.89504
	loss_value_2: 0.0572
	loss_reward_2: 0.00561
	loss_policy_3: 0.03436
	accuracy_policy_3: 0.89738
	loss_value_3: 0.05863
	loss_reward_3: 0.00676
	loss_policy_4: 0.03447
	accuracy_policy_4: 0.89676
	loss_value_4: 0.06021
	loss_reward_4: 0.00854
	loss_policy_5: 0.03436
	accuracy_policy_5: 0.90094
	loss_value_5: 0.06186
	loss_reward_5: 0.01031
	loss_policy: 0.34363
	loss_value: 0.56391
	loss_reward: 0.03589
[2025-05-11 14:05:59] nn step 20300, lr: 0.1.
	loss_policy_0: 0.18369
	accuracy_policy_0: 0.89457
	loss_value_0: 0.2887
	loss_policy_1: 0.03671
	accuracy_policy_1: 0.8925
	loss_value_1: 0.05877
	loss_reward_1: 0.00495
	loss_policy_2: 0.03672
	accuracy_policy_2: 0.89336
	loss_value_2: 0.06069
	loss_reward_2: 0.00597
	loss_policy_3: 0.03672
	accuracy_policy_3: 0.89707
	loss_value_3: 0.06198
	loss_reward_3: 0.00682
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.89785
	loss_value_4: 0.06363
	loss_reward_4: 0.00864
	loss_policy_5: 0.03662
	accuracy_policy_5: 0.90121
	loss_value_5: 0.06532
	loss_reward_5: 0.0105
	loss_policy: 0.36728
	loss_value: 0.59911
	loss_reward: 0.03689
[2025-05-11 14:06:08] nn step 20350, lr: 0.1.
	loss_policy_0: 0.1738
	accuracy_policy_0: 0.89199
	loss_value_0: 0.26705
	loss_policy_1: 0.03463
	accuracy_policy_1: 0.89312
	loss_value_1: 0.0545
	loss_reward_1: 0.00458
	loss_policy_2: 0.03435
	accuracy_policy_2: 0.8923
	loss_value_2: 0.05617
	loss_reward_2: 0.00552
	loss_policy_3: 0.03446
	accuracy_policy_3: 0.89133
	loss_value_3: 0.05751
	loss_reward_3: 0.00673
	loss_policy_4: 0.03429
	accuracy_policy_4: 0.89402
	loss_value_4: 0.05899
	loss_reward_4: 0.00832
	loss_policy_5: 0.0344
	accuracy_policy_5: 0.9
	loss_value_5: 0.06065
	loss_reward_5: 0.00987
	loss_policy: 0.34593
	loss_value: 0.55487
	loss_reward: 0.03503
[2025-05-11 14:06:16] nn step 20400, lr: 0.1.
	loss_policy_0: 0.18098
	accuracy_policy_0: 0.89332
	loss_value_0: 0.2825
	loss_policy_1: 0.03683
	accuracy_policy_1: 0.89367
	loss_value_1: 0.05765
	loss_reward_1: 0.0053
	loss_policy_2: 0.03661
	accuracy_policy_2: 0.89293
	loss_value_2: 0.05921
	loss_reward_2: 0.0061
	loss_policy_3: 0.03656
	accuracy_policy_3: 0.89305
	loss_value_3: 0.06074
	loss_reward_3: 0.00706
	loss_policy_4: 0.03634
	accuracy_policy_4: 0.89547
	loss_value_4: 0.06237
	loss_reward_4: 0.00884
	loss_policy_5: 0.03619
	accuracy_policy_5: 0.90211
	loss_value_5: 0.06387
	loss_reward_5: 0.01064
	loss_policy: 0.3635
	loss_value: 0.58633
	loss_reward: 0.03793
Optimization_Done 20400
[2025-05-11 14:07:32] [command] train weight_iter_20400.pkl 84 103
[2025-05-11 14:07:41] nn step 20450, lr: 0.1.
	loss_policy_0: 0.17404
	accuracy_policy_0: 0.88957
	loss_value_0: 0.27377
	loss_policy_1: 0.03493
	accuracy_policy_1: 0.8877
	loss_value_1: 0.05585
	loss_reward_1: 0.00472
	loss_policy_2: 0.03472
	accuracy_policy_2: 0.89359
	loss_value_2: 0.05725
	loss_reward_2: 0.00556
	loss_policy_3: 0.0347
	accuracy_policy_3: 0.89332
	loss_value_3: 0.05866
	loss_reward_3: 0.00687
	loss_policy_4: 0.03481
	accuracy_policy_4: 0.89875
	loss_value_4: 0.06052
	loss_reward_4: 0.00804
	loss_policy_5: 0.03471
	accuracy_policy_5: 0.89938
	loss_value_5: 0.06227
	loss_reward_5: 0.00992
	loss_policy: 0.34791
	loss_value: 0.56832
	loss_reward: 0.03511
[2025-05-11 14:07:48] nn step 20500, lr: 0.1.
	loss_policy_0: 0.17555
	accuracy_policy_0: 0.89777
	loss_value_0: 0.27389
	loss_policy_1: 0.03528
	accuracy_policy_1: 0.89605
	loss_value_1: 0.05565
	loss_reward_1: 0.00468
	loss_policy_2: 0.03506
	accuracy_policy_2: 0.89711
	loss_value_2: 0.05741
	loss_reward_2: 0.00563
	loss_policy_3: 0.03496
	accuracy_policy_3: 0.89836
	loss_value_3: 0.05894
	loss_reward_3: 0.00695
	loss_policy_4: 0.03527
	accuracy_policy_4: 0.89871
	loss_value_4: 0.06053
	loss_reward_4: 0.00823
	loss_policy_5: 0.03485
	accuracy_policy_5: 0.90297
	loss_value_5: 0.06213
	loss_reward_5: 0.00949
	loss_policy: 0.35096
	loss_value: 0.56854
	loss_reward: 0.03499
[2025-05-11 14:07:57] nn step 20550, lr: 0.1.
	loss_policy_0: 0.17445
	accuracy_policy_0: 0.89676
	loss_value_0: 0.27276
	loss_policy_1: 0.03464
	accuracy_policy_1: 0.8948
	loss_value_1: 0.05609
	loss_reward_1: 0.00467
	loss_policy_2: 0.03474
	accuracy_policy_2: 0.89398
	loss_value_2: 0.05762
	loss_reward_2: 0.00571
	loss_policy_3: 0.03469
	accuracy_policy_3: 0.89816
	loss_value_3: 0.05886
	loss_reward_3: 0.00676
	loss_policy_4: 0.03471
	accuracy_policy_4: 0.89758
	loss_value_4: 0.06064
	loss_reward_4: 0.00833
	loss_policy_5: 0.03473
	accuracy_policy_5: 0.90199
	loss_value_5: 0.06209
	loss_reward_5: 0.00971
	loss_policy: 0.34796
	loss_value: 0.56806
	loss_reward: 0.03517
[2025-05-11 14:08:06] nn step 20600, lr: 0.1.
	loss_policy_0: 0.17595
	accuracy_policy_0: 0.89391
	loss_value_0: 0.27305
	loss_policy_1: 0.03467
	accuracy_policy_1: 0.89652
	loss_value_1: 0.05589
	loss_reward_1: 0.00468
	loss_policy_2: 0.03454
	accuracy_policy_2: 0.89824
	loss_value_2: 0.05772
	loss_reward_2: 0.00553
	loss_policy_3: 0.03456
	accuracy_policy_3: 0.89988
	loss_value_3: 0.05916
	loss_reward_3: 0.00687
	loss_policy_4: 0.03444
	accuracy_policy_4: 0.90309
	loss_value_4: 0.0607
	loss_reward_4: 0.00844
	loss_policy_5: 0.0347
	accuracy_policy_5: 0.90422
	loss_value_5: 0.06252
	loss_reward_5: 0.01
	loss_policy: 0.34888
	loss_value: 0.56905
	loss_reward: 0.03552
Optimization_Done 20600
[2025-05-11 14:09:22] [command] train weight_iter_20600.pkl 85 104
[2025-05-11 14:09:31] nn step 20650, lr: 0.1.
	loss_policy_0: 0.168
	accuracy_policy_0: 0.89836
	loss_value_0: 0.27038
	loss_policy_1: 0.03359
	accuracy_policy_1: 0.89672
	loss_value_1: 0.05491
	loss_reward_1: 0.00464
	loss_policy_2: 0.03352
	accuracy_policy_2: 0.8948
	loss_value_2: 0.0561
	loss_reward_2: 0.00534
	loss_policy_3: 0.03356
	accuracy_policy_3: 0.90055
	loss_value_3: 0.0578
	loss_reward_3: 0.00617
	loss_policy_4: 0.03335
	accuracy_policy_4: 0.90016
	loss_value_4: 0.05928
	loss_reward_4: 0.00785
	loss_policy_5: 0.03363
	accuracy_policy_5: 0.90305
	loss_value_5: 0.06075
	loss_reward_5: 0.00945
	loss_policy: 0.33565
	loss_value: 0.55922
	loss_reward: 0.03345
[2025-05-11 14:09:39] nn step 20700, lr: 0.1.
	loss_policy_0: 0.17152
	accuracy_policy_0: 0.89695
	loss_value_0: 0.26973
	loss_policy_1: 0.03414
	accuracy_policy_1: 0.89785
	loss_value_1: 0.05511
	loss_reward_1: 0.00472
	loss_policy_2: 0.03415
	accuracy_policy_2: 0.89496
	loss_value_2: 0.05643
	loss_reward_2: 0.00535
	loss_policy_3: 0.03404
	accuracy_policy_3: 0.90066
	loss_value_3: 0.05807
	loss_reward_3: 0.00637
	loss_policy_4: 0.03432
	accuracy_policy_4: 0.90242
	loss_value_4: 0.05954
	loss_reward_4: 0.00791
	loss_policy_5: 0.03399
	accuracy_policy_5: 0.90543
	loss_value_5: 0.06105
	loss_reward_5: 0.00966
	loss_policy: 0.34215
	loss_value: 0.55992
	loss_reward: 0.03402
[2025-05-11 14:09:46] nn step 20750, lr: 0.1.
	loss_policy_0: 0.16664
	accuracy_policy_0: 0.89672
	loss_value_0: 0.26527
	loss_policy_1: 0.03353
	accuracy_policy_1: 0.90207
	loss_value_1: 0.05406
	loss_reward_1: 0.0047
	loss_policy_2: 0.03341
	accuracy_policy_2: 0.90117
	loss_value_2: 0.05542
	loss_reward_2: 0.00548
	loss_policy_3: 0.03347
	accuracy_policy_3: 0.90383
	loss_value_3: 0.05718
	loss_reward_3: 0.00671
	loss_policy_4: 0.0335
	accuracy_policy_4: 0.90055
	loss_value_4: 0.05883
	loss_reward_4: 0.00837
	loss_policy_5: 0.03342
	accuracy_policy_5: 0.90355
	loss_value_5: 0.06046
	loss_reward_5: 0.00992
	loss_policy: 0.33398
	loss_value: 0.55122
	loss_reward: 0.03518
[2025-05-11 14:09:55] nn step 20800, lr: 0.1.
	loss_policy_0: 0.1888
	accuracy_policy_0: 0.8952
	loss_value_0: 0.2993
	loss_policy_1: 0.03769
	accuracy_policy_1: 0.89703
	loss_value_1: 0.06065
	loss_reward_1: 0.00526
	loss_policy_2: 0.03742
	accuracy_policy_2: 0.90012
	loss_value_2: 0.06217
	loss_reward_2: 0.00611
	loss_policy_3: 0.03765
	accuracy_policy_3: 0.9
	loss_value_3: 0.064
	loss_reward_3: 0.00758
	loss_policy_4: 0.03775
	accuracy_policy_4: 0.89828
	loss_value_4: 0.06568
	loss_reward_4: 0.00938
	loss_policy_5: 0.0372
	accuracy_policy_5: 0.90504
	loss_value_5: 0.06749
	loss_reward_5: 0.01068
	loss_policy: 0.37651
	loss_value: 0.61928
	loss_reward: 0.039
Optimization_Done 20800
[2025-05-11 14:11:12] [command] train weight_iter_20800.pkl 86 105
[2025-05-11 14:11:21] nn step 20850, lr: 0.1.
	loss_policy_0: 0.16059
	accuracy_policy_0: 0.90082
	loss_value_0: 0.25998
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.89578
	loss_value_1: 0.05299
	loss_reward_1: 0.00446
	loss_policy_2: 0.03218
	accuracy_policy_2: 0.90109
	loss_value_2: 0.05455
	loss_reward_2: 0.00534
	loss_policy_3: 0.03214
	accuracy_policy_3: 0.90297
	loss_value_3: 0.0557
	loss_reward_3: 0.0064
	loss_policy_4: 0.03227
	accuracy_policy_4: 0.90223
	loss_value_4: 0.05742
	loss_reward_4: 0.00766
	loss_policy_5: 0.03199
	accuracy_policy_5: 0.90832
	loss_value_5: 0.05896
	loss_reward_5: 0.0091
	loss_policy: 0.32153
	loss_value: 0.53959
	loss_reward: 0.03295
[2025-05-11 14:11:30] nn step 20900, lr: 0.1.
	loss_policy_0: 0.17077
	accuracy_policy_0: 0.89969
	loss_value_0: 0.27182
	loss_policy_1: 0.03395
	accuracy_policy_1: 0.8993
	loss_value_1: 0.05533
	loss_reward_1: 0.00469
	loss_policy_2: 0.03406
	accuracy_policy_2: 0.90293
	loss_value_2: 0.0568
	loss_reward_2: 0.00565
	loss_policy_3: 0.03383
	accuracy_policy_3: 0.90625
	loss_value_3: 0.05813
	loss_reward_3: 0.00664
	loss_policy_4: 0.03436
	accuracy_policy_4: 0.90309
	loss_value_4: 0.05968
	loss_reward_4: 0.00819
	loss_policy_5: 0.03389
	accuracy_policy_5: 0.90809
	loss_value_5: 0.06146
	loss_reward_5: 0.00987
	loss_policy: 0.34086
	loss_value: 0.56323
	loss_reward: 0.03504
[2025-05-11 14:11:37] nn step 20950, lr: 0.1.
	loss_policy_0: 0.18489
	accuracy_policy_0: 0.90051
	loss_value_0: 0.29022
	loss_policy_1: 0.03699
	accuracy_policy_1: 0.89629
	loss_value_1: 0.05903
	loss_reward_1: 0.00499
	loss_policy_2: 0.03665
	accuracy_policy_2: 0.90125
	loss_value_2: 0.06055
	loss_reward_2: 0.00606
	loss_policy_3: 0.03646
	accuracy_policy_3: 0.90398
	loss_value_3: 0.06236
	loss_reward_3: 0.00724
	loss_policy_4: 0.0367
	accuracy_policy_4: 0.90535
	loss_value_4: 0.06416
	loss_reward_4: 0.0087
	loss_policy_5: 0.03649
	accuracy_policy_5: 0.90867
	loss_value_5: 0.06593
	loss_reward_5: 0.01038
	loss_policy: 0.36817
	loss_value: 0.60225
	loss_reward: 0.03736
[2025-05-11 14:11:45] nn step 21000, lr: 0.1.
	loss_policy_0: 0.17979
	accuracy_policy_0: 0.89961
	loss_value_0: 0.28554
	loss_policy_1: 0.03601
	accuracy_policy_1: 0.89848
	loss_value_1: 0.05803
	loss_reward_1: 0.00499
	loss_policy_2: 0.0358
	accuracy_policy_2: 0.9025
	loss_value_2: 0.05923
	loss_reward_2: 0.00604
	loss_policy_3: 0.036
	accuracy_policy_3: 0.90543
	loss_value_3: 0.06074
	loss_reward_3: 0.00726
	loss_policy_4: 0.03581
	accuracy_policy_4: 0.90586
	loss_value_4: 0.06251
	loss_reward_4: 0.00893
	loss_policy_5: 0.03551
	accuracy_policy_5: 0.90609
	loss_value_5: 0.06452
	loss_reward_5: 0.01088
	loss_policy: 0.35892
	loss_value: 0.59057
	loss_reward: 0.03811
Optimization_Done 21000
[2025-05-11 14:13:03] [command] train weight_iter_21000.pkl 87 106
[2025-05-11 14:13:12] nn step 21050, lr: 0.1.
	loss_policy_0: 0.1729
	accuracy_policy_0: 0.90082
	loss_value_0: 0.27802
	loss_policy_1: 0.0343
	accuracy_policy_1: 0.90449
	loss_value_1: 0.05668
	loss_reward_1: 0.00485
	loss_policy_2: 0.03459
	accuracy_policy_2: 0.90512
	loss_value_2: 0.0581
	loss_reward_2: 0.0058
	loss_policy_3: 0.03448
	accuracy_policy_3: 0.90602
	loss_value_3: 0.05946
	loss_reward_3: 0.00688
	loss_policy_4: 0.03441
	accuracy_policy_4: 0.90637
	loss_value_4: 0.06114
	loss_reward_4: 0.00859
	loss_policy_5: 0.03426
	accuracy_policy_5: 0.90629
	loss_value_5: 0.06261
	loss_reward_5: 0.00971
	loss_policy: 0.34494
	loss_value: 0.57601
	loss_reward: 0.03583
[2025-05-11 14:13:21] nn step 21100, lr: 0.1.
	loss_policy_0: 0.16663
	accuracy_policy_0: 0.90551
	loss_value_0: 0.26571
	loss_policy_1: 0.03353
	accuracy_policy_1: 0.90219
	loss_value_1: 0.05401
	loss_reward_1: 0.00459
	loss_policy_2: 0.0335
	accuracy_policy_2: 0.90508
	loss_value_2: 0.05542
	loss_reward_2: 0.00542
	loss_policy_3: 0.03342
	accuracy_policy_3: 0.90215
	loss_value_3: 0.05692
	loss_reward_3: 0.00642
	loss_policy_4: 0.03342
	accuracy_policy_4: 0.90371
	loss_value_4: 0.05839
	loss_reward_4: 0.00793
	loss_policy_5: 0.03358
	accuracy_policy_5: 0.90656
	loss_value_5: 0.06023
	loss_reward_5: 0.00921
	loss_policy: 0.33408
	loss_value: 0.55069
	loss_reward: 0.03357
[2025-05-11 14:13:28] nn step 21150, lr: 0.1.
	loss_policy_0: 0.15593
	accuracy_policy_0: 0.90621
	loss_value_0: 0.24727
	loss_policy_1: 0.03153
	accuracy_policy_1: 0.9016
	loss_value_1: 0.05018
	loss_reward_1: 0.00422
	loss_policy_2: 0.03123
	accuracy_policy_2: 0.90633
	loss_value_2: 0.05145
	loss_reward_2: 0.00497
	loss_policy_3: 0.03153
	accuracy_policy_3: 0.90527
	loss_value_3: 0.05267
	loss_reward_3: 0.00591
	loss_policy_4: 0.03155
	accuracy_policy_4: 0.90555
	loss_value_4: 0.05408
	loss_reward_4: 0.00687
	loss_policy_5: 0.0314
	accuracy_policy_5: 0.90316
	loss_value_5: 0.0558
	loss_reward_5: 0.0084
	loss_policy: 0.31318
	loss_value: 0.51145
	loss_reward: 0.03036
[2025-05-11 14:13:36] nn step 21200, lr: 0.1.
	loss_policy_0: 0.17928
	accuracy_policy_0: 0.90273
	loss_value_0: 0.27862
	loss_policy_1: 0.03579
	accuracy_policy_1: 0.90078
	loss_value_1: 0.05687
	loss_reward_1: 0.00498
	loss_policy_2: 0.03555
	accuracy_policy_2: 0.90406
	loss_value_2: 0.05827
	loss_reward_2: 0.00596
	loss_policy_3: 0.03602
	accuracy_policy_3: 0.90348
	loss_value_3: 0.05975
	loss_reward_3: 0.00699
	loss_policy_4: 0.03578
	accuracy_policy_4: 0.90578
	loss_value_4: 0.06145
	loss_reward_4: 0.00843
	loss_policy_5: 0.03548
	accuracy_policy_5: 0.9066
	loss_value_5: 0.06323
	loss_reward_5: 0.01018
	loss_policy: 0.3579
	loss_value: 0.5782
	loss_reward: 0.03655
Optimization_Done 21200
[2025-05-11 14:14:56] [command] train weight_iter_21200.pkl 88 107
[2025-05-11 14:15:05] nn step 21250, lr: 0.1.
	loss_policy_0: 0.17644
	accuracy_policy_0: 0.8991
	loss_value_0: 0.27984
	loss_policy_1: 0.03517
	accuracy_policy_1: 0.90047
	loss_value_1: 0.05692
	loss_reward_1: 0.00479
	loss_policy_2: 0.03504
	accuracy_policy_2: 0.89762
	loss_value_2: 0.05788
	loss_reward_2: 0.00554
	loss_policy_3: 0.03535
	accuracy_policy_3: 0.90086
	loss_value_3: 0.05903
	loss_reward_3: 0.00659
	loss_policy_4: 0.03544
	accuracy_policy_4: 0.89969
	loss_value_4: 0.06083
	loss_reward_4: 0.00819
	loss_policy_5: 0.03527
	accuracy_policy_5: 0.90152
	loss_value_5: 0.06239
	loss_reward_5: 0.0096
	loss_policy: 0.35271
	loss_value: 0.57689
	loss_reward: 0.03472
[2025-05-11 14:15:14] nn step 21300, lr: 0.1.
	loss_policy_0: 0.16078
	accuracy_policy_0: 0.90547
	loss_value_0: 0.24933
	loss_policy_1: 0.03231
	accuracy_policy_1: 0.90078
	loss_value_1: 0.05131
	loss_reward_1: 0.00436
	loss_policy_2: 0.0322
	accuracy_policy_2: 0.9018
	loss_value_2: 0.05252
	loss_reward_2: 0.00535
	loss_policy_3: 0.03232
	accuracy_policy_3: 0.90562
	loss_value_3: 0.05369
	loss_reward_3: 0.00608
	loss_policy_4: 0.03242
	accuracy_policy_4: 0.90504
	loss_value_4: 0.05519
	loss_reward_4: 0.0073
	loss_policy_5: 0.03232
	accuracy_policy_5: 0.9093
	loss_value_5: 0.05684
	loss_reward_5: 0.00893
	loss_policy: 0.32235
	loss_value: 0.51887
	loss_reward: 0.03202
[2025-05-11 14:15:21] nn step 21350, lr: 0.1.
	loss_policy_0: 0.15997
	accuracy_policy_0: 0.90312
	loss_value_0: 0.24796
	loss_policy_1: 0.0322
	accuracy_policy_1: 0.90039
	loss_value_1: 0.05073
	loss_reward_1: 0.00432
	loss_policy_2: 0.03198
	accuracy_policy_2: 0.90125
	loss_value_2: 0.0521
	loss_reward_2: 0.00519
	loss_policy_3: 0.03206
	accuracy_policy_3: 0.90641
	loss_value_3: 0.05306
	loss_reward_3: 0.006
	loss_policy_4: 0.03214
	accuracy_policy_4: 0.90711
	loss_value_4: 0.05459
	loss_reward_4: 0.00726
	loss_policy_5: 0.03202
	accuracy_policy_5: 0.90766
	loss_value_5: 0.05618
	loss_reward_5: 0.00864
	loss_policy: 0.32036
	loss_value: 0.51461
	loss_reward: 0.0314
[2025-05-11 14:15:29] nn step 21400, lr: 0.1.
	loss_policy_0: 0.17912
	accuracy_policy_0: 0.90422
	loss_value_0: 0.27457
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.90219
	loss_value_1: 0.05624
	loss_reward_1: 0.0049
	loss_policy_2: 0.03614
	accuracy_policy_2: 0.90172
	loss_value_2: 0.05777
	loss_reward_2: 0.00568
	loss_policy_3: 0.0362
	accuracy_policy_3: 0.90629
	loss_value_3: 0.05939
	loss_reward_3: 0.00667
	loss_policy_4: 0.03612
	accuracy_policy_4: 0.90617
	loss_value_4: 0.06078
	loss_reward_4: 0.0082
	loss_policy_5: 0.03577
	accuracy_policy_5: 0.90703
	loss_value_5: 0.06256
	loss_reward_5: 0.00991
	loss_policy: 0.35941
	loss_value: 0.57132
	loss_reward: 0.03537
Optimization_Done 21400
[2025-05-11 14:16:45] [command] train weight_iter_21400.pkl 89 108
[2025-05-11 14:16:54] nn step 21450, lr: 0.1.
	loss_policy_0: 0.1604
	accuracy_policy_0: 0.90652
	loss_value_0: 0.25382
	loss_policy_1: 0.03183
	accuracy_policy_1: 0.90691
	loss_value_1: 0.0515
	loss_reward_1: 0.00465
	loss_policy_2: 0.03185
	accuracy_policy_2: 0.90434
	loss_value_2: 0.05257
	loss_reward_2: 0.00521
	loss_policy_3: 0.03192
	accuracy_policy_3: 0.91062
	loss_value_3: 0.05354
	loss_reward_3: 0.00605
	loss_policy_4: 0.03235
	accuracy_policy_4: 0.90719
	loss_value_4: 0.05486
	loss_reward_4: 0.00736
	loss_policy_5: 0.03207
	accuracy_policy_5: 0.90625
	loss_value_5: 0.05647
	loss_reward_5: 0.00886
	loss_policy: 0.32042
	loss_value: 0.52275
	loss_reward: 0.03213
[2025-05-11 14:17:03] nn step 21500, lr: 0.1.
	loss_policy_0: 0.16984
	accuracy_policy_0: 0.90281
	loss_value_0: 0.26329
	loss_policy_1: 0.03404
	accuracy_policy_1: 0.9034
	loss_value_1: 0.05334
	loss_reward_1: 0.00469
	loss_policy_2: 0.03361
	accuracy_policy_2: 0.90527
	loss_value_2: 0.05491
	loss_reward_2: 0.00537
	loss_policy_3: 0.03387
	accuracy_policy_3: 0.90445
	loss_value_3: 0.05631
	loss_reward_3: 0.00645
	loss_policy_4: 0.03405
	accuracy_policy_4: 0.90578
	loss_value_4: 0.05779
	loss_reward_4: 0.00773
	loss_policy_5: 0.03407
	accuracy_policy_5: 0.90348
	loss_value_5: 0.05958
	loss_reward_5: 0.0091
	loss_policy: 0.33949
	loss_value: 0.54522
	loss_reward: 0.03333
[2025-05-11 14:17:11] nn step 21550, lr: 0.1.
	loss_policy_0: 0.17073
	accuracy_policy_0: 0.90262
	loss_value_0: 0.26173
	loss_policy_1: 0.03371
	accuracy_policy_1: 0.90156
	loss_value_1: 0.05314
	loss_reward_1: 0.00474
	loss_policy_2: 0.03427
	accuracy_policy_2: 0.90148
	loss_value_2: 0.05435
	loss_reward_2: 0.00558
	loss_policy_3: 0.03377
	accuracy_policy_3: 0.90578
	loss_value_3: 0.05575
	loss_reward_3: 0.0062
	loss_policy_4: 0.03419
	accuracy_policy_4: 0.90402
	loss_value_4: 0.05695
	loss_reward_4: 0.00821
	loss_policy_5: 0.0338
	accuracy_policy_5: 0.90699
	loss_value_5: 0.05889
	loss_reward_5: 0.00968
	loss_policy: 0.34046
	loss_value: 0.54081
	loss_reward: 0.03441
[2025-05-11 14:17:18] nn step 21600, lr: 0.1.
	loss_policy_0: 0.16713
	accuracy_policy_0: 0.90484
	loss_value_0: 0.25692
	loss_policy_1: 0.03339
	accuracy_policy_1: 0.9034
	loss_value_1: 0.05213
	loss_reward_1: 0.00443
	loss_policy_2: 0.03353
	accuracy_policy_2: 0.9023
	loss_value_2: 0.05362
	loss_reward_2: 0.00536
	loss_policy_3: 0.03318
	accuracy_policy_3: 0.90625
	loss_value_3: 0.05474
	loss_reward_3: 0.00642
	loss_policy_4: 0.0335
	accuracy_policy_4: 0.90816
	loss_value_4: 0.0564
	loss_reward_4: 0.00773
	loss_policy_5: 0.03336
	accuracy_policy_5: 0.90352
	loss_value_5: 0.05804
	loss_reward_5: 0.00904
	loss_policy: 0.33409
	loss_value: 0.53187
	loss_reward: 0.03298
Optimization_Done 21600
[2025-05-11 14:18:37] [command] train weight_iter_21600.pkl 90 109
[2025-05-11 14:18:45] nn step 21650, lr: 0.1.
	loss_policy_0: 0.16471
	accuracy_policy_0: 0.90508
	loss_value_0: 0.26505
	loss_policy_1: 0.03335
	accuracy_policy_1: 0.89977
	loss_value_1: 0.05388
	loss_reward_1: 0.0047
	loss_policy_2: 0.03317
	accuracy_policy_2: 0.90574
	loss_value_2: 0.05506
	loss_reward_2: 0.00532
	loss_policy_3: 0.03301
	accuracy_policy_3: 0.90531
	loss_value_3: 0.05622
	loss_reward_3: 0.00619
	loss_policy_4: 0.03305
	accuracy_policy_4: 0.90703
	loss_value_4: 0.05759
	loss_reward_4: 0.00781
	loss_policy_5: 0.03309
	accuracy_policy_5: 0.91016
	loss_value_5: 0.05921
	loss_reward_5: 0.00917
	loss_policy: 0.33038
	loss_value: 0.547
	loss_reward: 0.03319
[2025-05-11 14:18:53] nn step 21700, lr: 0.1.
	loss_policy_0: 0.16164
	accuracy_policy_0: 0.90398
	loss_value_0: 0.25284
	loss_policy_1: 0.03237
	accuracy_policy_1: 0.90477
	loss_value_1: 0.05113
	loss_reward_1: 0.00439
	loss_policy_2: 0.03209
	accuracy_policy_2: 0.90199
	loss_value_2: 0.05234
	loss_reward_2: 0.00525
	loss_policy_3: 0.03239
	accuracy_policy_3: 0.90887
	loss_value_3: 0.05348
	loss_reward_3: 0.00606
	loss_policy_4: 0.03237
	accuracy_policy_4: 0.90578
	loss_value_4: 0.05479
	loss_reward_4: 0.00748
	loss_policy_5: 0.03221
	accuracy_policy_5: 0.90785
	loss_value_5: 0.05644
	loss_reward_5: 0.00879
	loss_policy: 0.32306
	loss_value: 0.52101
	loss_reward: 0.03198
[2025-05-11 14:19:02] nn step 21750, lr: 0.1.
	loss_policy_0: 0.15703
	accuracy_policy_0: 0.905
	loss_value_0: 0.24374
	loss_policy_1: 0.03152
	accuracy_policy_1: 0.90457
	loss_value_1: 0.04976
	loss_reward_1: 0.00427
	loss_policy_2: 0.03122
	accuracy_policy_2: 0.90801
	loss_value_2: 0.05089
	loss_reward_2: 0.00507
	loss_policy_3: 0.03148
	accuracy_policy_3: 0.90824
	loss_value_3: 0.05194
	loss_reward_3: 0.00606
	loss_policy_4: 0.03129
	accuracy_policy_4: 0.90434
	loss_value_4: 0.05335
	loss_reward_4: 0.00726
	loss_policy_5: 0.03127
	accuracy_policy_5: 0.90531
	loss_value_5: 0.05483
	loss_reward_5: 0.00864
	loss_policy: 0.3138
	loss_value: 0.50451
	loss_reward: 0.03131
[2025-05-11 14:19:09] nn step 21800, lr: 0.1.
	loss_policy_0: 0.17295
	accuracy_policy_0: 0.90543
	loss_value_0: 0.2687
	loss_policy_1: 0.03437
	accuracy_policy_1: 0.90727
	loss_value_1: 0.05449
	loss_reward_1: 0.00466
	loss_policy_2: 0.03409
	accuracy_policy_2: 0.90887
	loss_value_2: 0.05536
	loss_reward_2: 0.00543
	loss_policy_3: 0.03447
	accuracy_policy_3: 0.90738
	loss_value_3: 0.05679
	loss_reward_3: 0.00668
	loss_policy_4: 0.03443
	accuracy_policy_4: 0.90766
	loss_value_4: 0.0583
	loss_reward_4: 0.00786
	loss_policy_5: 0.03461
	accuracy_policy_5: 0.90887
	loss_value_5: 0.06013
	loss_reward_5: 0.00928
	loss_policy: 0.34492
	loss_value: 0.55377
	loss_reward: 0.03391
Optimization_Done 21800
[2025-05-11 14:20:26] [command] train weight_iter_21800.pkl 91 110
[2025-05-11 14:20:34] nn step 21850, lr: 0.1.
	loss_policy_0: 0.16885
	accuracy_policy_0: 0.9093
	loss_value_0: 0.26644
	loss_policy_1: 0.03358
	accuracy_policy_1: 0.90559
	loss_value_1: 0.05405
	loss_reward_1: 0.00474
	loss_policy_2: 0.03358
	accuracy_policy_2: 0.90926
	loss_value_2: 0.05523
	loss_reward_2: 0.00551
	loss_policy_3: 0.03341
	accuracy_policy_3: 0.90742
	loss_value_3: 0.05643
	loss_reward_3: 0.00635
	loss_policy_4: 0.03373
	accuracy_policy_4: 0.90887
	loss_value_4: 0.05796
	loss_reward_4: 0.00776
	loss_policy_5: 0.0333
	accuracy_policy_5: 0.9098
	loss_value_5: 0.05966
	loss_reward_5: 0.00924
	loss_policy: 0.33646
	loss_value: 0.54977
	loss_reward: 0.0336
[2025-05-11 14:20:43] nn step 21900, lr: 0.1.
	loss_policy_0: 0.1638
	accuracy_policy_0: 0.90547
	loss_value_0: 0.2595
	loss_policy_1: 0.03268
	accuracy_policy_1: 0.90605
	loss_value_1: 0.05296
	loss_reward_1: 0.00454
	loss_policy_2: 0.03277
	accuracy_policy_2: 0.90625
	loss_value_2: 0.0542
	loss_reward_2: 0.00539
	loss_policy_3: 0.03287
	accuracy_policy_3: 0.91094
	loss_value_3: 0.05536
	loss_reward_3: 0.00643
	loss_policy_4: 0.03291
	accuracy_policy_4: 0.90898
	loss_value_4: 0.05683
	loss_reward_4: 0.00765
	loss_policy_5: 0.0324
	accuracy_policy_5: 0.91168
	loss_value_5: 0.05851
	loss_reward_5: 0.00884
	loss_policy: 0.32743
	loss_value: 0.53735
	loss_reward: 0.03286
[2025-05-11 14:20:51] nn step 21950, lr: 0.1.
	loss_policy_0: 0.1788
	accuracy_policy_0: 0.90664
	loss_value_0: 0.27688
	loss_policy_1: 0.0355
	accuracy_policy_1: 0.90492
	loss_value_1: 0.0565
	loss_reward_1: 0.00505
	loss_policy_2: 0.03515
	accuracy_policy_2: 0.9057
	loss_value_2: 0.05763
	loss_reward_2: 0.00573
	loss_policy_3: 0.03522
	accuracy_policy_3: 0.90988
	loss_value_3: 0.05883
	loss_reward_3: 0.00676
	loss_policy_4: 0.03549
	accuracy_policy_4: 0.90676
	loss_value_4: 0.06023
	loss_reward_4: 0.00821
	loss_policy_5: 0.03527
	accuracy_policy_5: 0.91078
	loss_value_5: 0.06208
	loss_reward_5: 0.00972
	loss_policy: 0.35544
	loss_value: 0.57215
	loss_reward: 0.03547
[2025-05-11 14:21:00] nn step 22000, lr: 0.1.
	loss_policy_0: 0.16632
	accuracy_policy_0: 0.9084
	loss_value_0: 0.26181
	loss_policy_1: 0.03334
	accuracy_policy_1: 0.90609
	loss_value_1: 0.05318
	loss_reward_1: 0.00474
	loss_policy_2: 0.03319
	accuracy_policy_2: 0.9107
	loss_value_2: 0.05448
	loss_reward_2: 0.0054
	loss_policy_3: 0.03335
	accuracy_policy_3: 0.91023
	loss_value_3: 0.05552
	loss_reward_3: 0.00637
	loss_policy_4: 0.03353
	accuracy_policy_4: 0.90543
	loss_value_4: 0.05727
	loss_reward_4: 0.00791
	loss_policy_5: 0.03306
	accuracy_policy_5: 0.90922
	loss_value_5: 0.05905
	loss_reward_5: 0.00937
	loss_policy: 0.33279
	loss_value: 0.54131
	loss_reward: 0.03379
Optimization_Done 22000
[2025-05-11 14:22:16] [command] train weight_iter_22000.pkl 92 111
[2025-05-11 14:22:25] nn step 22050, lr: 0.1.
	loss_policy_0: 0.1653
	accuracy_policy_0: 0.91031
	loss_value_0: 0.25938
	loss_policy_1: 0.03279
	accuracy_policy_1: 0.90898
	loss_value_1: 0.05275
	loss_reward_1: 0.00461
	loss_policy_2: 0.03295
	accuracy_policy_2: 0.90574
	loss_value_2: 0.05427
	loss_reward_2: 0.00532
	loss_policy_3: 0.03312
	accuracy_policy_3: 0.90855
	loss_value_3: 0.05532
	loss_reward_3: 0.00613
	loss_policy_4: 0.03338
	accuracy_policy_4: 0.90824
	loss_value_4: 0.05646
	loss_reward_4: 0.00789
	loss_policy_5: 0.03272
	accuracy_policy_5: 0.9132
	loss_value_5: 0.05815
	loss_reward_5: 0.00869
	loss_policy: 0.33026
	loss_value: 0.53633
	loss_reward: 0.03263
[2025-05-11 14:22:32] nn step 22100, lr: 0.1.
	loss_policy_0: 0.16859
	accuracy_policy_0: 0.90641
	loss_value_0: 0.2603
	loss_policy_1: 0.03341
	accuracy_policy_1: 0.90551
	loss_value_1: 0.0529
	loss_reward_1: 0.00484
	loss_policy_2: 0.0334
	accuracy_policy_2: 0.90871
	loss_value_2: 0.05423
	loss_reward_2: 0.00548
	loss_policy_3: 0.03368
	accuracy_policy_3: 0.90719
	loss_value_3: 0.05562
	loss_reward_3: 0.00616
	loss_policy_4: 0.03343
	accuracy_policy_4: 0.90852
	loss_value_4: 0.0569
	loss_reward_4: 0.00778
	loss_policy_5: 0.03315
	accuracy_policy_5: 0.91016
	loss_value_5: 0.05825
	loss_reward_5: 0.0089
	loss_policy: 0.33566
	loss_value: 0.5382
	loss_reward: 0.03317
[2025-05-11 14:22:40] nn step 22150, lr: 0.1.
	loss_policy_0: 0.1724
	accuracy_policy_0: 0.91051
	loss_value_0: 0.27095
	loss_policy_1: 0.03437
	accuracy_policy_1: 0.91188
	loss_value_1: 0.0553
	loss_reward_1: 0.00505
	loss_policy_2: 0.03476
	accuracy_policy_2: 0.91141
	loss_value_2: 0.05652
	loss_reward_2: 0.00575
	loss_policy_3: 0.03468
	accuracy_policy_3: 0.91008
	loss_value_3: 0.05792
	loss_reward_3: 0.00638
	loss_policy_4: 0.03463
	accuracy_policy_4: 0.90945
	loss_value_4: 0.05939
	loss_reward_4: 0.0082
	loss_policy_5: 0.03431
	accuracy_policy_5: 0.91188
	loss_value_5: 0.06118
	loss_reward_5: 0.00958
	loss_policy: 0.34514
	loss_value: 0.56126
	loss_reward: 0.03495
[2025-05-11 14:22:49] nn step 22200, lr: 0.1.
	loss_policy_0: 0.17669
	accuracy_policy_0: 0.91105
	loss_value_0: 0.27774
	loss_policy_1: 0.0353
	accuracy_policy_1: 0.90332
	loss_value_1: 0.05647
	loss_reward_1: 0.00509
	loss_policy_2: 0.03513
	accuracy_policy_2: 0.9093
	loss_value_2: 0.05761
	loss_reward_2: 0.00611
	loss_policy_3: 0.03581
	accuracy_policy_3: 0.90633
	loss_value_3: 0.05901
	loss_reward_3: 0.00704
	loss_policy_4: 0.03559
	accuracy_policy_4: 0.90781
	loss_value_4: 0.06067
	loss_reward_4: 0.00864
	loss_policy_5: 0.03514
	accuracy_policy_5: 0.9116
	loss_value_5: 0.06269
	loss_reward_5: 0.01012
	loss_policy: 0.35366
	loss_value: 0.57419
	loss_reward: 0.037
Optimization_Done 22200
[2025-05-11 14:24:06] [command] train weight_iter_22200.pkl 93 112
[2025-05-11 14:24:15] nn step 22250, lr: 0.1.
	loss_policy_0: 0.16183
	accuracy_policy_0: 0.91406
	loss_value_0: 0.26128
	loss_policy_1: 0.03257
	accuracy_policy_1: 0.90816
	loss_value_1: 0.05304
	loss_reward_1: 0.00462
	loss_policy_2: 0.03286
	accuracy_policy_2: 0.91074
	loss_value_2: 0.05425
	loss_reward_2: 0.00549
	loss_policy_3: 0.03283
	accuracy_policy_3: 0.90969
	loss_value_3: 0.05567
	loss_reward_3: 0.00636
	loss_policy_4: 0.03249
	accuracy_policy_4: 0.90863
	loss_value_4: 0.05695
	loss_reward_4: 0.00734
	loss_policy_5: 0.03274
	accuracy_policy_5: 0.90906
	loss_value_5: 0.05841
	loss_reward_5: 0.00893
	loss_policy: 0.32532
	loss_value: 0.53961
	loss_reward: 0.03275
[2025-05-11 14:24:22] nn step 22300, lr: 0.1.
	loss_policy_0: 0.16218
	accuracy_policy_0: 0.91188
	loss_value_0: 0.25055
	loss_policy_1: 0.03197
	accuracy_policy_1: 0.90742
	loss_value_1: 0.05108
	loss_reward_1: 0.00449
	loss_policy_2: 0.03184
	accuracy_policy_2: 0.91434
	loss_value_2: 0.05228
	loss_reward_2: 0.00507
	loss_policy_3: 0.03179
	accuracy_policy_3: 0.91305
	loss_value_3: 0.0536
	loss_reward_3: 0.0058
	loss_policy_4: 0.03198
	accuracy_policy_4: 0.91348
	loss_value_4: 0.05528
	loss_reward_4: 0.00714
	loss_policy_5: 0.03209
	accuracy_policy_5: 0.91543
	loss_value_5: 0.05669
	loss_reward_5: 0.0087
	loss_policy: 0.32185
	loss_value: 0.51949
	loss_reward: 0.0312
[2025-05-11 14:24:30] nn step 22350, lr: 0.1.
	loss_policy_0: 0.16397
	accuracy_policy_0: 0.91062
	loss_value_0: 0.25736
	loss_policy_1: 0.03275
	accuracy_policy_1: 0.90672
	loss_value_1: 0.05222
	loss_reward_1: 0.00457
	loss_policy_2: 0.03294
	accuracy_policy_2: 0.90977
	loss_value_2: 0.05317
	loss_reward_2: 0.00527
	loss_policy_3: 0.03262
	accuracy_policy_3: 0.91242
	loss_value_3: 0.05411
	loss_reward_3: 0.00611
	loss_policy_4: 0.03283
	accuracy_policy_4: 0.91117
	loss_value_4: 0.05562
	loss_reward_4: 0.00729
	loss_policy_5: 0.03248
	accuracy_policy_5: 0.91047
	loss_value_5: 0.05723
	loss_reward_5: 0.00857
	loss_policy: 0.3276
	loss_value: 0.52971
	loss_reward: 0.0318
[2025-05-11 14:24:39] nn step 22400, lr: 0.1.
	loss_policy_0: 0.16586
	accuracy_policy_0: 0.91238
	loss_value_0: 0.2579
	loss_policy_1: 0.03336
	accuracy_policy_1: 0.90918
	loss_value_1: 0.05248
	loss_reward_1: 0.00479
	loss_policy_2: 0.03342
	accuracy_policy_2: 0.91133
	loss_value_2: 0.05376
	loss_reward_2: 0.00539
	loss_policy_3: 0.03315
	accuracy_policy_3: 0.9116
	loss_value_3: 0.05492
	loss_reward_3: 0.00633
	loss_policy_4: 0.03344
	accuracy_policy_4: 0.91137
	loss_value_4: 0.05639
	loss_reward_4: 0.00743
	loss_policy_5: 0.03312
	accuracy_policy_5: 0.91156
	loss_value_5: 0.05797
	loss_reward_5: 0.00901
	loss_policy: 0.33236
	loss_value: 0.53342
	loss_reward: 0.03295
Optimization_Done 22400
[2025-05-11 14:25:56] [command] train weight_iter_22400.pkl 94 113
[2025-05-11 14:26:05] nn step 22450, lr: 0.1.
	loss_policy_0: 0.16664
	accuracy_policy_0: 0.91176
	loss_value_0: 0.26413
	loss_policy_1: 0.03305
	accuracy_policy_1: 0.90867
	loss_value_1: 0.05377
	loss_reward_1: 0.00477
	loss_policy_2: 0.03311
	accuracy_policy_2: 0.91109
	loss_value_2: 0.05489
	loss_reward_2: 0.00551
	loss_policy_3: 0.03299
	accuracy_policy_3: 0.90953
	loss_value_3: 0.05626
	loss_reward_3: 0.0066
	loss_policy_4: 0.03307
	accuracy_policy_4: 0.90848
	loss_value_4: 0.05754
	loss_reward_4: 0.00807
	loss_policy_5: 0.03283
	accuracy_policy_5: 0.91305
	loss_value_5: 0.05937
	loss_reward_5: 0.00957
	loss_policy: 0.33169
	loss_value: 0.54596
	loss_reward: 0.03452
[2025-05-11 14:26:14] nn step 22500, lr: 0.1.
	loss_policy_0: 0.16637
	accuracy_policy_0: 0.91348
	loss_value_0: 0.26185
	loss_policy_1: 0.03298
	accuracy_policy_1: 0.91543
	loss_value_1: 0.05332
	loss_reward_1: 0.00473
	loss_policy_2: 0.03321
	accuracy_policy_2: 0.91262
	loss_value_2: 0.05433
	loss_reward_2: 0.00532
	loss_policy_3: 0.03335
	accuracy_policy_3: 0.91102
	loss_value_3: 0.05552
	loss_reward_3: 0.00622
	loss_policy_4: 0.03335
	accuracy_policy_4: 0.91176
	loss_value_4: 0.05698
	loss_reward_4: 0.00772
	loss_policy_5: 0.03293
	accuracy_policy_5: 0.91512
	loss_value_5: 0.05863
	loss_reward_5: 0.00892
	loss_policy: 0.33219
	loss_value: 0.54063
	loss_reward: 0.03291
[2025-05-11 14:26:21] nn step 22550, lr: 0.1.
	loss_policy_0: 0.15891
	accuracy_policy_0: 0.91105
	loss_value_0: 0.2482
	loss_policy_1: 0.03179
	accuracy_policy_1: 0.90992
	loss_value_1: 0.05047
	loss_reward_1: 0.00466
	loss_policy_2: 0.032
	accuracy_policy_2: 0.90988
	loss_value_2: 0.05154
	loss_reward_2: 0.00526
	loss_policy_3: 0.03174
	accuracy_policy_3: 0.91281
	loss_value_3: 0.0527
	loss_reward_3: 0.00611
	loss_policy_4: 0.03154
	accuracy_policy_4: 0.91203
	loss_value_4: 0.05388
	loss_reward_4: 0.00722
	loss_policy_5: 0.03172
	accuracy_policy_5: 0.91367
	loss_value_5: 0.05553
	loss_reward_5: 0.00863
	loss_policy: 0.3177
	loss_value: 0.51233
	loss_reward: 0.03187
[2025-05-11 14:26:29] nn step 22600, lr: 0.1.
	loss_policy_0: 0.15717
	accuracy_policy_0: 0.91703
	loss_value_0: 0.2434
	loss_policy_1: 0.03141
	accuracy_policy_1: 0.9148
	loss_value_1: 0.04971
	loss_reward_1: 0.00456
	loss_policy_2: 0.03131
	accuracy_policy_2: 0.90984
	loss_value_2: 0.05097
	loss_reward_2: 0.00498
	loss_policy_3: 0.03139
	accuracy_policy_3: 0.91312
	loss_value_3: 0.05209
	loss_reward_3: 0.00577
	loss_policy_4: 0.0313
	accuracy_policy_4: 0.91562
	loss_value_4: 0.05373
	loss_reward_4: 0.00679
	loss_policy_5: 0.03127
	accuracy_policy_5: 0.91473
	loss_value_5: 0.05514
	loss_reward_5: 0.00816
	loss_policy: 0.31386
	loss_value: 0.50505
	loss_reward: 0.03027
Optimization_Done 22600
[2025-05-11 14:27:42] [command] train weight_iter_22600.pkl 95 114
[2025-05-11 14:27:52] nn step 22650, lr: 0.1.
	loss_policy_0: 0.15838
	accuracy_policy_0: 0.91914
	loss_value_0: 0.252
	loss_policy_1: 0.0319
	accuracy_policy_1: 0.9141
	loss_value_1: 0.05094
	loss_reward_1: 0.00458
	loss_policy_2: 0.03195
	accuracy_policy_2: 0.91344
	loss_value_2: 0.05213
	loss_reward_2: 0.00519
	loss_policy_3: 0.03171
	accuracy_policy_3: 0.91773
	loss_value_3: 0.05332
	loss_reward_3: 0.00602
	loss_policy_4: 0.03185
	accuracy_policy_4: 0.9127
	loss_value_4: 0.0547
	loss_reward_4: 0.00721
	loss_policy_5: 0.03148
	accuracy_policy_5: 0.91695
	loss_value_5: 0.05609
	loss_reward_5: 0.00847
	loss_policy: 0.31727
	loss_value: 0.51917
	loss_reward: 0.03146
[2025-05-11 14:28:00] nn step 22700, lr: 0.1.
	loss_policy_0: 0.16232
	accuracy_policy_0: 0.92039
	loss_value_0: 0.25488
	loss_policy_1: 0.03267
	accuracy_policy_1: 0.91254
	loss_value_1: 0.05207
	loss_reward_1: 0.00481
	loss_policy_2: 0.03267
	accuracy_policy_2: 0.91648
	loss_value_2: 0.05321
	loss_reward_2: 0.00531
	loss_policy_3: 0.03245
	accuracy_policy_3: 0.91621
	loss_value_3: 0.05438
	loss_reward_3: 0.00622
	loss_policy_4: 0.03258
	accuracy_policy_4: 0.91691
	loss_value_4: 0.05585
	loss_reward_4: 0.00753
	loss_policy_5: 0.03236
	accuracy_policy_5: 0.91809
	loss_value_5: 0.05739
	loss_reward_5: 0.00882
	loss_policy: 0.32506
	loss_value: 0.52777
	loss_reward: 0.0327
[2025-05-11 14:28:09] nn step 22750, lr: 0.1.
	loss_policy_0: 0.158
	accuracy_policy_0: 0.91992
	loss_value_0: 0.24568
	loss_policy_1: 0.03095
	accuracy_policy_1: 0.91734
	loss_value_1: 0.05022
	loss_reward_1: 0.00448
	loss_policy_2: 0.03137
	accuracy_policy_2: 0.91797
	loss_value_2: 0.05133
	loss_reward_2: 0.00507
	loss_policy_3: 0.03124
	accuracy_policy_3: 0.91414
	loss_value_3: 0.05246
	loss_reward_3: 0.00583
	loss_policy_4: 0.03136
	accuracy_policy_4: 0.91641
	loss_value_4: 0.05418
	loss_reward_4: 0.00702
	loss_policy_5: 0.03105
	accuracy_policy_5: 0.91984
	loss_value_5: 0.05558
	loss_reward_5: 0.00851
	loss_policy: 0.31397
	loss_value: 0.50945
	loss_reward: 0.03091
[2025-05-11 14:28:15] nn step 22800, lr: 0.1.
	loss_policy_0: 0.15923
	accuracy_policy_0: 0.91848
	loss_value_0: 0.24954
	loss_policy_1: 0.03179
	accuracy_policy_1: 0.9152
	loss_value_1: 0.051
	loss_reward_1: 0.00437
	loss_policy_2: 0.03203
	accuracy_policy_2: 0.91445
	loss_value_2: 0.0518
	loss_reward_2: 0.00524
	loss_policy_3: 0.03198
	accuracy_policy_3: 0.92094
	loss_value_3: 0.05274
	loss_reward_3: 0.0061
	loss_policy_4: 0.03217
	accuracy_policy_4: 0.91859
	loss_value_4: 0.05425
	loss_reward_4: 0.00723
	loss_policy_5: 0.03176
	accuracy_policy_5: 0.91848
	loss_value_5: 0.0558
	loss_reward_5: 0.00857
	loss_policy: 0.31896
	loss_value: 0.51513
	loss_reward: 0.03152
Optimization_Done 22800
[2025-05-11 14:29:34] [command] train weight_iter_22800.pkl 96 115
[2025-05-11 14:29:42] nn step 22850, lr: 0.1.
	loss_policy_0: 0.16156
	accuracy_policy_0: 0.91785
	loss_value_0: 0.25538
	loss_policy_1: 0.03228
	accuracy_policy_1: 0.91891
	loss_value_1: 0.0524
	loss_reward_1: 0.00482
	loss_policy_2: 0.03204
	accuracy_policy_2: 0.91688
	loss_value_2: 0.05332
	loss_reward_2: 0.00541
	loss_policy_3: 0.03205
	accuracy_policy_3: 0.91926
	loss_value_3: 0.05453
	loss_reward_3: 0.00628
	loss_policy_4: 0.03225
	accuracy_policy_4: 0.91988
	loss_value_4: 0.0557
	loss_reward_4: 0.00728
	loss_policy_5: 0.03137
	accuracy_policy_5: 0.91867
	loss_value_5: 0.05668
	loss_reward_5: 0.00868
	loss_policy: 0.32155
	loss_value: 0.52801
	loss_reward: 0.03247
[2025-05-11 14:29:50] nn step 22900, lr: 0.1.
	loss_policy_0: 0.15191
	accuracy_policy_0: 0.91965
	loss_value_0: 0.23701
	loss_policy_1: 0.03018
	accuracy_policy_1: 0.91832
	loss_value_1: 0.04824
	loss_reward_1: 0.00427
	loss_policy_2: 0.0305
	accuracy_policy_2: 0.91754
	loss_value_2: 0.04941
	loss_reward_2: 0.00496
	loss_policy_3: 0.03039
	accuracy_policy_3: 0.91418
	loss_value_3: 0.05047
	loss_reward_3: 0.00568
	loss_policy_4: 0.03028
	accuracy_policy_4: 0.91777
	loss_value_4: 0.05169
	loss_reward_4: 0.00684
	loss_policy_5: 0.03026
	accuracy_policy_5: 0.9207
	loss_value_5: 0.05313
	loss_reward_5: 0.00809
	loss_policy: 0.30352
	loss_value: 0.48996
	loss_reward: 0.02984
[2025-05-11 14:29:59] nn step 22950, lr: 0.1.
	loss_policy_0: 0.15657
	accuracy_policy_0: 0.92086
	loss_value_0: 0.24339
	loss_policy_1: 0.03117
	accuracy_policy_1: 0.91707
	loss_value_1: 0.04967
	loss_reward_1: 0.00517
	loss_policy_2: 0.03114
	accuracy_policy_2: 0.91891
	loss_value_2: 0.0507
	loss_reward_2: 0.00575
	loss_policy_3: 0.03118
	accuracy_policy_3: 0.91805
	loss_value_3: 0.05179
	loss_reward_3: 0.00621
	loss_policy_4: 0.03124
	accuracy_policy_4: 0.91668
	loss_value_4: 0.05338
	loss_reward_4: 0.00767
	loss_policy_5: 0.03104
	accuracy_policy_5: 0.91973
	loss_value_5: 0.05477
	loss_reward_5: 0.00872
	loss_policy: 0.31233
	loss_value: 0.50371
	loss_reward: 0.03352
[2025-05-11 14:30:06] nn step 23000, lr: 0.1.
	loss_policy_0: 0.16154
	accuracy_policy_0: 0.91895
	loss_value_0: 0.24993
	loss_policy_1: 0.03212
	accuracy_policy_1: 0.91906
	loss_value_1: 0.05065
	loss_reward_1: 0.0048
	loss_policy_2: 0.03256
	accuracy_policy_2: 0.91734
	loss_value_2: 0.05186
	loss_reward_2: 0.00549
	loss_policy_3: 0.03209
	accuracy_policy_3: 0.91938
	loss_value_3: 0.05314
	loss_reward_3: 0.00614
	loss_policy_4: 0.03202
	accuracy_policy_4: 0.91855
	loss_value_4: 0.05421
	loss_reward_4: 0.00732
	loss_policy_5: 0.03201
	accuracy_policy_5: 0.91957
	loss_value_5: 0.05628
	loss_reward_5: 0.00867
	loss_policy: 0.32234
	loss_value: 0.51608
	loss_reward: 0.03241
Optimization_Done 23000
[2025-05-11 14:31:23] [command] train weight_iter_23000.pkl 97 116
[2025-05-11 14:31:31] nn step 23050, lr: 0.1.
	loss_policy_0: 0.1523
	accuracy_policy_0: 0.9225
	loss_value_0: 0.24449
	loss_policy_1: 0.03051
	accuracy_policy_1: 0.92059
	loss_value_1: 0.0498
	loss_reward_1: 0.00437
	loss_policy_2: 0.03035
	accuracy_policy_2: 0.92246
	loss_value_2: 0.05096
	loss_reward_2: 0.00515
	loss_policy_3: 0.03092
	accuracy_policy_3: 0.92125
	loss_value_3: 0.05184
	loss_reward_3: 0.00586
	loss_policy_4: 0.03047
	accuracy_policy_4: 0.9202
	loss_value_4: 0.05319
	loss_reward_4: 0.0068
	loss_policy_5: 0.03051
	accuracy_policy_5: 0.92469
	loss_value_5: 0.05465
	loss_reward_5: 0.00839
	loss_policy: 0.30506
	loss_value: 0.50493
	loss_reward: 0.03057
[2025-05-11 14:31:40] nn step 23100, lr: 0.1.
	loss_policy_0: 0.16062
	accuracy_policy_0: 0.92461
	loss_value_0: 0.25595
	loss_policy_1: 0.03212
	accuracy_policy_1: 0.91871
	loss_value_1: 0.0519
	loss_reward_1: 0.00495
	loss_policy_2: 0.03218
	accuracy_policy_2: 0.92113
	loss_value_2: 0.05314
	loss_reward_2: 0.00563
	loss_policy_3: 0.03205
	accuracy_policy_3: 0.92203
	loss_value_3: 0.05432
	loss_reward_3: 0.0065
	loss_policy_4: 0.03242
	accuracy_policy_4: 0.91852
	loss_value_4: 0.05539
	loss_reward_4: 0.00747
	loss_policy_5: 0.03185
	accuracy_policy_5: 0.9202
	loss_value_5: 0.05677
	loss_reward_5: 0.00902
	loss_policy: 0.32123
	loss_value: 0.52746
	loss_reward: 0.03356
[2025-05-11 14:31:48] nn step 23150, lr: 0.1.
	loss_policy_0: 0.16429
	accuracy_policy_0: 0.92207
	loss_value_0: 0.25927
	loss_policy_1: 0.03286
	accuracy_policy_1: 0.91699
	loss_value_1: 0.05277
	loss_reward_1: 0.00491
	loss_policy_2: 0.03262
	accuracy_policy_2: 0.91883
	loss_value_2: 0.05401
	loss_reward_2: 0.00544
	loss_policy_3: 0.03249
	accuracy_policy_3: 0.92172
	loss_value_3: 0.05486
	loss_reward_3: 0.00635
	loss_policy_4: 0.03248
	accuracy_policy_4: 0.92066
	loss_value_4: 0.05639
	loss_reward_4: 0.00768
	loss_policy_5: 0.03241
	accuracy_policy_5: 0.92312
	loss_value_5: 0.05769
	loss_reward_5: 0.00926
	loss_policy: 0.32716
	loss_value: 0.53498
	loss_reward: 0.03365
[2025-05-11 14:31:57] nn step 23200, lr: 0.1.
	loss_policy_0: 0.16089
	accuracy_policy_0: 0.92285
	loss_value_0: 0.25445
	loss_policy_1: 0.03235
	accuracy_policy_1: 0.91805
	loss_value_1: 0.05178
	loss_reward_1: 0.00478
	loss_policy_2: 0.03212
	accuracy_policy_2: 0.91605
	loss_value_2: 0.05294
	loss_reward_2: 0.00545
	loss_policy_3: 0.0323
	accuracy_policy_3: 0.91934
	loss_value_3: 0.054
	loss_reward_3: 0.00634
	loss_policy_4: 0.03248
	accuracy_policy_4: 0.9198
	loss_value_4: 0.05537
	loss_reward_4: 0.00767
	loss_policy_5: 0.0321
	accuracy_policy_5: 0.92168
	loss_value_5: 0.05672
	loss_reward_5: 0.00879
	loss_policy: 0.32223
	loss_value: 0.52526
	loss_reward: 0.03303
Optimization_Done 23200
[2025-05-11 14:33:10] [command] train weight_iter_23200.pkl 98 117
[2025-05-11 14:33:20] nn step 23250, lr: 0.1.
	loss_policy_0: 0.16193
	accuracy_policy_0: 0.92168
	loss_value_0: 0.25696
	loss_policy_1: 0.03214
	accuracy_policy_1: 0.92262
	loss_value_1: 0.0523
	loss_reward_1: 0.0048
	loss_policy_2: 0.03219
	accuracy_policy_2: 0.91926
	loss_value_2: 0.05326
	loss_reward_2: 0.00551
	loss_policy_3: 0.03259
	accuracy_policy_3: 0.92258
	loss_value_3: 0.05425
	loss_reward_3: 0.00623
	loss_policy_4: 0.03242
	accuracy_policy_4: 0.92121
	loss_value_4: 0.05582
	loss_reward_4: 0.00771
	loss_policy_5: 0.03194
	accuracy_policy_5: 0.925
	loss_value_5: 0.05715
	loss_reward_5: 0.00865
	loss_policy: 0.3232
	loss_value: 0.52974
	loss_reward: 0.0329
[2025-05-11 14:33:27] nn step 23300, lr: 0.1.
	loss_policy_0: 0.15217
	accuracy_policy_0: 0.92207
	loss_value_0: 0.23614
	loss_policy_1: 0.03005
	accuracy_policy_1: 0.92094
	loss_value_1: 0.04814
	loss_reward_1: 0.0045
	loss_policy_2: 0.03024
	accuracy_policy_2: 0.91875
	loss_value_2: 0.04926
	loss_reward_2: 0.00521
	loss_policy_3: 0.03021
	accuracy_policy_3: 0.92184
	loss_value_3: 0.05033
	loss_reward_3: 0.0057
	loss_policy_4: 0.03032
	accuracy_policy_4: 0.92125
	loss_value_4: 0.0516
	loss_reward_4: 0.00705
	loss_policy_5: 0.03001
	accuracy_policy_5: 0.92445
	loss_value_5: 0.05298
	loss_reward_5: 0.00819
	loss_policy: 0.30301
	loss_value: 0.48846
	loss_reward: 0.03065
[2025-05-11 14:33:35] nn step 23350, lr: 0.1.
	loss_policy_0: 0.16377
	accuracy_policy_0: 0.92586
	loss_value_0: 0.25781
	loss_policy_1: 0.03263
	accuracy_policy_1: 0.91984
	loss_value_1: 0.05249
	loss_reward_1: 0.00519
	loss_policy_2: 0.03269
	accuracy_policy_2: 0.91992
	loss_value_2: 0.05352
	loss_reward_2: 0.0057
	loss_policy_3: 0.03279
	accuracy_policy_3: 0.92316
	loss_value_3: 0.05475
	loss_reward_3: 0.00674
	loss_policy_4: 0.03279
	accuracy_policy_4: 0.91852
	loss_value_4: 0.05625
	loss_reward_4: 0.00787
	loss_policy_5: 0.03261
	accuracy_policy_5: 0.92477
	loss_value_5: 0.05752
	loss_reward_5: 0.00943
	loss_policy: 0.32727
	loss_value: 0.53233
	loss_reward: 0.03493
[2025-05-11 14:33:44] nn step 23400, lr: 0.1.
	loss_policy_0: 0.1603
	accuracy_policy_0: 0.9232
	loss_value_0: 0.2509
	loss_policy_1: 0.03184
	accuracy_policy_1: 0.92254
	loss_value_1: 0.051
	loss_reward_1: 0.00485
	loss_policy_2: 0.03203
	accuracy_policy_2: 0.92426
	loss_value_2: 0.05262
	loss_reward_2: 0.00568
	loss_policy_3: 0.03236
	accuracy_policy_3: 0.92066
	loss_value_3: 0.0538
	loss_reward_3: 0.00622
	loss_policy_4: 0.03192
	accuracy_policy_4: 0.92117
	loss_value_4: 0.05483
	loss_reward_4: 0.00756
	loss_policy_5: 0.03206
	accuracy_policy_5: 0.92152
	loss_value_5: 0.05632
	loss_reward_5: 0.00925
	loss_policy: 0.3205
	loss_value: 0.51947
	loss_reward: 0.03356
Optimization_Done 23400
[2025-05-11 14:35:00] [command] train weight_iter_23400.pkl 99 118
[2025-05-11 14:35:09] nn step 23450, lr: 0.1.
	loss_policy_0: 0.153
	accuracy_policy_0: 0.93102
	loss_value_0: 0.24683
	loss_policy_1: 0.03068
	accuracy_policy_1: 0.92309
	loss_value_1: 0.05017
	loss_reward_1: 0.00483
	loss_policy_2: 0.03084
	accuracy_policy_2: 0.92082
	loss_value_2: 0.05107
	loss_reward_2: 0.00524
	loss_policy_3: 0.03043
	accuracy_policy_3: 0.92516
	loss_value_3: 0.05215
	loss_reward_3: 0.00598
	loss_policy_4: 0.0305
	accuracy_policy_4: 0.92664
	loss_value_4: 0.0532
	loss_reward_4: 0.00733
	loss_policy_5: 0.03019
	accuracy_policy_5: 0.9284
	loss_value_5: 0.05476
	loss_reward_5: 0.00861
	loss_policy: 0.30563
	loss_value: 0.50818
	loss_reward: 0.03199
[2025-05-11 14:35:16] nn step 23500, lr: 0.1.
	loss_policy_0: 0.15303
	accuracy_policy_0: 0.92703
	loss_value_0: 0.24198
	loss_policy_1: 0.03038
	accuracy_policy_1: 0.92684
	loss_value_1: 0.04917
	loss_reward_1: 0.00469
	loss_policy_2: 0.03067
	accuracy_policy_2: 0.92422
	loss_value_2: 0.05038
	loss_reward_2: 0.00545
	loss_policy_3: 0.03061
	accuracy_policy_3: 0.92629
	loss_value_3: 0.05145
	loss_reward_3: 0.00576
	loss_policy_4: 0.03077
	accuracy_policy_4: 0.9234
	loss_value_4: 0.05275
	loss_reward_4: 0.00704
	loss_policy_5: 0.02988
	accuracy_policy_5: 0.92988
	loss_value_5: 0.05417
	loss_reward_5: 0.00849
	loss_policy: 0.30534
	loss_value: 0.4999
	loss_reward: 0.03143
[2025-05-11 14:35:25] nn step 23550, lr: 0.1.
	loss_policy_0: 0.15283
	accuracy_policy_0: 0.92824
	loss_value_0: 0.23695
	loss_policy_1: 0.0304
	accuracy_policy_1: 0.92293
	loss_value_1: 0.04831
	loss_reward_1: 0.00471
	loss_policy_2: 0.03062
	accuracy_policy_2: 0.92422
	loss_value_2: 0.04931
	loss_reward_2: 0.00498
	loss_policy_3: 0.03091
	accuracy_policy_3: 0.92164
	loss_value_3: 0.05017
	loss_reward_3: 0.00609
	loss_policy_4: 0.03036
	accuracy_policy_4: 0.92273
	loss_value_4: 0.05133
	loss_reward_4: 0.00737
	loss_policy_5: 0.03051
	accuracy_policy_5: 0.92789
	loss_value_5: 0.05277
	loss_reward_5: 0.00828
	loss_policy: 0.30564
	loss_value: 0.48883
	loss_reward: 0.03143
[2025-05-11 14:35:33] nn step 23600, lr: 0.1.
	loss_policy_0: 0.15319
	accuracy_policy_0: 0.92711
	loss_value_0: 0.23602
	loss_policy_1: 0.03024
	accuracy_policy_1: 0.92223
	loss_value_1: 0.04811
	loss_reward_1: 0.00467
	loss_policy_2: 0.03015
	accuracy_policy_2: 0.92219
	loss_value_2: 0.04919
	loss_reward_2: 0.00525
	loss_policy_3: 0.03057
	accuracy_policy_3: 0.92168
	loss_value_3: 0.05011
	loss_reward_3: 0.00589
	loss_policy_4: 0.03038
	accuracy_policy_4: 0.92516
	loss_value_4: 0.05102
	loss_reward_4: 0.00711
	loss_policy_5: 0.02998
	accuracy_policy_5: 0.92754
	loss_value_5: 0.05224
	loss_reward_5: 0.00836
	loss_policy: 0.30451
	loss_value: 0.48669
	loss_reward: 0.03129
Optimization_Done 23600
[2025-05-11 14:36:49] [command] train weight_iter_23600.pkl 100 119
[2025-05-11 14:36:58] nn step 23650, lr: 0.1.
	loss_policy_0: 0.15715
	accuracy_policy_0: 0.93008
	loss_value_0: 0.25134
	loss_policy_1: 0.03134
	accuracy_policy_1: 0.92543
	loss_value_1: 0.05082
	loss_reward_1: 0.00481
	loss_policy_2: 0.03145
	accuracy_policy_2: 0.92754
	loss_value_2: 0.05177
	loss_reward_2: 0.00551
	loss_policy_3: 0.03133
	accuracy_policy_3: 0.92516
	loss_value_3: 0.05305
	loss_reward_3: 0.00637
	loss_policy_4: 0.03118
	accuracy_policy_4: 0.92539
	loss_value_4: 0.05428
	loss_reward_4: 0.00771
	loss_policy_5: 0.03081
	accuracy_policy_5: 0.9298
	loss_value_5: 0.05537
	loss_reward_5: 0.009
	loss_policy: 0.31326
	loss_value: 0.51662
	loss_reward: 0.0334
[2025-05-11 14:37:07] nn step 23700, lr: 0.1.
	loss_policy_0: 0.15935
	accuracy_policy_0: 0.92762
	loss_value_0: 0.2491
	loss_policy_1: 0.03136
	accuracy_policy_1: 0.92641
	loss_value_1: 0.05047
	loss_reward_1: 0.00481
	loss_policy_2: 0.03148
	accuracy_policy_2: 0.92227
	loss_value_2: 0.05176
	loss_reward_2: 0.0054
	loss_policy_3: 0.03174
	accuracy_policy_3: 0.92582
	loss_value_3: 0.05273
	loss_reward_3: 0.00631
	loss_policy_4: 0.03176
	accuracy_policy_4: 0.9257
	loss_value_4: 0.05378
	loss_reward_4: 0.00773
	loss_policy_5: 0.03132
	accuracy_policy_5: 0.92887
	loss_value_5: 0.05536
	loss_reward_5: 0.0092
	loss_policy: 0.31702
	loss_value: 0.51321
	loss_reward: 0.03344
[2025-05-11 14:37:13] nn step 23750, lr: 0.1.
	loss_policy_0: 0.15014
	accuracy_policy_0: 0.92859
	loss_value_0: 0.23473
	loss_policy_1: 0.0299
	accuracy_policy_1: 0.92422
	loss_value_1: 0.04759
	loss_reward_1: 0.00473
	loss_policy_2: 0.02953
	accuracy_policy_2: 0.92523
	loss_value_2: 0.04873
	loss_reward_2: 0.00523
	loss_policy_3: 0.0296
	accuracy_policy_3: 0.9291
	loss_value_3: 0.04966
	loss_reward_3: 0.00584
	loss_policy_4: 0.02956
	accuracy_policy_4: 0.93098
	loss_value_4: 0.05085
	loss_reward_4: 0.00682
	loss_policy_5: 0.02952
	accuracy_policy_5: 0.93031
	loss_value_5: 0.05241
	loss_reward_5: 0.00816
	loss_policy: 0.29825
	loss_value: 0.48398
	loss_reward: 0.03077
[2025-05-11 14:37:22] nn step 23800, lr: 0.1.
	loss_policy_0: 0.16488
	accuracy_policy_0: 0.92965
	loss_value_0: 0.25825
	loss_policy_1: 0.03274
	accuracy_policy_1: 0.92793
	loss_value_1: 0.05268
	loss_reward_1: 0.00509
	loss_policy_2: 0.03262
	accuracy_policy_2: 0.92648
	loss_value_2: 0.05393
	loss_reward_2: 0.0057
	loss_policy_3: 0.03278
	accuracy_policy_3: 0.92602
	loss_value_3: 0.05537
	loss_reward_3: 0.00667
	loss_policy_4: 0.03304
	accuracy_policy_4: 0.92457
	loss_value_4: 0.05681
	loss_reward_4: 0.00799
	loss_policy_5: 0.03271
	accuracy_policy_5: 0.93129
	loss_value_5: 0.05811
	loss_reward_5: 0.00945
	loss_policy: 0.32877
	loss_value: 0.53516
	loss_reward: 0.03489
Optimization_Done 23800
[2025-05-11 14:38:40] [command] train weight_iter_23800.pkl 101 120
[2025-05-11 14:38:50] nn step 23850, lr: 0.1.
	loss_policy_0: 0.15743
	accuracy_policy_0: 0.93242
	loss_value_0: 0.25049
	loss_policy_1: 0.03119
	accuracy_policy_1: 0.92996
	loss_value_1: 0.0509
	loss_reward_1: 0.0047
	loss_policy_2: 0.0313
	accuracy_policy_2: 0.92781
	loss_value_2: 0.05182
	loss_reward_2: 0.0055
	loss_policy_3: 0.03104
	accuracy_policy_3: 0.92953
	loss_value_3: 0.05296
	loss_reward_3: 0.006
	loss_policy_4: 0.03151
	accuracy_policy_4: 0.92875
	loss_value_4: 0.05402
	loss_reward_4: 0.00733
	loss_policy_5: 0.03086
	accuracy_policy_5: 0.93145
	loss_value_5: 0.0553
	loss_reward_5: 0.00823
	loss_policy: 0.31333
	loss_value: 0.51549
	loss_reward: 0.03176
[2025-05-11 14:38:58] nn step 23900, lr: 0.1.
	loss_policy_0: 0.16618
	accuracy_policy_0: 0.93102
	loss_value_0: 0.2582
	loss_policy_1: 0.03261
	accuracy_policy_1: 0.92801
	loss_value_1: 0.05257
	loss_reward_1: 0.0051
	loss_policy_2: 0.03283
	accuracy_policy_2: 0.92867
	loss_value_2: 0.05395
	loss_reward_2: 0.00568
	loss_policy_3: 0.03252
	accuracy_policy_3: 0.92992
	loss_value_3: 0.05505
	loss_reward_3: 0.00646
	loss_policy_4: 0.03276
	accuracy_policy_4: 0.9298
	loss_value_4: 0.05648
	loss_reward_4: 0.00787
	loss_policy_5: 0.0323
	accuracy_policy_5: 0.93219
	loss_value_5: 0.05794
	loss_reward_5: 0.0093
	loss_policy: 0.32921
	loss_value: 0.53419
	loss_reward: 0.0344
[2025-05-11 14:39:05] nn step 23950, lr: 0.1.
	loss_policy_0: 0.15819
	accuracy_policy_0: 0.92801
	loss_value_0: 0.24404
	loss_policy_1: 0.03129
	accuracy_policy_1: 0.93141
	loss_value_1: 0.04984
	loss_reward_1: 0.00491
	loss_policy_2: 0.03134
	accuracy_policy_2: 0.92645
	loss_value_2: 0.05127
	loss_reward_2: 0.00539
	loss_policy_3: 0.03124
	accuracy_policy_3: 0.92969
	loss_value_3: 0.05238
	loss_reward_3: 0.00611
	loss_policy_4: 0.03115
	accuracy_policy_4: 0.92676
	loss_value_4: 0.05374
	loss_reward_4: 0.00759
	loss_policy_5: 0.0313
	accuracy_policy_5: 0.92902
	loss_value_5: 0.05487
	loss_reward_5: 0.00878
	loss_policy: 0.31451
	loss_value: 0.50614
	loss_reward: 0.0328
[2025-05-11 14:39:13] nn step 24000, lr: 0.1.
	loss_policy_0: 0.15368
	accuracy_policy_0: 0.92859
	loss_value_0: 0.23728
	loss_policy_1: 0.03046
	accuracy_policy_1: 0.93062
	loss_value_1: 0.04844
	loss_reward_1: 0.00464
	loss_policy_2: 0.03029
	accuracy_policy_2: 0.9302
	loss_value_2: 0.04967
	loss_reward_2: 0.0054
	loss_policy_3: 0.03039
	accuracy_policy_3: 0.92879
	loss_value_3: 0.05086
	loss_reward_3: 0.00593
	loss_policy_4: 0.03037
	accuracy_policy_4: 0.9293
	loss_value_4: 0.05163
	loss_reward_4: 0.00714
	loss_policy_5: 0.03036
	accuracy_policy_5: 0.9316
	loss_value_5: 0.05335
	loss_reward_5: 0.00812
	loss_policy: 0.30555
	loss_value: 0.49122
	loss_reward: 0.03123
Optimization_Done 24000
[2025-05-11 14:40:30] [command] train weight_iter_24000.pkl 102 121
[2025-05-11 14:40:39] nn step 24050, lr: 0.1.
	loss_policy_0: 0.15734
	accuracy_policy_0: 0.92789
	loss_value_0: 0.24815
	loss_policy_1: 0.03095
	accuracy_policy_1: 0.92727
	loss_value_1: 0.05068
	loss_reward_1: 0.00492
	loss_policy_2: 0.0307
	accuracy_policy_2: 0.92996
	loss_value_2: 0.05169
	loss_reward_2: 0.00547
	loss_policy_3: 0.03092
	accuracy_policy_3: 0.9273
	loss_value_3: 0.05295
	loss_reward_3: 0.00631
	loss_policy_4: 0.03089
	accuracy_policy_4: 0.9291
	loss_value_4: 0.05413
	loss_reward_4: 0.00743
	loss_policy_5: 0.03065
	accuracy_policy_5: 0.93195
	loss_value_5: 0.05552
	loss_reward_5: 0.00863
	loss_policy: 0.31145
	loss_value: 0.51313
	loss_reward: 0.03277
[2025-05-11 14:40:47] nn step 24100, lr: 0.1.
	loss_policy_0: 0.16306
	accuracy_policy_0: 0.93203
	loss_value_0: 0.25719
	loss_policy_1: 0.03221
	accuracy_policy_1: 0.92836
	loss_value_1: 0.05228
	loss_reward_1: 0.00519
	loss_policy_2: 0.03225
	accuracy_policy_2: 0.93094
	loss_value_2: 0.05345
	loss_reward_2: 0.0058
	loss_policy_3: 0.0323
	accuracy_policy_3: 0.92867
	loss_value_3: 0.05469
	loss_reward_3: 0.00643
	loss_policy_4: 0.03246
	accuracy_policy_4: 0.93
	loss_value_4: 0.05591
	loss_reward_4: 0.00788
	loss_policy_5: 0.03226
	accuracy_policy_5: 0.93145
	loss_value_5: 0.05754
	loss_reward_5: 0.00944
	loss_policy: 0.32454
	loss_value: 0.53106
	loss_reward: 0.03473
[2025-05-11 14:40:56] nn step 24150, lr: 0.1.
	loss_policy_0: 0.14718
	accuracy_policy_0: 0.93363
	loss_value_0: 0.23297
	loss_policy_1: 0.02961
	accuracy_policy_1: 0.92844
	loss_value_1: 0.04731
	loss_reward_1: 0.0047
	loss_policy_2: 0.02926
	accuracy_policy_2: 0.92941
	loss_value_2: 0.04853
	loss_reward_2: 0.00499
	loss_policy_3: 0.02944
	accuracy_policy_3: 0.92836
	loss_value_3: 0.04948
	loss_reward_3: 0.0057
	loss_policy_4: 0.02926
	accuracy_policy_4: 0.93082
	loss_value_4: 0.05067
	loss_reward_4: 0.00707
	loss_policy_5: 0.02924
	accuracy_policy_5: 0.93234
	loss_value_5: 0.05192
	loss_reward_5: 0.0081
	loss_policy: 0.29399
	loss_value: 0.48088
	loss_reward: 0.03057
[2025-05-11 14:41:03] nn step 24200, lr: 0.1.
	loss_policy_0: 0.15269
	accuracy_policy_0: 0.9309
	loss_value_0: 0.23925
	loss_policy_1: 0.03065
	accuracy_policy_1: 0.92875
	loss_value_1: 0.04877
	loss_reward_1: 0.00479
	loss_policy_2: 0.03062
	accuracy_policy_2: 0.93137
	loss_value_2: 0.04958
	loss_reward_2: 0.00567
	loss_policy_3: 0.03061
	accuracy_policy_3: 0.93184
	loss_value_3: 0.05072
	loss_reward_3: 0.00632
	loss_policy_4: 0.03063
	accuracy_policy_4: 0.93066
	loss_value_4: 0.05184
	loss_reward_4: 0.00722
	loss_policy_5: 0.03001
	accuracy_policy_5: 0.93371
	loss_value_5: 0.05335
	loss_reward_5: 0.00867
	loss_policy: 0.30521
	loss_value: 0.49352
	loss_reward: 0.03267
Optimization_Done 24200
[2025-05-11 14:42:19] [command] train weight_iter_24200.pkl 103 122
[2025-05-11 14:42:27] nn step 24250, lr: 0.1.
	loss_policy_0: 0.15997
	accuracy_policy_0: 0.92977
	loss_value_0: 0.25795
	loss_policy_1: 0.03188
	accuracy_policy_1: 0.92875
	loss_value_1: 0.05253
	loss_reward_1: 0.00507
	loss_policy_2: 0.0318
	accuracy_policy_2: 0.93
	loss_value_2: 0.05381
	loss_reward_2: 0.0057
	loss_policy_3: 0.03215
	accuracy_policy_3: 0.9284
	loss_value_3: 0.055
	loss_reward_3: 0.00655
	loss_policy_4: 0.03215
	accuracy_policy_4: 0.92809
	loss_value_4: 0.05613
	loss_reward_4: 0.00813
	loss_policy_5: 0.03122
	accuracy_policy_5: 0.93375
	loss_value_5: 0.05762
	loss_reward_5: 0.00895
	loss_policy: 0.31917
	loss_value: 0.53304
	loss_reward: 0.03441
[2025-05-11 14:42:35] nn step 24300, lr: 0.1.
	loss_policy_0: 0.15025
	accuracy_policy_0: 0.93
	loss_value_0: 0.23531
	loss_policy_1: 0.02975
	accuracy_policy_1: 0.93016
	loss_value_1: 0.04791
	loss_reward_1: 0.00462
	loss_policy_2: 0.02963
	accuracy_policy_2: 0.92766
	loss_value_2: 0.04904
	loss_reward_2: 0.00517
	loss_policy_3: 0.02973
	accuracy_policy_3: 0.92582
	loss_value_3: 0.05011
	loss_reward_3: 0.006
	loss_policy_4: 0.0298
	accuracy_policy_4: 0.92633
	loss_value_4: 0.05135
	loss_reward_4: 0.00721
	loss_policy_5: 0.02963
	accuracy_policy_5: 0.9307
	loss_value_5: 0.05303
	loss_reward_5: 0.00871
	loss_policy: 0.29879
	loss_value: 0.48676
	loss_reward: 0.03171
[2025-05-11 14:42:44] nn step 24350, lr: 0.1.
	loss_policy_0: 0.15623
	accuracy_policy_0: 0.93055
	loss_value_0: 0.24392
	loss_policy_1: 0.03084
	accuracy_policy_1: 0.93062
	loss_value_1: 0.04971
	loss_reward_1: 0.00491
	loss_policy_2: 0.03089
	accuracy_policy_2: 0.93051
	loss_value_2: 0.05106
	loss_reward_2: 0.00556
	loss_policy_3: 0.03091
	accuracy_policy_3: 0.93219
	loss_value_3: 0.05207
	loss_reward_3: 0.00612
	loss_policy_4: 0.03113
	accuracy_policy_4: 0.9325
	loss_value_4: 0.05312
	loss_reward_4: 0.00728
	loss_policy_5: 0.03059
	accuracy_policy_5: 0.93324
	loss_value_5: 0.05467
	loss_reward_5: 0.00854
	loss_policy: 0.31058
	loss_value: 0.50456
	loss_reward: 0.03242
[2025-05-11 14:42:51] nn step 24400, lr: 0.1.
	loss_policy_0: 0.16531
	accuracy_policy_0: 0.93473
	loss_value_0: 0.25799
	loss_policy_1: 0.03307
	accuracy_policy_1: 0.9309
	loss_value_1: 0.05266
	loss_reward_1: 0.00532
	loss_policy_2: 0.03289
	accuracy_policy_2: 0.93043
	loss_value_2: 0.0539
	loss_reward_2: 0.00585
	loss_policy_3: 0.03291
	accuracy_policy_3: 0.9307
	loss_value_3: 0.05498
	loss_reward_3: 0.0067
	loss_policy_4: 0.03257
	accuracy_policy_4: 0.93176
	loss_value_4: 0.05657
	loss_reward_4: 0.00835
	loss_policy_5: 0.03243
	accuracy_policy_5: 0.9348
	loss_value_5: 0.0581
	loss_reward_5: 0.00945
	loss_policy: 0.32917
	loss_value: 0.53421
	loss_reward: 0.03568
Optimization_Done 24400
[2025-05-11 14:44:11] [command] train weight_iter_24400.pkl 104 123
[2025-05-11 14:44:19] nn step 24450, lr: 0.1.
	loss_policy_0: 0.15024
	accuracy_policy_0: 0.93168
	loss_value_0: 0.23586
	loss_policy_1: 0.03005
	accuracy_policy_1: 0.93078
	loss_value_1: 0.04809
	loss_reward_1: 0.00445
	loss_policy_2: 0.02987
	accuracy_policy_2: 0.93242
	loss_value_2: 0.04921
	loss_reward_2: 0.00529
	loss_policy_3: 0.02983
	accuracy_policy_3: 0.93117
	loss_value_3: 0.04999
	loss_reward_3: 0.00631
	loss_policy_4: 0.02989
	accuracy_policy_4: 0.93094
	loss_value_4: 0.05107
	loss_reward_4: 0.00716
	loss_policy_5: 0.02926
	accuracy_policy_5: 0.93402
	loss_value_5: 0.05246
	loss_reward_5: 0.00819
	loss_policy: 0.29914
	loss_value: 0.48669
	loss_reward: 0.0314
[2025-05-11 14:44:27] nn step 24500, lr: 0.1.
	loss_policy_0: 0.15744
	accuracy_policy_0: 0.9334
	loss_value_0: 0.24707
	loss_policy_1: 0.03124
	accuracy_policy_1: 0.93348
	loss_value_1: 0.05008
	loss_reward_1: 0.00488
	loss_policy_2: 0.03145
	accuracy_policy_2: 0.93707
	loss_value_2: 0.05131
	loss_reward_2: 0.00577
	loss_policy_3: 0.03145
	accuracy_policy_3: 0.93266
	loss_value_3: 0.05235
	loss_reward_3: 0.00647
	loss_policy_4: 0.03164
	accuracy_policy_4: 0.93434
	loss_value_4: 0.0536
	loss_reward_4: 0.00786
	loss_policy_5: 0.03138
	accuracy_policy_5: 0.9323
	loss_value_5: 0.05498
	loss_reward_5: 0.00903
	loss_policy: 0.3146
	loss_value: 0.50939
	loss_reward: 0.03402
[2025-05-11 14:44:36] nn step 24550, lr: 0.1.
	loss_policy_0: 0.16308
	accuracy_policy_0: 0.93277
	loss_value_0: 0.25223
	loss_policy_1: 0.03252
	accuracy_policy_1: 0.93176
	loss_value_1: 0.05135
	loss_reward_1: 0.00503
	loss_policy_2: 0.0325
	accuracy_policy_2: 0.93414
	loss_value_2: 0.0526
	loss_reward_2: 0.00559
	loss_policy_3: 0.03266
	accuracy_policy_3: 0.93191
	loss_value_3: 0.05388
	loss_reward_3: 0.00643
	loss_policy_4: 0.03294
	accuracy_policy_4: 0.93352
	loss_value_4: 0.05563
	loss_reward_4: 0.00775
	loss_policy_5: 0.03237
	accuracy_policy_5: 0.93609
	loss_value_5: 0.05691
	loss_reward_5: 0.00886
	loss_policy: 0.32608
	loss_value: 0.52259
	loss_reward: 0.03367
[2025-05-11 14:44:44] nn step 24600, lr: 0.1.
	loss_policy_0: 0.16626
	accuracy_policy_0: 0.93078
	loss_value_0: 0.26104
	loss_policy_1: 0.03325
	accuracy_policy_1: 0.93156
	loss_value_1: 0.05329
	loss_reward_1: 0.00538
	loss_policy_2: 0.03328
	accuracy_policy_2: 0.9293
	loss_value_2: 0.05469
	loss_reward_2: 0.00576
	loss_policy_3: 0.03329
	accuracy_policy_3: 0.93121
	loss_value_3: 0.05588
	loss_reward_3: 0.00681
	loss_policy_4: 0.03347
	accuracy_policy_4: 0.93215
	loss_value_4: 0.05722
	loss_reward_4: 0.00826
	loss_policy_5: 0.03284
	accuracy_policy_5: 0.93469
	loss_value_5: 0.05883
	loss_reward_5: 0.00961
	loss_policy: 0.33238
	loss_value: 0.54096
	loss_reward: 0.03582
Optimization_Done 24600
[2025-05-11 14:46:00] [command] train weight_iter_24600.pkl 105 124
[2025-05-11 14:46:09] nn step 24650, lr: 0.1.
	loss_policy_0: 0.15221
	accuracy_policy_0: 0.92781
	loss_value_0: 0.24022
	loss_policy_1: 0.03021
	accuracy_policy_1: 0.92805
	loss_value_1: 0.04907
	loss_reward_1: 0.00477
	loss_policy_2: 0.03026
	accuracy_policy_2: 0.92949
	loss_value_2: 0.0503
	loss_reward_2: 0.00527
	loss_policy_3: 0.03032
	accuracy_policy_3: 0.93172
	loss_value_3: 0.05145
	loss_reward_3: 0.00623
	loss_policy_4: 0.0305
	accuracy_policy_4: 0.92934
	loss_value_4: 0.0529
	loss_reward_4: 0.00735
	loss_policy_5: 0.03008
	accuracy_policy_5: 0.93195
	loss_value_5: 0.05409
	loss_reward_5: 0.00837
	loss_policy: 0.30358
	loss_value: 0.49804
	loss_reward: 0.03198
[2025-05-11 14:46:16] nn step 24700, lr: 0.1.
	loss_policy_0: 0.14697
	accuracy_policy_0: 0.93043
	loss_value_0: 0.23168
	loss_policy_1: 0.02922
	accuracy_policy_1: 0.92945
	loss_value_1: 0.04709
	loss_reward_1: 0.00461
	loss_policy_2: 0.02912
	accuracy_policy_2: 0.93125
	loss_value_2: 0.04827
	loss_reward_2: 0.00504
	loss_policy_3: 0.02921
	accuracy_policy_3: 0.9316
	loss_value_3: 0.04928
	loss_reward_3: 0.00599
	loss_policy_4: 0.02926
	accuracy_policy_4: 0.92992
	loss_value_4: 0.05031
	loss_reward_4: 0.00704
	loss_policy_5: 0.02906
	accuracy_policy_5: 0.93246
	loss_value_5: 0.05156
	loss_reward_5: 0.00814
	loss_policy: 0.29283
	loss_value: 0.4782
	loss_reward: 0.03081
[2025-05-11 14:46:25] nn step 24750, lr: 0.1.
	loss_policy_0: 0.16189
	accuracy_policy_0: 0.93227
	loss_value_0: 0.24727
	loss_policy_1: 0.03199
	accuracy_policy_1: 0.9291
	loss_value_1: 0.05043
	loss_reward_1: 0.0049
	loss_policy_2: 0.03193
	accuracy_policy_2: 0.93504
	loss_value_2: 0.05175
	loss_reward_2: 0.00555
	loss_policy_3: 0.03222
	accuracy_policy_3: 0.93359
	loss_value_3: 0.05351
	loss_reward_3: 0.00629
	loss_policy_4: 0.03213
	accuracy_policy_4: 0.93309
	loss_value_4: 0.05452
	loss_reward_4: 0.00768
	loss_policy_5: 0.03207
	accuracy_policy_5: 0.93461
	loss_value_5: 0.05621
	loss_reward_5: 0.00871
	loss_policy: 0.32224
	loss_value: 0.5137
	loss_reward: 0.03313
[2025-05-11 14:46:33] nn step 24800, lr: 0.1.
	loss_policy_0: 0.15485
	accuracy_policy_0: 0.9325
	loss_value_0: 0.23788
	loss_policy_1: 0.03094
	accuracy_policy_1: 0.93078
	loss_value_1: 0.04872
	loss_reward_1: 0.00466
	loss_policy_2: 0.03079
	accuracy_policy_2: 0.93035
	loss_value_2: 0.0497
	loss_reward_2: 0.00532
	loss_policy_3: 0.03082
	accuracy_policy_3: 0.92992
	loss_value_3: 0.05088
	loss_reward_3: 0.00619
	loss_policy_4: 0.03071
	accuracy_policy_4: 0.93203
	loss_value_4: 0.05209
	loss_reward_4: 0.00742
	loss_policy_5: 0.03051
	accuracy_policy_5: 0.93297
	loss_value_5: 0.05373
	loss_reward_5: 0.00839
	loss_policy: 0.30863
	loss_value: 0.493
	loss_reward: 0.03198
Optimization_Done 24800
[2025-05-11 14:47:50] [command] train weight_iter_24800.pkl 106 125
[2025-05-11 14:48:00] nn step 24850, lr: 0.1.
	loss_policy_0: 0.15394
	accuracy_policy_0: 0.9307
	loss_value_0: 0.24233
	loss_policy_1: 0.03003
	accuracy_policy_1: 0.93141
	loss_value_1: 0.049
	loss_reward_1: 0.00472
	loss_policy_2: 0.02992
	accuracy_policy_2: 0.93223
	loss_value_2: 0.0499
	loss_reward_2: 0.00537
	loss_policy_3: 0.03033
	accuracy_policy_3: 0.93367
	loss_value_3: 0.05096
	loss_reward_3: 0.00597
	loss_policy_4: 0.03008
	accuracy_policy_4: 0.93344
	loss_value_4: 0.05212
	loss_reward_4: 0.00721
	loss_policy_5: 0.02975
	accuracy_policy_5: 0.93676
	loss_value_5: 0.05352
	loss_reward_5: 0.00836
	loss_policy: 0.30406
	loss_value: 0.49782
	loss_reward: 0.03163
[2025-05-11 14:48:07] nn step 24900, lr: 0.1.
	loss_policy_0: 0.14926
	accuracy_policy_0: 0.93
	loss_value_0: 0.2285
	loss_policy_1: 0.02929
	accuracy_policy_1: 0.93234
	loss_value_1: 0.04638
	loss_reward_1: 0.00468
	loss_policy_2: 0.02963
	accuracy_policy_2: 0.93246
	loss_value_2: 0.04766
	loss_reward_2: 0.00519
	loss_policy_3: 0.0297
	accuracy_policy_3: 0.9309
	loss_value_3: 0.04862
	loss_reward_3: 0.00582
	loss_policy_4: 0.0294
	accuracy_policy_4: 0.92938
	loss_value_4: 0.05011
	loss_reward_4: 0.00702
	loss_policy_5: 0.02917
	accuracy_policy_5: 0.93242
	loss_value_5: 0.05129
	loss_reward_5: 0.00833
	loss_policy: 0.29644
	loss_value: 0.47256
	loss_reward: 0.03104
[2025-05-11 14:48:15] nn step 24950, lr: 0.1.
	loss_policy_0: 0.15075
	accuracy_policy_0: 0.92926
	loss_value_0: 0.24086
	loss_policy_1: 0.03011
	accuracy_policy_1: 0.9275
	loss_value_1: 0.04915
	loss_reward_1: 0.00638
	loss_policy_2: 0.02995
	accuracy_policy_2: 0.92754
	loss_value_2: 0.05031
	loss_reward_2: 0.00724
	loss_policy_3: 0.03022
	accuracy_policy_3: 0.92789
	loss_value_3: 0.05175
	loss_reward_3: 0.0081
	loss_policy_4: 0.03024
	accuracy_policy_4: 0.92871
	loss_value_4: 0.05306
	loss_reward_4: 0.00899
	loss_policy_5: 0.02972
	accuracy_policy_5: 0.9302
	loss_value_5: 0.05452
	loss_reward_5: 0.01028
	loss_policy: 0.30097
	loss_value: 0.49965
	loss_reward: 0.04098
[2025-05-11 14:48:24] nn step 25000, lr: 0.1.
	loss_policy_0: 0.15707
	accuracy_policy_0: 0.92766
	loss_value_0: 0.25826
	loss_policy_1: 0.031
	accuracy_policy_1: 0.92668
	loss_value_1: 0.05233
	loss_reward_1: 0.00644
	loss_policy_2: 0.031
	accuracy_policy_2: 0.92441
	loss_value_2: 0.0533
	loss_reward_2: 0.00715
	loss_policy_3: 0.03123
	accuracy_policy_3: 0.92699
	loss_value_3: 0.05445
	loss_reward_3: 0.00775
	loss_policy_4: 0.03122
	accuracy_policy_4: 0.92793
	loss_value_4: 0.05555
	loss_reward_4: 0.00934
	loss_policy_5: 0.03079
	accuracy_policy_5: 0.92988
	loss_value_5: 0.05714
	loss_reward_5: 0.01113
	loss_policy: 0.31231
	loss_value: 0.53103
	loss_reward: 0.04181
Optimization_Done 25000
[2025-05-11 14:49:37] [command] train weight_iter_25000.pkl 107 126
[2025-05-11 14:49:47] nn step 25050, lr: 0.1.
	loss_policy_0: 0.15289
	accuracy_policy_0: 0.92805
	loss_value_0: 0.24992
	loss_policy_1: 0.03022
	accuracy_policy_1: 0.92906
	loss_value_1: 0.04992
	loss_reward_1: 0.0053
	loss_policy_2: 0.03009
	accuracy_policy_2: 0.92957
	loss_value_2: 0.05114
	loss_reward_2: 0.00575
	loss_policy_3: 0.03016
	accuracy_policy_3: 0.92867
	loss_value_3: 0.05239
	loss_reward_3: 0.00687
	loss_policy_4: 0.03032
	accuracy_policy_4: 0.9323
	loss_value_4: 0.05338
	loss_reward_4: 0.008
	loss_policy_5: 0.03
	accuracy_policy_5: 0.93297
	loss_value_5: 0.05454
	loss_reward_5: 0.00937
	loss_policy: 0.30368
	loss_value: 0.51128
	loss_reward: 0.0353
[2025-05-11 14:49:55] nn step 25100, lr: 0.1.
	loss_policy_0: 0.16406
	accuracy_policy_0: 0.92633
	loss_value_0: 0.26132
	loss_policy_1: 0.03254
	accuracy_policy_1: 0.92797
	loss_value_1: 0.05262
	loss_reward_1: 0.00534
	loss_policy_2: 0.03275
	accuracy_policy_2: 0.93016
	loss_value_2: 0.05384
	loss_reward_2: 0.00619
	loss_policy_3: 0.03283
	accuracy_policy_3: 0.92578
	loss_value_3: 0.05494
	loss_reward_3: 0.00719
	loss_policy_4: 0.03273
	accuracy_policy_4: 0.92832
	loss_value_4: 0.05645
	loss_reward_4: 0.00835
	loss_policy_5: 0.03265
	accuracy_policy_5: 0.9318
	loss_value_5: 0.05791
	loss_reward_5: 0.00972
	loss_policy: 0.32756
	loss_value: 0.53709
	loss_reward: 0.03679
[2025-05-11 14:50:02] nn step 25150, lr: 0.1.
	loss_policy_0: 0.15221
	accuracy_policy_0: 0.93043
	loss_value_0: 0.23276
	loss_policy_1: 0.02987
	accuracy_policy_1: 0.92848
	loss_value_1: 0.04723
	loss_reward_1: 0.00474
	loss_policy_2: 0.02982
	accuracy_policy_2: 0.92879
	loss_value_2: 0.04842
	loss_reward_2: 0.00553
	loss_policy_3: 0.02993
	accuracy_policy_3: 0.92855
	loss_value_3: 0.04945
	loss_reward_3: 0.00623
	loss_policy_4: 0.03006
	accuracy_policy_4: 0.9293
	loss_value_4: 0.05083
	loss_reward_4: 0.00739
	loss_policy_5: 0.02992
	accuracy_policy_5: 0.93312
	loss_value_5: 0.05206
	loss_reward_5: 0.00851
	loss_policy: 0.3018
	loss_value: 0.48076
	loss_reward: 0.0324
[2025-05-11 14:50:11] nn step 25200, lr: 0.1.
	loss_policy_0: 0.15854
	accuracy_policy_0: 0.9307
	loss_value_0: 0.24868
	loss_policy_1: 0.03151
	accuracy_policy_1: 0.92977
	loss_value_1: 0.05045
	loss_reward_1: 0.0052
	loss_policy_2: 0.03137
	accuracy_policy_2: 0.92988
	loss_value_2: 0.05172
	loss_reward_2: 0.00578
	loss_policy_3: 0.03194
	accuracy_policy_3: 0.92918
	loss_value_3: 0.05265
	loss_reward_3: 0.00683
	loss_policy_4: 0.03155
	accuracy_policy_4: 0.93137
	loss_value_4: 0.05422
	loss_reward_4: 0.0079
	loss_policy_5: 0.0314
	accuracy_policy_5: 0.93062
	loss_value_5: 0.05552
	loss_reward_5: 0.00896
	loss_policy: 0.31632
	loss_value: 0.51323
	loss_reward: 0.03468
Optimization_Done 25200
[2025-05-11 14:51:27] [command] train weight_iter_25200.pkl 108 127
[2025-05-11 14:51:37] nn step 25250, lr: 0.1.
	loss_policy_0: 0.1547
	accuracy_policy_0: 0.92922
	loss_value_0: 0.25115
	loss_policy_1: 0.03066
	accuracy_policy_1: 0.93242
	loss_value_1: 0.05058
	loss_reward_1: 0.00498
	loss_policy_2: 0.03075
	accuracy_policy_2: 0.93164
	loss_value_2: 0.05188
	loss_reward_2: 0.00579
	loss_policy_3: 0.03086
	accuracy_policy_3: 0.92949
	loss_value_3: 0.05312
	loss_reward_3: 0.00652
	loss_policy_4: 0.03043
	accuracy_policy_4: 0.93082
	loss_value_4: 0.05442
	loss_reward_4: 0.0079
	loss_policy_5: 0.03062
	accuracy_policy_5: 0.9323
	loss_value_5: 0.05571
	loss_reward_5: 0.00907
	loss_policy: 0.30803
	loss_value: 0.51685
	loss_reward: 0.03426
[2025-05-11 14:51:45] nn step 25300, lr: 0.1.
	loss_policy_0: 0.14193
	accuracy_policy_0: 0.92996
	loss_value_0: 0.22616
	loss_policy_1: 0.02827
	accuracy_policy_1: 0.92961
	loss_value_1: 0.04594
	loss_reward_1: 0.00448
	loss_policy_2: 0.02845
	accuracy_policy_2: 0.93281
	loss_value_2: 0.04698
	loss_reward_2: 0.00499
	loss_policy_3: 0.02818
	accuracy_policy_3: 0.93074
	loss_value_3: 0.04807
	loss_reward_3: 0.00591
	loss_policy_4: 0.02834
	accuracy_policy_4: 0.92996
	loss_value_4: 0.04931
	loss_reward_4: 0.007
	loss_policy_5: 0.02819
	accuracy_policy_5: 0.93133
	loss_value_5: 0.0507
	loss_reward_5: 0.00798
	loss_policy: 0.28337
	loss_value: 0.46715
	loss_reward: 0.03036
[2025-05-11 14:51:52] nn step 25350, lr: 0.1.
	loss_policy_0: 0.15393
	accuracy_policy_0: 0.93062
	loss_value_0: 0.24573
	loss_policy_1: 0.03073
	accuracy_policy_1: 0.93352
	loss_value_1: 0.05007
	loss_reward_1: 0.00499
	loss_policy_2: 0.03073
	accuracy_policy_2: 0.93051
	loss_value_2: 0.05118
	loss_reward_2: 0.00589
	loss_policy_3: 0.03077
	accuracy_policy_3: 0.93336
	loss_value_3: 0.05236
	loss_reward_3: 0.00631
	loss_policy_4: 0.03082
	accuracy_policy_4: 0.92898
	loss_value_4: 0.05382
	loss_reward_4: 0.00764
	loss_policy_5: 0.03037
	accuracy_policy_5: 0.93422
	loss_value_5: 0.05494
	loss_reward_5: 0.0088
	loss_policy: 0.30735
	loss_value: 0.5081
	loss_reward: 0.03363
[2025-05-11 14:52:01] nn step 25400, lr: 0.1.
	loss_policy_0: 0.13907
	accuracy_policy_0: 0.93301
	loss_value_0: 0.22347
	loss_policy_1: 0.0277
	accuracy_policy_1: 0.93543
	loss_value_1: 0.04538
	loss_reward_1: 0.00447
	loss_policy_2: 0.02807
	accuracy_policy_2: 0.93434
	loss_value_2: 0.0465
	loss_reward_2: 0.00525
	loss_policy_3: 0.0281
	accuracy_policy_3: 0.93422
	loss_value_3: 0.04719
	loss_reward_3: 0.00599
	loss_policy_4: 0.0278
	accuracy_policy_4: 0.93445
	loss_value_4: 0.04882
	loss_reward_4: 0.00698
	loss_policy_5: 0.02788
	accuracy_policy_5: 0.93676
	loss_value_5: 0.04986
	loss_reward_5: 0.0078
	loss_policy: 0.27861
	loss_value: 0.46121
	loss_reward: 0.0305
Optimization_Done 25400
[2025-05-11 14:53:19] [command] train weight_iter_25400.pkl 109 128
[2025-05-11 14:53:28] nn step 25450, lr: 0.1.
	loss_policy_0: 0.15418
	accuracy_policy_0: 0.92801
	loss_value_0: 0.25355
	loss_policy_1: 0.03027
	accuracy_policy_1: 0.93129
	loss_value_1: 0.05183
	loss_reward_1: 0.00495
	loss_policy_2: 0.03057
	accuracy_policy_2: 0.93242
	loss_value_2: 0.0529
	loss_reward_2: 0.00568
	loss_policy_3: 0.03047
	accuracy_policy_3: 0.93191
	loss_value_3: 0.05378
	loss_reward_3: 0.00625
	loss_policy_4: 0.03038
	accuracy_policy_4: 0.93309
	loss_value_4: 0.05474
	loss_reward_4: 0.0075
	loss_policy_5: 0.03028
	accuracy_policy_5: 0.93348
	loss_value_5: 0.05606
	loss_reward_5: 0.00879
	loss_policy: 0.30616
	loss_value: 0.52286
	loss_reward: 0.03316
[2025-05-11 14:53:37] nn step 25500, lr: 0.1.
	loss_policy_0: 0.14574
	accuracy_policy_0: 0.93312
	loss_value_0: 0.23674
	loss_policy_1: 0.02882
	accuracy_policy_1: 0.92973
	loss_value_1: 0.04803
	loss_reward_1: 0.00487
	loss_policy_2: 0.02889
	accuracy_policy_2: 0.92914
	loss_value_2: 0.0491
	loss_reward_2: 0.00537
	loss_policy_3: 0.02895
	accuracy_policy_3: 0.93102
	loss_value_3: 0.05039
	loss_reward_3: 0.00597
	loss_policy_4: 0.02914
	accuracy_policy_4: 0.93098
	loss_value_4: 0.05166
	loss_reward_4: 0.0071
	loss_policy_5: 0.02871
	accuracy_policy_5: 0.93547
	loss_value_5: 0.05283
	loss_reward_5: 0.00874
	loss_policy: 0.29025
	loss_value: 0.48875
	loss_reward: 0.03205
[2025-05-11 14:53:45] nn step 25550, lr: 0.1.
	loss_policy_0: 0.15593
	accuracy_policy_0: 0.93254
	loss_value_0: 0.255
	loss_policy_1: 0.03108
	accuracy_policy_1: 0.93211
	loss_value_1: 0.05144
	loss_reward_1: 0.00501
	loss_policy_2: 0.03094
	accuracy_policy_2: 0.93285
	loss_value_2: 0.05269
	loss_reward_2: 0.00576
	loss_policy_3: 0.03086
	accuracy_policy_3: 0.93262
	loss_value_3: 0.05419
	loss_reward_3: 0.00668
	loss_policy_4: 0.03113
	accuracy_policy_4: 0.93461
	loss_value_4: 0.05579
	loss_reward_4: 0.00793
	loss_policy_5: 0.031
	accuracy_policy_5: 0.93402
	loss_value_5: 0.05716
	loss_reward_5: 0.00923
	loss_policy: 0.31094
	loss_value: 0.52626
	loss_reward: 0.03462
[2025-05-11 14:53:52] nn step 25600, lr: 0.1.
	loss_policy_0: 0.14612
	accuracy_policy_0: 0.9307
	loss_value_0: 0.23777
	loss_policy_1: 0.02922
	accuracy_policy_1: 0.92988
	loss_value_1: 0.04821
	loss_reward_1: 0.00465
	loss_policy_2: 0.02939
	accuracy_policy_2: 0.93066
	loss_value_2: 0.04924
	loss_reward_2: 0.00537
	loss_policy_3: 0.02917
	accuracy_policy_3: 0.93238
	loss_value_3: 0.05056
	loss_reward_3: 0.00618
	loss_policy_4: 0.02945
	accuracy_policy_4: 0.93492
	loss_value_4: 0.05205
	loss_reward_4: 0.00699
	loss_policy_5: 0.02899
	accuracy_policy_5: 0.93512
	loss_value_5: 0.05318
	loss_reward_5: 0.00832
	loss_policy: 0.29233
	loss_value: 0.49102
	loss_reward: 0.03152
Optimization_Done 25600
[2025-05-11 14:55:11] [command] train weight_iter_25600.pkl 110 129
[2025-05-11 14:55:19] nn step 25650, lr: 0.1.
	loss_policy_0: 0.15623
	accuracy_policy_0: 0.92645
	loss_value_0: 0.25484
	loss_policy_1: 0.03118
	accuracy_policy_1: 0.93219
	loss_value_1: 0.05174
	loss_reward_1: 0.00512
	loss_policy_2: 0.03073
	accuracy_policy_2: 0.93277
	loss_value_2: 0.05325
	loss_reward_2: 0.006
	loss_policy_3: 0.03083
	accuracy_policy_3: 0.93016
	loss_value_3: 0.05449
	loss_reward_3: 0.00678
	loss_policy_4: 0.0312
	accuracy_policy_4: 0.93137
	loss_value_4: 0.05567
	loss_reward_4: 0.00792
	loss_policy_5: 0.03097
	accuracy_policy_5: 0.9348
	loss_value_5: 0.05705
	loss_reward_5: 0.00931
	loss_policy: 0.31114
	loss_value: 0.52703
	loss_reward: 0.03513
[2025-05-11 14:55:27] nn step 25700, lr: 0.1.
	loss_policy_0: 0.15622
	accuracy_policy_0: 0.9309
	loss_value_0: 0.25407
	loss_policy_1: 0.03081
	accuracy_policy_1: 0.92977
	loss_value_1: 0.05143
	loss_reward_1: 0.00504
	loss_policy_2: 0.03105
	accuracy_policy_2: 0.92793
	loss_value_2: 0.0528
	loss_reward_2: 0.00565
	loss_policy_3: 0.03069
	accuracy_policy_3: 0.9307
	loss_value_3: 0.05416
	loss_reward_3: 0.00662
	loss_policy_4: 0.03088
	accuracy_policy_4: 0.93188
	loss_value_4: 0.05524
	loss_reward_4: 0.00787
	loss_policy_5: 0.03079
	accuracy_policy_5: 0.93324
	loss_value_5: 0.05665
	loss_reward_5: 0.00918
	loss_policy: 0.31043
	loss_value: 0.52437
	loss_reward: 0.03437
[2025-05-11 14:55:36] nn step 25750, lr: 0.1.
	loss_policy_0: 0.15409
	accuracy_policy_0: 0.93164
	loss_value_0: 0.25072
	loss_policy_1: 0.03041
	accuracy_policy_1: 0.92781
	loss_value_1: 0.05082
	loss_reward_1: 0.00504
	loss_policy_2: 0.03054
	accuracy_policy_2: 0.93125
	loss_value_2: 0.05192
	loss_reward_2: 0.00574
	loss_policy_3: 0.0308
	accuracy_policy_3: 0.93031
	loss_value_3: 0.05309
	loss_reward_3: 0.00652
	loss_policy_4: 0.03068
	accuracy_policy_4: 0.93449
	loss_value_4: 0.05419
	loss_reward_4: 0.00754
	loss_policy_5: 0.03022
	accuracy_policy_5: 0.93566
	loss_value_5: 0.05553
	loss_reward_5: 0.00877
	loss_policy: 0.30673
	loss_value: 0.51627
	loss_reward: 0.03361
[2025-05-11 14:55:43] nn step 25800, lr: 0.1.
	loss_policy_0: 0.14659
	accuracy_policy_0: 0.93074
	loss_value_0: 0.23845
	loss_policy_1: 0.02921
	accuracy_policy_1: 0.93082
	loss_value_1: 0.0484
	loss_reward_1: 0.00477
	loss_policy_2: 0.02928
	accuracy_policy_2: 0.93426
	loss_value_2: 0.04938
	loss_reward_2: 0.00544
	loss_policy_3: 0.02946
	accuracy_policy_3: 0.93363
	loss_value_3: 0.05026
	loss_reward_3: 0.00612
	loss_policy_4: 0.0295
	accuracy_policy_4: 0.93375
	loss_value_4: 0.05157
	loss_reward_4: 0.0072
	loss_policy_5: 0.02935
	accuracy_policy_5: 0.93332
	loss_value_5: 0.05288
	loss_reward_5: 0.00829
	loss_policy: 0.29339
	loss_value: 0.49094
	loss_reward: 0.03182
Optimization_Done 25800
[2025-05-11 14:57:01] [command] train weight_iter_25800.pkl 111 130
[2025-05-11 14:57:09] nn step 25850, lr: 0.1.
	loss_policy_0: 0.14771
	accuracy_policy_0: 0.92727
	loss_value_0: 0.2442
	loss_policy_1: 0.02929
	accuracy_policy_1: 0.92824
	loss_value_1: 0.04938
	loss_reward_1: 0.00481
	loss_policy_2: 0.02953
	accuracy_policy_2: 0.9307
	loss_value_2: 0.05062
	loss_reward_2: 0.00533
	loss_policy_3: 0.02922
	accuracy_policy_3: 0.93051
	loss_value_3: 0.05178
	loss_reward_3: 0.00617
	loss_policy_4: 0.02948
	accuracy_policy_4: 0.93363
	loss_value_4: 0.05298
	loss_reward_4: 0.00746
	loss_policy_5: 0.02898
	accuracy_policy_5: 0.93246
	loss_value_5: 0.05399
	loss_reward_5: 0.00865
	loss_policy: 0.29422
	loss_value: 0.50296
	loss_reward: 0.03242
[2025-05-11 14:57:18] nn step 25900, lr: 0.1.
	loss_policy_0: 0.1564
	accuracy_policy_0: 0.93035
	loss_value_0: 0.25558
	loss_policy_1: 0.0309
	accuracy_policy_1: 0.92793
	loss_value_1: 0.05178
	loss_reward_1: 0.00527
	loss_policy_2: 0.03096
	accuracy_policy_2: 0.93082
	loss_value_2: 0.05284
	loss_reward_2: 0.00574
	loss_policy_3: 0.03088
	accuracy_policy_3: 0.93113
	loss_value_3: 0.05413
	loss_reward_3: 0.00669
	loss_policy_4: 0.03105
	accuracy_policy_4: 0.93199
	loss_value_4: 0.05552
	loss_reward_4: 0.00803
	loss_policy_5: 0.0308
	accuracy_policy_5: 0.9332
	loss_value_5: 0.05674
	loss_reward_5: 0.00912
	loss_policy: 0.31099
	loss_value: 0.5266
	loss_reward: 0.03485
[2025-05-11 14:57:26] nn step 25950, lr: 0.1.
	loss_policy_0: 0.14644
	accuracy_policy_0: 0.92895
	loss_value_0: 0.23416
	loss_policy_1: 0.02918
	accuracy_policy_1: 0.92883
	loss_value_1: 0.04776
	loss_reward_1: 0.00479
	loss_policy_2: 0.02905
	accuracy_policy_2: 0.92785
	loss_value_2: 0.04896
	loss_reward_2: 0.00514
	loss_policy_3: 0.02915
	accuracy_policy_3: 0.92848
	loss_value_3: 0.04996
	loss_reward_3: 0.00611
	loss_policy_4: 0.02908
	accuracy_policy_4: 0.93137
	loss_value_4: 0.0511
	loss_reward_4: 0.00726
	loss_policy_5: 0.02888
	accuracy_policy_5: 0.93512
	loss_value_5: 0.05219
	loss_reward_5: 0.00831
	loss_policy: 0.29177
	loss_value: 0.48414
	loss_reward: 0.03162
[2025-05-11 14:57:33] nn step 26000, lr: 0.1.
	loss_policy_0: 0.15426
	accuracy_policy_0: 0.92836
	loss_value_0: 0.24844
	loss_policy_1: 0.03069
	accuracy_policy_1: 0.92754
	loss_value_1: 0.05041
	loss_reward_1: 0.00523
	loss_policy_2: 0.03063
	accuracy_policy_2: 0.93363
	loss_value_2: 0.05134
	loss_reward_2: 0.00557
	loss_policy_3: 0.03098
	accuracy_policy_3: 0.93289
	loss_value_3: 0.05272
	loss_reward_3: 0.00634
	loss_policy_4: 0.03096
	accuracy_policy_4: 0.9318
	loss_value_4: 0.05389
	loss_reward_4: 0.00773
	loss_policy_5: 0.03023
	accuracy_policy_5: 0.93309
	loss_value_5: 0.05527
	loss_reward_5: 0.00858
	loss_policy: 0.30775
	loss_value: 0.51208
	loss_reward: 0.03345
Optimization_Done 26000
[2025-05-11 14:58:50] [command] train weight_iter_26000.pkl 112 131
[2025-05-11 14:58:58] nn step 26050, lr: 0.1.
	loss_policy_0: 0.15679
	accuracy_policy_0: 0.93047
	loss_value_0: 0.25962
	loss_policy_1: 0.0309
	accuracy_policy_1: 0.93254
	loss_value_1: 0.05272
	loss_reward_1: 0.00512
	loss_policy_2: 0.03111
	accuracy_policy_2: 0.93289
	loss_value_2: 0.05367
	loss_reward_2: 0.00591
	loss_policy_3: 0.03115
	accuracy_policy_3: 0.92883
	loss_value_3: 0.05474
	loss_reward_3: 0.00677
	loss_policy_4: 0.03131
	accuracy_policy_4: 0.93504
	loss_value_4: 0.05607
	loss_reward_4: 0.00788
	loss_policy_5: 0.03073
	accuracy_policy_5: 0.93613
	loss_value_5: 0.05734
	loss_reward_5: 0.00885
	loss_policy: 0.312
	loss_value: 0.53417
	loss_reward: 0.03452
[2025-05-11 14:59:07] nn step 26100, lr: 0.1.
	loss_policy_0: 0.15565
	accuracy_policy_0: 0.93152
	loss_value_0: 0.25794
	loss_policy_1: 0.03121
	accuracy_policy_1: 0.93012
	loss_value_1: 0.05255
	loss_reward_1: 0.00529
	loss_policy_2: 0.03133
	accuracy_policy_2: 0.92855
	loss_value_2: 0.05337
	loss_reward_2: 0.00597
	loss_policy_3: 0.0316
	accuracy_policy_3: 0.93164
	loss_value_3: 0.05467
	loss_reward_3: 0.00677
	loss_policy_4: 0.03141
	accuracy_policy_4: 0.92918
	loss_value_4: 0.05622
	loss_reward_4: 0.00805
	loss_policy_5: 0.03107
	accuracy_policy_5: 0.93047
	loss_value_5: 0.05782
	loss_reward_5: 0.00937
	loss_policy: 0.31227
	loss_value: 0.53256
	loss_reward: 0.03546
[2025-05-11 14:59:15] nn step 26150, lr: 0.1.
	loss_policy_0: 0.14912
	accuracy_policy_0: 0.93086
	loss_value_0: 0.24121
	loss_policy_1: 0.02938
	accuracy_policy_1: 0.93172
	loss_value_1: 0.0491
	loss_reward_1: 0.00506
	loss_policy_2: 0.02922
	accuracy_policy_2: 0.93301
	loss_value_2: 0.05029
	loss_reward_2: 0.00534
	loss_policy_3: 0.02949
	accuracy_policy_3: 0.93316
	loss_value_3: 0.05131
	loss_reward_3: 0.00658
	loss_policy_4: 0.02988
	accuracy_policy_4: 0.93219
	loss_value_4: 0.05258
	loss_reward_4: 0.00755
	loss_policy_5: 0.02925
	accuracy_policy_5: 0.93758
	loss_value_5: 0.05395
	loss_reward_5: 0.00863
	loss_policy: 0.29634
	loss_value: 0.49844
	loss_reward: 0.03316
[2025-05-11 14:59:24] nn step 26200, lr: 0.1.
	loss_policy_0: 0.15083
	accuracy_policy_0: 0.9302
	loss_value_0: 0.24489
	loss_policy_1: 0.02987
	accuracy_policy_1: 0.92859
	loss_value_1: 0.04973
	loss_reward_1: 0.0051
	loss_policy_2: 0.02998
	accuracy_policy_2: 0.9325
	loss_value_2: 0.05065
	loss_reward_2: 0.00568
	loss_policy_3: 0.03004
	accuracy_policy_3: 0.93133
	loss_value_3: 0.05186
	loss_reward_3: 0.00639
	loss_policy_4: 0.02981
	accuracy_policy_4: 0.93145
	loss_value_4: 0.05286
	loss_reward_4: 0.00759
	loss_policy_5: 0.02951
	accuracy_policy_5: 0.93523
	loss_value_5: 0.05435
	loss_reward_5: 0.00858
	loss_policy: 0.30005
	loss_value: 0.50435
	loss_reward: 0.03333
Optimization_Done 26200
[2025-05-11 15:00:37] [command] train weight_iter_26200.pkl 113 132
[2025-05-11 15:00:47] nn step 26250, lr: 0.1.
	loss_policy_0: 0.14932
	accuracy_policy_0: 0.9309
	loss_value_0: 0.24888
	loss_policy_1: 0.02983
	accuracy_policy_1: 0.92824
	loss_value_1: 0.05054
	loss_reward_1: 0.00498
	loss_policy_2: 0.02968
	accuracy_policy_2: 0.93039
	loss_value_2: 0.0519
	loss_reward_2: 0.00557
	loss_policy_3: 0.02949
	accuracy_policy_3: 0.93188
	loss_value_3: 0.05258
	loss_reward_3: 0.00681
	loss_policy_4: 0.02964
	accuracy_policy_4: 0.93223
	loss_value_4: 0.05362
	loss_reward_4: 0.00762
	loss_policy_5: 0.02944
	accuracy_policy_5: 0.93605
	loss_value_5: 0.0549
	loss_reward_5: 0.00893
	loss_policy: 0.2974
	loss_value: 0.51242
	loss_reward: 0.03391
[2025-05-11 15:00:54] nn step 26300, lr: 0.1.
	loss_policy_0: 0.14134
	accuracy_policy_0: 0.9323
	loss_value_0: 0.23566
	loss_policy_1: 0.02821
	accuracy_policy_1: 0.93039
	loss_value_1: 0.04723
	loss_reward_1: 0.00489
	loss_policy_2: 0.02857
	accuracy_policy_2: 0.93359
	loss_value_2: 0.04845
	loss_reward_2: 0.00524
	loss_policy_3: 0.02836
	accuracy_policy_3: 0.93316
	loss_value_3: 0.04972
	loss_reward_3: 0.00597
	loss_policy_4: 0.02824
	accuracy_policy_4: 0.93418
	loss_value_4: 0.05102
	loss_reward_4: 0.0071
	loss_policy_5: 0.0279
	accuracy_policy_5: 0.9352
	loss_value_5: 0.05215
	loss_reward_5: 0.00863
	loss_policy: 0.28262
	loss_value: 0.48423
	loss_reward: 0.03183
[2025-05-11 15:01:03] nn step 26350, lr: 0.1.
	loss_policy_0: 0.15229
	accuracy_policy_0: 0.93051
	loss_value_0: 0.25217
	loss_policy_1: 0.03016
	accuracy_policy_1: 0.92945
	loss_value_1: 0.05118
	loss_reward_1: 0.00511
	loss_policy_2: 0.03031
	accuracy_policy_2: 0.93152
	loss_value_2: 0.0523
	loss_reward_2: 0.00581
	loss_policy_3: 0.03003
	accuracy_policy_3: 0.93148
	loss_value_3: 0.05338
	loss_reward_3: 0.00656
	loss_policy_4: 0.03007
	accuracy_policy_4: 0.93566
	loss_value_4: 0.05468
	loss_reward_4: 0.00807
	loss_policy_5: 0.03004
	accuracy_policy_5: 0.93449
	loss_value_5: 0.05604
	loss_reward_5: 0.00864
	loss_policy: 0.30291
	loss_value: 0.51976
	loss_reward: 0.03419
[2025-05-11 15:01:11] nn step 26400, lr: 0.1.
	loss_policy_0: 0.15555
	accuracy_policy_0: 0.93031
	loss_value_0: 0.2511
	loss_policy_1: 0.03047
	accuracy_policy_1: 0.92984
	loss_value_1: 0.05135
	loss_reward_1: 0.00531
	loss_policy_2: 0.03084
	accuracy_policy_2: 0.93324
	loss_value_2: 0.05196
	loss_reward_2: 0.00583
	loss_policy_3: 0.03035
	accuracy_policy_3: 0.93531
	loss_value_3: 0.05297
	loss_reward_3: 0.00688
	loss_policy_4: 0.0306
	accuracy_policy_4: 0.93391
	loss_value_4: 0.0542
	loss_reward_4: 0.00781
	loss_policy_5: 0.03043
	accuracy_policy_5: 0.93676
	loss_value_5: 0.05588
	loss_reward_5: 0.00889
	loss_policy: 0.30823
	loss_value: 0.51747
	loss_reward: 0.03471
Optimization_Done 26400
[2025-05-11 15:02:30] [command] train weight_iter_26400.pkl 114 133
[2025-05-11 15:02:39] nn step 26450, lr: 0.1.
	loss_policy_0: 0.14518
	accuracy_policy_0: 0.92848
	loss_value_0: 0.24074
	loss_policy_1: 0.02877
	accuracy_policy_1: 0.92758
	loss_value_1: 0.04884
	loss_reward_1: 0.00494
	loss_policy_2: 0.02886
	accuracy_policy_2: 0.93016
	loss_value_2: 0.04982
	loss_reward_2: 0.0056
	loss_policy_3: 0.02887
	accuracy_policy_3: 0.92727
	loss_value_3: 0.05059
	loss_reward_3: 0.00624
	loss_policy_4: 0.0287
	accuracy_policy_4: 0.9334
	loss_value_4: 0.0517
	loss_reward_4: 0.00739
	loss_policy_5: 0.02842
	accuracy_policy_5: 0.93668
	loss_value_5: 0.05317
	loss_reward_5: 0.00872
	loss_policy: 0.2888
	loss_value: 0.49487
	loss_reward: 0.03289
[2025-05-11 15:02:46] nn step 26500, lr: 0.1.
	loss_policy_0: 0.14789
	accuracy_policy_0: 0.9302
	loss_value_0: 0.24202
	loss_policy_1: 0.02951
	accuracy_policy_1: 0.92941
	loss_value_1: 0.049
	loss_reward_1: 0.00486
	loss_policy_2: 0.02937
	accuracy_policy_2: 0.93027
	loss_value_2: 0.04977
	loss_reward_2: 0.00544
	loss_policy_3: 0.0296
	accuracy_policy_3: 0.92977
	loss_value_3: 0.05096
	loss_reward_3: 0.00634
	loss_policy_4: 0.02952
	accuracy_policy_4: 0.93016
	loss_value_4: 0.0522
	loss_reward_4: 0.00755
	loss_policy_5: 0.02948
	accuracy_policy_5: 0.93574
	loss_value_5: 0.0537
	loss_reward_5: 0.00873
	loss_policy: 0.29538
	loss_value: 0.49765
	loss_reward: 0.03291
[2025-05-11 15:02:55] nn step 26550, lr: 0.1.
	loss_policy_0: 0.14776
	accuracy_policy_0: 0.92945
	loss_value_0: 0.24223
	loss_policy_1: 0.02939
	accuracy_policy_1: 0.92719
	loss_value_1: 0.04917
	loss_reward_1: 0.00508
	loss_policy_2: 0.02931
	accuracy_policy_2: 0.9318
	loss_value_2: 0.05007
	loss_reward_2: 0.00549
	loss_policy_3: 0.02944
	accuracy_policy_3: 0.92992
	loss_value_3: 0.05131
	loss_reward_3: 0.00632
	loss_policy_4: 0.02934
	accuracy_policy_4: 0.9293
	loss_value_4: 0.05282
	loss_reward_4: 0.00744
	loss_policy_5: 0.02915
	accuracy_policy_5: 0.93562
	loss_value_5: 0.0543
	loss_reward_5: 0.00888
	loss_policy: 0.29438
	loss_value: 0.49991
	loss_reward: 0.0332
[2025-05-11 15:03:03] nn step 26600, lr: 0.1.
	loss_policy_0: 0.15073
	accuracy_policy_0: 0.92863
	loss_value_0: 0.2468
	loss_policy_1: 0.02984
	accuracy_policy_1: 0.92938
	loss_value_1: 0.04974
	loss_reward_1: 0.00505
	loss_policy_2: 0.02996
	accuracy_policy_2: 0.93117
	loss_value_2: 0.05046
	loss_reward_2: 0.00561
	loss_policy_3: 0.02983
	accuracy_policy_3: 0.93113
	loss_value_3: 0.05146
	loss_reward_3: 0.00661
	loss_policy_4: 0.02975
	accuracy_policy_4: 0.9327
	loss_value_4: 0.05267
	loss_reward_4: 0.00775
	loss_policy_5: 0.02974
	accuracy_policy_5: 0.93375
	loss_value_5: 0.05402
	loss_reward_5: 0.00905
	loss_policy: 0.29985
	loss_value: 0.50515
	loss_reward: 0.03407
Optimization_Done 26600
[2025-05-11 15:04:18] [command] train weight_iter_26600.pkl 115 134
[2025-05-11 15:04:28] nn step 26650, lr: 0.1.
	loss_policy_0: 0.14348
	accuracy_policy_0: 0.92547
	loss_value_0: 0.24004
	loss_policy_1: 0.02811
	accuracy_policy_1: 0.92676
	loss_value_1: 0.04826
	loss_reward_1: 0.00467
	loss_policy_2: 0.02794
	accuracy_policy_2: 0.93105
	loss_value_2: 0.04898
	loss_reward_2: 0.0054
	loss_policy_3: 0.02855
	accuracy_policy_3: 0.93039
	loss_value_3: 0.05002
	loss_reward_3: 0.00629
	loss_policy_4: 0.0283
	accuracy_policy_4: 0.92996
	loss_value_4: 0.05099
	loss_reward_4: 0.00729
	loss_policy_5: 0.02834
	accuracy_policy_5: 0.93336
	loss_value_5: 0.05231
	loss_reward_5: 0.00809
	loss_policy: 0.28472
	loss_value: 0.49061
	loss_reward: 0.03175
[2025-05-11 15:04:36] nn step 26700, lr: 0.1.
	loss_policy_0: 0.14866
	accuracy_policy_0: 0.9277
	loss_value_0: 0.24425
	loss_policy_1: 0.02952
	accuracy_policy_1: 0.92555
	loss_value_1: 0.04935
	loss_reward_1: 0.0049
	loss_policy_2: 0.02924
	accuracy_policy_2: 0.93078
	loss_value_2: 0.05006
	loss_reward_2: 0.00559
	loss_policy_3: 0.02937
	accuracy_policy_3: 0.93223
	loss_value_3: 0.05123
	loss_reward_3: 0.00598
	loss_policy_4: 0.02923
	accuracy_policy_4: 0.93188
	loss_value_4: 0.05253
	loss_reward_4: 0.00717
	loss_policy_5: 0.02903
	accuracy_policy_5: 0.93418
	loss_value_5: 0.05354
	loss_reward_5: 0.00849
	loss_policy: 0.29505
	loss_value: 0.50095
	loss_reward: 0.03212
[2025-05-11 15:04:44] nn step 26750, lr: 0.1.
	loss_policy_0: 0.14334
	accuracy_policy_0: 0.92797
	loss_value_0: 0.23491
	loss_policy_1: 0.02828
	accuracy_policy_1: 0.92656
	loss_value_1: 0.0475
	loss_reward_1: 0.00492
	loss_policy_2: 0.02839
	accuracy_policy_2: 0.92656
	loss_value_2: 0.04826
	loss_reward_2: 0.00519
	loss_policy_3: 0.02852
	accuracy_policy_3: 0.92965
	loss_value_3: 0.04956
	loss_reward_3: 0.00615
	loss_policy_4: 0.02878
	accuracy_policy_4: 0.9316
	loss_value_4: 0.05055
	loss_reward_4: 0.00733
	loss_policy_5: 0.02827
	accuracy_policy_5: 0.93566
	loss_value_5: 0.05149
	loss_reward_5: 0.0081
	loss_policy: 0.28557
	loss_value: 0.48227
	loss_reward: 0.03169
[2025-05-11 15:04:52] nn step 26800, lr: 0.1.
	loss_policy_0: 0.14549
	accuracy_policy_0: 0.9266
	loss_value_0: 0.23563
	loss_policy_1: 0.02891
	accuracy_policy_1: 0.92809
	loss_value_1: 0.04763
	loss_reward_1: 0.00474
	loss_policy_2: 0.02853
	accuracy_policy_2: 0.93387
	loss_value_2: 0.04858
	loss_reward_2: 0.00552
	loss_policy_3: 0.02876
	accuracy_policy_3: 0.93031
	loss_value_3: 0.04952
	loss_reward_3: 0.00616
	loss_policy_4: 0.02874
	accuracy_policy_4: 0.93133
	loss_value_4: 0.05066
	loss_reward_4: 0.00716
	loss_policy_5: 0.0284
	accuracy_policy_5: 0.93473
	loss_value_5: 0.05176
	loss_reward_5: 0.00817
	loss_policy: 0.28884
	loss_value: 0.48378
	loss_reward: 0.03175
Optimization_Done 26800
[2025-05-11 15:06:10] [command] train weight_iter_26800.pkl 116 135
[2025-05-11 15:06:19] nn step 26850, lr: 0.1.
	loss_policy_0: 0.14664
	accuracy_policy_0: 0.93023
	loss_value_0: 0.24511
	loss_policy_1: 0.0289
	accuracy_policy_1: 0.92812
	loss_value_1: 0.04932
	loss_reward_1: 0.00485
	loss_policy_2: 0.0292
	accuracy_policy_2: 0.93121
	loss_value_2: 0.05028
	loss_reward_2: 0.0054
	loss_policy_3: 0.02907
	accuracy_policy_3: 0.93176
	loss_value_3: 0.05128
	loss_reward_3: 0.00619
	loss_policy_4: 0.02915
	accuracy_policy_4: 0.93121
	loss_value_4: 0.05254
	loss_reward_4: 0.00746
	loss_policy_5: 0.0288
	accuracy_policy_5: 0.93766
	loss_value_5: 0.05361
	loss_reward_5: 0.00902
	loss_policy: 0.29176
	loss_value: 0.50214
	loss_reward: 0.03293
[2025-05-11 15:06:28] nn step 26900, lr: 0.1.
	loss_policy_0: 0.15387
	accuracy_policy_0: 0.92895
	loss_value_0: 0.25759
	loss_policy_1: 0.0305
	accuracy_policy_1: 0.92734
	loss_value_1: 0.05226
	loss_reward_1: 0.00531
	loss_policy_2: 0.03045
	accuracy_policy_2: 0.93363
	loss_value_2: 0.05355
	loss_reward_2: 0.00565
	loss_policy_3: 0.03041
	accuracy_policy_3: 0.93387
	loss_value_3: 0.0544
	loss_reward_3: 0.00649
	loss_policy_4: 0.03037
	accuracy_policy_4: 0.9325
	loss_value_4: 0.05552
	loss_reward_4: 0.00796
	loss_policy_5: 0.03051
	accuracy_policy_5: 0.93641
	loss_value_5: 0.05695
	loss_reward_5: 0.00929
	loss_policy: 0.30611
	loss_value: 0.53027
	loss_reward: 0.0347
[2025-05-11 15:06:35] nn step 26950, lr: 0.1.
	loss_policy_0: 0.15081
	accuracy_policy_0: 0.9291
	loss_value_0: 0.25244
	loss_policy_1: 0.02993
	accuracy_policy_1: 0.9273
	loss_value_1: 0.05113
	loss_reward_1: 0.00514
	loss_policy_2: 0.02968
	accuracy_policy_2: 0.93441
	loss_value_2: 0.05216
	loss_reward_2: 0.00566
	loss_policy_3: 0.03033
	accuracy_policy_3: 0.93152
	loss_value_3: 0.05316
	loss_reward_3: 0.00644
	loss_policy_4: 0.03003
	accuracy_policy_4: 0.93152
	loss_value_4: 0.05441
	loss_reward_4: 0.00783
	loss_policy_5: 0.02973
	accuracy_policy_5: 0.93629
	loss_value_5: 0.05555
	loss_reward_5: 0.00903
	loss_policy: 0.3005
	loss_value: 0.51884
	loss_reward: 0.03409
[2025-05-11 15:06:43] nn step 27000, lr: 0.1.
	loss_policy_0: 0.14638
	accuracy_policy_0: 0.92965
	loss_value_0: 0.24219
	loss_policy_1: 0.02931
	accuracy_policy_1: 0.92898
	loss_value_1: 0.0488
	loss_reward_1: 0.00491
	loss_policy_2: 0.02898
	accuracy_policy_2: 0.9334
	loss_value_2: 0.04994
	loss_reward_2: 0.00516
	loss_policy_3: 0.02941
	accuracy_policy_3: 0.93074
	loss_value_3: 0.05108
	loss_reward_3: 0.00617
	loss_policy_4: 0.02944
	accuracy_policy_4: 0.93297
	loss_value_4: 0.0526
	loss_reward_4: 0.00734
	loss_policy_5: 0.02915
	accuracy_policy_5: 0.93684
	loss_value_5: 0.05407
	loss_reward_5: 0.00809
	loss_policy: 0.29268
	loss_value: 0.49867
	loss_reward: 0.03168
Optimization_Done 27000
[2025-05-11 15:08:01] [command] train weight_iter_27000.pkl 117 136
[2025-05-11 15:08:10] nn step 27050, lr: 0.1.
	loss_policy_0: 0.14132
	accuracy_policy_0: 0.92984
	loss_value_0: 0.2415
	loss_policy_1: 0.02839
	accuracy_policy_1: 0.92707
	loss_value_1: 0.04831
	loss_reward_1: 0.00468
	loss_policy_2: 0.02814
	accuracy_policy_2: 0.93004
	loss_value_2: 0.04935
	loss_reward_2: 0.00521
	loss_policy_3: 0.02817
	accuracy_policy_3: 0.9284
	loss_value_3: 0.05031
	loss_reward_3: 0.00576
	loss_policy_4: 0.0283
	accuracy_policy_4: 0.93172
	loss_value_4: 0.05138
	loss_reward_4: 0.00727
	loss_policy_5: 0.02777
	accuracy_policy_5: 0.93781
	loss_value_5: 0.05274
	loss_reward_5: 0.008
	loss_policy: 0.28208
	loss_value: 0.49358
	loss_reward: 0.03091
[2025-05-11 15:08:19] nn step 27100, lr: 0.1.
	loss_policy_0: 0.14503
	accuracy_policy_0: 0.93184
	loss_value_0: 0.24444
	loss_policy_1: 0.02848
	accuracy_policy_1: 0.92637
	loss_value_1: 0.04923
	loss_reward_1: 0.00483
	loss_policy_2: 0.02869
	accuracy_policy_2: 0.93203
	loss_value_2: 0.05009
	loss_reward_2: 0.00547
	loss_policy_3: 0.02906
	accuracy_policy_3: 0.93262
	loss_value_3: 0.05111
	loss_reward_3: 0.0062
	loss_policy_4: 0.02888
	accuracy_policy_4: 0.9366
	loss_value_4: 0.05247
	loss_reward_4: 0.00712
	loss_policy_5: 0.02845
	accuracy_policy_5: 0.93754
	loss_value_5: 0.05387
	loss_reward_5: 0.0083
	loss_policy: 0.28859
	loss_value: 0.5012
	loss_reward: 0.03192
[2025-05-11 15:08:27] nn step 27150, lr: 0.1.
	loss_policy_0: 0.14052
	accuracy_policy_0: 0.93137
	loss_value_0: 0.23561
	loss_policy_1: 0.02781
	accuracy_policy_1: 0.92422
	loss_value_1: 0.04769
	loss_reward_1: 0.00487
	loss_policy_2: 0.02796
	accuracy_policy_2: 0.93211
	loss_value_2: 0.04849
	loss_reward_2: 0.00511
	loss_policy_3: 0.02799
	accuracy_policy_3: 0.93094
	loss_value_3: 0.0494
	loss_reward_3: 0.0061
	loss_policy_4: 0.02823
	accuracy_policy_4: 0.93055
	loss_value_4: 0.05057
	loss_reward_4: 0.00701
	loss_policy_5: 0.02817
	accuracy_policy_5: 0.9334
	loss_value_5: 0.05215
	loss_reward_5: 0.00813
	loss_policy: 0.28067
	loss_value: 0.48391
	loss_reward: 0.03122
[2025-05-11 15:08:34] nn step 27200, lr: 0.1.
	loss_policy_0: 0.14525
	accuracy_policy_0: 0.92891
	loss_value_0: 0.24522
	loss_policy_1: 0.02895
	accuracy_policy_1: 0.92797
	loss_value_1: 0.04972
	loss_reward_1: 0.00487
	loss_policy_2: 0.02888
	accuracy_policy_2: 0.93094
	loss_value_2: 0.05106
	loss_reward_2: 0.00539
	loss_policy_3: 0.02863
	accuracy_policy_3: 0.93094
	loss_value_3: 0.05195
	loss_reward_3: 0.0062
	loss_policy_4: 0.02906
	accuracy_policy_4: 0.93379
	loss_value_4: 0.05296
	loss_reward_4: 0.0073
	loss_policy_5: 0.02902
	accuracy_policy_5: 0.93621
	loss_value_5: 0.0546
	loss_reward_5: 0.00847
	loss_policy: 0.2898
	loss_value: 0.50551
	loss_reward: 0.03223
Optimization_Done 27200
[2025-05-11 15:09:52] [command] train weight_iter_27200.pkl 118 137
[2025-05-11 15:10:00] nn step 27250, lr: 0.1.
	loss_policy_0: 0.1437
	accuracy_policy_0: 0.93113
	loss_value_0: 0.24531
	loss_policy_1: 0.02816
	accuracy_policy_1: 0.92855
	loss_value_1: 0.04928
	loss_reward_1: 0.00493
	loss_policy_2: 0.02847
	accuracy_policy_2: 0.93195
	loss_value_2: 0.05002
	loss_reward_2: 0.0052
	loss_policy_3: 0.0281
	accuracy_policy_3: 0.93414
	loss_value_3: 0.05084
	loss_reward_3: 0.00601
	loss_policy_4: 0.02848
	accuracy_policy_4: 0.93199
	loss_value_4: 0.05208
	loss_reward_4: 0.00731
	loss_policy_5: 0.02828
	accuracy_policy_5: 0.9373
	loss_value_5: 0.05371
	loss_reward_5: 0.00822
	loss_policy: 0.28518
	loss_value: 0.50125
	loss_reward: 0.03166
[2025-05-11 15:10:08] nn step 27300, lr: 0.1.
	loss_policy_0: 0.14117
	accuracy_policy_0: 0.9316
	loss_value_0: 0.23952
	loss_policy_1: 0.02788
	accuracy_policy_1: 0.92578
	loss_value_1: 0.04831
	loss_reward_1: 0.00471
	loss_policy_2: 0.0283
	accuracy_policy_2: 0.9316
	loss_value_2: 0.04941
	loss_reward_2: 0.00533
	loss_policy_3: 0.02811
	accuracy_policy_3: 0.92934
	loss_value_3: 0.05002
	loss_reward_3: 0.00614
	loss_policy_4: 0.02811
	accuracy_policy_4: 0.93266
	loss_value_4: 0.05118
	loss_reward_4: 0.00712
	loss_policy_5: 0.02822
	accuracy_policy_5: 0.93719
	loss_value_5: 0.0526
	loss_reward_5: 0.00823
	loss_policy: 0.28179
	loss_value: 0.49103
	loss_reward: 0.03153
[2025-05-11 15:10:17] nn step 27350, lr: 0.1.
	loss_policy_0: 0.13844
	accuracy_policy_0: 0.93133
	loss_value_0: 0.23624
	loss_policy_1: 0.02756
	accuracy_policy_1: 0.92836
	loss_value_1: 0.04742
	loss_reward_1: 0.00455
	loss_policy_2: 0.02769
	accuracy_policy_2: 0.93309
	loss_value_2: 0.0485
	loss_reward_2: 0.00509
	loss_policy_3: 0.02787
	accuracy_policy_3: 0.92938
	loss_value_3: 0.04937
	loss_reward_3: 0.00613
	loss_policy_4: 0.02792
	accuracy_policy_4: 0.93047
	loss_value_4: 0.05049
	loss_reward_4: 0.00698
	loss_policy_5: 0.02752
	accuracy_policy_5: 0.93543
	loss_value_5: 0.05173
	loss_reward_5: 0.00801
	loss_policy: 0.27698
	loss_value: 0.48375
	loss_reward: 0.03076
[2025-05-11 15:10:24] nn step 27400, lr: 0.1.
	loss_policy_0: 0.13519
	accuracy_policy_0: 0.92805
	loss_value_0: 0.22504
	loss_policy_1: 0.02655
	accuracy_policy_1: 0.92699
	loss_value_1: 0.04541
	loss_reward_1: 0.00444
	loss_policy_2: 0.0264
	accuracy_policy_2: 0.92918
	loss_value_2: 0.04612
	loss_reward_2: 0.0048
	loss_policy_3: 0.02683
	accuracy_policy_3: 0.92883
	loss_value_3: 0.04723
	loss_reward_3: 0.00564
	loss_policy_4: 0.02663
	accuracy_policy_4: 0.9334
	loss_value_4: 0.04827
	loss_reward_4: 0.00639
	loss_policy_5: 0.02672
	accuracy_policy_5: 0.9373
	loss_value_5: 0.04958
	loss_reward_5: 0.00748
	loss_policy: 0.26832
	loss_value: 0.46164
	loss_reward: 0.02876
Optimization_Done 27400
[2025-05-11 15:11:42] [command] train weight_iter_27400.pkl 119 138
[2025-05-11 15:11:50] nn step 27450, lr: 0.1.
	loss_policy_0: 0.13755
	accuracy_policy_0: 0.9352
	loss_value_0: 0.24572
	loss_policy_1: 0.02753
	accuracy_policy_1: 0.92793
	loss_value_1: 0.0491
	loss_reward_1: 0.00475
	loss_policy_2: 0.0273
	accuracy_policy_2: 0.93473
	loss_value_2: 0.04984
	loss_reward_2: 0.00531
	loss_policy_3: 0.02763
	accuracy_policy_3: 0.93375
	loss_value_3: 0.05081
	loss_reward_3: 0.00598
	loss_policy_4: 0.02757
	accuracy_policy_4: 0.93293
	loss_value_4: 0.052
	loss_reward_4: 0.00712
	loss_policy_5: 0.02757
	accuracy_policy_5: 0.9407
	loss_value_5: 0.05312
	loss_reward_5: 0.00814
	loss_policy: 0.27516
	loss_value: 0.50059
	loss_reward: 0.03129
[2025-05-11 15:11:58] nn step 27500, lr: 0.1.
	loss_policy_0: 0.14997
	accuracy_policy_0: 0.93254
	loss_value_0: 0.25664
	loss_policy_1: 0.02975
	accuracy_policy_1: 0.92566
	loss_value_1: 0.05161
	loss_reward_1: 0.00506
	loss_policy_2: 0.02981
	accuracy_policy_2: 0.93574
	loss_value_2: 0.05266
	loss_reward_2: 0.00564
	loss_policy_3: 0.02968
	accuracy_policy_3: 0.93281
	loss_value_3: 0.05368
	loss_reward_3: 0.00657
	loss_policy_4: 0.02975
	accuracy_policy_4: 0.93305
	loss_value_4: 0.05491
	loss_reward_4: 0.00806
	loss_policy_5: 0.02977
	accuracy_policy_5: 0.93641
	loss_value_5: 0.05619
	loss_reward_5: 0.00867
	loss_policy: 0.29873
	loss_value: 0.52569
	loss_reward: 0.03399
[2025-05-11 15:12:07] nn step 27550, lr: 0.1.
	loss_policy_0: 0.14288
	accuracy_policy_0: 0.93438
	loss_value_0: 0.24446
	loss_policy_1: 0.02878
	accuracy_policy_1: 0.92625
	loss_value_1: 0.04933
	loss_reward_1: 0.00498
	loss_policy_2: 0.02842
	accuracy_policy_2: 0.93266
	loss_value_2: 0.05052
	loss_reward_2: 0.00539
	loss_policy_3: 0.02852
	accuracy_policy_3: 0.93141
	loss_value_3: 0.05129
	loss_reward_3: 0.00628
	loss_policy_4: 0.02898
	accuracy_policy_4: 0.93254
	loss_value_4: 0.05251
	loss_reward_4: 0.0074
	loss_policy_5: 0.02851
	accuracy_policy_5: 0.93688
	loss_value_5: 0.05363
	loss_reward_5: 0.00833
	loss_policy: 0.2861
	loss_value: 0.50175
	loss_reward: 0.03238
[2025-05-11 15:12:15] nn step 27600, lr: 0.1.
	loss_policy_0: 0.13783
	accuracy_policy_0: 0.93113
	loss_value_0: 0.23107
	loss_policy_1: 0.02736
	accuracy_policy_1: 0.92746
	loss_value_1: 0.04665
	loss_reward_1: 0.00456
	loss_policy_2: 0.02724
	accuracy_policy_2: 0.93234
	loss_value_2: 0.04778
	loss_reward_2: 0.00504
	loss_policy_3: 0.02721
	accuracy_policy_3: 0.93418
	loss_value_3: 0.04886
	loss_reward_3: 0.00589
	loss_policy_4: 0.02756
	accuracy_policy_4: 0.93586
	loss_value_4: 0.04972
	loss_reward_4: 0.00673
	loss_policy_5: 0.02675
	accuracy_policy_5: 0.93906
	loss_value_5: 0.05101
	loss_reward_5: 0.00797
	loss_policy: 0.27394
	loss_value: 0.4751
	loss_reward: 0.03019
Optimization_Done 27600
[2025-05-11 15:13:31] [command] train weight_iter_27600.pkl 120 139
[2025-05-11 15:13:41] nn step 27650, lr: 0.1.
	loss_policy_0: 0.14321
	accuracy_policy_0: 0.93133
	loss_value_0: 0.25388
	loss_policy_1: 0.02848
	accuracy_policy_1: 0.92785
	loss_value_1: 0.051
	loss_reward_1: 0.00485
	loss_policy_2: 0.02885
	accuracy_policy_2: 0.92953
	loss_value_2: 0.05181
	loss_reward_2: 0.00538
	loss_policy_3: 0.02856
	accuracy_policy_3: 0.93027
	loss_value_3: 0.05278
	loss_reward_3: 0.00636
	loss_policy_4: 0.02844
	accuracy_policy_4: 0.93453
	loss_value_4: 0.05425
	loss_reward_4: 0.00711
	loss_policy_5: 0.02852
	accuracy_policy_5: 0.93672
	loss_value_5: 0.05565
	loss_reward_5: 0.00822
	loss_policy: 0.28606
	loss_value: 0.51938
	loss_reward: 0.03191
[2025-05-11 15:13:48] nn step 27700, lr: 0.1.
	loss_policy_0: 0.143
	accuracy_policy_0: 0.9273
	loss_value_0: 0.24839
	loss_policy_1: 0.02835
	accuracy_policy_1: 0.92395
	loss_value_1: 0.05004
	loss_reward_1: 0.00469
	loss_policy_2: 0.02834
	accuracy_policy_2: 0.93012
	loss_value_2: 0.05117
	loss_reward_2: 0.0053
	loss_policy_3: 0.02812
	accuracy_policy_3: 0.92992
	loss_value_3: 0.05226
	loss_reward_3: 0.0062
	loss_policy_4: 0.02831
	accuracy_policy_4: 0.93008
	loss_value_4: 0.05343
	loss_reward_4: 0.00716
	loss_policy_5: 0.02878
	accuracy_policy_5: 0.93121
	loss_value_5: 0.05484
	loss_reward_5: 0.00811
	loss_policy: 0.2849
	loss_value: 0.51014
	loss_reward: 0.03145
[2025-05-11 15:13:56] nn step 27750, lr: 0.1.
	loss_policy_0: 0.15037
	accuracy_policy_0: 0.92949
	loss_value_0: 0.26129
	loss_policy_1: 0.02978
	accuracy_policy_1: 0.92434
	loss_value_1: 0.05284
	loss_reward_1: 0.0049
	loss_policy_2: 0.02983
	accuracy_policy_2: 0.93082
	loss_value_2: 0.05399
	loss_reward_2: 0.00557
	loss_policy_3: 0.02987
	accuracy_policy_3: 0.92891
	loss_value_3: 0.05476
	loss_reward_3: 0.00645
	loss_policy_4: 0.02978
	accuracy_policy_4: 0.93191
	loss_value_4: 0.05612
	loss_reward_4: 0.00744
	loss_policy_5: 0.02951
	accuracy_policy_5: 0.93648
	loss_value_5: 0.05722
	loss_reward_5: 0.00887
	loss_policy: 0.29913
	loss_value: 0.53622
	loss_reward: 0.03324
[2025-05-11 15:14:05] nn step 27800, lr: 0.1.
	loss_policy_0: 0.13579
	accuracy_policy_0: 0.92789
	loss_value_0: 0.23459
	loss_policy_1: 0.0268
	accuracy_policy_1: 0.92543
	loss_value_1: 0.04731
	loss_reward_1: 0.00479
	loss_policy_2: 0.02693
	accuracy_policy_2: 0.93074
	loss_value_2: 0.0485
	loss_reward_2: 0.00499
	loss_policy_3: 0.027
	accuracy_policy_3: 0.92797
	loss_value_3: 0.04934
	loss_reward_3: 0.0058
	loss_policy_4: 0.02709
	accuracy_policy_4: 0.93047
	loss_value_4: 0.05022
	loss_reward_4: 0.007
	loss_policy_5: 0.02707
	accuracy_policy_5: 0.93145
	loss_value_5: 0.0514
	loss_reward_5: 0.00777
	loss_policy: 0.27068
	loss_value: 0.48136
	loss_reward: 0.03036
Optimization_Done 27800
[2025-05-11 15:15:23] [command] train weight_iter_27800.pkl 121 140
[2025-05-11 15:15:32] nn step 27850, lr: 0.1.
	loss_policy_0: 0.14395
	accuracy_policy_0: 0.9302
	loss_value_0: 0.25737
	loss_policy_1: 0.02831
	accuracy_policy_1: 0.92754
	loss_value_1: 0.05158
	loss_reward_1: 0.00475
	loss_policy_2: 0.02871
	accuracy_policy_2: 0.93047
	loss_value_2: 0.05214
	loss_reward_2: 0.00536
	loss_policy_3: 0.02875
	accuracy_policy_3: 0.93086
	loss_value_3: 0.0534
	loss_reward_3: 0.00616
	loss_policy_4: 0.02834
	accuracy_policy_4: 0.93371
	loss_value_4: 0.05427
	loss_reward_4: 0.00742
	loss_policy_5: 0.02872
	accuracy_policy_5: 0.93453
	loss_value_5: 0.05541
	loss_reward_5: 0.0085
	loss_policy: 0.28678
	loss_value: 0.52417
	loss_reward: 0.0322
[2025-05-11 15:15:40] nn step 27900, lr: 0.1.
	loss_policy_0: 0.13794
	accuracy_policy_0: 0.92969
	loss_value_0: 0.24112
	loss_policy_1: 0.02744
	accuracy_policy_1: 0.92164
	loss_value_1: 0.0485
	loss_reward_1: 0.00457
	loss_policy_2: 0.02713
	accuracy_policy_2: 0.93094
	loss_value_2: 0.04946
	loss_reward_2: 0.0051
	loss_policy_3: 0.02754
	accuracy_policy_3: 0.92754
	loss_value_3: 0.05044
	loss_reward_3: 0.00572
	loss_policy_4: 0.02741
	accuracy_policy_4: 0.92988
	loss_value_4: 0.05158
	loss_reward_4: 0.00698
	loss_policy_5: 0.02734
	accuracy_policy_5: 0.93629
	loss_value_5: 0.05274
	loss_reward_5: 0.00783
	loss_policy: 0.27481
	loss_value: 0.49383
	loss_reward: 0.03021
[2025-05-11 15:15:48] nn step 27950, lr: 0.1.
	loss_policy_0: 0.14281
	accuracy_policy_0: 0.92816
	loss_value_0: 0.25343
	loss_policy_1: 0.02901
	accuracy_policy_1: 0.92445
	loss_value_1: 0.05085
	loss_reward_1: 0.00507
	loss_policy_2: 0.02861
	accuracy_policy_2: 0.92871
	loss_value_2: 0.05191
	loss_reward_2: 0.00583
	loss_policy_3: 0.02838
	accuracy_policy_3: 0.93023
	loss_value_3: 0.0532
	loss_reward_3: 0.00675
	loss_policy_4: 0.02876
	accuracy_policy_4: 0.93277
	loss_value_4: 0.05428
	loss_reward_4: 0.0078
	loss_policy_5: 0.02842
	accuracy_policy_5: 0.93449
	loss_value_5: 0.0557
	loss_reward_5: 0.00885
	loss_policy: 0.28599
	loss_value: 0.51937
	loss_reward: 0.0343
[2025-05-11 15:15:57] nn step 28000, lr: 0.1.
	loss_policy_0: 0.14639
	accuracy_policy_0: 0.9275
	loss_value_0: 0.2544
	loss_policy_1: 0.02895
	accuracy_policy_1: 0.92379
	loss_value_1: 0.05135
	loss_reward_1: 0.00484
	loss_policy_2: 0.02931
	accuracy_policy_2: 0.92848
	loss_value_2: 0.05247
	loss_reward_2: 0.00547
	loss_policy_3: 0.02916
	accuracy_policy_3: 0.92738
	loss_value_3: 0.05341
	loss_reward_3: 0.00631
	loss_policy_4: 0.02929
	accuracy_policy_4: 0.93129
	loss_value_4: 0.05438
	loss_reward_4: 0.00744
	loss_policy_5: 0.02891
	accuracy_policy_5: 0.93113
	loss_value_5: 0.05568
	loss_reward_5: 0.00826
	loss_policy: 0.29202
	loss_value: 0.52169
	loss_reward: 0.03233
Optimization_Done 28000
[2025-05-11 15:17:14] [command] train weight_iter_28000.pkl 122 141
[2025-05-11 15:17:24] nn step 28050, lr: 0.1.
	loss_policy_0: 0.14072
	accuracy_policy_0: 0.92406
	loss_value_0: 0.25153
	loss_policy_1: 0.02798
	accuracy_policy_1: 0.92316
	loss_value_1: 0.05042
	loss_reward_1: 0.00482
	loss_policy_2: 0.02794
	accuracy_policy_2: 0.92562
	loss_value_2: 0.05135
	loss_reward_2: 0.00546
	loss_policy_3: 0.0281
	accuracy_policy_3: 0.92941
	loss_value_3: 0.05213
	loss_reward_3: 0.00599
	loss_policy_4: 0.02814
	accuracy_policy_4: 0.93012
	loss_value_4: 0.05324
	loss_reward_4: 0.00737
	loss_policy_5: 0.02792
	accuracy_policy_5: 0.93141
	loss_value_5: 0.05459
	loss_reward_5: 0.00819
	loss_policy: 0.2808
	loss_value: 0.51327
	loss_reward: 0.03182
[2025-05-11 15:17:31] nn step 28100, lr: 0.1.
	loss_policy_0: 0.13439
	accuracy_policy_0: 0.92512
	loss_value_0: 0.2353
	loss_policy_1: 0.02677
	accuracy_policy_1: 0.92367
	loss_value_1: 0.04739
	loss_reward_1: 0.0045
	loss_policy_2: 0.02671
	accuracy_policy_2: 0.92688
	loss_value_2: 0.04801
	loss_reward_2: 0.005
	loss_policy_3: 0.02695
	accuracy_policy_3: 0.93125
	loss_value_3: 0.04881
	loss_reward_3: 0.0057
	loss_policy_4: 0.0267
	accuracy_policy_4: 0.93191
	loss_value_4: 0.0497
	loss_reward_4: 0.00664
	loss_policy_5: 0.02633
	accuracy_policy_5: 0.93449
	loss_value_5: 0.05095
	loss_reward_5: 0.00761
	loss_policy: 0.26785
	loss_value: 0.48014
	loss_reward: 0.02944
[2025-05-11 15:17:40] nn step 28150, lr: 0.1.
	loss_policy_0: 0.13598
	accuracy_policy_0: 0.92965
	loss_value_0: 0.23693
	loss_policy_1: 0.02717
	accuracy_policy_1: 0.92414
	loss_value_1: 0.04786
	loss_reward_1: 0.0045
	loss_policy_2: 0.02753
	accuracy_policy_2: 0.93043
	loss_value_2: 0.04864
	loss_reward_2: 0.00533
	loss_policy_3: 0.02726
	accuracy_policy_3: 0.92883
	loss_value_3: 0.04971
	loss_reward_3: 0.00594
	loss_policy_4: 0.02738
	accuracy_policy_4: 0.93121
	loss_value_4: 0.05083
	loss_reward_4: 0.00696
	loss_policy_5: 0.02695
	accuracy_policy_5: 0.93668
	loss_value_5: 0.05218
	loss_reward_5: 0.00794
	loss_policy: 0.27227
	loss_value: 0.48615
	loss_reward: 0.03068
[2025-05-11 15:17:48] nn step 28200, lr: 0.1.
	loss_policy_0: 0.13762
	accuracy_policy_0: 0.92469
	loss_value_0: 0.24004
	loss_policy_1: 0.02768
	accuracy_policy_1: 0.92332
	loss_value_1: 0.04841
	loss_reward_1: 0.00475
	loss_policy_2: 0.02793
	accuracy_policy_2: 0.92695
	loss_value_2: 0.04901
	loss_reward_2: 0.0052
	loss_policy_3: 0.02785
	accuracy_policy_3: 0.92727
	loss_value_3: 0.04997
	loss_reward_3: 0.0059
	loss_policy_4: 0.02774
	accuracy_policy_4: 0.93176
	loss_value_4: 0.0512
	loss_reward_4: 0.00697
	loss_policy_5: 0.02762
	accuracy_policy_5: 0.93504
	loss_value_5: 0.05244
	loss_reward_5: 0.00821
	loss_policy: 0.27644
	loss_value: 0.49106
	loss_reward: 0.03103
Optimization_Done 28200
[2025-05-11 15:19:05] [command] train weight_iter_28200.pkl 123 142
[2025-05-11 15:19:15] nn step 28250, lr: 0.1.
	loss_policy_0: 0.14021
	accuracy_policy_0: 0.93
	loss_value_0: 0.25054
	loss_policy_1: 0.02792
	accuracy_policy_1: 0.92629
	loss_value_1: 0.05048
	loss_reward_1: 0.00462
	loss_policy_2: 0.02797
	accuracy_policy_2: 0.93156
	loss_value_2: 0.05103
	loss_reward_2: 0.00524
	loss_policy_3: 0.02817
	accuracy_policy_3: 0.93055
	loss_value_3: 0.05204
	loss_reward_3: 0.00618
	loss_policy_4: 0.02819
	accuracy_policy_4: 0.93516
	loss_value_4: 0.05312
	loss_reward_4: 0.00747
	loss_policy_5: 0.02779
	accuracy_policy_5: 0.93941
	loss_value_5: 0.0541
	loss_reward_5: 0.00824
	loss_policy: 0.28025
	loss_value: 0.51132
	loss_reward: 0.03175
[2025-05-11 15:19:22] nn step 28300, lr: 0.1.
	loss_policy_0: 0.15127
	accuracy_policy_0: 0.92871
	loss_value_0: 0.26185
	loss_policy_1: 0.02977
	accuracy_policy_1: 0.92551
	loss_value_1: 0.05252
	loss_reward_1: 0.00501
	loss_policy_2: 0.02967
	accuracy_policy_2: 0.9298
	loss_value_2: 0.05335
	loss_reward_2: 0.00568
	loss_policy_3: 0.03
	accuracy_policy_3: 0.92852
	loss_value_3: 0.05441
	loss_reward_3: 0.00626
	loss_policy_4: 0.02979
	accuracy_policy_4: 0.93262
	loss_value_4: 0.05566
	loss_reward_4: 0.00766
	loss_policy_5: 0.02956
	accuracy_policy_5: 0.93691
	loss_value_5: 0.05685
	loss_reward_5: 0.0087
	loss_policy: 0.30005
	loss_value: 0.53463
	loss_reward: 0.03331
[2025-05-11 15:19:31] nn step 28350, lr: 0.1.
	loss_policy_0: 0.13182
	accuracy_policy_0: 0.92973
	loss_value_0: 0.2297
	loss_policy_1: 0.02645
	accuracy_policy_1: 0.92262
	loss_value_1: 0.04646
	loss_reward_1: 0.00444
	loss_policy_2: 0.02654
	accuracy_policy_2: 0.92996
	loss_value_2: 0.04758
	loss_reward_2: 0.00487
	loss_policy_3: 0.02681
	accuracy_policy_3: 0.93008
	loss_value_3: 0.04858
	loss_reward_3: 0.00591
	loss_policy_4: 0.0264
	accuracy_policy_4: 0.93004
	loss_value_4: 0.04933
	loss_reward_4: 0.00695
	loss_policy_5: 0.02626
	accuracy_policy_5: 0.93645
	loss_value_5: 0.05021
	loss_reward_5: 0.00798
	loss_policy: 0.26427
	loss_value: 0.47186
	loss_reward: 0.03015
[2025-05-11 15:19:39] nn step 28400, lr: 0.1.
	loss_policy_0: 0.14438
	accuracy_policy_0: 0.92867
	loss_value_0: 0.25009
	loss_policy_1: 0.02856
	accuracy_policy_1: 0.92398
	loss_value_1: 0.05046
	loss_reward_1: 0.00513
	loss_policy_2: 0.02893
	accuracy_policy_2: 0.9282
	loss_value_2: 0.05113
	loss_reward_2: 0.00526
	loss_policy_3: 0.02888
	accuracy_policy_3: 0.93156
	loss_value_3: 0.05206
	loss_reward_3: 0.00613
	loss_policy_4: 0.02863
	accuracy_policy_4: 0.93336
	loss_value_4: 0.05324
	loss_reward_4: 0.00734
	loss_policy_5: 0.02854
	accuracy_policy_5: 0.93867
	loss_value_5: 0.05459
	loss_reward_5: 0.00861
	loss_policy: 0.28792
	loss_value: 0.51156
	loss_reward: 0.03247
Optimization_Done 28400
[2025-05-11 15:20:57] [command] train weight_iter_28400.pkl 124 143
[2025-05-11 15:21:06] nn step 28450, lr: 0.1.
	loss_policy_0: 0.13532
	accuracy_policy_0: 0.92973
	loss_value_0: 0.2394
	loss_policy_1: 0.02671
	accuracy_policy_1: 0.92496
	loss_value_1: 0.04796
	loss_reward_1: 0.00456
	loss_policy_2: 0.02679
	accuracy_policy_2: 0.9284
	loss_value_2: 0.04859
	loss_reward_2: 0.00491
	loss_policy_3: 0.02661
	accuracy_policy_3: 0.92953
	loss_value_3: 0.04935
	loss_reward_3: 0.0056
	loss_policy_4: 0.02676
	accuracy_policy_4: 0.92883
	loss_value_4: 0.05035
	loss_reward_4: 0.0069
	loss_policy_5: 0.02662
	accuracy_policy_5: 0.93836
	loss_value_5: 0.05147
	loss_reward_5: 0.00793
	loss_policy: 0.26881
	loss_value: 0.48712
	loss_reward: 0.02992
[2025-05-11 15:21:15] nn step 28500, lr: 0.1.
	loss_policy_0: 0.13447
	accuracy_policy_0: 0.92961
	loss_value_0: 0.23632
	loss_policy_1: 0.02692
	accuracy_policy_1: 0.92289
	loss_value_1: 0.04763
	loss_reward_1: 0.00446
	loss_policy_2: 0.0269
	accuracy_policy_2: 0.92758
	loss_value_2: 0.04817
	loss_reward_2: 0.00491
	loss_policy_3: 0.02672
	accuracy_policy_3: 0.92914
	loss_value_3: 0.04938
	loss_reward_3: 0.00577
	loss_policy_4: 0.02709
	accuracy_policy_4: 0.93234
	loss_value_4: 0.05004
	loss_reward_4: 0.00683
	loss_policy_5: 0.02648
	accuracy_policy_5: 0.9368
	loss_value_5: 0.05093
	loss_reward_5: 0.00755
	loss_policy: 0.26857
	loss_value: 0.48247
	loss_reward: 0.02953
[2025-05-11 15:21:22] nn step 28550, lr: 0.1.
	loss_policy_0: 0.14493
	accuracy_policy_0: 0.92578
	loss_value_0: 0.25371
	loss_policy_1: 0.02901
	accuracy_policy_1: 0.92176
	loss_value_1: 0.05109
	loss_reward_1: 0.00491
	loss_policy_2: 0.02885
	accuracy_policy_2: 0.92836
	loss_value_2: 0.05186
	loss_reward_2: 0.00548
	loss_policy_3: 0.02931
	accuracy_policy_3: 0.93008
	loss_value_3: 0.05269
	loss_reward_3: 0.0061
	loss_policy_4: 0.02908
	accuracy_policy_4: 0.93137
	loss_value_4: 0.05373
	loss_reward_4: 0.00741
	loss_policy_5: 0.02887
	accuracy_policy_5: 0.93934
	loss_value_5: 0.05533
	loss_reward_5: 0.00848
	loss_policy: 0.29004
	loss_value: 0.5184
	loss_reward: 0.03238
[2025-05-11 15:21:30] nn step 28600, lr: 0.1.
	loss_policy_0: 0.14128
	accuracy_policy_0: 0.92656
	loss_value_0: 0.24801
	loss_policy_1: 0.02784
	accuracy_policy_1: 0.92406
	loss_value_1: 0.04964
	loss_reward_1: 0.0048
	loss_policy_2: 0.02793
	accuracy_policy_2: 0.92816
	loss_value_2: 0.05054
	loss_reward_2: 0.00526
	loss_policy_3: 0.02768
	accuracy_policy_3: 0.92992
	loss_value_3: 0.05173
	loss_reward_3: 0.00603
	loss_policy_4: 0.02779
	accuracy_policy_4: 0.93191
	loss_value_4: 0.0527
	loss_reward_4: 0.00734
	loss_policy_5: 0.02753
	accuracy_policy_5: 0.93805
	loss_value_5: 0.05419
	loss_reward_5: 0.00862
	loss_policy: 0.28007
	loss_value: 0.50681
	loss_reward: 0.03205
Optimization_Done 28600
[2025-05-11 15:22:46] [command] train weight_iter_28600.pkl 125 144
[2025-05-11 15:22:56] nn step 28650, lr: 0.1.
	loss_policy_0: 0.13446
	accuracy_policy_0: 0.92965
	loss_value_0: 0.24156
	loss_policy_1: 0.02681
	accuracy_policy_1: 0.92312
	loss_value_1: 0.04829
	loss_reward_1: 0.00463
	loss_policy_2: 0.0268
	accuracy_policy_2: 0.93145
	loss_value_2: 0.04917
	loss_reward_2: 0.00496
	loss_policy_3: 0.02659
	accuracy_policy_3: 0.92949
	loss_value_3: 0.05026
	loss_reward_3: 0.00589
	loss_policy_4: 0.02672
	accuracy_policy_4: 0.93352
	loss_value_4: 0.05141
	loss_reward_4: 0.0066
	loss_policy_5: 0.02672
	accuracy_policy_5: 0.93762
	loss_value_5: 0.05249
	loss_reward_5: 0.00775
	loss_policy: 0.2681
	loss_value: 0.49318
	loss_reward: 0.02983
[2025-05-11 15:23:05] nn step 28700, lr: 0.1.
	loss_policy_0: 0.13994
	accuracy_policy_0: 0.92898
	loss_value_0: 0.24291
	loss_policy_1: 0.02763
	accuracy_policy_1: 0.92727
	loss_value_1: 0.04893
	loss_reward_1: 0.00469
	loss_policy_2: 0.02755
	accuracy_policy_2: 0.93031
	loss_value_2: 0.04971
	loss_reward_2: 0.00509
	loss_policy_3: 0.02778
	accuracy_policy_3: 0.9298
	loss_value_3: 0.05081
	loss_reward_3: 0.0058
	loss_policy_4: 0.02754
	accuracy_policy_4: 0.93375
	loss_value_4: 0.05194
	loss_reward_4: 0.00705
	loss_policy_5: 0.02777
	accuracy_policy_5: 0.93793
	loss_value_5: 0.05321
	loss_reward_5: 0.00786
	loss_policy: 0.27821
	loss_value: 0.49752
	loss_reward: 0.0305
[2025-05-11 15:23:12] nn step 28750, lr: 0.1.
	loss_policy_0: 0.143
	accuracy_policy_0: 0.92629
	loss_value_0: 0.25246
	loss_policy_1: 0.02838
	accuracy_policy_1: 0.92457
	loss_value_1: 0.05087
	loss_reward_1: 0.00492
	loss_policy_2: 0.02832
	accuracy_policy_2: 0.93043
	loss_value_2: 0.05184
	loss_reward_2: 0.00532
	loss_policy_3: 0.02835
	accuracy_policy_3: 0.92879
	loss_value_3: 0.05316
	loss_reward_3: 0.00603
	loss_policy_4: 0.0288
	accuracy_policy_4: 0.93324
	loss_value_4: 0.05412
	loss_reward_4: 0.0074
	loss_policy_5: 0.02867
	accuracy_policy_5: 0.9391
	loss_value_5: 0.05531
	loss_reward_5: 0.00799
	loss_policy: 0.28552
	loss_value: 0.51775
	loss_reward: 0.03166
[2025-05-11 15:23:20] nn step 28800, lr: 0.1.
	loss_policy_0: 0.13186
	accuracy_policy_0: 0.92836
	loss_value_0: 0.22715
	loss_policy_1: 0.02626
	accuracy_policy_1: 0.92414
	loss_value_1: 0.0459
	loss_reward_1: 0.00441
	loss_policy_2: 0.02614
	accuracy_policy_2: 0.93164
	loss_value_2: 0.04681
	loss_reward_2: 0.00488
	loss_policy_3: 0.02631
	accuracy_policy_3: 0.93441
	loss_value_3: 0.04774
	loss_reward_3: 0.00569
	loss_policy_4: 0.02627
	accuracy_policy_4: 0.93223
	loss_value_4: 0.04889
	loss_reward_4: 0.0068
	loss_policy_5: 0.02618
	accuracy_policy_5: 0.93656
	loss_value_5: 0.04999
	loss_reward_5: 0.00753
	loss_policy: 0.26302
	loss_value: 0.46648
	loss_reward: 0.02931
Optimization_Done 28800
[2025-05-11 15:24:38] [command] train weight_iter_28800.pkl 126 145
[2025-05-11 15:24:47] nn step 28850, lr: 0.1.
	loss_policy_0: 0.13611
	accuracy_policy_0: 0.93055
	loss_value_0: 0.24716
	loss_policy_1: 0.0269
	accuracy_policy_1: 0.92547
	loss_value_1: 0.04965
	loss_reward_1: 0.0048
	loss_policy_2: 0.02706
	accuracy_policy_2: 0.92836
	loss_value_2: 0.05055
	loss_reward_2: 0.00534
	loss_policy_3: 0.02724
	accuracy_policy_3: 0.93281
	loss_value_3: 0.05111
	loss_reward_3: 0.00571
	loss_policy_4: 0.02751
	accuracy_policy_4: 0.93238
	loss_value_4: 0.05225
	loss_reward_4: 0.00706
	loss_policy_5: 0.02741
	accuracy_policy_5: 0.93832
	loss_value_5: 0.05341
	loss_reward_5: 0.00823
	loss_policy: 0.27223
	loss_value: 0.50413
	loss_reward: 0.03114
[2025-05-11 15:24:56] nn step 28900, lr: 0.1.
	loss_policy_0: 0.13018
	accuracy_policy_0: 0.92738
	loss_value_0: 0.22962
	loss_policy_1: 0.02576
	accuracy_policy_1: 0.92355
	loss_value_1: 0.04604
	loss_reward_1: 0.00447
	loss_policy_2: 0.02566
	accuracy_policy_2: 0.9282
	loss_value_2: 0.04684
	loss_reward_2: 0.00517
	loss_policy_3: 0.02571
	accuracy_policy_3: 0.93426
	loss_value_3: 0.04758
	loss_reward_3: 0.0059
	loss_policy_4: 0.02571
	accuracy_policy_4: 0.93438
	loss_value_4: 0.04829
	loss_reward_4: 0.00697
	loss_policy_5: 0.02551
	accuracy_policy_5: 0.93816
	loss_value_5: 0.04979
	loss_reward_5: 0.0078
	loss_policy: 0.25853
	loss_value: 0.46817
	loss_reward: 0.0303
[2025-05-11 15:25:03] nn step 28950, lr: 0.1.
	loss_policy_0: 0.13825
	accuracy_policy_0: 0.92824
	loss_value_0: 0.24722
	loss_policy_1: 0.02718
	accuracy_policy_1: 0.92523
	loss_value_1: 0.04963
	loss_reward_1: 0.0047
	loss_policy_2: 0.02759
	accuracy_policy_2: 0.92766
	loss_value_2: 0.05033
	loss_reward_2: 0.00526
	loss_policy_3: 0.02737
	accuracy_policy_3: 0.93059
	loss_value_3: 0.05092
	loss_reward_3: 0.00605
	loss_policy_4: 0.0274
	accuracy_policy_4: 0.93676
	loss_value_4: 0.05216
	loss_reward_4: 0.00715
	loss_policy_5: 0.02793
	accuracy_policy_5: 0.93906
	loss_value_5: 0.05366
	loss_reward_5: 0.00803
	loss_policy: 0.27572
	loss_value: 0.50391
	loss_reward: 0.03119
[2025-05-11 15:25:11] nn step 29000, lr: 0.1.
	loss_policy_0: 0.13603
	accuracy_policy_0: 0.92715
	loss_value_0: 0.23789
	loss_policy_1: 0.0271
	accuracy_policy_1: 0.92602
	loss_value_1: 0.04782
	loss_reward_1: 0.00462
	loss_policy_2: 0.02718
	accuracy_policy_2: 0.9309
	loss_value_2: 0.04876
	loss_reward_2: 0.00501
	loss_policy_3: 0.02757
	accuracy_policy_3: 0.93
	loss_value_3: 0.0497
	loss_reward_3: 0.00574
	loss_policy_4: 0.02721
	accuracy_policy_4: 0.93293
	loss_value_4: 0.05083
	loss_reward_4: 0.00676
	loss_policy_5: 0.02741
	accuracy_policy_5: 0.93668
	loss_value_5: 0.05185
	loss_reward_5: 0.00782
	loss_policy: 0.2725
	loss_value: 0.48685
	loss_reward: 0.02995
Optimization_Done 29000
[2025-05-11 15:26:30] [command] train weight_iter_29000.pkl 127 146
[2025-05-11 15:26:40] nn step 29050, lr: 0.1.
	loss_policy_0: 0.12907
	accuracy_policy_0: 0.93207
	loss_value_0: 0.23391
	loss_policy_1: 0.02584
	accuracy_policy_1: 0.92449
	loss_value_1: 0.04676
	loss_reward_1: 0.00438
	loss_policy_2: 0.02554
	accuracy_policy_2: 0.9318
	loss_value_2: 0.04774
	loss_reward_2: 0.00469
	loss_policy_3: 0.02589
	accuracy_policy_3: 0.9318
	loss_value_3: 0.04857
	loss_reward_3: 0.00584
	loss_policy_4: 0.02575
	accuracy_policy_4: 0.93766
	loss_value_4: 0.0497
	loss_reward_4: 0.00653
	loss_policy_5: 0.02533
	accuracy_policy_5: 0.9368
	loss_value_5: 0.05086
	loss_reward_5: 0.00743
	loss_policy: 0.25742
	loss_value: 0.47755
	loss_reward: 0.02887
[2025-05-11 15:26:48] nn step 29100, lr: 0.1.
	loss_policy_0: 0.13202
	accuracy_policy_0: 0.93199
	loss_value_0: 0.23113
	loss_policy_1: 0.02599
	accuracy_policy_1: 0.92844
	loss_value_1: 0.04641
	loss_reward_1: 0.0045
	loss_policy_2: 0.02636
	accuracy_policy_2: 0.93633
	loss_value_2: 0.04715
	loss_reward_2: 0.00477
	loss_policy_3: 0.02615
	accuracy_policy_3: 0.93691
	loss_value_3: 0.0483
	loss_reward_3: 0.0054
	loss_policy_4: 0.02628
	accuracy_policy_4: 0.93621
	loss_value_4: 0.04912
	loss_reward_4: 0.00631
	loss_policy_5: 0.02634
	accuracy_policy_5: 0.94059
	loss_value_5: 0.05036
	loss_reward_5: 0.00712
	loss_policy: 0.26315
	loss_value: 0.47247
	loss_reward: 0.02809
[2025-05-11 15:26:57] nn step 29150, lr: 0.1.
	loss_policy_0: 0.13468
	accuracy_policy_0: 0.93219
	loss_value_0: 0.23521
	loss_policy_1: 0.02682
	accuracy_policy_1: 0.92816
	loss_value_1: 0.0477
	loss_reward_1: 0.00454
	loss_policy_2: 0.02672
	accuracy_policy_2: 0.93277
	loss_value_2: 0.04869
	loss_reward_2: 0.00485
	loss_policy_3: 0.02667
	accuracy_policy_3: 0.93289
	loss_value_3: 0.0494
	loss_reward_3: 0.00559
	loss_policy_4: 0.0267
	accuracy_policy_4: 0.93699
	loss_value_4: 0.05039
	loss_reward_4: 0.00697
	loss_policy_5: 0.02687
	accuracy_policy_5: 0.93852
	loss_value_5: 0.05154
	loss_reward_5: 0.00762
	loss_policy: 0.26846
	loss_value: 0.48293
	loss_reward: 0.02957
[2025-05-11 15:27:04] nn step 29200, lr: 0.1.
	loss_policy_0: 0.14025
	accuracy_policy_0: 0.92969
	loss_value_0: 0.24394
	loss_policy_1: 0.02782
	accuracy_policy_1: 0.92609
	loss_value_1: 0.04921
	loss_reward_1: 0.00474
	loss_policy_2: 0.02812
	accuracy_policy_2: 0.9291
	loss_value_2: 0.05
	loss_reward_2: 0.00507
	loss_policy_3: 0.02828
	accuracy_policy_3: 0.92891
	loss_value_3: 0.05083
	loss_reward_3: 0.00588
	loss_policy_4: 0.02797
	accuracy_policy_4: 0.93305
	loss_value_4: 0.05191
	loss_reward_4: 0.00685
	loss_policy_5: 0.02802
	accuracy_policy_5: 0.93637
	loss_value_5: 0.05314
	loss_reward_5: 0.0078
	loss_policy: 0.28044
	loss_value: 0.49903
	loss_reward: 0.03034
Optimization_Done 29200
[2025-05-11 15:28:22] [command] train weight_iter_29200.pkl 128 147
[2025-05-11 15:28:30] nn step 29250, lr: 0.1.
	loss_policy_0: 0.13095
	accuracy_policy_0: 0.93605
	loss_value_0: 0.22551
	loss_policy_1: 0.02551
	accuracy_policy_1: 0.93281
	loss_value_1: 0.04515
	loss_reward_1: 0.00422
	loss_policy_2: 0.0256
	accuracy_policy_2: 0.93668
	loss_value_2: 0.04608
	loss_reward_2: 0.00461
	loss_policy_3: 0.02574
	accuracy_policy_3: 0.93539
	loss_value_3: 0.04687
	loss_reward_3: 0.00531
	loss_policy_4: 0.02555
	accuracy_policy_4: 0.93906
	loss_value_4: 0.0476
	loss_reward_4: 0.00598
	loss_policy_5: 0.02541
	accuracy_policy_5: 0.93699
	loss_value_5: 0.04876
	loss_reward_5: 0.00696
	loss_policy: 0.25875
	loss_value: 0.45997
	loss_reward: 0.02709
[2025-05-11 15:28:39] nn step 29300, lr: 0.1.
	loss_policy_0: 0.13826
	accuracy_policy_0: 0.9348
	loss_value_0: 0.2347
	loss_policy_1: 0.02732
	accuracy_policy_1: 0.92887
	loss_value_1: 0.04736
	loss_reward_1: 0.00475
	loss_policy_2: 0.02748
	accuracy_policy_2: 0.93379
	loss_value_2: 0.04841
	loss_reward_2: 0.00515
	loss_policy_3: 0.02718
	accuracy_policy_3: 0.93395
	loss_value_3: 0.04933
	loss_reward_3: 0.00566
	loss_policy_4: 0.02739
	accuracy_policy_4: 0.93637
	loss_value_4: 0.05037
	loss_reward_4: 0.0066
	loss_policy_5: 0.02728
	accuracy_policy_5: 0.93766
	loss_value_5: 0.05169
	loss_reward_5: 0.00758
	loss_policy: 0.2749
	loss_value: 0.48187
	loss_reward: 0.02973
[2025-05-11 15:28:47] nn step 29350, lr: 0.1.
	loss_policy_0: 0.14145
	accuracy_policy_0: 0.93363
	loss_value_0: 0.24261
	loss_policy_1: 0.02806
	accuracy_policy_1: 0.93
	loss_value_1: 0.04908
	loss_reward_1: 0.00474
	loss_policy_2: 0.02826
	accuracy_policy_2: 0.93215
	loss_value_2: 0.04993
	loss_reward_2: 0.00575
	loss_policy_3: 0.02836
	accuracy_policy_3: 0.93109
	loss_value_3: 0.05092
	loss_reward_3: 0.00596
	loss_policy_4: 0.02839
	accuracy_policy_4: 0.93391
	loss_value_4: 0.0521
	loss_reward_4: 0.00738
	loss_policy_5: 0.02803
	accuracy_policy_5: 0.93863
	loss_value_5: 0.05348
	loss_reward_5: 0.00865
	loss_policy: 0.28256
	loss_value: 0.49813
	loss_reward: 0.03248
[2025-05-11 15:28:54] nn step 29400, lr: 0.1.
	loss_policy_0: 0.14204
	accuracy_policy_0: 0.93355
	loss_value_0: 0.24063
	loss_policy_1: 0.02798
	accuracy_policy_1: 0.93078
	loss_value_1: 0.04849
	loss_reward_1: 0.00466
	loss_policy_2: 0.02838
	accuracy_policy_2: 0.93406
	loss_value_2: 0.04959
	loss_reward_2: 0.00502
	loss_policy_3: 0.02838
	accuracy_policy_3: 0.93543
	loss_value_3: 0.05071
	loss_reward_3: 0.00579
	loss_policy_4: 0.02806
	accuracy_policy_4: 0.93645
	loss_value_4: 0.05159
	loss_reward_4: 0.00703
	loss_policy_5: 0.0279
	accuracy_policy_5: 0.93824
	loss_value_5: 0.05272
	loss_reward_5: 0.00772
	loss_policy: 0.28275
	loss_value: 0.49374
	loss_reward: 0.03022
Optimization_Done 29400
[2025-05-11 15:30:13] [command] train weight_iter_29400.pkl 129 148
[2025-05-11 15:30:21] nn step 29450, lr: 0.1.
	loss_policy_0: 0.14022
	accuracy_policy_0: 0.93664
	loss_value_0: 0.24223
	loss_policy_1: 0.02726
	accuracy_policy_1: 0.93512
	loss_value_1: 0.0486
	loss_reward_1: 0.00458
	loss_policy_2: 0.02724
	accuracy_policy_2: 0.93449
	loss_value_2: 0.04949
	loss_reward_2: 0.00512
	loss_policy_3: 0.02752
	accuracy_policy_3: 0.93621
	loss_value_3: 0.05014
	loss_reward_3: 0.00573
	loss_policy_4: 0.02734
	accuracy_policy_4: 0.93715
	loss_value_4: 0.05161
	loss_reward_4: 0.00691
	loss_policy_5: 0.02767
	accuracy_policy_5: 0.93859
	loss_value_5: 0.05273
	loss_reward_5: 0.00782
	loss_policy: 0.27724
	loss_value: 0.49479
	loss_reward: 0.03017
[2025-05-11 15:30:30] nn step 29500, lr: 0.1.
	loss_policy_0: 0.13291
	accuracy_policy_0: 0.93816
	loss_value_0: 0.2257
	loss_policy_1: 0.02645
	accuracy_policy_1: 0.93305
	loss_value_1: 0.0456
	loss_reward_1: 0.00433
	loss_policy_2: 0.02654
	accuracy_policy_2: 0.93375
	loss_value_2: 0.04659
	loss_reward_2: 0.00473
	loss_policy_3: 0.02676
	accuracy_policy_3: 0.93312
	loss_value_3: 0.04766
	loss_reward_3: 0.00547
	loss_policy_4: 0.02669
	accuracy_policy_4: 0.93637
	loss_value_4: 0.04901
	loss_reward_4: 0.00649
	loss_policy_5: 0.02676
	accuracy_policy_5: 0.94047
	loss_value_5: 0.05029
	loss_reward_5: 0.00726
	loss_policy: 0.26611
	loss_value: 0.46484
	loss_reward: 0.02827
[2025-05-11 15:30:38] nn step 29550, lr: 0.1.
	loss_policy_0: 0.13746
	accuracy_policy_0: 0.93516
	loss_value_0: 0.23444
	loss_policy_1: 0.02741
	accuracy_policy_1: 0.93293
	loss_value_1: 0.04747
	loss_reward_1: 0.0043
	loss_policy_2: 0.02728
	accuracy_policy_2: 0.93324
	loss_value_2: 0.04834
	loss_reward_2: 0.00505
	loss_policy_3: 0.02745
	accuracy_policy_3: 0.93504
	loss_value_3: 0.04914
	loss_reward_3: 0.0058
	loss_policy_4: 0.02761
	accuracy_policy_4: 0.93566
	loss_value_4: 0.05032
	loss_reward_4: 0.00691
	loss_policy_5: 0.02708
	accuracy_policy_5: 0.94027
	loss_value_5: 0.05175
	loss_reward_5: 0.00785
	loss_policy: 0.2743
	loss_value: 0.48147
	loss_reward: 0.02991
[2025-05-11 15:30:45] nn step 29600, lr: 0.1.
	loss_policy_0: 0.14766
	accuracy_policy_0: 0.93801
	loss_value_0: 0.25048
	loss_policy_1: 0.0295
	accuracy_policy_1: 0.93504
	loss_value_1: 0.05042
	loss_reward_1: 0.00484
	loss_policy_2: 0.02959
	accuracy_policy_2: 0.93348
	loss_value_2: 0.05142
	loss_reward_2: 0.00553
	loss_policy_3: 0.02931
	accuracy_policy_3: 0.93527
	loss_value_3: 0.05232
	loss_reward_3: 0.00629
	loss_policy_4: 0.0294
	accuracy_policy_4: 0.93809
	loss_value_4: 0.05367
	loss_reward_4: 0.00703
	loss_policy_5: 0.02943
	accuracy_policy_5: 0.93918
	loss_value_5: 0.05508
	loss_reward_5: 0.0085
	loss_policy: 0.29489
	loss_value: 0.5134
	loss_reward: 0.0322
Optimization_Done 29600
[2025-05-11 15:32:02] [command] train weight_iter_29600.pkl 130 149
[2025-05-11 15:32:09] nn step 29650, lr: 0.1.
	loss_policy_0: 0.14345
	accuracy_policy_0: 0.9377
	loss_value_0: 0.24784
	loss_policy_1: 0.02797
	accuracy_policy_1: 0.93551
	loss_value_1: 0.0497
	loss_reward_1: 0.00484
	loss_policy_2: 0.0284
	accuracy_policy_2: 0.93438
	loss_value_2: 0.05085
	loss_reward_2: 0.00543
	loss_policy_3: 0.02819
	accuracy_policy_3: 0.93566
	loss_value_3: 0.05169
	loss_reward_3: 0.00587
	loss_policy_4: 0.02829
	accuracy_policy_4: 0.94152
	loss_value_4: 0.05256
	loss_reward_4: 0.00704
	loss_policy_5: 0.02828
	accuracy_policy_5: 0.94164
	loss_value_5: 0.05374
	loss_reward_5: 0.00822
	loss_policy: 0.28457
	loss_value: 0.50638
	loss_reward: 0.0314
[2025-05-11 15:32:18] nn step 29700, lr: 0.1.
	loss_policy_0: 0.13944
	accuracy_policy_0: 0.93895
	loss_value_0: 0.23824
	loss_policy_1: 0.0278
	accuracy_policy_1: 0.93508
	loss_value_1: 0.04791
	loss_reward_1: 0.00479
	loss_policy_2: 0.0278
	accuracy_policy_2: 0.93523
	loss_value_2: 0.04889
	loss_reward_2: 0.00543
	loss_policy_3: 0.02769
	accuracy_policy_3: 0.93598
	loss_value_3: 0.04981
	loss_reward_3: 0.00578
	loss_policy_4: 0.02786
	accuracy_policy_4: 0.93965
	loss_value_4: 0.05109
	loss_reward_4: 0.00693
	loss_policy_5: 0.02742
	accuracy_policy_5: 0.94004
	loss_value_5: 0.05239
	loss_reward_5: 0.00801
	loss_policy: 0.27801
	loss_value: 0.48832
	loss_reward: 0.03093
[2025-05-11 15:32:27] nn step 29750, lr: 0.1.
	loss_policy_0: 0.1382
	accuracy_policy_0: 0.9382
	loss_value_0: 0.23654
	loss_policy_1: 0.02751
	accuracy_policy_1: 0.93336
	loss_value_1: 0.04781
	loss_reward_1: 0.00463
	loss_policy_2: 0.02768
	accuracy_policy_2: 0.93543
	loss_value_2: 0.04846
	loss_reward_2: 0.00498
	loss_policy_3: 0.02757
	accuracy_policy_3: 0.93672
	loss_value_3: 0.04906
	loss_reward_3: 0.00571
	loss_policy_4: 0.02763
	accuracy_policy_4: 0.94023
	loss_value_4: 0.05029
	loss_reward_4: 0.00678
	loss_policy_5: 0.02762
	accuracy_policy_5: 0.94289
	loss_value_5: 0.05151
	loss_reward_5: 0.00815
	loss_policy: 0.27621
	loss_value: 0.48368
	loss_reward: 0.03025
[2025-05-11 15:32:36] nn step 29800, lr: 0.1.
	loss_policy_0: 0.13852
	accuracy_policy_0: 0.93809
	loss_value_0: 0.23515
	loss_policy_1: 0.02766
	accuracy_policy_1: 0.93309
	loss_value_1: 0.04783
	loss_reward_1: 0.0046
	loss_policy_2: 0.0278
	accuracy_policy_2: 0.93309
	loss_value_2: 0.04892
	loss_reward_2: 0.00534
	loss_policy_3: 0.02762
	accuracy_policy_3: 0.93355
	loss_value_3: 0.04985
	loss_reward_3: 0.00591
	loss_policy_4: 0.02787
	accuracy_policy_4: 0.93844
	loss_value_4: 0.05094
	loss_reward_4: 0.00681
	loss_policy_5: 0.02783
	accuracy_policy_5: 0.93828
	loss_value_5: 0.05252
	loss_reward_5: 0.00804
	loss_policy: 0.2773
	loss_value: 0.48521
	loss_reward: 0.03071
Optimization_Done 29800
[2025-05-11 15:33:52] [command] train weight_iter_29800.pkl 131 150
[2025-05-11 15:34:01] nn step 29850, lr: 0.1.
	loss_policy_0: 0.1368
	accuracy_policy_0: 0.93863
	loss_value_0: 0.24111
	loss_policy_1: 0.02728
	accuracy_policy_1: 0.93457
	loss_value_1: 0.04844
	loss_reward_1: 0.00467
	loss_policy_2: 0.02723
	accuracy_policy_2: 0.93523
	loss_value_2: 0.04922
	loss_reward_2: 0.00521
	loss_policy_3: 0.02746
	accuracy_policy_3: 0.93715
	loss_value_3: 0.05049
	loss_reward_3: 0.00611
	loss_policy_4: 0.02737
	accuracy_policy_4: 0.93777
	loss_value_4: 0.05147
	loss_reward_4: 0.00686
	loss_policy_5: 0.02732
	accuracy_policy_5: 0.94043
	loss_value_5: 0.05247
	loss_reward_5: 0.0079
	loss_policy: 0.27346
	loss_value: 0.4932
	loss_reward: 0.03075
[2025-05-11 15:34:09] nn step 29900, lr: 0.1.
	loss_policy_0: 0.14066
	accuracy_policy_0: 0.93609
	loss_value_0: 0.24105
	loss_policy_1: 0.02834
	accuracy_policy_1: 0.93305
	loss_value_1: 0.04873
	loss_reward_1: 0.00467
	loss_policy_2: 0.02817
	accuracy_policy_2: 0.93211
	loss_value_2: 0.04962
	loss_reward_2: 0.00521
	loss_policy_3: 0.02806
	accuracy_policy_3: 0.93629
	loss_value_3: 0.05034
	loss_reward_3: 0.00595
	loss_policy_4: 0.02795
	accuracy_policy_4: 0.93809
	loss_value_4: 0.05152
	loss_reward_4: 0.00686
	loss_policy_5: 0.02822
	accuracy_policy_5: 0.93945
	loss_value_5: 0.05282
	loss_reward_5: 0.00803
	loss_policy: 0.2814
	loss_value: 0.49408
	loss_reward: 0.03072
[2025-05-11 15:34:17] nn step 29950, lr: 0.1.
	loss_policy_0: 0.13187
	accuracy_policy_0: 0.93781
	loss_value_0: 0.22271
	loss_policy_1: 0.02587
	accuracy_policy_1: 0.93387
	loss_value_1: 0.04489
	loss_reward_1: 0.00434
	loss_policy_2: 0.02581
	accuracy_policy_2: 0.93641
	loss_value_2: 0.04598
	loss_reward_2: 0.00495
	loss_policy_3: 0.02627
	accuracy_policy_3: 0.93391
	loss_value_3: 0.04698
	loss_reward_3: 0.00555
	loss_policy_4: 0.02614
	accuracy_policy_4: 0.94102
	loss_value_4: 0.04835
	loss_reward_4: 0.00626
	loss_policy_5: 0.02606
	accuracy_policy_5: 0.93797
	loss_value_5: 0.04961
	loss_reward_5: 0.00743
	loss_policy: 0.26202
	loss_value: 0.4585
	loss_reward: 0.02853
[2025-05-11 15:34:26] nn step 30000, lr: 0.1.
	loss_policy_0: 0.14299
	accuracy_policy_0: 0.93863
	loss_value_0: 0.24297
	loss_policy_1: 0.0287
	accuracy_policy_1: 0.93551
	loss_value_1: 0.04893
	loss_reward_1: 0.00465
	loss_policy_2: 0.02909
	accuracy_policy_2: 0.93527
	loss_value_2: 0.05
	loss_reward_2: 0.00544
	loss_policy_3: 0.02914
	accuracy_policy_3: 0.93496
	loss_value_3: 0.05104
	loss_reward_3: 0.00583
	loss_policy_4: 0.02889
	accuracy_policy_4: 0.9418
	loss_value_4: 0.05206
	loss_reward_4: 0.00721
	loss_policy_5: 0.02869
	accuracy_policy_5: 0.94078
	loss_value_5: 0.05351
	loss_reward_5: 0.00812
	loss_policy: 0.28749
	loss_value: 0.4985
	loss_reward: 0.03124
Optimization_Done 30000
[2025-05-11 15:35:44] [command] train weight_iter_30000.pkl 132 151
[2025-05-11 15:35:53] nn step 30050, lr: 0.1.
	loss_policy_0: 0.12648
	accuracy_policy_0: 0.94055
	loss_value_0: 0.22165
	loss_policy_1: 0.02523
	accuracy_policy_1: 0.93828
	loss_value_1: 0.04451
	loss_reward_1: 0.00438
	loss_policy_2: 0.02514
	accuracy_policy_2: 0.94027
	loss_value_2: 0.04538
	loss_reward_2: 0.00471
	loss_policy_3: 0.02552
	accuracy_policy_3: 0.93734
	loss_value_3: 0.04601
	loss_reward_3: 0.00521
	loss_policy_4: 0.02512
	accuracy_policy_4: 0.94164
	loss_value_4: 0.04691
	loss_reward_4: 0.00616
	loss_policy_5: 0.02546
	accuracy_policy_5: 0.94434
	loss_value_5: 0.04772
	loss_reward_5: 0.00761
	loss_policy: 0.25296
	loss_value: 0.45217
	loss_reward: 0.02807
[2025-05-11 15:36:00] nn step 30100, lr: 0.1.
	loss_policy_0: 0.13358
	accuracy_policy_0: 0.94191
	loss_value_0: 0.22263
	loss_policy_1: 0.0264
	accuracy_policy_1: 0.9352
	loss_value_1: 0.04512
	loss_reward_1: 0.00462
	loss_policy_2: 0.02613
	accuracy_policy_2: 0.93848
	loss_value_2: 0.046
	loss_reward_2: 0.0049
	loss_policy_3: 0.02662
	accuracy_policy_3: 0.93605
	loss_value_3: 0.04685
	loss_reward_3: 0.00577
	loss_policy_4: 0.02655
	accuracy_policy_4: 0.93926
	loss_value_4: 0.04787
	loss_reward_4: 0.00652
	loss_policy_5: 0.02619
	accuracy_policy_5: 0.9434
	loss_value_5: 0.04896
	loss_reward_5: 0.00774
	loss_policy: 0.26547
	loss_value: 0.45744
	loss_reward: 0.02955
[2025-05-11 15:36:09] nn step 30150, lr: 0.1.
	loss_policy_0: 0.13108
	accuracy_policy_0: 0.94098
	loss_value_0: 0.22524
	loss_policy_1: 0.02642
	accuracy_policy_1: 0.93578
	loss_value_1: 0.0456
	loss_reward_1: 0.00426
	loss_policy_2: 0.02649
	accuracy_policy_2: 0.93801
	loss_value_2: 0.04653
	loss_reward_2: 0.00497
	loss_policy_3: 0.02637
	accuracy_policy_3: 0.93938
	loss_value_3: 0.04737
	loss_reward_3: 0.00551
	loss_policy_4: 0.02661
	accuracy_policy_4: 0.94047
	loss_value_4: 0.04841
	loss_reward_4: 0.00634
	loss_policy_5: 0.02628
	accuracy_policy_5: 0.94363
	loss_value_5: 0.04972
	loss_reward_5: 0.00755
	loss_policy: 0.26324
	loss_value: 0.46287
	loss_reward: 0.02864
[2025-05-11 15:36:17] nn step 30200, lr: 0.1.
	loss_policy_0: 0.13804
	accuracy_policy_0: 0.94055
	loss_value_0: 0.23131
	loss_policy_1: 0.02744
	accuracy_policy_1: 0.93648
	loss_value_1: 0.04678
	loss_reward_1: 0.00467
	loss_policy_2: 0.02771
	accuracy_policy_2: 0.93695
	loss_value_2: 0.04756
	loss_reward_2: 0.00515
	loss_policy_3: 0.02734
	accuracy_policy_3: 0.93664
	loss_value_3: 0.04845
	loss_reward_3: 0.00563
	loss_policy_4: 0.02726
	accuracy_policy_4: 0.93992
	loss_value_4: 0.04968
	loss_reward_4: 0.00697
	loss_policy_5: 0.02723
	accuracy_policy_5: 0.94125
	loss_value_5: 0.05066
	loss_reward_5: 0.00826
	loss_policy: 0.27503
	loss_value: 0.47443
	loss_reward: 0.03068
Optimization_Done 30200
[2025-05-11 15:37:36] [command] train weight_iter_30200.pkl 133 152
[2025-05-11 15:37:46] nn step 30250, lr: 0.1.
	loss_policy_0: 0.12973
	accuracy_policy_0: 0.94133
	loss_value_0: 0.22421
	loss_policy_1: 0.0257
	accuracy_policy_1: 0.93785
	loss_value_1: 0.04496
	loss_reward_1: 0.00446
	loss_policy_2: 0.02583
	accuracy_policy_2: 0.9377
	loss_value_2: 0.04563
	loss_reward_2: 0.00464
	loss_policy_3: 0.02561
	accuracy_policy_3: 0.9393
	loss_value_3: 0.0465
	loss_reward_3: 0.00525
	loss_policy_4: 0.026
	accuracy_policy_4: 0.9402
	loss_value_4: 0.04765
	loss_reward_4: 0.00623
	loss_policy_5: 0.02572
	accuracy_policy_5: 0.94512
	loss_value_5: 0.04883
	loss_reward_5: 0.00734
	loss_policy: 0.25857
	loss_value: 0.45779
	loss_reward: 0.02793
[2025-05-11 15:37:52] nn step 30300, lr: 0.1.
	loss_policy_0: 0.13705
	accuracy_policy_0: 0.94047
	loss_value_0: 0.23277
	loss_policy_1: 0.02728
	accuracy_policy_1: 0.93801
	loss_value_1: 0.0465
	loss_reward_1: 0.00454
	loss_policy_2: 0.02788
	accuracy_policy_2: 0.93672
	loss_value_2: 0.04717
	loss_reward_2: 0.00514
	loss_policy_3: 0.02773
	accuracy_policy_3: 0.9398
	loss_value_3: 0.04808
	loss_reward_3: 0.00578
	loss_policy_4: 0.02744
	accuracy_policy_4: 0.94219
	loss_value_4: 0.04893
	loss_reward_4: 0.00668
	loss_policy_5: 0.02728
	accuracy_policy_5: 0.94332
	loss_value_5: 0.05049
	loss_reward_5: 0.0081
	loss_policy: 0.27465
	loss_value: 0.47395
	loss_reward: 0.03024
[2025-05-11 15:38:01] nn step 30350, lr: 0.1.
	loss_policy_0: 0.1409
	accuracy_policy_0: 0.94148
	loss_value_0: 0.23742
	loss_policy_1: 0.02843
	accuracy_policy_1: 0.93461
	loss_value_1: 0.04785
	loss_reward_1: 0.00483
	loss_policy_2: 0.02833
	accuracy_policy_2: 0.93445
	loss_value_2: 0.04868
	loss_reward_2: 0.00527
	loss_policy_3: 0.02814
	accuracy_policy_3: 0.93707
	loss_value_3: 0.04962
	loss_reward_3: 0.00606
	loss_policy_4: 0.02887
	accuracy_policy_4: 0.93867
	loss_value_4: 0.05085
	loss_reward_4: 0.00719
	loss_policy_5: 0.02849
	accuracy_policy_5: 0.94039
	loss_value_5: 0.05226
	loss_reward_5: 0.00821
	loss_policy: 0.28316
	loss_value: 0.48668
	loss_reward: 0.03157
[2025-05-11 15:38:10] nn step 30400, lr: 0.1.
	loss_policy_0: 0.13783
	accuracy_policy_0: 0.93945
	loss_value_0: 0.23379
	loss_policy_1: 0.02734
	accuracy_policy_1: 0.93902
	loss_value_1: 0.04716
	loss_reward_1: 0.00476
	loss_policy_2: 0.02748
	accuracy_policy_2: 0.93742
	loss_value_2: 0.048
	loss_reward_2: 0.00522
	loss_policy_3: 0.02778
	accuracy_policy_3: 0.93934
	loss_value_3: 0.04879
	loss_reward_3: 0.00559
	loss_policy_4: 0.02725
	accuracy_policy_4: 0.94125
	loss_value_4: 0.0502
	loss_reward_4: 0.0069
	loss_policy_5: 0.02759
	accuracy_policy_5: 0.94617
	loss_value_5: 0.05144
	loss_reward_5: 0.00821
	loss_policy: 0.27526
	loss_value: 0.47938
	loss_reward: 0.03068
Optimization_Done 30400
[2025-05-11 15:39:28] [command] train weight_iter_30400.pkl 134 153
[2025-05-11 15:39:37] nn step 30450, lr: 0.1.
	loss_policy_0: 0.13597
	accuracy_policy_0: 0.94043
	loss_value_0: 0.23376
	loss_policy_1: 0.02666
	accuracy_policy_1: 0.93914
	loss_value_1: 0.04666
	loss_reward_1: 0.00464
	loss_policy_2: 0.02672
	accuracy_policy_2: 0.94141
	loss_value_2: 0.04742
	loss_reward_2: 0.00494
	loss_policy_3: 0.02722
	accuracy_policy_3: 0.94305
	loss_value_3: 0.04831
	loss_reward_3: 0.00542
	loss_policy_4: 0.02709
	accuracy_policy_4: 0.94246
	loss_value_4: 0.04948
	loss_reward_4: 0.00674
	loss_policy_5: 0.02707
	accuracy_policy_5: 0.94621
	loss_value_5: 0.05078
	loss_reward_5: 0.00767
	loss_policy: 0.27073
	loss_value: 0.47641
	loss_reward: 0.02942
[2025-05-11 15:39:44] nn step 30500, lr: 0.1.
	loss_policy_0: 0.13671
	accuracy_policy_0: 0.94305
	loss_value_0: 0.22455
	loss_policy_1: 0.02652
	accuracy_policy_1: 0.93773
	loss_value_1: 0.04516
	loss_reward_1: 0.00465
	loss_policy_2: 0.0265
	accuracy_policy_2: 0.94066
	loss_value_2: 0.04603
	loss_reward_2: 0.00497
	loss_policy_3: 0.02655
	accuracy_policy_3: 0.94129
	loss_value_3: 0.04698
	loss_reward_3: 0.00558
	loss_policy_4: 0.02697
	accuracy_policy_4: 0.94379
	loss_value_4: 0.04786
	loss_reward_4: 0.00645
	loss_policy_5: 0.02684
	accuracy_policy_5: 0.94227
	loss_value_5: 0.04903
	loss_reward_5: 0.00761
	loss_policy: 0.2701
	loss_value: 0.45961
	loss_reward: 0.02925
[2025-05-11 15:39:53] nn step 30550, lr: 0.1.
	loss_policy_0: 0.12833
	accuracy_policy_0: 0.94375
	loss_value_0: 0.21359
	loss_policy_1: 0.02569
	accuracy_policy_1: 0.93926
	loss_value_1: 0.04299
	loss_reward_1: 0.0043
	loss_policy_2: 0.02573
	accuracy_policy_2: 0.93906
	loss_value_2: 0.04376
	loss_reward_2: 0.00494
	loss_policy_3: 0.02583
	accuracy_policy_3: 0.94012
	loss_value_3: 0.04474
	loss_reward_3: 0.00536
	loss_policy_4: 0.02583
	accuracy_policy_4: 0.94223
	loss_value_4: 0.04568
	loss_reward_4: 0.0064
	loss_policy_5: 0.02586
	accuracy_policy_5: 0.94375
	loss_value_5: 0.04682
	loss_reward_5: 0.00738
	loss_policy: 0.25727
	loss_value: 0.43759
	loss_reward: 0.02838
[2025-05-11 15:40:01] nn step 30600, lr: 0.1.
	loss_policy_0: 0.13557
	accuracy_policy_0: 0.93938
	loss_value_0: 0.22428
	loss_policy_1: 0.02654
	accuracy_policy_1: 0.93699
	loss_value_1: 0.0451
	loss_reward_1: 0.00451
	loss_policy_2: 0.02675
	accuracy_policy_2: 0.93672
	loss_value_2: 0.04572
	loss_reward_2: 0.00518
	loss_policy_3: 0.02673
	accuracy_policy_3: 0.93891
	loss_value_3: 0.04676
	loss_reward_3: 0.00554
	loss_policy_4: 0.02667
	accuracy_policy_4: 0.94156
	loss_value_4: 0.0478
	loss_reward_4: 0.00631
	loss_policy_5: 0.02661
	accuracy_policy_5: 0.94449
	loss_value_5: 0.0493
	loss_reward_5: 0.00789
	loss_policy: 0.26887
	loss_value: 0.45896
	loss_reward: 0.02943
Optimization_Done 30600
[2025-05-11 15:41:18] [command] train weight_iter_30600.pkl 135 154
[2025-05-11 15:41:27] nn step 30650, lr: 0.1.
	loss_policy_0: 0.13165
	accuracy_policy_0: 0.94469
	loss_value_0: 0.22659
	loss_policy_1: 0.02621
	accuracy_policy_1: 0.93992
	loss_value_1: 0.04546
	loss_reward_1: 0.00439
	loss_policy_2: 0.02598
	accuracy_policy_2: 0.94332
	loss_value_2: 0.04649
	loss_reward_2: 0.00491
	loss_policy_3: 0.02607
	accuracy_policy_3: 0.94367
	loss_value_3: 0.04729
	loss_reward_3: 0.00545
	loss_policy_4: 0.02597
	accuracy_policy_4: 0.94727
	loss_value_4: 0.04842
	loss_reward_4: 0.00649
	loss_policy_5: 0.02598
	accuracy_policy_5: 0.94539
	loss_value_5: 0.04982
	loss_reward_5: 0.0074
	loss_policy: 0.26187
	loss_value: 0.46407
	loss_reward: 0.02864
[2025-05-11 15:41:36] nn step 30700, lr: 0.1.
	loss_policy_0: 0.1309
	accuracy_policy_0: 0.94438
	loss_value_0: 0.22313
	loss_policy_1: 0.02618
	accuracy_policy_1: 0.93789
	loss_value_1: 0.04478
	loss_reward_1: 0.0044
	loss_policy_2: 0.02627
	accuracy_policy_2: 0.94062
	loss_value_2: 0.04591
	loss_reward_2: 0.00491
	loss_policy_3: 0.0264
	accuracy_policy_3: 0.94016
	loss_value_3: 0.04666
	loss_reward_3: 0.00551
	loss_policy_4: 0.02636
	accuracy_policy_4: 0.94391
	loss_value_4: 0.04782
	loss_reward_4: 0.0066
	loss_policy_5: 0.02619
	accuracy_policy_5: 0.94387
	loss_value_5: 0.04902
	loss_reward_5: 0.00753
	loss_policy: 0.2623
	loss_value: 0.45732
	loss_reward: 0.02895
[2025-05-11 15:41:43] nn step 30750, lr: 0.1.
	loss_policy_0: 0.14457
	accuracy_policy_0: 0.94363
	loss_value_0: 0.24249
	loss_policy_1: 0.02868
	accuracy_policy_1: 0.94207
	loss_value_1: 0.04893
	loss_reward_1: 0.00467
	loss_policy_2: 0.0287
	accuracy_policy_2: 0.93961
	loss_value_2: 0.05003
	loss_reward_2: 0.00542
	loss_policy_3: 0.02881
	accuracy_policy_3: 0.94113
	loss_value_3: 0.05108
	loss_reward_3: 0.00612
	loss_policy_4: 0.02938
	accuracy_policy_4: 0.94383
	loss_value_4: 0.05259
	loss_reward_4: 0.00707
	loss_policy_5: 0.02863
	accuracy_policy_5: 0.94402
	loss_value_5: 0.05402
	loss_reward_5: 0.00833
	loss_policy: 0.28877
	loss_value: 0.49914
	loss_reward: 0.03161
[2025-05-11 15:41:51] nn step 30800, lr: 0.1.
	loss_policy_0: 0.1411
	accuracy_policy_0: 0.9448
	loss_value_0: 0.23782
	loss_policy_1: 0.02787
	accuracy_policy_1: 0.94125
	loss_value_1: 0.04813
	loss_reward_1: 0.00482
	loss_policy_2: 0.02805
	accuracy_policy_2: 0.94188
	loss_value_2: 0.04922
	loss_reward_2: 0.00527
	loss_policy_3: 0.02814
	accuracy_policy_3: 0.94047
	loss_value_3: 0.05024
	loss_reward_3: 0.00615
	loss_policy_4: 0.02794
	accuracy_policy_4: 0.94242
	loss_value_4: 0.05134
	loss_reward_4: 0.00722
	loss_policy_5: 0.02783
	accuracy_policy_5: 0.9459
	loss_value_5: 0.05288
	loss_reward_5: 0.00853
	loss_policy: 0.28093
	loss_value: 0.48964
	loss_reward: 0.03199
Optimization_Done 30800
[2025-05-11 15:43:07] [command] train weight_iter_30800.pkl 136 155
[2025-05-11 15:43:17] nn step 30850, lr: 0.1.
	loss_policy_0: 0.1365
	accuracy_policy_0: 0.94387
	loss_value_0: 0.23061
	loss_policy_1: 0.02692
	accuracy_policy_1: 0.9402
	loss_value_1: 0.04608
	loss_reward_1: 0.00444
	loss_policy_2: 0.0269
	accuracy_policy_2: 0.94367
	loss_value_2: 0.04715
	loss_reward_2: 0.00496
	loss_policy_3: 0.02673
	accuracy_policy_3: 0.94219
	loss_value_3: 0.04802
	loss_reward_3: 0.00555
	loss_policy_4: 0.02695
	accuracy_policy_4: 0.94562
	loss_value_4: 0.0492
	loss_reward_4: 0.00656
	loss_policy_5: 0.02678
	accuracy_policy_5: 0.94582
	loss_value_5: 0.05025
	loss_reward_5: 0.00732
	loss_policy: 0.27078
	loss_value: 0.47131
	loss_reward: 0.02883
[2025-05-11 15:43:25] nn step 30900, lr: 0.1.
	loss_policy_0: 0.13239
	accuracy_policy_0: 0.94258
	loss_value_0: 0.22331
	loss_policy_1: 0.02654
	accuracy_policy_1: 0.93945
	loss_value_1: 0.04491
	loss_reward_1: 0.00446
	loss_policy_2: 0.0263
	accuracy_policy_2: 0.94055
	loss_value_2: 0.04594
	loss_reward_2: 0.00493
	loss_policy_3: 0.02668
	accuracy_policy_3: 0.9373
	loss_value_3: 0.04659
	loss_reward_3: 0.00556
	loss_policy_4: 0.02661
	accuracy_policy_4: 0.94219
	loss_value_4: 0.04792
	loss_reward_4: 0.00664
	loss_policy_5: 0.02652
	accuracy_policy_5: 0.94367
	loss_value_5: 0.04887
	loss_reward_5: 0.00747
	loss_policy: 0.26504
	loss_value: 0.45753
	loss_reward: 0.02906
[2025-05-11 15:43:33] nn step 30950, lr: 0.1.
	loss_policy_0: 0.13463
	accuracy_policy_0: 0.94508
	loss_value_0: 0.22436
	loss_policy_1: 0.02673
	accuracy_policy_1: 0.94004
	loss_value_1: 0.04542
	loss_reward_1: 0.0046
	loss_policy_2: 0.02657
	accuracy_policy_2: 0.94027
	loss_value_2: 0.04605
	loss_reward_2: 0.00509
	loss_policy_3: 0.02695
	accuracy_policy_3: 0.94156
	loss_value_3: 0.04705
	loss_reward_3: 0.00529
	loss_policy_4: 0.02685
	accuracy_policy_4: 0.94555
	loss_value_4: 0.04834
	loss_reward_4: 0.0066
	loss_policy_5: 0.02658
	accuracy_policy_5: 0.94668
	loss_value_5: 0.04951
	loss_reward_5: 0.00749
	loss_policy: 0.26831
	loss_value: 0.46074
	loss_reward: 0.02906
[2025-05-11 15:43:41] nn step 31000, lr: 0.1.
	loss_policy_0: 0.14125
	accuracy_policy_0: 0.94156
	loss_value_0: 0.23562
	loss_policy_1: 0.02842
	accuracy_policy_1: 0.94082
	loss_value_1: 0.04754
	loss_reward_1: 0.00475
	loss_policy_2: 0.02805
	accuracy_policy_2: 0.94078
	loss_value_2: 0.04841
	loss_reward_2: 0.00515
	loss_policy_3: 0.02829
	accuracy_policy_3: 0.93887
	loss_value_3: 0.04917
	loss_reward_3: 0.0061
	loss_policy_4: 0.02868
	accuracy_policy_4: 0.93984
	loss_value_4: 0.05065
	loss_reward_4: 0.00691
	loss_policy_5: 0.02802
	accuracy_policy_5: 0.94238
	loss_value_5: 0.05188
	loss_reward_5: 0.00807
	loss_policy: 0.28272
	loss_value: 0.48329
	loss_reward: 0.03099
Optimization_Done 31000
[2025-05-11 15:44:58] [command] train weight_iter_31000.pkl 137 156
[2025-05-11 15:45:06] nn step 31050, lr: 0.1.
	loss_policy_0: 0.13939
	accuracy_policy_0: 0.94391
	loss_value_0: 0.23832
	loss_policy_1: 0.02753
	accuracy_policy_1: 0.94141
	loss_value_1: 0.04781
	loss_reward_1: 0.00468
	loss_policy_2: 0.0272
	accuracy_policy_2: 0.93941
	loss_value_2: 0.04872
	loss_reward_2: 0.00524
	loss_policy_3: 0.02741
	accuracy_policy_3: 0.94172
	loss_value_3: 0.04978
	loss_reward_3: 0.00555
	loss_policy_4: 0.02772
	accuracy_policy_4: 0.94535
	loss_value_4: 0.05102
	loss_reward_4: 0.00676
	loss_policy_5: 0.02773
	accuracy_policy_5: 0.94418
	loss_value_5: 0.0521
	loss_reward_5: 0.00793
	loss_policy: 0.27698
	loss_value: 0.48776
	loss_reward: 0.03016
[2025-05-11 15:45:15] nn step 31100, lr: 0.1.
	loss_policy_0: 0.13717
	accuracy_policy_0: 0.94336
	loss_value_0: 0.23326
	loss_policy_1: 0.02716
	accuracy_policy_1: 0.94234
	loss_value_1: 0.04656
	loss_reward_1: 0.00476
	loss_policy_2: 0.02723
	accuracy_policy_2: 0.94148
	loss_value_2: 0.04734
	loss_reward_2: 0.00535
	loss_policy_3: 0.02714
	accuracy_policy_3: 0.94277
	loss_value_3: 0.04811
	loss_reward_3: 0.0057
	loss_policy_4: 0.02757
	accuracy_policy_4: 0.94176
	loss_value_4: 0.04939
	loss_reward_4: 0.007
	loss_policy_5: 0.027
	accuracy_policy_5: 0.94598
	loss_value_5: 0.05065
	loss_reward_5: 0.00829
	loss_policy: 0.27327
	loss_value: 0.47531
	loss_reward: 0.03111
[2025-05-11 15:45:23] nn step 31150, lr: 0.1.
	loss_policy_0: 0.12796
	accuracy_policy_0: 0.94297
	loss_value_0: 0.21337
	loss_policy_1: 0.02525
	accuracy_policy_1: 0.94066
	loss_value_1: 0.04298
	loss_reward_1: 0.00426
	loss_policy_2: 0.02512
	accuracy_policy_2: 0.93996
	loss_value_2: 0.04386
	loss_reward_2: 0.00473
	loss_policy_3: 0.02512
	accuracy_policy_3: 0.94066
	loss_value_3: 0.04463
	loss_reward_3: 0.00533
	loss_policy_4: 0.02527
	accuracy_policy_4: 0.94168
	loss_value_4: 0.04571
	loss_reward_4: 0.00607
	loss_policy_5: 0.02546
	accuracy_policy_5: 0.94242
	loss_value_5: 0.04651
	loss_reward_5: 0.00725
	loss_policy: 0.25418
	loss_value: 0.43706
	loss_reward: 0.02763
[2025-05-11 15:45:30] nn step 31200, lr: 0.1.
	loss_policy_0: 0.14176
	accuracy_policy_0: 0.9416
	loss_value_0: 0.22948
	loss_policy_1: 0.0276
	accuracy_policy_1: 0.94086
	loss_value_1: 0.0462
	loss_reward_1: 0.00469
	loss_policy_2: 0.02798
	accuracy_policy_2: 0.93805
	loss_value_2: 0.04743
	loss_reward_2: 0.00512
	loss_policy_3: 0.02809
	accuracy_policy_3: 0.94113
	loss_value_3: 0.04821
	loss_reward_3: 0.00577
	loss_policy_4: 0.02784
	accuracy_policy_4: 0.94113
	loss_value_4: 0.0494
	loss_reward_4: 0.00699
	loss_policy_5: 0.02769
	accuracy_policy_5: 0.94223
	loss_value_5: 0.05097
	loss_reward_5: 0.00777
	loss_policy: 0.28097
	loss_value: 0.47171
	loss_reward: 0.03033
Optimization_Done 31200
[2025-05-11 15:46:50] [command] train weight_iter_31200.pkl 138 157
[2025-05-11 15:46:58] nn step 31250, lr: 0.1.
	loss_policy_0: 0.14349
	accuracy_policy_0: 0.94441
	loss_value_0: 0.25405
	loss_policy_1: 0.02857
	accuracy_policy_1: 0.94188
	loss_value_1: 0.05102
	loss_reward_1: 0.00475
	loss_policy_2: 0.02866
	accuracy_policy_2: 0.94109
	loss_value_2: 0.05232
	loss_reward_2: 0.00542
	loss_policy_3: 0.02853
	accuracy_policy_3: 0.94496
	loss_value_3: 0.05326
	loss_reward_3: 0.00618
	loss_policy_4: 0.02883
	accuracy_policy_4: 0.94203
	loss_value_4: 0.05461
	loss_reward_4: 0.00751
	loss_policy_5: 0.02838
	accuracy_policy_5: 0.94469
	loss_value_5: 0.05608
	loss_reward_5: 0.00869
	loss_policy: 0.28645
	loss_value: 0.52134
	loss_reward: 0.03256
[2025-05-11 15:47:07] nn step 31300, lr: 0.1.
	loss_policy_0: 0.13188
	accuracy_policy_0: 0.94191
	loss_value_0: 0.22563
	loss_policy_1: 0.02605
	accuracy_policy_1: 0.93996
	loss_value_1: 0.04551
	loss_reward_1: 0.00441
	loss_policy_2: 0.02587
	accuracy_policy_2: 0.94125
	loss_value_2: 0.04668
	loss_reward_2: 0.00508
	loss_policy_3: 0.02574
	accuracy_policy_3: 0.94355
	loss_value_3: 0.04791
	loss_reward_3: 0.00564
	loss_policy_4: 0.02605
	accuracy_policy_4: 0.94191
	loss_value_4: 0.04894
	loss_reward_4: 0.00667
	loss_policy_5: 0.02592
	accuracy_policy_5: 0.94664
	loss_value_5: 0.05029
	loss_reward_5: 0.00799
	loss_policy: 0.26152
	loss_value: 0.46495
	loss_reward: 0.0298
[2025-05-11 15:47:16] nn step 31350, lr: 0.1.
	loss_policy_0: 0.1481
	accuracy_policy_0: 0.94344
	loss_value_0: 0.25661
	loss_policy_1: 0.0297
	accuracy_policy_1: 0.94031
	loss_value_1: 0.05161
	loss_reward_1: 0.00496
	loss_policy_2: 0.02972
	accuracy_policy_2: 0.94074
	loss_value_2: 0.05295
	loss_reward_2: 0.00567
	loss_policy_3: 0.02957
	accuracy_policy_3: 0.94113
	loss_value_3: 0.05373
	loss_reward_3: 0.0062
	loss_policy_4: 0.02972
	accuracy_policy_4: 0.94133
	loss_value_4: 0.05515
	loss_reward_4: 0.00752
	loss_policy_5: 0.02943
	accuracy_policy_5: 0.94332
	loss_value_5: 0.05688
	loss_reward_5: 0.00887
	loss_policy: 0.29624
	loss_value: 0.52694
	loss_reward: 0.03324
[2025-05-11 15:47:23] nn step 31400, lr: 0.1.
	loss_policy_0: 0.14272
	accuracy_policy_0: 0.9393
	loss_value_0: 0.24499
	loss_policy_1: 0.02796
	accuracy_policy_1: 0.93922
	loss_value_1: 0.04922
	loss_reward_1: 0.00481
	loss_policy_2: 0.02798
	accuracy_policy_2: 0.94145
	loss_value_2: 0.05056
	loss_reward_2: 0.00559
	loss_policy_3: 0.02806
	accuracy_policy_3: 0.94
	loss_value_3: 0.05174
	loss_reward_3: 0.00617
	loss_policy_4: 0.02824
	accuracy_policy_4: 0.9425
	loss_value_4: 0.05251
	loss_reward_4: 0.0072
	loss_policy_5: 0.02825
	accuracy_policy_5: 0.94246
	loss_value_5: 0.0539
	loss_reward_5: 0.00859
	loss_policy: 0.28321
	loss_value: 0.50293
	loss_reward: 0.03237
Optimization_Done 31400
[2025-05-11 15:48:44] [command] train weight_iter_31400.pkl 139 158
[2025-05-11 15:48:52] nn step 31450, lr: 0.1.
	loss_policy_0: 0.12374
	accuracy_policy_0: 0.94301
	loss_value_0: 0.22177
	loss_policy_1: 0.0247
	accuracy_policy_1: 0.93863
	loss_value_1: 0.0443
	loss_reward_1: 0.00422
	loss_policy_2: 0.02443
	accuracy_policy_2: 0.94004
	loss_value_2: 0.04517
	loss_reward_2: 0.00457
	loss_policy_3: 0.0245
	accuracy_policy_3: 0.94039
	loss_value_3: 0.04623
	loss_reward_3: 0.00529
	loss_policy_4: 0.02418
	accuracy_policy_4: 0.9434
	loss_value_4: 0.04741
	loss_reward_4: 0.00635
	loss_policy_5: 0.02453
	accuracy_policy_5: 0.94504
	loss_value_5: 0.04832
	loss_reward_5: 0.00692
	loss_policy: 0.24607
	loss_value: 0.45319
	loss_reward: 0.02735
[2025-05-11 15:49:01] nn step 31500, lr: 0.1.
	loss_policy_0: 0.13949
	accuracy_policy_0: 0.9448
	loss_value_0: 0.24613
	loss_policy_1: 0.02806
	accuracy_policy_1: 0.93809
	loss_value_1: 0.04963
	loss_reward_1: 0.00475
	loss_policy_2: 0.0275
	accuracy_policy_2: 0.94203
	loss_value_2: 0.05054
	loss_reward_2: 0.00539
	loss_policy_3: 0.02791
	accuracy_policy_3: 0.94055
	loss_value_3: 0.05154
	loss_reward_3: 0.00584
	loss_policy_4: 0.02788
	accuracy_policy_4: 0.9432
	loss_value_4: 0.05255
	loss_reward_4: 0.00696
	loss_policy_5: 0.02795
	accuracy_policy_5: 0.94484
	loss_value_5: 0.05379
	loss_reward_5: 0.00809
	loss_policy: 0.27878
	loss_value: 0.50418
	loss_reward: 0.03103
[2025-05-11 15:49:09] nn step 31550, lr: 0.1.
	loss_policy_0: 0.13539
	accuracy_policy_0: 0.94441
	loss_value_0: 0.23781
	loss_policy_1: 0.02714
	accuracy_policy_1: 0.93875
	loss_value_1: 0.04789
	loss_reward_1: 0.00463
	loss_policy_2: 0.02729
	accuracy_policy_2: 0.94137
	loss_value_2: 0.04913
	loss_reward_2: 0.00514
	loss_policy_3: 0.02702
	accuracy_policy_3: 0.94367
	loss_value_3: 0.0502
	loss_reward_3: 0.00591
	loss_policy_4: 0.02716
	accuracy_policy_4: 0.9423
	loss_value_4: 0.0512
	loss_reward_4: 0.00718
	loss_policy_5: 0.02722
	accuracy_policy_5: 0.94707
	loss_value_5: 0.05261
	loss_reward_5: 0.00792
	loss_policy: 0.27122
	loss_value: 0.48884
	loss_reward: 0.03078
[2025-05-11 15:49:16] nn step 31600, lr: 0.1.
	loss_policy_0: 0.13646
	accuracy_policy_0: 0.94234
	loss_value_0: 0.23724
	loss_policy_1: 0.02734
	accuracy_policy_1: 0.93832
	loss_value_1: 0.04763
	loss_reward_1: 0.00477
	loss_policy_2: 0.02728
	accuracy_policy_2: 0.93988
	loss_value_2: 0.0485
	loss_reward_2: 0.00506
	loss_policy_3: 0.02707
	accuracy_policy_3: 0.93969
	loss_value_3: 0.04946
	loss_reward_3: 0.00575
	loss_policy_4: 0.02736
	accuracy_policy_4: 0.94141
	loss_value_4: 0.0509
	loss_reward_4: 0.00713
	loss_policy_5: 0.02728
	accuracy_policy_5: 0.9477
	loss_value_5: 0.05254
	loss_reward_5: 0.00843
	loss_policy: 0.27279
	loss_value: 0.48627
	loss_reward: 0.03114
Optimization_Done 31600
[2025-05-11 15:50:35] [command] train weight_iter_31600.pkl 140 159
[2025-05-11 15:50:43] nn step 31650, lr: 0.1.
	loss_policy_0: 0.13273
	accuracy_policy_0: 0.94535
	loss_value_0: 0.23673
	loss_policy_1: 0.02616
	accuracy_policy_1: 0.94203
	loss_value_1: 0.04737
	loss_reward_1: 0.00447
	loss_policy_2: 0.02617
	accuracy_policy_2: 0.94277
	loss_value_2: 0.04834
	loss_reward_2: 0.00495
	loss_policy_3: 0.02606
	accuracy_policy_3: 0.94273
	loss_value_3: 0.04928
	loss_reward_3: 0.0056
	loss_policy_4: 0.02595
	accuracy_policy_4: 0.94762
	loss_value_4: 0.05078
	loss_reward_4: 0.00669
	loss_policy_5: 0.0264
	accuracy_policy_5: 0.94789
	loss_value_5: 0.05195
	loss_reward_5: 0.00763
	loss_policy: 0.26347
	loss_value: 0.48445
	loss_reward: 0.02934
[2025-05-11 15:50:51] nn step 31700, lr: 0.1.
	loss_policy_0: 0.13435
	accuracy_policy_0: 0.94609
	loss_value_0: 0.23611
	loss_policy_1: 0.02681
	accuracy_policy_1: 0.94359
	loss_value_1: 0.04746
	loss_reward_1: 0.00452
	loss_policy_2: 0.02701
	accuracy_policy_2: 0.94047
	loss_value_2: 0.04853
	loss_reward_2: 0.0053
	loss_policy_3: 0.02744
	accuracy_policy_3: 0.94086
	loss_value_3: 0.0497
	loss_reward_3: 0.00591
	loss_policy_4: 0.02703
	accuracy_policy_4: 0.94242
	loss_value_4: 0.05075
	loss_reward_4: 0.00697
	loss_policy_5: 0.0271
	accuracy_policy_5: 0.94684
	loss_value_5: 0.05203
	loss_reward_5: 0.00824
	loss_policy: 0.26976
	loss_value: 0.48459
	loss_reward: 0.03095
[2025-05-11 15:51:00] nn step 31750, lr: 0.1.
	loss_policy_0: 0.13417
	accuracy_policy_0: 0.9409
	loss_value_0: 0.23201
	loss_policy_1: 0.02662
	accuracy_policy_1: 0.94
	loss_value_1: 0.04707
	loss_reward_1: 0.00455
	loss_policy_2: 0.02659
	accuracy_policy_2: 0.94273
	loss_value_2: 0.04806
	loss_reward_2: 0.00518
	loss_policy_3: 0.02653
	accuracy_policy_3: 0.94121
	loss_value_3: 0.04905
	loss_reward_3: 0.00596
	loss_policy_4: 0.02654
	accuracy_policy_4: 0.94488
	loss_value_4: 0.05051
	loss_reward_4: 0.0071
	loss_policy_5: 0.02679
	accuracy_policy_5: 0.94527
	loss_value_5: 0.05171
	loss_reward_5: 0.00834
	loss_policy: 0.26724
	loss_value: 0.47841
	loss_reward: 0.03114
[2025-05-11 15:51:07] nn step 31800, lr: 0.1.
	loss_policy_0: 0.13061
	accuracy_policy_0: 0.94527
	loss_value_0: 0.22516
	loss_policy_1: 0.02623
	accuracy_policy_1: 0.9416
	loss_value_1: 0.04515
	loss_reward_1: 0.00423
	loss_policy_2: 0.02591
	accuracy_policy_2: 0.94125
	loss_value_2: 0.04626
	loss_reward_2: 0.00496
	loss_policy_3: 0.02618
	accuracy_policy_3: 0.94008
	loss_value_3: 0.04703
	loss_reward_3: 0.00549
	loss_policy_4: 0.02587
	accuracy_policy_4: 0.94543
	loss_value_4: 0.0482
	loss_reward_4: 0.00648
	loss_policy_5: 0.02601
	accuracy_policy_5: 0.94777
	loss_value_5: 0.04964
	loss_reward_5: 0.00771
	loss_policy: 0.26081
	loss_value: 0.46144
	loss_reward: 0.02888
Optimization_Done 31800
[2025-05-11 15:52:40] [command] train weight_iter_31800.pkl 141 160
[2025-05-11 15:52:49] nn step 31850, lr: 0.1.
	loss_policy_0: 0.13421
	accuracy_policy_0: 0.94789
	loss_value_0: 0.24466
	loss_policy_1: 0.02641
	accuracy_policy_1: 0.94426
	loss_value_1: 0.04873
	loss_reward_1: 0.00456
	loss_policy_2: 0.02672
	accuracy_policy_2: 0.94121
	loss_value_2: 0.04979
	loss_reward_2: 0.00508
	loss_policy_3: 0.02653
	accuracy_policy_3: 0.94504
	loss_value_3: 0.05096
	loss_reward_3: 0.0058
	loss_policy_4: 0.02659
	accuracy_policy_4: 0.94621
	loss_value_4: 0.05189
	loss_reward_4: 0.00707
	loss_policy_5: 0.02675
	accuracy_policy_5: 0.94875
	loss_value_5: 0.0531
	loss_reward_5: 0.00815
	loss_policy: 0.26722
	loss_value: 0.49913
	loss_reward: 0.03066
[2025-05-11 15:52:58] nn step 31900, lr: 0.1.
	loss_policy_0: 0.13642
	accuracy_policy_0: 0.94578
	loss_value_0: 0.23749
	loss_policy_1: 0.02685
	accuracy_policy_1: 0.94477
	loss_value_1: 0.04762
	loss_reward_1: 0.00443
	loss_policy_2: 0.02709
	accuracy_policy_2: 0.94383
	loss_value_2: 0.04858
	loss_reward_2: 0.00522
	loss_policy_3: 0.02724
	accuracy_policy_3: 0.94395
	loss_value_3: 0.04958
	loss_reward_3: 0.00581
	loss_policy_4: 0.02727
	accuracy_policy_4: 0.94402
	loss_value_4: 0.05083
	loss_reward_4: 0.00713
	loss_policy_5: 0.02674
	accuracy_policy_5: 0.94629
	loss_value_5: 0.05245
	loss_reward_5: 0.00798
	loss_policy: 0.27161
	loss_value: 0.48655
	loss_reward: 0.03057
[2025-05-11 15:53:05] nn step 31950, lr: 0.1.
	loss_policy_0: 0.12816
	accuracy_policy_0: 0.94543
	loss_value_0: 0.22273
	loss_policy_1: 0.0256
	accuracy_policy_1: 0.945
	loss_value_1: 0.04494
	loss_reward_1: 0.0042
	loss_policy_2: 0.02603
	accuracy_policy_2: 0.94223
	loss_value_2: 0.04611
	loss_reward_2: 0.00492
	loss_policy_3: 0.02581
	accuracy_policy_3: 0.94141
	loss_value_3: 0.04716
	loss_reward_3: 0.00567
	loss_policy_4: 0.0256
	accuracy_policy_4: 0.94621
	loss_value_4: 0.04838
	loss_reward_4: 0.00646
	loss_policy_5: 0.02545
	accuracy_policy_5: 0.94957
	loss_value_5: 0.04942
	loss_reward_5: 0.00755
	loss_policy: 0.25666
	loss_value: 0.45873
	loss_reward: 0.0288
[2025-05-11 15:53:13] nn step 32000, lr: 0.1.
	loss_policy_0: 0.13482
	accuracy_policy_0: 0.94395
	loss_value_0: 0.23226
	loss_policy_1: 0.02709
	accuracy_policy_1: 0.94043
	loss_value_1: 0.04684
	loss_reward_1: 0.00434
	loss_policy_2: 0.02701
	accuracy_policy_2: 0.94148
	loss_value_2: 0.04786
	loss_reward_2: 0.00508
	loss_policy_3: 0.02697
	accuracy_policy_3: 0.94219
	loss_value_3: 0.04881
	loss_reward_3: 0.00587
	loss_policy_4: 0.02686
	accuracy_policy_4: 0.94406
	loss_value_4: 0.04997
	loss_reward_4: 0.00647
	loss_policy_5: 0.02686
	accuracy_policy_5: 0.94668
	loss_value_5: 0.051
	loss_reward_5: 0.00748
	loss_policy: 0.26961
	loss_value: 0.47674
	loss_reward: 0.02923
Optimization_Done 32000
[2025-05-11 15:54:52] [command] train weight_iter_32000.pkl 142 161
[2025-05-11 15:55:00] nn step 32050, lr: 0.1.
	loss_policy_0: 0.12812
	accuracy_policy_0: 0.9498
	loss_value_0: 0.22825
	loss_policy_1: 0.02548
	accuracy_policy_1: 0.9473
	loss_value_1: 0.04562
	loss_reward_1: 0.00426
	loss_policy_2: 0.02513
	accuracy_policy_2: 0.94438
	loss_value_2: 0.04635
	loss_reward_2: 0.00461
	loss_policy_3: 0.02557
	accuracy_policy_3: 0.94656
	loss_value_3: 0.04731
	loss_reward_3: 0.00541
	loss_policy_4: 0.02531
	accuracy_policy_4: 0.94781
	loss_value_4: 0.0483
	loss_reward_4: 0.00625
	loss_policy_5: 0.02515
	accuracy_policy_5: 0.9493
	loss_value_5: 0.04961
	loss_reward_5: 0.00735
	loss_policy: 0.25476
	loss_value: 0.46545
	loss_reward: 0.02788
[2025-05-11 15:55:09] nn step 32100, lr: 0.1.
	loss_policy_0: 0.13333
	accuracy_policy_0: 0.94801
	loss_value_0: 0.23206
	loss_policy_1: 0.02622
	accuracy_policy_1: 0.94266
	loss_value_1: 0.04658
	loss_reward_1: 0.00449
	loss_policy_2: 0.02634
	accuracy_policy_2: 0.94168
	loss_value_2: 0.04779
	loss_reward_2: 0.00489
	loss_policy_3: 0.02654
	accuracy_policy_3: 0.94168
	loss_value_3: 0.04856
	loss_reward_3: 0.00563
	loss_policy_4: 0.02616
	accuracy_policy_4: 0.94711
	loss_value_4: 0.04953
	loss_reward_4: 0.00669
	loss_policy_5: 0.0263
	accuracy_policy_5: 0.95227
	loss_value_5: 0.05053
	loss_reward_5: 0.00766
	loss_policy: 0.26487
	loss_value: 0.47505
	loss_reward: 0.02937
[2025-05-11 15:55:17] nn step 32150, lr: 0.1.
	loss_policy_0: 0.13721
	accuracy_policy_0: 0.94672
	loss_value_0: 0.23931
	loss_policy_1: 0.02736
	accuracy_policy_1: 0.94301
	loss_value_1: 0.0483
	loss_reward_1: 0.00461
	loss_policy_2: 0.02724
	accuracy_policy_2: 0.94387
	loss_value_2: 0.04923
	loss_reward_2: 0.00533
	loss_policy_3: 0.02709
	accuracy_policy_3: 0.94543
	loss_value_3: 0.05033
	loss_reward_3: 0.00587
	loss_policy_4: 0.02752
	accuracy_policy_4: 0.94664
	loss_value_4: 0.0519
	loss_reward_4: 0.00693
	loss_policy_5: 0.02733
	accuracy_policy_5: 0.94844
	loss_value_5: 0.05297
	loss_reward_5: 0.00812
	loss_policy: 0.27375
	loss_value: 0.49204
	loss_reward: 0.03085
[2025-05-11 15:55:25] nn step 32200, lr: 0.1.
	loss_policy_0: 0.1355
	accuracy_policy_0: 0.94684
	loss_value_0: 0.23489
	loss_policy_1: 0.02702
	accuracy_policy_1: 0.94547
	loss_value_1: 0.0473
	loss_reward_1: 0.00454
	loss_policy_2: 0.0274
	accuracy_policy_2: 0.93984
	loss_value_2: 0.04857
	loss_reward_2: 0.00506
	loss_policy_3: 0.02723
	accuracy_policy_3: 0.94238
	loss_value_3: 0.04955
	loss_reward_3: 0.00568
	loss_policy_4: 0.02713
	accuracy_policy_4: 0.9434
	loss_value_4: 0.05066
	loss_reward_4: 0.00698
	loss_policy_5: 0.02658
	accuracy_policy_5: 0.95059
	loss_value_5: 0.05214
	loss_reward_5: 0.00783
	loss_policy: 0.27087
	loss_value: 0.48311
	loss_reward: 0.03009
Optimization_Done 32200
[2025-05-11 15:56:59] [command] train weight_iter_32200.pkl 143 162
[2025-05-11 15:57:09] nn step 32250, lr: 0.1.
	loss_policy_0: 0.12276
	accuracy_policy_0: 0.94566
	loss_value_0: 0.22099
	loss_policy_1: 0.02429
	accuracy_policy_1: 0.94207
	loss_value_1: 0.04423
	loss_reward_1: 0.00409
	loss_policy_2: 0.02432
	accuracy_policy_2: 0.9423
	loss_value_2: 0.04541
	loss_reward_2: 0.00472
	loss_policy_3: 0.02428
	accuracy_policy_3: 0.94102
	loss_value_3: 0.04621
	loss_reward_3: 0.00545
	loss_policy_4: 0.02438
	accuracy_policy_4: 0.94238
	loss_value_4: 0.04732
	loss_reward_4: 0.00632
	loss_policy_5: 0.02414
	accuracy_policy_5: 0.94695
	loss_value_5: 0.04836
	loss_reward_5: 0.00709
	loss_policy: 0.24418
	loss_value: 0.45252
	loss_reward: 0.02767
[2025-05-11 15:57:17] nn step 32300, lr: 0.1.
	loss_policy_0: 0.13715
	accuracy_policy_0: 0.94625
	loss_value_0: 0.23844
	loss_policy_1: 0.02699
	accuracy_policy_1: 0.94426
	loss_value_1: 0.04781
	loss_reward_1: 0.00436
	loss_policy_2: 0.02719
	accuracy_policy_2: 0.94109
	loss_value_2: 0.04893
	loss_reward_2: 0.0051
	loss_policy_3: 0.027
	accuracy_policy_3: 0.94277
	loss_value_3: 0.05012
	loss_reward_3: 0.00568
	loss_policy_4: 0.02696
	accuracy_policy_4: 0.94398
	loss_value_4: 0.05139
	loss_reward_4: 0.00664
	loss_policy_5: 0.02695
	accuracy_policy_5: 0.94691
	loss_value_5: 0.05257
	loss_reward_5: 0.00814
	loss_policy: 0.27224
	loss_value: 0.48927
	loss_reward: 0.02991
[2025-05-11 15:57:24] nn step 32350, lr: 0.1.
	loss_policy_0: 0.13364
	accuracy_policy_0: 0.94379
	loss_value_0: 0.23823
	loss_policy_1: 0.02677
	accuracy_policy_1: 0.94164
	loss_value_1: 0.04782
	loss_reward_1: 0.00462
	loss_policy_2: 0.02685
	accuracy_policy_2: 0.93926
	loss_value_2: 0.0487
	loss_reward_2: 0.00525
	loss_policy_3: 0.02701
	accuracy_policy_3: 0.93801
	loss_value_3: 0.04975
	loss_reward_3: 0.00578
	loss_policy_4: 0.02686
	accuracy_policy_4: 0.94215
	loss_value_4: 0.05072
	loss_reward_4: 0.00682
	loss_policy_5: 0.0269
	accuracy_policy_5: 0.94543
	loss_value_5: 0.05191
	loss_reward_5: 0.0081
	loss_policy: 0.26804
	loss_value: 0.48714
	loss_reward: 0.03057
[2025-05-11 15:57:33] nn step 32400, lr: 0.1.
	loss_policy_0: 0.13031
	accuracy_policy_0: 0.94453
	loss_value_0: 0.22818
	loss_policy_1: 0.0259
	accuracy_policy_1: 0.94145
	loss_value_1: 0.04614
	loss_reward_1: 0.00427
	loss_policy_2: 0.02593
	accuracy_policy_2: 0.94082
	loss_value_2: 0.04707
	loss_reward_2: 0.00483
	loss_policy_3: 0.02586
	accuracy_policy_3: 0.94387
	loss_value_3: 0.04779
	loss_reward_3: 0.00549
	loss_policy_4: 0.02624
	accuracy_policy_4: 0.94449
	loss_value_4: 0.04884
	loss_reward_4: 0.00629
	loss_policy_5: 0.0258
	accuracy_policy_5: 0.94707
	loss_value_5: 0.04992
	loss_reward_5: 0.00755
	loss_policy: 0.26003
	loss_value: 0.46794
	loss_reward: 0.02843
Optimization_Done 32400
[2025-05-11 15:59:07] [command] train weight_iter_32400.pkl 144 163
[2025-05-11 15:59:15] nn step 32450, lr: 0.1.
	loss_policy_0: 0.11963
	accuracy_policy_0: 0.94953
	loss_value_0: 0.2251
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.94898
	loss_value_1: 0.04502
	loss_reward_1: 0.00412
	loss_policy_2: 0.02355
	accuracy_policy_2: 0.94836
	loss_value_2: 0.04569
	loss_reward_2: 0.00469
	loss_policy_3: 0.024
	accuracy_policy_3: 0.94461
	loss_value_3: 0.04677
	loss_reward_3: 0.00534
	loss_policy_4: 0.02405
	accuracy_policy_4: 0.94719
	loss_value_4: 0.04762
	loss_reward_4: 0.00612
	loss_policy_5: 0.02341
	accuracy_policy_5: 0.95273
	loss_value_5: 0.04857
	loss_reward_5: 0.00772
	loss_policy: 0.23821
	loss_value: 0.45878
	loss_reward: 0.02799
[2025-05-11 15:59:23] nn step 32500, lr: 0.1.
	loss_policy_0: 0.13334
	accuracy_policy_0: 0.94754
	loss_value_0: 0.23693
	loss_policy_1: 0.02637
	accuracy_policy_1: 0.9427
	loss_value_1: 0.04759
	loss_reward_1: 0.00437
	loss_policy_2: 0.02657
	accuracy_policy_2: 0.94605
	loss_value_2: 0.04895
	loss_reward_2: 0.00511
	loss_policy_3: 0.02634
	accuracy_policy_3: 0.94254
	loss_value_3: 0.0499
	loss_reward_3: 0.00556
	loss_policy_4: 0.02658
	accuracy_policy_4: 0.94574
	loss_value_4: 0.05104
	loss_reward_4: 0.00663
	loss_policy_5: 0.02624
	accuracy_policy_5: 0.95133
	loss_value_5: 0.05246
	loss_reward_5: 0.00788
	loss_policy: 0.26545
	loss_value: 0.48687
	loss_reward: 0.02955
[2025-05-11 15:59:32] nn step 32550, lr: 0.1.
	loss_policy_0: 0.14036
	accuracy_policy_0: 0.94754
	loss_value_0: 0.24341
	loss_policy_1: 0.02752
	accuracy_policy_1: 0.9457
	loss_value_1: 0.04886
	loss_reward_1: 0.0046
	loss_policy_2: 0.02766
	accuracy_policy_2: 0.94266
	loss_value_2: 0.04994
	loss_reward_2: 0.00542
	loss_policy_3: 0.02772
	accuracy_policy_3: 0.94145
	loss_value_3: 0.05131
	loss_reward_3: 0.00609
	loss_policy_4: 0.02795
	accuracy_policy_4: 0.9466
	loss_value_4: 0.05262
	loss_reward_4: 0.00698
	loss_policy_5: 0.02767
	accuracy_policy_5: 0.95004
	loss_value_5: 0.05388
	loss_reward_5: 0.0081
	loss_policy: 0.27888
	loss_value: 0.50001
	loss_reward: 0.03119
[2025-05-11 15:59:39] nn step 32600, lr: 0.1.
	loss_policy_0: 0.13044
	accuracy_policy_0: 0.94449
	loss_value_0: 0.2327
	loss_policy_1: 0.02647
	accuracy_policy_1: 0.93988
	loss_value_1: 0.04651
	loss_reward_1: 0.00445
	loss_policy_2: 0.0263
	accuracy_policy_2: 0.94332
	loss_value_2: 0.04749
	loss_reward_2: 0.00496
	loss_policy_3: 0.02612
	accuracy_policy_3: 0.94031
	loss_value_3: 0.04861
	loss_reward_3: 0.00584
	loss_policy_4: 0.02626
	accuracy_policy_4: 0.94426
	loss_value_4: 0.04979
	loss_reward_4: 0.0068
	loss_policy_5: 0.0262
	accuracy_policy_5: 0.94695
	loss_value_5: 0.05117
	loss_reward_5: 0.00756
	loss_policy: 0.26179
	loss_value: 0.47628
	loss_reward: 0.02961
Optimization_Done 32600
[2025-05-11 16:01:18] [command] train weight_iter_32600.pkl 145 164
[2025-05-11 16:01:28] nn step 32650, lr: 0.1.
	loss_policy_0: 0.13122
	accuracy_policy_0: 0.94969
	loss_value_0: 0.23567
	loss_policy_1: 0.02628
	accuracy_policy_1: 0.94711
	loss_value_1: 0.04684
	loss_reward_1: 0.00414
	loss_policy_2: 0.02626
	accuracy_policy_2: 0.94453
	loss_value_2: 0.04763
	loss_reward_2: 0.00501
	loss_policy_3: 0.02621
	accuracy_policy_3: 0.94633
	loss_value_3: 0.04873
	loss_reward_3: 0.00554
	loss_policy_4: 0.02591
	accuracy_policy_4: 0.94656
	loss_value_4: 0.05025
	loss_reward_4: 0.00668
	loss_policy_5: 0.02631
	accuracy_policy_5: 0.94879
	loss_value_5: 0.05128
	loss_reward_5: 0.00756
	loss_policy: 0.2622
	loss_value: 0.4804
	loss_reward: 0.02892
[2025-05-11 16:01:35] nn step 32700, lr: 0.1.
	loss_policy_0: 0.13476
	accuracy_policy_0: 0.94684
	loss_value_0: 0.23667
	loss_policy_1: 0.02631
	accuracy_policy_1: 0.94648
	loss_value_1: 0.04753
	loss_reward_1: 0.00428
	loss_policy_2: 0.02679
	accuracy_policy_2: 0.94613
	loss_value_2: 0.04863
	loss_reward_2: 0.00525
	loss_policy_3: 0.02697
	accuracy_policy_3: 0.94395
	loss_value_3: 0.04946
	loss_reward_3: 0.00574
	loss_policy_4: 0.02654
	accuracy_policy_4: 0.94473
	loss_value_4: 0.05025
	loss_reward_4: 0.0066
	loss_policy_5: 0.02669
	accuracy_policy_5: 0.94902
	loss_value_5: 0.05165
	loss_reward_5: 0.00796
	loss_policy: 0.26805
	loss_value: 0.48419
	loss_reward: 0.02984
[2025-05-11 16:01:43] nn step 32750, lr: 0.1.
	loss_policy_0: 0.13427
	accuracy_policy_0: 0.94637
	loss_value_0: 0.2334
	loss_policy_1: 0.0265
	accuracy_policy_1: 0.9443
	loss_value_1: 0.04717
	loss_reward_1: 0.00441
	loss_policy_2: 0.02667
	accuracy_policy_2: 0.94496
	loss_value_2: 0.04806
	loss_reward_2: 0.00501
	loss_policy_3: 0.0267
	accuracy_policy_3: 0.94387
	loss_value_3: 0.04887
	loss_reward_3: 0.00556
	loss_policy_4: 0.02643
	accuracy_policy_4: 0.94758
	loss_value_4: 0.05011
	loss_reward_4: 0.00663
	loss_policy_5: 0.02682
	accuracy_policy_5: 0.94738
	loss_value_5: 0.05149
	loss_reward_5: 0.00794
	loss_policy: 0.26739
	loss_value: 0.4791
	loss_reward: 0.02955
[2025-05-11 16:01:52] nn step 32800, lr: 0.1.
	loss_policy_0: 0.13488
	accuracy_policy_0: 0.94906
	loss_value_0: 0.23757
	loss_policy_1: 0.02674
	accuracy_policy_1: 0.94551
	loss_value_1: 0.048
	loss_reward_1: 0.00445
	loss_policy_2: 0.02686
	accuracy_policy_2: 0.94309
	loss_value_2: 0.04885
	loss_reward_2: 0.00517
	loss_policy_3: 0.02712
	accuracy_policy_3: 0.94406
	loss_value_3: 0.04969
	loss_reward_3: 0.00584
	loss_policy_4: 0.02697
	accuracy_policy_4: 0.94734
	loss_value_4: 0.05102
	loss_reward_4: 0.0068
	loss_policy_5: 0.02695
	accuracy_policy_5: 0.9498
	loss_value_5: 0.05245
	loss_reward_5: 0.00793
	loss_policy: 0.26952
	loss_value: 0.48758
	loss_reward: 0.03019
Optimization_Done 32800
[2025-05-11 16:03:27] [command] train weight_iter_32800.pkl 146 165
[2025-05-11 16:03:35] nn step 32850, lr: 0.1.
	loss_policy_0: 0.1344
	accuracy_policy_0: 0.9527
	loss_value_0: 0.24768
	loss_policy_1: 0.02661
	accuracy_policy_1: 0.94793
	loss_value_1: 0.04954
	loss_reward_1: 0.00439
	loss_policy_2: 0.02687
	accuracy_policy_2: 0.94738
	loss_value_2: 0.05023
	loss_reward_2: 0.00491
	loss_policy_3: 0.02666
	accuracy_policy_3: 0.94914
	loss_value_3: 0.0514
	loss_reward_3: 0.00581
	loss_policy_4: 0.02686
	accuracy_policy_4: 0.94844
	loss_value_4: 0.05243
	loss_reward_4: 0.00707
	loss_policy_5: 0.02665
	accuracy_policy_5: 0.95355
	loss_value_5: 0.05347
	loss_reward_5: 0.00795
	loss_policy: 0.26805
	loss_value: 0.50475
	loss_reward: 0.03014
[2025-05-11 16:03:43] nn step 32900, lr: 0.1.
	loss_policy_0: 0.13244
	accuracy_policy_0: 0.94988
	loss_value_0: 0.23377
	loss_policy_1: 0.02627
	accuracy_policy_1: 0.94707
	loss_value_1: 0.04715
	loss_reward_1: 0.00428
	loss_policy_2: 0.02602
	accuracy_policy_2: 0.94617
	loss_value_2: 0.04799
	loss_reward_2: 0.00501
	loss_policy_3: 0.02625
	accuracy_policy_3: 0.9473
	loss_value_3: 0.04873
	loss_reward_3: 0.00571
	loss_policy_4: 0.026
	accuracy_policy_4: 0.95219
	loss_value_4: 0.04972
	loss_reward_4: 0.00654
	loss_policy_5: 0.02637
	accuracy_policy_5: 0.95086
	loss_value_5: 0.05098
	loss_reward_5: 0.00757
	loss_policy: 0.26335
	loss_value: 0.47834
	loss_reward: 0.0291
[2025-05-11 16:03:52] nn step 32950, lr: 0.1.
	loss_policy_0: 0.12687
	accuracy_policy_0: 0.95117
	loss_value_0: 0.2244
	loss_policy_1: 0.02529
	accuracy_policy_1: 0.94652
	loss_value_1: 0.04528
	loss_reward_1: 0.00445
	loss_policy_2: 0.0259
	accuracy_policy_2: 0.94484
	loss_value_2: 0.04625
	loss_reward_2: 0.00487
	loss_policy_3: 0.02563
	accuracy_policy_3: 0.94594
	loss_value_3: 0.04732
	loss_reward_3: 0.00576
	loss_policy_4: 0.02528
	accuracy_policy_4: 0.94953
	loss_value_4: 0.04819
	loss_reward_4: 0.00674
	loss_policy_5: 0.02504
	accuracy_policy_5: 0.95203
	loss_value_5: 0.04942
	loss_reward_5: 0.00766
	loss_policy: 0.25402
	loss_value: 0.46086
	loss_reward: 0.02948
[2025-05-11 16:03:59] nn step 33000, lr: 0.1.
	loss_policy_0: 0.12968
	accuracy_policy_0: 0.95
	loss_value_0: 0.22573
	loss_policy_1: 0.02563
	accuracy_policy_1: 0.9468
	loss_value_1: 0.04545
	loss_reward_1: 0.00433
	loss_policy_2: 0.02591
	accuracy_policy_2: 0.94734
	loss_value_2: 0.04668
	loss_reward_2: 0.00487
	loss_policy_3: 0.02561
	accuracy_policy_3: 0.94773
	loss_value_3: 0.04745
	loss_reward_3: 0.00548
	loss_policy_4: 0.02581
	accuracy_policy_4: 0.95008
	loss_value_4: 0.04836
	loss_reward_4: 0.00646
	loss_policy_5: 0.02566
	accuracy_policy_5: 0.95121
	loss_value_5: 0.04971
	loss_reward_5: 0.00772
	loss_policy: 0.25829
	loss_value: 0.46338
	loss_reward: 0.02885
Optimization_Done 33000
[2025-05-11 16:05:36] [command] train weight_iter_33000.pkl 147 166
[2025-05-11 16:05:46] nn step 33050, lr: 0.1.
	loss_policy_0: 0.12289
	accuracy_policy_0: 0.94855
	loss_value_0: 0.22658
	loss_policy_1: 0.0242
	accuracy_policy_1: 0.94895
	loss_value_1: 0.04517
	loss_reward_1: 0.00418
	loss_policy_2: 0.02452
	accuracy_policy_2: 0.94523
	loss_value_2: 0.04617
	loss_reward_2: 0.0046
	loss_policy_3: 0.02455
	accuracy_policy_3: 0.94602
	loss_value_3: 0.04698
	loss_reward_3: 0.00507
	loss_policy_4: 0.02466
	accuracy_policy_4: 0.94738
	loss_value_4: 0.04794
	loss_reward_4: 0.0063
	loss_policy_5: 0.02472
	accuracy_policy_5: 0.95555
	loss_value_5: 0.04902
	loss_reward_5: 0.00747
	loss_policy: 0.24554
	loss_value: 0.46186
	loss_reward: 0.02762
[2025-05-11 16:05:53] nn step 33100, lr: 0.1.
	loss_policy_0: 0.12878
	accuracy_policy_0: 0.94871
	loss_value_0: 0.22906
	loss_policy_1: 0.02523
	accuracy_policy_1: 0.94543
	loss_value_1: 0.04622
	loss_reward_1: 0.00416
	loss_policy_2: 0.02556
	accuracy_policy_2: 0.94441
	loss_value_2: 0.04688
	loss_reward_2: 0.0049
	loss_policy_3: 0.02597
	accuracy_policy_3: 0.9425
	loss_value_3: 0.04766
	loss_reward_3: 0.00543
	loss_policy_4: 0.02581
	accuracy_policy_4: 0.94754
	loss_value_4: 0.04865
	loss_reward_4: 0.00634
	loss_policy_5: 0.0256
	accuracy_policy_5: 0.95215
	loss_value_5: 0.04984
	loss_reward_5: 0.00739
	loss_policy: 0.25696
	loss_value: 0.46831
	loss_reward: 0.02821
[2025-05-11 16:06:02] nn step 33150, lr: 0.1.
	loss_policy_0: 0.13393
	accuracy_policy_0: 0.94793
	loss_value_0: 0.23699
	loss_policy_1: 0.02673
	accuracy_policy_1: 0.9493
	loss_value_1: 0.04747
	loss_reward_1: 0.00471
	loss_policy_2: 0.02656
	accuracy_policy_2: 0.94715
	loss_value_2: 0.04849
	loss_reward_2: 0.00508
	loss_policy_3: 0.02637
	accuracy_policy_3: 0.94648
	loss_value_3: 0.04931
	loss_reward_3: 0.00592
	loss_policy_4: 0.02678
	accuracy_policy_4: 0.94887
	loss_value_4: 0.05014
	loss_reward_4: 0.00731
	loss_policy_5: 0.02668
	accuracy_policy_5: 0.95332
	loss_value_5: 0.05179
	loss_reward_5: 0.00809
	loss_policy: 0.26706
	loss_value: 0.4842
	loss_reward: 0.03111
[2025-05-11 16:06:10] nn step 33200, lr: 0.1.
	loss_policy_0: 0.13248
	accuracy_policy_0: 0.94738
	loss_value_0: 0.23432
	loss_policy_1: 0.02656
	accuracy_policy_1: 0.94332
	loss_value_1: 0.0473
	loss_reward_1: 0.00424
	loss_policy_2: 0.02654
	accuracy_policy_2: 0.94152
	loss_value_2: 0.04792
	loss_reward_2: 0.00489
	loss_policy_3: 0.0263
	accuracy_policy_3: 0.94305
	loss_value_3: 0.04907
	loss_reward_3: 0.00578
	loss_policy_4: 0.02654
	accuracy_policy_4: 0.94289
	loss_value_4: 0.05038
	loss_reward_4: 0.00696
	loss_policy_5: 0.02619
	accuracy_policy_5: 0.9498
	loss_value_5: 0.05147
	loss_reward_5: 0.00797
	loss_policy: 0.26462
	loss_value: 0.48047
	loss_reward: 0.02984
Optimization_Done 33200
[2025-05-11 16:07:45] [command] train weight_iter_33200.pkl 148 167
[2025-05-11 16:07:55] nn step 33250, lr: 0.1.
	loss_policy_0: 0.12762
	accuracy_policy_0: 0.94809
	loss_value_0: 0.22965
	loss_policy_1: 0.02519
	accuracy_policy_1: 0.94613
	loss_value_1: 0.04582
	loss_reward_1: 0.00426
	loss_policy_2: 0.02518
	accuracy_policy_2: 0.94559
	loss_value_2: 0.04652
	loss_reward_2: 0.00465
	loss_policy_3: 0.0255
	accuracy_policy_3: 0.94637
	loss_value_3: 0.04741
	loss_reward_3: 0.00534
	loss_policy_4: 0.02527
	accuracy_policy_4: 0.9491
	loss_value_4: 0.04858
	loss_reward_4: 0.00631
	loss_policy_5: 0.02521
	accuracy_policy_5: 0.95246
	loss_value_5: 0.04959
	loss_reward_5: 0.00747
	loss_policy: 0.25396
	loss_value: 0.46758
	loss_reward: 0.02804
[2025-05-11 16:08:03] nn step 33300, lr: 0.1.
	loss_policy_0: 0.13529
	accuracy_policy_0: 0.94844
	loss_value_0: 0.24141
	loss_policy_1: 0.02674
	accuracy_policy_1: 0.9466
	loss_value_1: 0.0486
	loss_reward_1: 0.00453
	loss_policy_2: 0.02697
	accuracy_policy_2: 0.94523
	loss_value_2: 0.04934
	loss_reward_2: 0.00491
	loss_policy_3: 0.02706
	accuracy_policy_3: 0.94523
	loss_value_3: 0.05045
	loss_reward_3: 0.00568
	loss_policy_4: 0.02724
	accuracy_policy_4: 0.94664
	loss_value_4: 0.05171
	loss_reward_4: 0.00676
	loss_policy_5: 0.02681
	accuracy_policy_5: 0.94977
	loss_value_5: 0.05274
	loss_reward_5: 0.00768
	loss_policy: 0.2701
	loss_value: 0.49423
	loss_reward: 0.02955
[2025-05-11 16:08:10] nn step 33350, lr: 0.1.
	loss_policy_0: 0.1253
	accuracy_policy_0: 0.94934
	loss_value_0: 0.2218
	loss_policy_1: 0.02515
	accuracy_policy_1: 0.94715
	loss_value_1: 0.04467
	loss_reward_1: 0.00417
	loss_policy_2: 0.0248
	accuracy_policy_2: 0.9457
	loss_value_2: 0.04547
	loss_reward_2: 0.00467
	loss_policy_3: 0.02471
	accuracy_policy_3: 0.94707
	loss_value_3: 0.04634
	loss_reward_3: 0.00553
	loss_policy_4: 0.02486
	accuracy_policy_4: 0.94793
	loss_value_4: 0.0478
	loss_reward_4: 0.00639
	loss_policy_5: 0.02483
	accuracy_policy_5: 0.9498
	loss_value_5: 0.04885
	loss_reward_5: 0.00755
	loss_policy: 0.24964
	loss_value: 0.45493
	loss_reward: 0.02832
[2025-05-11 16:08:19] nn step 33400, lr: 0.1.
	loss_policy_0: 0.12933
	accuracy_policy_0: 0.94875
	loss_value_0: 0.22768
	loss_policy_1: 0.02541
	accuracy_policy_1: 0.94715
	loss_value_1: 0.04573
	loss_reward_1: 0.00413
	loss_policy_2: 0.02557
	accuracy_policy_2: 0.94484
	loss_value_2: 0.04676
	loss_reward_2: 0.00498
	loss_policy_3: 0.02554
	accuracy_policy_3: 0.9443
	loss_value_3: 0.04756
	loss_reward_3: 0.00541
	loss_policy_4: 0.02573
	accuracy_policy_4: 0.94977
	loss_value_4: 0.04877
	loss_reward_4: 0.0063
	loss_policy_5: 0.02525
	accuracy_policy_5: 0.95152
	loss_value_5: 0.05046
	loss_reward_5: 0.00775
	loss_policy: 0.25683
	loss_value: 0.46696
	loss_reward: 0.02859
Optimization_Done 33400
[2025-05-11 16:09:56] [command] train weight_iter_33400.pkl 149 168
[2025-05-11 16:10:04] nn step 33450, lr: 0.1.
	loss_policy_0: 0.13517
	accuracy_policy_0: 0.9502
	loss_value_0: 0.24479
	loss_policy_1: 0.02637
	accuracy_policy_1: 0.94781
	loss_value_1: 0.04904
	loss_reward_1: 0.00454
	loss_policy_2: 0.02645
	accuracy_policy_2: 0.9457
	loss_value_2: 0.04986
	loss_reward_2: 0.00505
	loss_policy_3: 0.02659
	accuracy_policy_3: 0.94656
	loss_value_3: 0.05055
	loss_reward_3: 0.00561
	loss_policy_4: 0.02679
	accuracy_policy_4: 0.9468
	loss_value_4: 0.05162
	loss_reward_4: 0.0065
	loss_policy_5: 0.02645
	accuracy_policy_5: 0.95223
	loss_value_5: 0.05252
	loss_reward_5: 0.00789
	loss_policy: 0.26783
	loss_value: 0.49838
	loss_reward: 0.02959
[2025-05-11 16:10:13] nn step 33500, lr: 0.1.
	loss_policy_0: 0.1297
	accuracy_policy_0: 0.95273
	loss_value_0: 0.22951
	loss_policy_1: 0.02571
	accuracy_policy_1: 0.94766
	loss_value_1: 0.04568
	loss_reward_1: 0.00418
	loss_policy_2: 0.02549
	accuracy_policy_2: 0.94562
	loss_value_2: 0.04667
	loss_reward_2: 0.00484
	loss_policy_3: 0.02578
	accuracy_policy_3: 0.94566
	loss_value_3: 0.04768
	loss_reward_3: 0.00535
	loss_policy_4: 0.02567
	accuracy_policy_4: 0.94906
	loss_value_4: 0.04886
	loss_reward_4: 0.00662
	loss_policy_5: 0.02565
	accuracy_policy_5: 0.9525
	loss_value_5: 0.05039
	loss_reward_5: 0.00768
	loss_policy: 0.25801
	loss_value: 0.46881
	loss_reward: 0.02866
[2025-05-11 16:10:21] nn step 33550, lr: 0.1.
	loss_policy_0: 0.1295
	accuracy_policy_0: 0.94953
	loss_value_0: 0.23189
	loss_policy_1: 0.02575
	accuracy_policy_1: 0.94406
	loss_value_1: 0.04653
	loss_reward_1: 0.00424
	loss_policy_2: 0.02604
	accuracy_policy_2: 0.94387
	loss_value_2: 0.0476
	loss_reward_2: 0.00498
	loss_policy_3: 0.0255
	accuracy_policy_3: 0.945
	loss_value_3: 0.04836
	loss_reward_3: 0.00555
	loss_policy_4: 0.02614
	accuracy_policy_4: 0.94535
	loss_value_4: 0.04942
	loss_reward_4: 0.00674
	loss_policy_5: 0.02555
	accuracy_policy_5: 0.95098
	loss_value_5: 0.05054
	loss_reward_5: 0.00776
	loss_policy: 0.25848
	loss_value: 0.47434
	loss_reward: 0.02927
[2025-05-11 16:10:30] nn step 33600, lr: 0.1.
	loss_policy_0: 0.13419
	accuracy_policy_0: 0.94781
	loss_value_0: 0.23802
	loss_policy_1: 0.0267
	accuracy_policy_1: 0.94469
	loss_value_1: 0.0479
	loss_reward_1: 0.00454
	loss_policy_2: 0.02692
	accuracy_policy_2: 0.94156
	loss_value_2: 0.0486
	loss_reward_2: 0.00507
	loss_policy_3: 0.02677
	accuracy_policy_3: 0.94328
	loss_value_3: 0.04926
	loss_reward_3: 0.00547
	loss_policy_4: 0.02666
	accuracy_policy_4: 0.94809
	loss_value_4: 0.05021
	loss_reward_4: 0.00686
	loss_policy_5: 0.02649
	accuracy_policy_5: 0.94914
	loss_value_5: 0.05181
	loss_reward_5: 0.00801
	loss_policy: 0.26772
	loss_value: 0.48579
	loss_reward: 0.02996
Optimization_Done 33600
[2025-05-11 16:12:06] [command] train weight_iter_33600.pkl 150 169
[2025-05-11 16:12:16] nn step 33650, lr: 0.1.
	loss_policy_0: 0.12044
	accuracy_policy_0: 0.95285
	loss_value_0: 0.22082
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.95281
	loss_value_1: 0.04401
	loss_reward_1: 0.00385
	loss_policy_2: 0.02383
	accuracy_policy_2: 0.94906
	loss_value_2: 0.04505
	loss_reward_2: 0.00455
	loss_policy_3: 0.02369
	accuracy_policy_3: 0.94949
	loss_value_3: 0.04565
	loss_reward_3: 0.00497
	loss_policy_4: 0.02399
	accuracy_policy_4: 0.94879
	loss_value_4: 0.0467
	loss_reward_4: 0.00586
	loss_policy_5: 0.02389
	accuracy_policy_5: 0.95273
	loss_value_5: 0.04773
	loss_reward_5: 0.00723
	loss_policy: 0.2397
	loss_value: 0.44996
	loss_reward: 0.02647
[2025-05-11 16:12:25] nn step 33700, lr: 0.1.
	loss_policy_0: 0.13067
	accuracy_policy_0: 0.95238
	loss_value_0: 0.23426
	loss_policy_1: 0.02579
	accuracy_policy_1: 0.94863
	loss_value_1: 0.04691
	loss_reward_1: 0.00445
	loss_policy_2: 0.02594
	accuracy_policy_2: 0.9457
	loss_value_2: 0.04785
	loss_reward_2: 0.00481
	loss_policy_3: 0.02559
	accuracy_policy_3: 0.94859
	loss_value_3: 0.04887
	loss_reward_3: 0.00558
	loss_policy_4: 0.02592
	accuracy_policy_4: 0.94867
	loss_value_4: 0.05012
	loss_reward_4: 0.00676
	loss_policy_5: 0.02563
	accuracy_policy_5: 0.95121
	loss_value_5: 0.05134
	loss_reward_5: 0.00765
	loss_policy: 0.25954
	loss_value: 0.47935
	loss_reward: 0.02925
[2025-05-11 16:12:32] nn step 33750, lr: 0.1.
	loss_policy_0: 0.13137
	accuracy_policy_0: 0.94785
	loss_value_0: 0.23152
	loss_policy_1: 0.02584
	accuracy_policy_1: 0.94746
	loss_value_1: 0.04658
	loss_reward_1: 0.00419
	loss_policy_2: 0.02616
	accuracy_policy_2: 0.94746
	loss_value_2: 0.04767
	loss_reward_2: 0.00501
	loss_policy_3: 0.02611
	accuracy_policy_3: 0.94559
	loss_value_3: 0.0487
	loss_reward_3: 0.00555
	loss_policy_4: 0.02617
	accuracy_policy_4: 0.94957
	loss_value_4: 0.04939
	loss_reward_4: 0.00664
	loss_policy_5: 0.02627
	accuracy_policy_5: 0.95086
	loss_value_5: 0.05091
	loss_reward_5: 0.00795
	loss_policy: 0.26192
	loss_value: 0.47477
	loss_reward: 0.02933
[2025-05-11 16:12:40] nn step 33800, lr: 0.1.
	loss_policy_0: 0.13855
	accuracy_policy_0: 0.94797
	loss_value_0: 0.24612
	loss_policy_1: 0.02743
	accuracy_policy_1: 0.94602
	loss_value_1: 0.04946
	loss_reward_1: 0.00444
	loss_policy_2: 0.02766
	accuracy_policy_2: 0.94508
	loss_value_2: 0.0504
	loss_reward_2: 0.00531
	loss_policy_3: 0.02726
	accuracy_policy_3: 0.94711
	loss_value_3: 0.05119
	loss_reward_3: 0.00619
	loss_policy_4: 0.02761
	accuracy_policy_4: 0.94793
	loss_value_4: 0.05235
	loss_reward_4: 0.00722
	loss_policy_5: 0.02768
	accuracy_policy_5: 0.95254
	loss_value_5: 0.05376
	loss_reward_5: 0.00842
	loss_policy: 0.27619
	loss_value: 0.50328
	loss_reward: 0.03158
Optimization_Done 33800
[2025-05-11 16:14:16] [command] train weight_iter_33800.pkl 151 170
[2025-05-11 16:14:24] nn step 33850, lr: 0.1.
	loss_policy_0: 0.12785
	accuracy_policy_0: 0.94957
	loss_value_0: 0.23737
	loss_policy_1: 0.02567
	accuracy_policy_1: 0.94777
	loss_value_1: 0.04754
	loss_reward_1: 0.00416
	loss_policy_2: 0.02569
	accuracy_policy_2: 0.94309
	loss_value_2: 0.04821
	loss_reward_2: 0.00488
	loss_policy_3: 0.02588
	accuracy_policy_3: 0.94348
	loss_value_3: 0.04917
	loss_reward_3: 0.00567
	loss_policy_4: 0.02603
	accuracy_policy_4: 0.94535
	loss_value_4: 0.05016
	loss_reward_4: 0.00676
	loss_policy_5: 0.02604
	accuracy_policy_5: 0.94805
	loss_value_5: 0.05102
	loss_reward_5: 0.00774
	loss_policy: 0.25717
	loss_value: 0.48348
	loss_reward: 0.0292
[2025-05-11 16:14:33] nn step 33900, lr: 0.1.
	loss_policy_0: 0.1381
	accuracy_policy_0: 0.95227
	loss_value_0: 0.24757
	loss_policy_1: 0.02788
	accuracy_policy_1: 0.94738
	loss_value_1: 0.04978
	loss_reward_1: 0.00462
	loss_policy_2: 0.02746
	accuracy_policy_2: 0.94445
	loss_value_2: 0.05057
	loss_reward_2: 0.00525
	loss_policy_3: 0.02773
	accuracy_policy_3: 0.94672
	loss_value_3: 0.05157
	loss_reward_3: 0.0059
	loss_policy_4: 0.02745
	accuracy_policy_4: 0.94887
	loss_value_4: 0.05279
	loss_reward_4: 0.00729
	loss_policy_5: 0.0277
	accuracy_policy_5: 0.95094
	loss_value_5: 0.05402
	loss_reward_5: 0.00835
	loss_policy: 0.27632
	loss_value: 0.50629
	loss_reward: 0.0314
[2025-05-11 16:14:41] nn step 33950, lr: 0.1.
	loss_policy_0: 0.14196
	accuracy_policy_0: 0.9475
	loss_value_0: 0.25528
	loss_policy_1: 0.02795
	accuracy_policy_1: 0.94727
	loss_value_1: 0.05167
	loss_reward_1: 0.00476
	loss_policy_2: 0.02809
	accuracy_policy_2: 0.94508
	loss_value_2: 0.05266
	loss_reward_2: 0.00553
	loss_policy_3: 0.02834
	accuracy_policy_3: 0.94719
	loss_value_3: 0.05336
	loss_reward_3: 0.00649
	loss_policy_4: 0.02795
	accuracy_policy_4: 0.94773
	loss_value_4: 0.05461
	loss_reward_4: 0.00743
	loss_policy_5: 0.02842
	accuracy_policy_5: 0.95016
	loss_value_5: 0.05591
	loss_reward_5: 0.00858
	loss_policy: 0.28271
	loss_value: 0.52349
	loss_reward: 0.0328
[2025-05-11 16:14:50] nn step 34000, lr: 0.1.
	loss_policy_0: 0.12338
	accuracy_policy_0: 0.95074
	loss_value_0: 0.22152
	loss_policy_1: 0.02444
	accuracy_policy_1: 0.94785
	loss_value_1: 0.04453
	loss_reward_1: 0.00407
	loss_policy_2: 0.02437
	accuracy_policy_2: 0.94703
	loss_value_2: 0.04534
	loss_reward_2: 0.00482
	loss_policy_3: 0.02464
	accuracy_policy_3: 0.9459
	loss_value_3: 0.04604
	loss_reward_3: 0.00528
	loss_policy_4: 0.0245
	accuracy_policy_4: 0.95004
	loss_value_4: 0.04697
	loss_reward_4: 0.00608
	loss_policy_5: 0.02439
	accuracy_policy_5: 0.95215
	loss_value_5: 0.04822
	loss_reward_5: 0.00735
	loss_policy: 0.24572
	loss_value: 0.45263
	loss_reward: 0.02761
Optimization_Done 34000
[2025-05-11 16:16:26] [command] train weight_iter_34000.pkl 152 171
[2025-05-11 16:16:35] nn step 34050, lr: 0.1.
	loss_policy_0: 0.13376
	accuracy_policy_0: 0.95
	loss_value_0: 0.25017
	loss_policy_1: 0.02657
	accuracy_policy_1: 0.94711
	loss_value_1: 0.04971
	loss_reward_1: 0.00461
	loss_policy_2: 0.02702
	accuracy_policy_2: 0.94332
	loss_value_2: 0.05065
	loss_reward_2: 0.00521
	loss_policy_3: 0.02636
	accuracy_policy_3: 0.94652
	loss_value_3: 0.05144
	loss_reward_3: 0.00595
	loss_policy_4: 0.02658
	accuracy_policy_4: 0.94941
	loss_value_4: 0.0524
	loss_reward_4: 0.00719
	loss_policy_5: 0.02677
	accuracy_policy_5: 0.95188
	loss_value_5: 0.05365
	loss_reward_5: 0.00817
	loss_policy: 0.26705
	loss_value: 0.50801
	loss_reward: 0.03114
[2025-05-11 16:16:43] nn step 34100, lr: 0.1.
	loss_policy_0: 0.12962
	accuracy_policy_0: 0.95012
	loss_value_0: 0.23387
	loss_policy_1: 0.02568
	accuracy_policy_1: 0.94605
	loss_value_1: 0.04686
	loss_reward_1: 0.00429
	loss_policy_2: 0.02617
	accuracy_policy_2: 0.94305
	loss_value_2: 0.04775
	loss_reward_2: 0.00508
	loss_policy_3: 0.02585
	accuracy_policy_3: 0.94531
	loss_value_3: 0.04871
	loss_reward_3: 0.00583
	loss_policy_4: 0.02582
	accuracy_policy_4: 0.94793
	loss_value_4: 0.04964
	loss_reward_4: 0.00652
	loss_policy_5: 0.02561
	accuracy_policy_5: 0.95
	loss_value_5: 0.05062
	loss_reward_5: 0.00806
	loss_policy: 0.25874
	loss_value: 0.47746
	loss_reward: 0.02978
[2025-05-11 16:16:51] nn step 34150, lr: 0.1.
	loss_policy_0: 0.12645
	accuracy_policy_0: 0.94809
	loss_value_0: 0.22611
	loss_policy_1: 0.02507
	accuracy_policy_1: 0.94492
	loss_value_1: 0.0453
	loss_reward_1: 0.00425
	loss_policy_2: 0.02507
	accuracy_policy_2: 0.94434
	loss_value_2: 0.0462
	loss_reward_2: 0.00488
	loss_policy_3: 0.02476
	accuracy_policy_3: 0.94527
	loss_value_3: 0.04723
	loss_reward_3: 0.00551
	loss_policy_4: 0.02505
	accuracy_policy_4: 0.94797
	loss_value_4: 0.04816
	loss_reward_4: 0.00646
	loss_policy_5: 0.02498
	accuracy_policy_5: 0.95242
	loss_value_5: 0.04958
	loss_reward_5: 0.00753
	loss_policy: 0.25138
	loss_value: 0.46258
	loss_reward: 0.02863
[2025-05-11 16:16:59] nn step 34200, lr: 0.1.
	loss_policy_0: 0.13689
	accuracy_policy_0: 0.94996
	loss_value_0: 0.24687
	loss_policy_1: 0.02716
	accuracy_policy_1: 0.9452
	loss_value_1: 0.04961
	loss_reward_1: 0.00467
	loss_policy_2: 0.02734
	accuracy_policy_2: 0.94348
	loss_value_2: 0.05092
	loss_reward_2: 0.00523
	loss_policy_3: 0.02763
	accuracy_policy_3: 0.94449
	loss_value_3: 0.05172
	loss_reward_3: 0.00598
	loss_policy_4: 0.0275
	accuracy_policy_4: 0.94617
	loss_value_4: 0.05276
	loss_reward_4: 0.0071
	loss_policy_5: 0.02741
	accuracy_policy_5: 0.95164
	loss_value_5: 0.05405
	loss_reward_5: 0.00856
	loss_policy: 0.27394
	loss_value: 0.50592
	loss_reward: 0.03154
Optimization_Done 34200
[2025-05-11 16:18:34] [command] train weight_iter_34200.pkl 153 172
[2025-05-11 16:18:42] nn step 34250, lr: 0.1.
	loss_policy_0: 0.12098
	accuracy_policy_0: 0.9502
	loss_value_0: 0.22213
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.94559
	loss_value_1: 0.04436
	loss_reward_1: 0.00403
	loss_policy_2: 0.02395
	accuracy_policy_2: 0.9432
	loss_value_2: 0.04502
	loss_reward_2: 0.00455
	loss_policy_3: 0.0241
	accuracy_policy_3: 0.94445
	loss_value_3: 0.046
	loss_reward_3: 0.00511
	loss_policy_4: 0.02418
	accuracy_policy_4: 0.94828
	loss_value_4: 0.04717
	loss_reward_4: 0.00619
	loss_policy_5: 0.02374
	accuracy_policy_5: 0.95125
	loss_value_5: 0.04838
	loss_reward_5: 0.0074
	loss_policy: 0.24088
	loss_value: 0.45306
	loss_reward: 0.02728
[2025-05-11 16:18:50] nn step 34300, lr: 0.1.
	loss_policy_0: 0.12712
	accuracy_policy_0: 0.94691
	loss_value_0: 0.2261
	loss_policy_1: 0.02513
	accuracy_policy_1: 0.94473
	loss_value_1: 0.04548
	loss_reward_1: 0.00431
	loss_policy_2: 0.02508
	accuracy_policy_2: 0.94199
	loss_value_2: 0.04643
	loss_reward_2: 0.00468
	loss_policy_3: 0.02492
	accuracy_policy_3: 0.94191
	loss_value_3: 0.04761
	loss_reward_3: 0.00519
	loss_policy_4: 0.02531
	accuracy_policy_4: 0.94738
	loss_value_4: 0.04855
	loss_reward_4: 0.0064
	loss_policy_5: 0.02533
	accuracy_policy_5: 0.94953
	loss_value_5: 0.04958
	loss_reward_5: 0.00758
	loss_policy: 0.25288
	loss_value: 0.46375
	loss_reward: 0.02815
[2025-05-11 16:18:59] nn step 34350, lr: 0.1.
	loss_policy_0: 0.13514
	accuracy_policy_0: 0.94621
	loss_value_0: 0.23891
	loss_policy_1: 0.02608
	accuracy_policy_1: 0.9425
	loss_value_1: 0.048
	loss_reward_1: 0.00447
	loss_policy_2: 0.02619
	accuracy_policy_2: 0.94199
	loss_value_2: 0.04892
	loss_reward_2: 0.00508
	loss_policy_3: 0.0265
	accuracy_policy_3: 0.94281
	loss_value_3: 0.05008
	loss_reward_3: 0.00578
	loss_policy_4: 0.02625
	accuracy_policy_4: 0.94492
	loss_value_4: 0.05127
	loss_reward_4: 0.00699
	loss_policy_5: 0.02634
	accuracy_policy_5: 0.95
	loss_value_5: 0.05265
	loss_reward_5: 0.00806
	loss_policy: 0.26651
	loss_value: 0.48983
	loss_reward: 0.03037
[2025-05-11 16:19:06] nn step 34400, lr: 0.1.
	loss_policy_0: 0.125
	accuracy_policy_0: 0.94688
	loss_value_0: 0.22303
	loss_policy_1: 0.02422
	accuracy_policy_1: 0.94383
	loss_value_1: 0.04496
	loss_reward_1: 0.0041
	loss_policy_2: 0.02472
	accuracy_policy_2: 0.94277
	loss_value_2: 0.0457
	loss_reward_2: 0.00444
	loss_policy_3: 0.02452
	accuracy_policy_3: 0.94199
	loss_value_3: 0.04658
	loss_reward_3: 0.00514
	loss_policy_4: 0.02445
	accuracy_policy_4: 0.94703
	loss_value_4: 0.04751
	loss_reward_4: 0.00644
	loss_policy_5: 0.0246
	accuracy_policy_5: 0.95074
	loss_value_5: 0.04828
	loss_reward_5: 0.00726
	loss_policy: 0.24751
	loss_value: 0.45606
	loss_reward: 0.02737
Optimization_Done 34400
[2025-05-11 16:20:42] [command] train weight_iter_34400.pkl 154 173
[2025-05-11 16:20:51] nn step 34450, lr: 0.1.
	loss_policy_0: 0.12274
	accuracy_policy_0: 0.94828
	loss_value_0: 0.22472
	loss_policy_1: 0.02424
	accuracy_policy_1: 0.9459
	loss_value_1: 0.04483
	loss_reward_1: 0.004
	loss_policy_2: 0.02478
	accuracy_policy_2: 0.94352
	loss_value_2: 0.04604
	loss_reward_2: 0.00441
	loss_policy_3: 0.02458
	accuracy_policy_3: 0.94301
	loss_value_3: 0.04662
	loss_reward_3: 0.00528
	loss_policy_4: 0.02434
	accuracy_policy_4: 0.94551
	loss_value_4: 0.04754
	loss_reward_4: 0.00634
	loss_policy_5: 0.02433
	accuracy_policy_5: 0.95238
	loss_value_5: 0.04823
	loss_reward_5: 0.00699
	loss_policy: 0.245
	loss_value: 0.45798
	loss_reward: 0.02701
[2025-05-11 16:20:58] nn step 34500, lr: 0.1.
	loss_policy_0: 0.13186
	accuracy_policy_0: 0.94793
	loss_value_0: 0.23484
	loss_policy_1: 0.02562
	accuracy_policy_1: 0.94727
	loss_value_1: 0.04735
	loss_reward_1: 0.00433
	loss_policy_2: 0.02606
	accuracy_policy_2: 0.94422
	loss_value_2: 0.04823
	loss_reward_2: 0.00511
	loss_policy_3: 0.02632
	accuracy_policy_3: 0.94188
	loss_value_3: 0.04914
	loss_reward_3: 0.00554
	loss_policy_4: 0.02629
	accuracy_policy_4: 0.94672
	loss_value_4: 0.04993
	loss_reward_4: 0.00629
	loss_policy_5: 0.02598
	accuracy_policy_5: 0.95215
	loss_value_5: 0.05101
	loss_reward_5: 0.00765
	loss_policy: 0.26213
	loss_value: 0.48051
	loss_reward: 0.02893
[2025-05-11 16:21:07] nn step 34550, lr: 0.1.
	loss_policy_0: 0.12699
	accuracy_policy_0: 0.94598
	loss_value_0: 0.22902
	loss_policy_1: 0.02512
	accuracy_policy_1: 0.94242
	loss_value_1: 0.04597
	loss_reward_1: 0.00416
	loss_policy_2: 0.02508
	accuracy_policy_2: 0.94168
	loss_value_2: 0.047
	loss_reward_2: 0.00485
	loss_policy_3: 0.02533
	accuracy_policy_3: 0.9423
	loss_value_3: 0.04819
	loss_reward_3: 0.00552
	loss_policy_4: 0.02509
	accuracy_policy_4: 0.94492
	loss_value_4: 0.04887
	loss_reward_4: 0.00641
	loss_policy_5: 0.02511
	accuracy_policy_5: 0.9509
	loss_value_5: 0.05035
	loss_reward_5: 0.00746
	loss_policy: 0.25271
	loss_value: 0.46941
	loss_reward: 0.02841
[2025-05-11 16:21:16] nn step 34600, lr: 0.1.
	loss_policy_0: 0.13079
	accuracy_policy_0: 0.94723
	loss_value_0: 0.23317
	loss_policy_1: 0.02576
	accuracy_policy_1: 0.94445
	loss_value_1: 0.04683
	loss_reward_1: 0.00404
	loss_policy_2: 0.02591
	accuracy_policy_2: 0.9441
	loss_value_2: 0.04761
	loss_reward_2: 0.00495
	loss_policy_3: 0.02579
	accuracy_policy_3: 0.94391
	loss_value_3: 0.04883
	loss_reward_3: 0.00604
	loss_policy_4: 0.02621
	accuracy_policy_4: 0.94316
	loss_value_4: 0.04978
	loss_reward_4: 0.00675
	loss_policy_5: 0.02587
	accuracy_policy_5: 0.94824
	loss_value_5: 0.05101
	loss_reward_5: 0.00785
	loss_policy: 0.26033
	loss_value: 0.47722
	loss_reward: 0.02963
Optimization_Done 34600
[2025-05-11 16:22:56] [command] train weight_iter_34600.pkl 155 174
[2025-05-11 16:23:06] nn step 34650, lr: 0.1.
	loss_policy_0: 0.12865
	accuracy_policy_0: 0.94086
	loss_value_0: 0.23915
	loss_policy_1: 0.02548
	accuracy_policy_1: 0.94074
	loss_value_1: 0.04783
	loss_reward_1: 0.00423
	loss_policy_2: 0.02579
	accuracy_policy_2: 0.94164
	loss_value_2: 0.04858
	loss_reward_2: 0.00484
	loss_policy_3: 0.02579
	accuracy_policy_3: 0.94035
	loss_value_3: 0.04944
	loss_reward_3: 0.00582
	loss_policy_4: 0.02558
	accuracy_policy_4: 0.94203
	loss_value_4: 0.05017
	loss_reward_4: 0.00662
	loss_policy_5: 0.02558
	accuracy_policy_5: 0.94598
	loss_value_5: 0.05106
	loss_reward_5: 0.00778
	loss_policy: 0.25686
	loss_value: 0.48622
	loss_reward: 0.02929
[2025-05-11 16:23:14] nn step 34700, lr: 0.1.
	loss_policy_0: 0.13025
	accuracy_policy_0: 0.94508
	loss_value_0: 0.23588
	loss_policy_1: 0.02577
	accuracy_policy_1: 0.94176
	loss_value_1: 0.04712
	loss_reward_1: 0.00421
	loss_policy_2: 0.02573
	accuracy_policy_2: 0.94125
	loss_value_2: 0.04803
	loss_reward_2: 0.00492
	loss_policy_3: 0.02626
	accuracy_policy_3: 0.9432
	loss_value_3: 0.04892
	loss_reward_3: 0.0056
	loss_policy_4: 0.02598
	accuracy_policy_4: 0.94484
	loss_value_4: 0.04989
	loss_reward_4: 0.00643
	loss_policy_5: 0.02575
	accuracy_policy_5: 0.95152
	loss_value_5: 0.051
	loss_reward_5: 0.00757
	loss_policy: 0.25973
	loss_value: 0.48084
	loss_reward: 0.02873
[2025-05-11 16:23:22] nn step 34750, lr: 0.1.
	loss_policy_0: 0.12846
	accuracy_policy_0: 0.94633
	loss_value_0: 0.23006
	loss_policy_1: 0.0253
	accuracy_policy_1: 0.9402
	loss_value_1: 0.04623
	loss_reward_1: 0.00435
	loss_policy_2: 0.02537
	accuracy_policy_2: 0.93984
	loss_value_2: 0.04703
	loss_reward_2: 0.00514
	loss_policy_3: 0.02535
	accuracy_policy_3: 0.94195
	loss_value_3: 0.04758
	loss_reward_3: 0.00552
	loss_policy_4: 0.02536
	accuracy_policy_4: 0.94344
	loss_value_4: 0.04848
	loss_reward_4: 0.00653
	loss_policy_5: 0.025
	accuracy_policy_5: 0.94988
	loss_value_5: 0.0496
	loss_reward_5: 0.00766
	loss_policy: 0.25484
	loss_value: 0.46897
	loss_reward: 0.0292
[2025-05-11 16:23:30] nn step 34800, lr: 0.1.
	loss_policy_0: 0.13998
	accuracy_policy_0: 0.94289
	loss_value_0: 0.2524
	loss_policy_1: 0.02791
	accuracy_policy_1: 0.93938
	loss_value_1: 0.05072
	loss_reward_1: 0.0046
	loss_policy_2: 0.02772
	accuracy_policy_2: 0.94238
	loss_value_2: 0.05167
	loss_reward_2: 0.00536
	loss_policy_3: 0.02815
	accuracy_policy_3: 0.93926
	loss_value_3: 0.05244
	loss_reward_3: 0.00602
	loss_policy_4: 0.0281
	accuracy_policy_4: 0.94391
	loss_value_4: 0.05321
	loss_reward_4: 0.00716
	loss_policy_5: 0.02783
	accuracy_policy_5: 0.94734
	loss_value_5: 0.05452
	loss_reward_5: 0.00825
	loss_policy: 0.27969
	loss_value: 0.51495
	loss_reward: 0.03139
Optimization_Done 34800
[2025-05-11 16:25:08] [command] train weight_iter_34800.pkl 156 175
[2025-05-11 16:25:16] nn step 34850, lr: 0.1.
	loss_policy_0: 0.12718
	accuracy_policy_0: 0.94332
	loss_value_0: 0.2365
	loss_policy_1: 0.02533
	accuracy_policy_1: 0.94027
	loss_value_1: 0.04721
	loss_reward_1: 0.0041
	loss_policy_2: 0.02526
	accuracy_policy_2: 0.93891
	loss_value_2: 0.04805
	loss_reward_2: 0.00498
	loss_policy_3: 0.02522
	accuracy_policy_3: 0.94141
	loss_value_3: 0.04881
	loss_reward_3: 0.00562
	loss_policy_4: 0.02503
	accuracy_policy_4: 0.94629
	loss_value_4: 0.04974
	loss_reward_4: 0.00639
	loss_policy_5: 0.02469
	accuracy_policy_5: 0.95023
	loss_value_5: 0.05111
	loss_reward_5: 0.0077
	loss_policy: 0.25272
	loss_value: 0.48142
	loss_reward: 0.02879
[2025-05-11 16:25:24] nn step 34900, lr: 0.1.
	loss_policy_0: 0.13252
	accuracy_policy_0: 0.94555
	loss_value_0: 0.2399
	loss_policy_1: 0.02639
	accuracy_policy_1: 0.94113
	loss_value_1: 0.04838
	loss_reward_1: 0.00445
	loss_policy_2: 0.02631
	accuracy_policy_2: 0.94168
	loss_value_2: 0.04936
	loss_reward_2: 0.00503
	loss_policy_3: 0.02641
	accuracy_policy_3: 0.94328
	loss_value_3: 0.05031
	loss_reward_3: 0.0057
	loss_policy_4: 0.02628
	accuracy_policy_4: 0.94562
	loss_value_4: 0.05125
	loss_reward_4: 0.00678
	loss_policy_5: 0.02609
	accuracy_policy_5: 0.94887
	loss_value_5: 0.05262
	loss_reward_5: 0.00814
	loss_policy: 0.264
	loss_value: 0.49183
	loss_reward: 0.0301
[2025-05-11 16:25:33] nn step 34950, lr: 0.1.
	loss_policy_0: 0.12423
	accuracy_policy_0: 0.94562
	loss_value_0: 0.22523
	loss_policy_1: 0.02491
	accuracy_policy_1: 0.94129
	loss_value_1: 0.04505
	loss_reward_1: 0.00403
	loss_policy_2: 0.02484
	accuracy_policy_2: 0.94199
	loss_value_2: 0.04597
	loss_reward_2: 0.00458
	loss_policy_3: 0.02482
	accuracy_policy_3: 0.94078
	loss_value_3: 0.04704
	loss_reward_3: 0.00542
	loss_policy_4: 0.02481
	accuracy_policy_4: 0.94457
	loss_value_4: 0.04767
	loss_reward_4: 0.00667
	loss_policy_5: 0.02483
	accuracy_policy_5: 0.95039
	loss_value_5: 0.04888
	loss_reward_5: 0.00738
	loss_policy: 0.24845
	loss_value: 0.45985
	loss_reward: 0.02807
[2025-05-11 16:25:41] nn step 35000, lr: 0.1.
	loss_policy_0: 0.13522
	accuracy_policy_0: 0.94809
	loss_value_0: 0.24499
	loss_policy_1: 0.02676
	accuracy_policy_1: 0.94328
	loss_value_1: 0.04906
	loss_reward_1: 0.00442
	loss_policy_2: 0.02724
	accuracy_policy_2: 0.93949
	loss_value_2: 0.05
	loss_reward_2: 0.00528
	loss_policy_3: 0.02715
	accuracy_policy_3: 0.94418
	loss_value_3: 0.05071
	loss_reward_3: 0.00595
	loss_policy_4: 0.02693
	accuracy_policy_4: 0.94391
	loss_value_4: 0.05194
	loss_reward_4: 0.00677
	loss_policy_5: 0.02713
	accuracy_policy_5: 0.94914
	loss_value_5: 0.05332
	loss_reward_5: 0.00799
	loss_policy: 0.27044
	loss_value: 0.50002
	loss_reward: 0.03042
Optimization_Done 35000
[2025-05-11 16:27:15] [command] train weight_iter_35000.pkl 157 176
[2025-05-11 16:27:24] nn step 35050, lr: 0.1.
	loss_policy_0: 0.12738
	accuracy_policy_0: 0.94609
	loss_value_0: 0.2354
	loss_policy_1: 0.02513
	accuracy_policy_1: 0.94473
	loss_value_1: 0.04707
	loss_reward_1: 0.00421
	loss_policy_2: 0.02534
	accuracy_policy_2: 0.93988
	loss_value_2: 0.04805
	loss_reward_2: 0.00465
	loss_policy_3: 0.02532
	accuracy_policy_3: 0.9434
	loss_value_3: 0.04877
	loss_reward_3: 0.00547
	loss_policy_4: 0.02516
	accuracy_policy_4: 0.94488
	loss_value_4: 0.04969
	loss_reward_4: 0.00633
	loss_policy_5: 0.0248
	accuracy_policy_5: 0.95059
	loss_value_5: 0.05096
	loss_reward_5: 0.00753
	loss_policy: 0.25313
	loss_value: 0.47995
	loss_reward: 0.02819
[2025-05-11 16:27:33] nn step 35100, lr: 0.1.
	loss_policy_0: 0.14005
	accuracy_policy_0: 0.94441
	loss_value_0: 0.25545
	loss_policy_1: 0.02757
	accuracy_policy_1: 0.94312
	loss_value_1: 0.05154
	loss_reward_1: 0.00462
	loss_policy_2: 0.02785
	accuracy_policy_2: 0.93887
	loss_value_2: 0.05244
	loss_reward_2: 0.00547
	loss_policy_3: 0.02769
	accuracy_policy_3: 0.94426
	loss_value_3: 0.05331
	loss_reward_3: 0.00609
	loss_policy_4: 0.02781
	accuracy_policy_4: 0.94504
	loss_value_4: 0.05439
	loss_reward_4: 0.00727
	loss_policy_5: 0.02773
	accuracy_policy_5: 0.95098
	loss_value_5: 0.05547
	loss_reward_5: 0.00858
	loss_policy: 0.27871
	loss_value: 0.5226
	loss_reward: 0.03203
[2025-05-11 16:27:40] nn step 35150, lr: 0.1.
	loss_policy_0: 0.13381
	accuracy_policy_0: 0.94574
	loss_value_0: 0.24438
	loss_policy_1: 0.02672
	accuracy_policy_1: 0.9418
	loss_value_1: 0.04925
	loss_reward_1: 0.00448
	loss_policy_2: 0.02658
	accuracy_policy_2: 0.94074
	loss_value_2: 0.05017
	loss_reward_2: 0.00522
	loss_policy_3: 0.0267
	accuracy_policy_3: 0.94141
	loss_value_3: 0.05107
	loss_reward_3: 0.00594
	loss_policy_4: 0.02651
	accuracy_policy_4: 0.94637
	loss_value_4: 0.05229
	loss_reward_4: 0.00698
	loss_policy_5: 0.02674
	accuracy_policy_5: 0.94996
	loss_value_5: 0.05349
	loss_reward_5: 0.00834
	loss_policy: 0.26707
	loss_value: 0.50065
	loss_reward: 0.03097
[2025-05-11 16:27:49] nn step 35200, lr: 0.1.
	loss_policy_0: 0.13197
	accuracy_policy_0: 0.94684
	loss_value_0: 0.2365
	loss_policy_1: 0.02592
	accuracy_policy_1: 0.94391
	loss_value_1: 0.04767
	loss_reward_1: 0.00434
	loss_policy_2: 0.02601
	accuracy_policy_2: 0.94152
	loss_value_2: 0.04862
	loss_reward_2: 0.00483
	loss_policy_3: 0.02615
	accuracy_policy_3: 0.94227
	loss_value_3: 0.04943
	loss_reward_3: 0.00575
	loss_policy_4: 0.0262
	accuracy_policy_4: 0.94359
	loss_value_4: 0.05054
	loss_reward_4: 0.00684
	loss_policy_5: 0.0261
	accuracy_policy_5: 0.95078
	loss_value_5: 0.05167
	loss_reward_5: 0.00795
	loss_policy: 0.26233
	loss_value: 0.48442
	loss_reward: 0.02971
Optimization_Done 35200
[2025-05-11 16:29:27] [command] train weight_iter_35200.pkl 158 177
[2025-05-11 16:29:35] nn step 35250, lr: 0.1.
	loss_policy_0: 0.13428
	accuracy_policy_0: 0.94504
	loss_value_0: 0.25603
	loss_policy_1: 0.02696
	accuracy_policy_1: 0.93672
	loss_value_1: 0.05111
	loss_reward_1: 0.0044
	loss_policy_2: 0.02733
	accuracy_policy_2: 0.93809
	loss_value_2: 0.05207
	loss_reward_2: 0.00528
	loss_policy_3: 0.02706
	accuracy_policy_3: 0.93785
	loss_value_3: 0.05297
	loss_reward_3: 0.00609
	loss_policy_4: 0.02716
	accuracy_policy_4: 0.94098
	loss_value_4: 0.05412
	loss_reward_4: 0.00717
	loss_policy_5: 0.02731
	accuracy_policy_5: 0.94273
	loss_value_5: 0.05542
	loss_reward_5: 0.00838
	loss_policy: 0.27009
	loss_value: 0.52171
	loss_reward: 0.03133
[2025-05-11 16:29:44] nn step 35300, lr: 0.1.
	loss_policy_0: 0.13472
	accuracy_policy_0: 0.94379
	loss_value_0: 0.24867
	loss_policy_1: 0.02686
	accuracy_policy_1: 0.94023
	loss_value_1: 0.05009
	loss_reward_1: 0.00458
	loss_policy_2: 0.02696
	accuracy_policy_2: 0.93852
	loss_value_2: 0.05126
	loss_reward_2: 0.00528
	loss_policy_3: 0.02687
	accuracy_policy_3: 0.94109
	loss_value_3: 0.05249
	loss_reward_3: 0.00604
	loss_policy_4: 0.02708
	accuracy_policy_4: 0.94375
	loss_value_4: 0.05367
	loss_reward_4: 0.00725
	loss_policy_5: 0.02711
	accuracy_policy_5: 0.94645
	loss_value_5: 0.05513
	loss_reward_5: 0.00849
	loss_policy: 0.2696
	loss_value: 0.51131
	loss_reward: 0.03163
[2025-05-11 16:29:52] nn step 35350, lr: 0.1.
	loss_policy_0: 0.13369
	accuracy_policy_0: 0.9425
	loss_value_0: 0.24256
	loss_policy_1: 0.02675
	accuracy_policy_1: 0.94012
	loss_value_1: 0.0487
	loss_reward_1: 0.00448
	loss_policy_2: 0.02657
	accuracy_policy_2: 0.93863
	loss_value_2: 0.04957
	loss_reward_2: 0.00508
	loss_policy_3: 0.02671
	accuracy_policy_3: 0.93762
	loss_value_3: 0.05061
	loss_reward_3: 0.00582
	loss_policy_4: 0.0267
	accuracy_policy_4: 0.94324
	loss_value_4: 0.05148
	loss_reward_4: 0.00688
	loss_policy_5: 0.02657
	accuracy_policy_5: 0.94582
	loss_value_5: 0.05296
	loss_reward_5: 0.00848
	loss_policy: 0.26698
	loss_value: 0.49587
	loss_reward: 0.03074
[2025-05-11 16:29:59] nn step 35400, lr: 0.1.
	loss_policy_0: 0.14278
	accuracy_policy_0: 0.94109
	loss_value_0: 0.25786
	loss_policy_1: 0.02839
	accuracy_policy_1: 0.93586
	loss_value_1: 0.05184
	loss_reward_1: 0.00459
	loss_policy_2: 0.02818
	accuracy_policy_2: 0.93652
	loss_value_2: 0.0528
	loss_reward_2: 0.00529
	loss_policy_3: 0.02829
	accuracy_policy_3: 0.9375
	loss_value_3: 0.0536
	loss_reward_3: 0.00618
	loss_policy_4: 0.02833
	accuracy_policy_4: 0.94266
	loss_value_4: 0.05481
	loss_reward_4: 0.00728
	loss_policy_5: 0.02832
	accuracy_policy_5: 0.94383
	loss_value_5: 0.0562
	loss_reward_5: 0.00883
	loss_policy: 0.28428
	loss_value: 0.52712
	loss_reward: 0.03217
Optimization_Done 35400
[2025-05-11 16:31:35] [command] train weight_iter_35400.pkl 159 178
[2025-05-11 16:31:45] nn step 35450, lr: 0.1.
	loss_policy_0: 0.13824
	accuracy_policy_0: 0.94246
	loss_value_0: 0.25668
	loss_policy_1: 0.02744
	accuracy_policy_1: 0.93754
	loss_value_1: 0.05112
	loss_reward_1: 0.00437
	loss_policy_2: 0.02709
	accuracy_policy_2: 0.93805
	loss_value_2: 0.05223
	loss_reward_2: 0.00524
	loss_policy_3: 0.02743
	accuracy_policy_3: 0.94059
	loss_value_3: 0.05301
	loss_reward_3: 0.00607
	loss_policy_4: 0.02754
	accuracy_policy_4: 0.9432
	loss_value_4: 0.05386
	loss_reward_4: 0.00725
	loss_policy_5: 0.02719
	accuracy_policy_5: 0.94613
	loss_value_5: 0.05508
	loss_reward_5: 0.00871
	loss_policy: 0.27493
	loss_value: 0.52199
	loss_reward: 0.03165
[2025-05-11 16:31:54] nn step 35500, lr: 0.1.
	loss_policy_0: 0.13376
	accuracy_policy_0: 0.94398
	loss_value_0: 0.24639
	loss_policy_1: 0.02634
	accuracy_policy_1: 0.9407
	loss_value_1: 0.04952
	loss_reward_1: 0.00425
	loss_policy_2: 0.02652
	accuracy_policy_2: 0.93867
	loss_value_2: 0.05063
	loss_reward_2: 0.00513
	loss_policy_3: 0.02669
	accuracy_policy_3: 0.94094
	loss_value_3: 0.05162
	loss_reward_3: 0.00562
	loss_policy_4: 0.0264
	accuracy_policy_4: 0.94258
	loss_value_4: 0.0525
	loss_reward_4: 0.00685
	loss_policy_5: 0.02639
	accuracy_policy_5: 0.94789
	loss_value_5: 0.05413
	loss_reward_5: 0.0081
	loss_policy: 0.2661
	loss_value: 0.50478
	loss_reward: 0.02994
[2025-05-11 16:32:01] nn step 35550, lr: 0.1.
	loss_policy_0: 0.13034
	accuracy_policy_0: 0.9384
	loss_value_0: 0.23407
	loss_policy_1: 0.02566
	accuracy_policy_1: 0.9377
	loss_value_1: 0.04704
	loss_reward_1: 0.00414
	loss_policy_2: 0.02574
	accuracy_policy_2: 0.93617
	loss_value_2: 0.04794
	loss_reward_2: 0.00473
	loss_policy_3: 0.02574
	accuracy_policy_3: 0.93879
	loss_value_3: 0.04904
	loss_reward_3: 0.00567
	loss_policy_4: 0.02577
	accuracy_policy_4: 0.93926
	loss_value_4: 0.0501
	loss_reward_4: 0.00655
	loss_policy_5: 0.0256
	accuracy_policy_5: 0.9441
	loss_value_5: 0.05119
	loss_reward_5: 0.00777
	loss_policy: 0.25885
	loss_value: 0.47937
	loss_reward: 0.02885
[2025-05-11 16:32:09] nn step 35600, lr: 0.1.
	loss_policy_0: 0.13047
	accuracy_policy_0: 0.94059
	loss_value_0: 0.23526
	loss_policy_1: 0.02556
	accuracy_policy_1: 0.94074
	loss_value_1: 0.04723
	loss_reward_1: 0.00416
	loss_policy_2: 0.02593
	accuracy_policy_2: 0.93832
	loss_value_2: 0.04825
	loss_reward_2: 0.00483
	loss_policy_3: 0.02633
	accuracy_policy_3: 0.93938
	loss_value_3: 0.04878
	loss_reward_3: 0.0056
	loss_policy_4: 0.02586
	accuracy_policy_4: 0.9427
	loss_value_4: 0.04972
	loss_reward_4: 0.00642
	loss_policy_5: 0.02591
	accuracy_policy_5: 0.945
	loss_value_5: 0.05094
	loss_reward_5: 0.00778
	loss_policy: 0.26006
	loss_value: 0.48018
	loss_reward: 0.02879
Optimization_Done 35600
[2025-05-11 16:33:48] [command] train weight_iter_35600.pkl 160 179
[2025-05-11 16:33:57] nn step 35650, lr: 0.1.
	loss_policy_0: 0.13941
	accuracy_policy_0: 0.94184
	loss_value_0: 0.26265
	loss_policy_1: 0.02732
	accuracy_policy_1: 0.93676
	loss_value_1: 0.05238
	loss_reward_1: 0.00475
	loss_policy_2: 0.02751
	accuracy_policy_2: 0.93633
	loss_value_2: 0.05311
	loss_reward_2: 0.00532
	loss_policy_3: 0.0276
	accuracy_policy_3: 0.94129
	loss_value_3: 0.054
	loss_reward_3: 0.00597
	loss_policy_4: 0.02761
	accuracy_policy_4: 0.94398
	loss_value_4: 0.05503
	loss_reward_4: 0.00685
	loss_policy_5: 0.02784
	accuracy_policy_5: 0.9448
	loss_value_5: 0.05634
	loss_reward_5: 0.00856
	loss_policy: 0.27729
	loss_value: 0.53351
	loss_reward: 0.03145
[2025-05-11 16:34:06] nn step 35700, lr: 0.1.
	loss_policy_0: 0.13586
	accuracy_policy_0: 0.93832
	loss_value_0: 0.24472
	loss_policy_1: 0.02641
	accuracy_policy_1: 0.93898
	loss_value_1: 0.04908
	loss_reward_1: 0.00435
	loss_policy_2: 0.02651
	accuracy_policy_2: 0.9375
	loss_value_2: 0.05003
	loss_reward_2: 0.00507
	loss_policy_3: 0.02663
	accuracy_policy_3: 0.93848
	loss_value_3: 0.05093
	loss_reward_3: 0.00571
	loss_policy_4: 0.02664
	accuracy_policy_4: 0.9416
	loss_value_4: 0.052
	loss_reward_4: 0.00675
	loss_policy_5: 0.02685
	accuracy_policy_5: 0.94602
	loss_value_5: 0.05329
	loss_reward_5: 0.00786
	loss_policy: 0.26889
	loss_value: 0.50006
	loss_reward: 0.02974
[2025-05-11 16:34:15] nn step 35750, lr: 0.1.
	loss_policy_0: 0.1294
	accuracy_policy_0: 0.9407
	loss_value_0: 0.23261
	loss_policy_1: 0.02556
	accuracy_policy_1: 0.93984
	loss_value_1: 0.04666
	loss_reward_1: 0.0042
	loss_policy_2: 0.02582
	accuracy_policy_2: 0.93996
	loss_value_2: 0.04751
	loss_reward_2: 0.00485
	loss_policy_3: 0.02553
	accuracy_policy_3: 0.94004
	loss_value_3: 0.04859
	loss_reward_3: 0.0056
	loss_policy_4: 0.02543
	accuracy_policy_4: 0.94449
	loss_value_4: 0.0495
	loss_reward_4: 0.00645
	loss_policy_5: 0.0255
	accuracy_policy_5: 0.94699
	loss_value_5: 0.05059
	loss_reward_5: 0.00774
	loss_policy: 0.25724
	loss_value: 0.47546
	loss_reward: 0.02884
[2025-05-11 16:34:22] nn step 35800, lr: 0.1.
	loss_policy_0: 0.14034
	accuracy_policy_0: 0.94062
	loss_value_0: 0.25045
	loss_policy_1: 0.02768
	accuracy_policy_1: 0.93609
	loss_value_1: 0.05039
	loss_reward_1: 0.0046
	loss_policy_2: 0.02771
	accuracy_policy_2: 0.93387
	loss_value_2: 0.05123
	loss_reward_2: 0.00554
	loss_policy_3: 0.02814
	accuracy_policy_3: 0.93902
	loss_value_3: 0.05252
	loss_reward_3: 0.00613
	loss_policy_4: 0.02798
	accuracy_policy_4: 0.94117
	loss_value_4: 0.05359
	loss_reward_4: 0.00729
	loss_policy_5: 0.02762
	accuracy_policy_5: 0.9475
	loss_value_5: 0.05492
	loss_reward_5: 0.0083
	loss_policy: 0.27948
	loss_value: 0.51311
	loss_reward: 0.03186
Optimization_Done 35800
[2025-05-11 16:35:56] [command] train weight_iter_35800.pkl 161 180
[2025-05-11 16:36:06] nn step 35850, lr: 0.1.
	loss_policy_0: 0.13427
	accuracy_policy_0: 0.93926
	loss_value_0: 0.24333
	loss_policy_1: 0.02612
	accuracy_policy_1: 0.93496
	loss_value_1: 0.0486
	loss_reward_1: 0.00454
	loss_policy_2: 0.02606
	accuracy_policy_2: 0.93691
	loss_value_2: 0.04939
	loss_reward_2: 0.00503
	loss_policy_3: 0.02612
	accuracy_policy_3: 0.93781
	loss_value_3: 0.05011
	loss_reward_3: 0.00564
	loss_policy_4: 0.02623
	accuracy_policy_4: 0.93934
	loss_value_4: 0.0514
	loss_reward_4: 0.00673
	loss_policy_5: 0.02631
	accuracy_policy_5: 0.94605
	loss_value_5: 0.05225
	loss_reward_5: 0.00799
	loss_policy: 0.2651
	loss_value: 0.49507
	loss_reward: 0.02993
[2025-05-11 16:36:13] nn step 35900, lr: 0.1.
	loss_policy_0: 0.127
	accuracy_policy_0: 0.93926
	loss_value_0: 0.23383
	loss_policy_1: 0.02516
	accuracy_policy_1: 0.93688
	loss_value_1: 0.04675
	loss_reward_1: 0.00413
	loss_policy_2: 0.02539
	accuracy_policy_2: 0.93582
	loss_value_2: 0.04761
	loss_reward_2: 0.0049
	loss_policy_3: 0.02532
	accuracy_policy_3: 0.9357
	loss_value_3: 0.04829
	loss_reward_3: 0.00564
	loss_policy_4: 0.02539
	accuracy_policy_4: 0.94012
	loss_value_4: 0.04938
	loss_reward_4: 0.00674
	loss_policy_5: 0.02523
	accuracy_policy_5: 0.94426
	loss_value_5: 0.0503
	loss_reward_5: 0.00795
	loss_policy: 0.25348
	loss_value: 0.47617
	loss_reward: 0.02936
[2025-05-11 16:36:21] nn step 35950, lr: 0.1.
	loss_policy_0: 0.12779
	accuracy_policy_0: 0.93891
	loss_value_0: 0.23125
	loss_policy_1: 0.02534
	accuracy_policy_1: 0.93266
	loss_value_1: 0.04673
	loss_reward_1: 0.0041
	loss_policy_2: 0.02548
	accuracy_policy_2: 0.93402
	loss_value_2: 0.04778
	loss_reward_2: 0.00508
	loss_policy_3: 0.02583
	accuracy_policy_3: 0.93465
	loss_value_3: 0.0485
	loss_reward_3: 0.0058
	loss_policy_4: 0.02558
	accuracy_policy_4: 0.9377
	loss_value_4: 0.04939
	loss_reward_4: 0.00672
	loss_policy_5: 0.02544
	accuracy_policy_5: 0.94559
	loss_value_5: 0.05043
	loss_reward_5: 0.0078
	loss_policy: 0.25545
	loss_value: 0.47407
	loss_reward: 0.0295
[2025-05-11 16:36:29] nn step 36000, lr: 0.1.
	loss_policy_0: 0.13116
	accuracy_policy_0: 0.9373
	loss_value_0: 0.23459
	loss_policy_1: 0.02586
	accuracy_policy_1: 0.93773
	loss_value_1: 0.04724
	loss_reward_1: 0.00435
	loss_policy_2: 0.02607
	accuracy_policy_2: 0.93746
	loss_value_2: 0.04786
	loss_reward_2: 0.00524
	loss_policy_3: 0.02623
	accuracy_policy_3: 0.93656
	loss_value_3: 0.0488
	loss_reward_3: 0.0056
	loss_policy_4: 0.02627
	accuracy_policy_4: 0.93863
	loss_value_4: 0.04971
	loss_reward_4: 0.00631
	loss_policy_5: 0.02635
	accuracy_policy_5: 0.9434
	loss_value_5: 0.05094
	loss_reward_5: 0.00781
	loss_policy: 0.26194
	loss_value: 0.47914
	loss_reward: 0.02931
Optimization_Done 36000
[2025-05-11 16:38:03] [command] train weight_iter_36000.pkl 162 181
[2025-05-11 16:38:11] nn step 36050, lr: 0.1.
	loss_policy_0: 0.13638
	accuracy_policy_0: 0.9382
	loss_value_0: 0.25268
	loss_policy_1: 0.02692
	accuracy_policy_1: 0.93723
	loss_value_1: 0.05059
	loss_reward_1: 0.00467
	loss_policy_2: 0.0269
	accuracy_policy_2: 0.93676
	loss_value_2: 0.05135
	loss_reward_2: 0.00521
	loss_policy_3: 0.02711
	accuracy_policy_3: 0.9377
	loss_value_3: 0.05256
	loss_reward_3: 0.00592
	loss_policy_4: 0.02727
	accuracy_policy_4: 0.93926
	loss_value_4: 0.05339
	loss_reward_4: 0.00729
	loss_policy_5: 0.02695
	accuracy_policy_5: 0.9457
	loss_value_5: 0.05481
	loss_reward_5: 0.00807
	loss_policy: 0.27154
	loss_value: 0.51539
	loss_reward: 0.03117
[2025-05-11 16:38:19] nn step 36100, lr: 0.1.
	loss_policy_0: 0.13416
	accuracy_policy_0: 0.94094
	loss_value_0: 0.2444
	loss_policy_1: 0.02701
	accuracy_policy_1: 0.93574
	loss_value_1: 0.04886
	loss_reward_1: 0.00435
	loss_policy_2: 0.02657
	accuracy_policy_2: 0.93488
	loss_value_2: 0.04987
	loss_reward_2: 0.00514
	loss_policy_3: 0.02683
	accuracy_policy_3: 0.93613
	loss_value_3: 0.05101
	loss_reward_3: 0.00566
	loss_policy_4: 0.02642
	accuracy_policy_4: 0.94207
	loss_value_4: 0.05202
	loss_reward_4: 0.00681
	loss_policy_5: 0.02677
	accuracy_policy_5: 0.94543
	loss_value_5: 0.0532
	loss_reward_5: 0.00786
	loss_policy: 0.26776
	loss_value: 0.49936
	loss_reward: 0.02982
[2025-05-11 16:38:28] nn step 36150, lr: 0.1.
	loss_policy_0: 0.1306
	accuracy_policy_0: 0.94156
	loss_value_0: 0.236
	loss_policy_1: 0.02607
	accuracy_policy_1: 0.9352
	loss_value_1: 0.04729
	loss_reward_1: 0.00425
	loss_policy_2: 0.0261
	accuracy_policy_2: 0.93402
	loss_value_2: 0.04812
	loss_reward_2: 0.00495
	loss_policy_3: 0.02618
	accuracy_policy_3: 0.93711
	loss_value_3: 0.04895
	loss_reward_3: 0.00545
	loss_policy_4: 0.02581
	accuracy_policy_4: 0.94047
	loss_value_4: 0.04974
	loss_reward_4: 0.00645
	loss_policy_5: 0.02563
	accuracy_policy_5: 0.94723
	loss_value_5: 0.05063
	loss_reward_5: 0.00779
	loss_policy: 0.2604
	loss_value: 0.48072
	loss_reward: 0.02888
[2025-05-11 16:38:35] nn step 36200, lr: 0.1.
	loss_policy_0: 0.12987
	accuracy_policy_0: 0.9373
	loss_value_0: 0.23335
	loss_policy_1: 0.0258
	accuracy_policy_1: 0.93266
	loss_value_1: 0.0468
	loss_reward_1: 0.00437
	loss_policy_2: 0.02577
	accuracy_policy_2: 0.93414
	loss_value_2: 0.04756
	loss_reward_2: 0.00481
	loss_policy_3: 0.02584
	accuracy_policy_3: 0.93422
	loss_value_3: 0.04812
	loss_reward_3: 0.00527
	loss_policy_4: 0.02538
	accuracy_policy_4: 0.93871
	loss_value_4: 0.04914
	loss_reward_4: 0.00645
	loss_policy_5: 0.02568
	accuracy_policy_5: 0.94328
	loss_value_5: 0.05033
	loss_reward_5: 0.00778
	loss_policy: 0.25834
	loss_value: 0.47531
	loss_reward: 0.02869
Optimization_Done 36200
[2025-05-11 16:40:11] [command] train weight_iter_36200.pkl 163 182
[2025-05-11 16:40:20] nn step 36250, lr: 0.1.
	loss_policy_0: 0.13165
	accuracy_policy_0: 0.94035
	loss_value_0: 0.25945
	loss_policy_1: 0.02639
	accuracy_policy_1: 0.9382
	loss_value_1: 0.05181
	loss_reward_1: 0.00467
	loss_policy_2: 0.02635
	accuracy_policy_2: 0.9382
	loss_value_2: 0.05305
	loss_reward_2: 0.00528
	loss_policy_3: 0.02653
	accuracy_policy_3: 0.94035
	loss_value_3: 0.05401
	loss_reward_3: 0.00585
	loss_policy_4: 0.02637
	accuracy_policy_4: 0.94422
	loss_value_4: 0.05455
	loss_reward_4: 0.00746
	loss_policy_5: 0.02648
	accuracy_policy_5: 0.95066
	loss_value_5: 0.05572
	loss_reward_5: 0.00847
	loss_policy: 0.26376
	loss_value: 0.52859
	loss_reward: 0.03174
[2025-05-11 16:40:28] nn step 36300, lr: 0.1.
	loss_policy_0: 0.1276
	accuracy_policy_0: 0.94031
	loss_value_0: 0.24259
	loss_policy_1: 0.02507
	accuracy_policy_1: 0.9373
	loss_value_1: 0.04908
	loss_reward_1: 0.00423
	loss_policy_2: 0.0253
	accuracy_policy_2: 0.93852
	loss_value_2: 0.04982
	loss_reward_2: 0.00503
	loss_policy_3: 0.0258
	accuracy_policy_3: 0.93957
	loss_value_3: 0.0507
	loss_reward_3: 0.00568
	loss_policy_4: 0.0252
	accuracy_policy_4: 0.94371
	loss_value_4: 0.05165
	loss_reward_4: 0.00667
	loss_policy_5: 0.02534
	accuracy_policy_5: 0.95141
	loss_value_5: 0.05301
	loss_reward_5: 0.0082
	loss_policy: 0.25432
	loss_value: 0.49685
	loss_reward: 0.02981
[2025-05-11 16:40:37] nn step 36350, lr: 0.1.
	loss_policy_0: 0.12979
	accuracy_policy_0: 0.94082
	loss_value_0: 0.24209
	loss_policy_1: 0.02545
	accuracy_policy_1: 0.93453
	loss_value_1: 0.04886
	loss_reward_1: 0.0044
	loss_policy_2: 0.02553
	accuracy_policy_2: 0.9377
	loss_value_2: 0.04957
	loss_reward_2: 0.00499
	loss_policy_3: 0.02535
	accuracy_policy_3: 0.9391
	loss_value_3: 0.05001
	loss_reward_3: 0.0056
	loss_policy_4: 0.0256
	accuracy_policy_4: 0.94379
	loss_value_4: 0.05105
	loss_reward_4: 0.00665
	loss_policy_5: 0.02518
	accuracy_policy_5: 0.94879
	loss_value_5: 0.05223
	loss_reward_5: 0.00776
	loss_policy: 0.25691
	loss_value: 0.49382
	loss_reward: 0.0294
[2025-05-11 16:40:46] nn step 36400, lr: 0.1.
	loss_policy_0: 0.12583
	accuracy_policy_0: 0.94047
	loss_value_0: 0.23718
	loss_policy_1: 0.02484
	accuracy_policy_1: 0.93672
	loss_value_1: 0.04769
	loss_reward_1: 0.00428
	loss_policy_2: 0.02529
	accuracy_policy_2: 0.93754
	loss_value_2: 0.0487
	loss_reward_2: 0.00493
	loss_policy_3: 0.02519
	accuracy_policy_3: 0.94008
	loss_value_3: 0.04937
	loss_reward_3: 0.00584
	loss_policy_4: 0.02535
	accuracy_policy_4: 0.94418
	loss_value_4: 0.0504
	loss_reward_4: 0.00676
	loss_policy_5: 0.02513
	accuracy_policy_5: 0.94887
	loss_value_5: 0.05158
	loss_reward_5: 0.00751
	loss_policy: 0.25162
	loss_value: 0.48492
	loss_reward: 0.02932
Optimization_Done 36400
[2025-05-11 16:42:21] [command] train weight_iter_36400.pkl 164 183
[2025-05-11 16:42:30] nn step 36450, lr: 0.1.
	loss_policy_0: 0.1189
	accuracy_policy_0: 0.94285
	loss_value_0: 0.23556
	loss_policy_1: 0.02381
	accuracy_policy_1: 0.93953
	loss_value_1: 0.04691
	loss_reward_1: 0.00405
	loss_policy_2: 0.02419
	accuracy_policy_2: 0.93914
	loss_value_2: 0.04778
	loss_reward_2: 0.00468
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.94352
	loss_value_3: 0.04831
	loss_reward_3: 0.00536
	loss_policy_4: 0.02362
	accuracy_policy_4: 0.94828
	loss_value_4: 0.04912
	loss_reward_4: 0.00653
	loss_policy_5: 0.02403
	accuracy_policy_5: 0.95078
	loss_value_5: 0.05015
	loss_reward_5: 0.00755
	loss_policy: 0.23842
	loss_value: 0.47783
	loss_reward: 0.02817
[2025-05-11 16:42:39] nn step 36500, lr: 0.1.
	loss_policy_0: 0.12435
	accuracy_policy_0: 0.94297
	loss_value_0: 0.23429
	loss_policy_1: 0.02454
	accuracy_policy_1: 0.93816
	loss_value_1: 0.04688
	loss_reward_1: 0.00415
	loss_policy_2: 0.0246
	accuracy_policy_2: 0.94059
	loss_value_2: 0.04786
	loss_reward_2: 0.00466
	loss_policy_3: 0.02443
	accuracy_policy_3: 0.94254
	loss_value_3: 0.04853
	loss_reward_3: 0.00546
	loss_policy_4: 0.02431
	accuracy_policy_4: 0.9452
	loss_value_4: 0.04941
	loss_reward_4: 0.00655
	loss_policy_5: 0.02453
	accuracy_policy_5: 0.94938
	loss_value_5: 0.05043
	loss_reward_5: 0.00736
	loss_policy: 0.24676
	loss_value: 0.4774
	loss_reward: 0.02819
[2025-05-11 16:42:48] nn step 36550, lr: 0.1.
	loss_policy_0: 0.1346
	accuracy_policy_0: 0.94266
	loss_value_0: 0.25414
	loss_policy_1: 0.0268
	accuracy_policy_1: 0.93695
	loss_value_1: 0.05109
	loss_reward_1: 0.00459
	loss_policy_2: 0.02678
	accuracy_policy_2: 0.94078
	loss_value_2: 0.05197
	loss_reward_2: 0.00523
	loss_policy_3: 0.02681
	accuracy_policy_3: 0.9432
	loss_value_3: 0.053
	loss_reward_3: 0.00591
	loss_policy_4: 0.0268
	accuracy_policy_4: 0.94539
	loss_value_4: 0.05416
	loss_reward_4: 0.00723
	loss_policy_5: 0.02678
	accuracy_policy_5: 0.95285
	loss_value_5: 0.05552
	loss_reward_5: 0.00827
	loss_policy: 0.26858
	loss_value: 0.51988
	loss_reward: 0.03122
[2025-05-11 16:42:54] nn step 36600, lr: 0.1.
	loss_policy_0: 0.12233
	accuracy_policy_0: 0.94043
	loss_value_0: 0.22778
	loss_policy_1: 0.024
	accuracy_policy_1: 0.93652
	loss_value_1: 0.04552
	loss_reward_1: 0.00418
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.93855
	loss_value_2: 0.04611
	loss_reward_2: 0.00473
	loss_policy_3: 0.02421
	accuracy_policy_3: 0.9402
	loss_value_3: 0.04723
	loss_reward_3: 0.00536
	loss_policy_4: 0.02398
	accuracy_policy_4: 0.94562
	loss_value_4: 0.04807
	loss_reward_4: 0.00645
	loss_policy_5: 0.02389
	accuracy_policy_5: 0.95156
	loss_value_5: 0.04905
	loss_reward_5: 0.0076
	loss_policy: 0.24258
	loss_value: 0.46376
	loss_reward: 0.02832
Optimization_Done 36600
[2025-05-11 16:44:32] [command] train weight_iter_36600.pkl 165 184
[2025-05-11 16:44:40] nn step 36650, lr: 0.1.
	loss_policy_0: 0.12628
	accuracy_policy_0: 0.94438
	loss_value_0: 0.24704
	loss_policy_1: 0.02518
	accuracy_policy_1: 0.93633
	loss_value_1: 0.04936
	loss_reward_1: 0.00437
	loss_policy_2: 0.02501
	accuracy_policy_2: 0.93914
	loss_value_2: 0.05027
	loss_reward_2: 0.00504
	loss_policy_3: 0.02515
	accuracy_policy_3: 0.93973
	loss_value_3: 0.05113
	loss_reward_3: 0.00576
	loss_policy_4: 0.02524
	accuracy_policy_4: 0.94738
	loss_value_4: 0.05164
	loss_reward_4: 0.00661
	loss_policy_5: 0.02517
	accuracy_policy_5: 0.95121
	loss_value_5: 0.05279
	loss_reward_5: 0.00779
	loss_policy: 0.25203
	loss_value: 0.50224
	loss_reward: 0.02956
[2025-05-11 16:44:49] nn step 36700, lr: 0.1.
	loss_policy_0: 0.12327
	accuracy_policy_0: 0.94164
	loss_value_0: 0.23832
	loss_policy_1: 0.02454
	accuracy_policy_1: 0.93578
	loss_value_1: 0.04776
	loss_reward_1: 0.00423
	loss_policy_2: 0.02465
	accuracy_policy_2: 0.93832
	loss_value_2: 0.04857
	loss_reward_2: 0.00513
	loss_policy_3: 0.02499
	accuracy_policy_3: 0.935
	loss_value_3: 0.04957
	loss_reward_3: 0.00566
	loss_policy_4: 0.02485
	accuracy_policy_4: 0.94137
	loss_value_4: 0.05046
	loss_reward_4: 0.00642
	loss_policy_5: 0.02421
	accuracy_policy_5: 0.94625
	loss_value_5: 0.05159
	loss_reward_5: 0.00786
	loss_policy: 0.24651
	loss_value: 0.48628
	loss_reward: 0.02931
[2025-05-11 16:44:57] nn step 36750, lr: 0.1.
	loss_policy_0: 0.13328
	accuracy_policy_0: 0.93934
	loss_value_0: 0.25481
	loss_policy_1: 0.02654
	accuracy_policy_1: 0.93699
	loss_value_1: 0.05118
	loss_reward_1: 0.00466
	loss_policy_2: 0.02645
	accuracy_policy_2: 0.93824
	loss_value_2: 0.05195
	loss_reward_2: 0.00521
	loss_policy_3: 0.02688
	accuracy_policy_3: 0.9398
	loss_value_3: 0.05281
	loss_reward_3: 0.00585
	loss_policy_4: 0.02646
	accuracy_policy_4: 0.94332
	loss_value_4: 0.05371
	loss_reward_4: 0.0072
	loss_policy_5: 0.0268
	accuracy_policy_5: 0.94742
	loss_value_5: 0.05502
	loss_reward_5: 0.00838
	loss_policy: 0.26641
	loss_value: 0.51947
	loss_reward: 0.03129
[2025-05-11 16:45:06] nn step 36800, lr: 0.1.
	loss_policy_0: 0.12655
	accuracy_policy_0: 0.94059
	loss_value_0: 0.24239
	loss_policy_1: 0.02539
	accuracy_policy_1: 0.93422
	loss_value_1: 0.04869
	loss_reward_1: 0.0044
	loss_policy_2: 0.02539
	accuracy_policy_2: 0.93496
	loss_value_2: 0.04954
	loss_reward_2: 0.0051
	loss_policy_3: 0.02543
	accuracy_policy_3: 0.93699
	loss_value_3: 0.05045
	loss_reward_3: 0.00581
	loss_policy_4: 0.02551
	accuracy_policy_4: 0.93855
	loss_value_4: 0.05125
	loss_reward_4: 0.00689
	loss_policy_5: 0.02511
	accuracy_policy_5: 0.94832
	loss_value_5: 0.05255
	loss_reward_5: 0.00796
	loss_policy: 0.25337
	loss_value: 0.49488
	loss_reward: 0.03016
Optimization_Done 36800
[2025-05-11 16:46:41] [command] train weight_iter_36800.pkl 166 185
[2025-05-11 16:46:51] nn step 36850, lr: 0.1.
	loss_policy_0: 0.12365
	accuracy_policy_0: 0.93895
	loss_value_0: 0.23856
	loss_policy_1: 0.02443
	accuracy_policy_1: 0.93863
	loss_value_1: 0.0478
	loss_reward_1: 0.00408
	loss_policy_2: 0.02479
	accuracy_policy_2: 0.93719
	loss_value_2: 0.0485
	loss_reward_2: 0.00477
	loss_policy_3: 0.0248
	accuracy_policy_3: 0.93766
	loss_value_3: 0.04941
	loss_reward_3: 0.00541
	loss_policy_4: 0.02453
	accuracy_policy_4: 0.94031
	loss_value_4: 0.05017
	loss_reward_4: 0.00657
	loss_policy_5: 0.02444
	accuracy_policy_5: 0.94645
	loss_value_5: 0.05137
	loss_reward_5: 0.00757
	loss_policy: 0.24664
	loss_value: 0.48579
	loss_reward: 0.02841
[2025-05-11 16:46:59] nn step 36900, lr: 0.1.
	loss_policy_0: 0.12353
	accuracy_policy_0: 0.93883
	loss_value_0: 0.23529
	loss_policy_1: 0.02448
	accuracy_policy_1: 0.93785
	loss_value_1: 0.04732
	loss_reward_1: 0.00421
	loss_policy_2: 0.02471
	accuracy_policy_2: 0.93922
	loss_value_2: 0.04789
	loss_reward_2: 0.00484
	loss_policy_3: 0.02472
	accuracy_policy_3: 0.93926
	loss_value_3: 0.04852
	loss_reward_3: 0.00552
	loss_policy_4: 0.02492
	accuracy_policy_4: 0.94043
	loss_value_4: 0.04932
	loss_reward_4: 0.00631
	loss_policy_5: 0.02432
	accuracy_policy_5: 0.94625
	loss_value_5: 0.05057
	loss_reward_5: 0.00733
	loss_policy: 0.24667
	loss_value: 0.47892
	loss_reward: 0.02821
[2025-05-11 16:47:06] nn step 36950, lr: 0.1.
	loss_policy_0: 0.12688
	accuracy_policy_0: 0.93625
	loss_value_0: 0.23585
	loss_policy_1: 0.02499
	accuracy_policy_1: 0.93543
	loss_value_1: 0.04722
	loss_reward_1: 0.00429
	loss_policy_2: 0.02529
	accuracy_policy_2: 0.93902
	loss_value_2: 0.04825
	loss_reward_2: 0.00488
	loss_policy_3: 0.02526
	accuracy_policy_3: 0.9368
	loss_value_3: 0.04886
	loss_reward_3: 0.00552
	loss_policy_4: 0.02509
	accuracy_policy_4: 0.94172
	loss_value_4: 0.05008
	loss_reward_4: 0.00654
	loss_policy_5: 0.02528
	accuracy_policy_5: 0.95016
	loss_value_5: 0.05117
	loss_reward_5: 0.0077
	loss_policy: 0.25279
	loss_value: 0.48145
	loss_reward: 0.02893
[2025-05-11 16:47:15] nn step 37000, lr: 0.1.
	loss_policy_0: 0.13144
	accuracy_policy_0: 0.93855
	loss_value_0: 0.24755
	loss_policy_1: 0.02626
	accuracy_policy_1: 0.93355
	loss_value_1: 0.04977
	loss_reward_1: 0.0045
	loss_policy_2: 0.0264
	accuracy_policy_2: 0.93641
	loss_value_2: 0.05039
	loss_reward_2: 0.00532
	loss_policy_3: 0.02637
	accuracy_policy_3: 0.9393
	loss_value_3: 0.05128
	loss_reward_3: 0.00597
	loss_policy_4: 0.02667
	accuracy_policy_4: 0.94078
	loss_value_4: 0.05219
	loss_reward_4: 0.00678
	loss_policy_5: 0.02629
	accuracy_policy_5: 0.94703
	loss_value_5: 0.05355
	loss_reward_5: 0.00809
	loss_policy: 0.26343
	loss_value: 0.50473
	loss_reward: 0.03066
Optimization_Done 37000
[2025-05-11 16:48:51] [command] train weight_iter_37000.pkl 167 186
[2025-05-11 16:48:59] nn step 37050, lr: 0.1.
	loss_policy_0: 0.12537
	accuracy_policy_0: 0.94125
	loss_value_0: 0.24354
	loss_policy_1: 0.0251
	accuracy_policy_1: 0.93711
	loss_value_1: 0.04867
	loss_reward_1: 0.00428
	loss_policy_2: 0.02499
	accuracy_policy_2: 0.93984
	loss_value_2: 0.04963
	loss_reward_2: 0.00487
	loss_policy_3: 0.02511
	accuracy_policy_3: 0.94148
	loss_value_3: 0.0504
	loss_reward_3: 0.00562
	loss_policy_4: 0.02481
	accuracy_policy_4: 0.94129
	loss_value_4: 0.0512
	loss_reward_4: 0.00669
	loss_policy_5: 0.02492
	accuracy_policy_5: 0.9498
	loss_value_5: 0.05205
	loss_reward_5: 0.00783
	loss_policy: 0.25031
	loss_value: 0.49549
	loss_reward: 0.02929
[2025-05-11 16:49:07] nn step 37100, lr: 0.1.
	loss_policy_0: 0.12733
	accuracy_policy_0: 0.94047
	loss_value_0: 0.23954
	loss_policy_1: 0.02518
	accuracy_policy_1: 0.93914
	loss_value_1: 0.04796
	loss_reward_1: 0.0045
	loss_policy_2: 0.02556
	accuracy_policy_2: 0.93762
	loss_value_2: 0.04897
	loss_reward_2: 0.00519
	loss_policy_3: 0.02554
	accuracy_policy_3: 0.93676
	loss_value_3: 0.04983
	loss_reward_3: 0.00584
	loss_policy_4: 0.02554
	accuracy_policy_4: 0.93797
	loss_value_4: 0.05047
	loss_reward_4: 0.00637
	loss_policy_5: 0.02521
	accuracy_policy_5: 0.94734
	loss_value_5: 0.05148
	loss_reward_5: 0.00784
	loss_policy: 0.25437
	loss_value: 0.48825
	loss_reward: 0.02974
[2025-05-11 16:49:16] nn step 37150, lr: 0.1.
	loss_policy_0: 0.12578
	accuracy_policy_0: 0.93789
	loss_value_0: 0.23292
	loss_policy_1: 0.02441
	accuracy_policy_1: 0.93996
	loss_value_1: 0.04693
	loss_reward_1: 0.00424
	loss_policy_2: 0.02478
	accuracy_policy_2: 0.9368
	loss_value_2: 0.04781
	loss_reward_2: 0.00485
	loss_policy_3: 0.02486
	accuracy_policy_3: 0.93531
	loss_value_3: 0.04854
	loss_reward_3: 0.00555
	loss_policy_4: 0.02493
	accuracy_policy_4: 0.94105
	loss_value_4: 0.04927
	loss_reward_4: 0.00656
	loss_policy_5: 0.02471
	accuracy_policy_5: 0.94961
	loss_value_5: 0.05003
	loss_reward_5: 0.00759
	loss_policy: 0.24949
	loss_value: 0.4755
	loss_reward: 0.02879
[2025-05-11 16:49:25] nn step 37200, lr: 0.1.
	loss_policy_0: 0.12733
	accuracy_policy_0: 0.93832
	loss_value_0: 0.23495
	loss_policy_1: 0.02505
	accuracy_policy_1: 0.93594
	loss_value_1: 0.04733
	loss_reward_1: 0.00451
	loss_policy_2: 0.02499
	accuracy_policy_2: 0.93836
	loss_value_2: 0.04812
	loss_reward_2: 0.00503
	loss_policy_3: 0.02512
	accuracy_policy_3: 0.93781
	loss_value_3: 0.04876
	loss_reward_3: 0.00557
	loss_policy_4: 0.02486
	accuracy_policy_4: 0.94156
	loss_value_4: 0.04975
	loss_reward_4: 0.0066
	loss_policy_5: 0.02505
	accuracy_policy_5: 0.95047
	loss_value_5: 0.05094
	loss_reward_5: 0.00796
	loss_policy: 0.2524
	loss_value: 0.47984
	loss_reward: 0.02968
Optimization_Done 37200
[2025-05-11 16:51:01] [command] train weight_iter_37200.pkl 168 187
[2025-05-11 16:51:11] nn step 37250, lr: 0.1.
	loss_policy_0: 0.12372
	accuracy_policy_0: 0.94008
	loss_value_0: 0.24275
	loss_policy_1: 0.02471
	accuracy_policy_1: 0.93996
	loss_value_1: 0.04855
	loss_reward_1: 0.00433
	loss_policy_2: 0.02485
	accuracy_policy_2: 0.93777
	loss_value_2: 0.04892
	loss_reward_2: 0.00515
	loss_policy_3: 0.02495
	accuracy_policy_3: 0.93797
	loss_value_3: 0.04983
	loss_reward_3: 0.00573
	loss_policy_4: 0.02494
	accuracy_policy_4: 0.94086
	loss_value_4: 0.05051
	loss_reward_4: 0.00664
	loss_policy_5: 0.02474
	accuracy_policy_5: 0.94883
	loss_value_5: 0.05153
	loss_reward_5: 0.00775
	loss_policy: 0.24792
	loss_value: 0.4921
	loss_reward: 0.02961
[2025-05-11 16:51:18] nn step 37300, lr: 0.1.
	loss_policy_0: 0.12026
	accuracy_policy_0: 0.93828
	loss_value_0: 0.22399
	loss_policy_1: 0.02366
	accuracy_policy_1: 0.93434
	loss_value_1: 0.04501
	loss_reward_1: 0.00424
	loss_policy_2: 0.02376
	accuracy_policy_2: 0.93711
	loss_value_2: 0.04574
	loss_reward_2: 0.00464
	loss_policy_3: 0.02355
	accuracy_policy_3: 0.94055
	loss_value_3: 0.04677
	loss_reward_3: 0.0051
	loss_policy_4: 0.02364
	accuracy_policy_4: 0.94094
	loss_value_4: 0.04754
	loss_reward_4: 0.00627
	loss_policy_5: 0.02338
	accuracy_policy_5: 0.94969
	loss_value_5: 0.04825
	loss_reward_5: 0.00749
	loss_policy: 0.23825
	loss_value: 0.45731
	loss_reward: 0.02774
[2025-05-11 16:51:27] nn step 37350, lr: 0.1.
	loss_policy_0: 0.12895
	accuracy_policy_0: 0.94008
	loss_value_0: 0.24254
	loss_policy_1: 0.02542
	accuracy_policy_1: 0.94012
	loss_value_1: 0.04866
	loss_reward_1: 0.00462
	loss_policy_2: 0.02576
	accuracy_policy_2: 0.94016
	loss_value_2: 0.04964
	loss_reward_2: 0.00517
	loss_policy_3: 0.02588
	accuracy_policy_3: 0.9384
	loss_value_3: 0.05025
	loss_reward_3: 0.00575
	loss_policy_4: 0.02612
	accuracy_policy_4: 0.94051
	loss_value_4: 0.05127
	loss_reward_4: 0.00698
	loss_policy_5: 0.02565
	accuracy_policy_5: 0.94992
	loss_value_5: 0.05196
	loss_reward_5: 0.00785
	loss_policy: 0.25778
	loss_value: 0.49431
	loss_reward: 0.03037
[2025-05-11 16:51:35] nn step 37400, lr: 0.1.
	loss_policy_0: 0.12479
	accuracy_policy_0: 0.9382
	loss_value_0: 0.23524
	loss_policy_1: 0.02487
	accuracy_policy_1: 0.9382
	loss_value_1: 0.04719
	loss_reward_1: 0.00428
	loss_policy_2: 0.02521
	accuracy_policy_2: 0.93816
	loss_value_2: 0.04793
	loss_reward_2: 0.005
	loss_policy_3: 0.02513
	accuracy_policy_3: 0.93688
	loss_value_3: 0.04889
	loss_reward_3: 0.00549
	loss_policy_4: 0.02501
	accuracy_policy_4: 0.93961
	loss_value_4: 0.04959
	loss_reward_4: 0.00664
	loss_policy_5: 0.02482
	accuracy_policy_5: 0.94832
	loss_value_5: 0.05036
	loss_reward_5: 0.00782
	loss_policy: 0.24983
	loss_value: 0.4792
	loss_reward: 0.02923
Optimization_Done 37400
[2025-05-11 16:53:08] [command] train weight_iter_37400.pkl 169 188
[2025-05-11 16:53:16] nn step 37450, lr: 0.1.
	loss_policy_0: 0.12905
	accuracy_policy_0: 0.94184
	loss_value_0: 0.24767
	loss_policy_1: 0.02538
	accuracy_policy_1: 0.93922
	loss_value_1: 0.04936
	loss_reward_1: 0.00434
	loss_policy_2: 0.02551
	accuracy_policy_2: 0.93945
	loss_value_2: 0.05026
	loss_reward_2: 0.00523
	loss_policy_3: 0.02593
	accuracy_policy_3: 0.9377
	loss_value_3: 0.05096
	loss_reward_3: 0.00566
	loss_policy_4: 0.02524
	accuracy_policy_4: 0.94352
	loss_value_4: 0.0522
	loss_reward_4: 0.00663
	loss_policy_5: 0.02513
	accuracy_policy_5: 0.9518
	loss_value_5: 0.05304
	loss_reward_5: 0.00785
	loss_policy: 0.25624
	loss_value: 0.50349
	loss_reward: 0.02971
[2025-05-11 16:53:24] nn step 37500, lr: 0.1.
	loss_policy_0: 0.12551
	accuracy_policy_0: 0.94305
	loss_value_0: 0.23983
	loss_policy_1: 0.02491
	accuracy_policy_1: 0.93863
	loss_value_1: 0.04798
	loss_reward_1: 0.00421
	loss_policy_2: 0.0249
	accuracy_policy_2: 0.93953
	loss_value_2: 0.04867
	loss_reward_2: 0.005
	loss_policy_3: 0.02503
	accuracy_policy_3: 0.94141
	loss_value_3: 0.04948
	loss_reward_3: 0.00544
	loss_policy_4: 0.02493
	accuracy_policy_4: 0.94508
	loss_value_4: 0.05022
	loss_reward_4: 0.0064
	loss_policy_5: 0.0251
	accuracy_policy_5: 0.95066
	loss_value_5: 0.05133
	loss_reward_5: 0.00748
	loss_policy: 0.25038
	loss_value: 0.4875
	loss_reward: 0.02853
[2025-05-11 16:53:33] nn step 37550, lr: 0.1.
	loss_policy_0: 0.12522
	accuracy_policy_0: 0.94012
	loss_value_0: 0.23801
	loss_policy_1: 0.02429
	accuracy_policy_1: 0.93859
	loss_value_1: 0.04753
	loss_reward_1: 0.00449
	loss_policy_2: 0.0247
	accuracy_policy_2: 0.94125
	loss_value_2: 0.04862
	loss_reward_2: 0.00503
	loss_policy_3: 0.02463
	accuracy_policy_3: 0.94309
	loss_value_3: 0.04942
	loss_reward_3: 0.00567
	loss_policy_4: 0.02456
	accuracy_policy_4: 0.94344
	loss_value_4: 0.05026
	loss_reward_4: 0.00668
	loss_policy_5: 0.02459
	accuracy_policy_5: 0.95211
	loss_value_5: 0.05129
	loss_reward_5: 0.00774
	loss_policy: 0.24798
	loss_value: 0.48514
	loss_reward: 0.02961
[2025-05-11 16:53:40] nn step 37600, lr: 0.1.
	loss_policy_0: 0.13287
	accuracy_policy_0: 0.94059
	loss_value_0: 0.25055
	loss_policy_1: 0.02644
	accuracy_policy_1: 0.93602
	loss_value_1: 0.05026
	loss_reward_1: 0.00463
	loss_policy_2: 0.02612
	accuracy_policy_2: 0.94176
	loss_value_2: 0.05108
	loss_reward_2: 0.0053
	loss_policy_3: 0.02639
	accuracy_policy_3: 0.93957
	loss_value_3: 0.05178
	loss_reward_3: 0.00603
	loss_policy_4: 0.02658
	accuracy_policy_4: 0.94078
	loss_value_4: 0.05299
	loss_reward_4: 0.00716
	loss_policy_5: 0.02644
	accuracy_policy_5: 0.9498
	loss_value_5: 0.05405
	loss_reward_5: 0.00821
	loss_policy: 0.26484
	loss_value: 0.51071
	loss_reward: 0.03132
Optimization_Done 37600
[2025-05-11 16:55:18] [command] train weight_iter_37600.pkl 170 189
[2025-05-11 16:55:28] nn step 37650, lr: 0.1.
	loss_policy_0: 0.12376
	accuracy_policy_0: 0.94211
	loss_value_0: 0.23818
	loss_policy_1: 0.02452
	accuracy_policy_1: 0.93848
	loss_value_1: 0.04751
	loss_reward_1: 0.00427
	loss_policy_2: 0.0243
	accuracy_policy_2: 0.94
	loss_value_2: 0.04827
	loss_reward_2: 0.00502
	loss_policy_3: 0.02458
	accuracy_policy_3: 0.94184
	loss_value_3: 0.04886
	loss_reward_3: 0.00576
	loss_policy_4: 0.02464
	accuracy_policy_4: 0.94305
	loss_value_4: 0.04993
	loss_reward_4: 0.00644
	loss_policy_5: 0.0246
	accuracy_policy_5: 0.94855
	loss_value_5: 0.05072
	loss_reward_5: 0.0077
	loss_policy: 0.24639
	loss_value: 0.48347
	loss_reward: 0.02921
[2025-05-11 16:55:34] nn step 37700, lr: 0.1.
	loss_policy_0: 0.12727
	accuracy_policy_0: 0.93867
	loss_value_0: 0.24044
	loss_policy_1: 0.02478
	accuracy_policy_1: 0.93793
	loss_value_1: 0.0485
	loss_reward_1: 0.00432
	loss_policy_2: 0.02521
	accuracy_policy_2: 0.94066
	loss_value_2: 0.04914
	loss_reward_2: 0.00492
	loss_policy_3: 0.02489
	accuracy_policy_3: 0.94176
	loss_value_3: 0.05024
	loss_reward_3: 0.00562
	loss_policy_4: 0.02485
	accuracy_policy_4: 0.9443
	loss_value_4: 0.0509
	loss_reward_4: 0.00671
	loss_policy_5: 0.02488
	accuracy_policy_5: 0.95254
	loss_value_5: 0.05194
	loss_reward_5: 0.00766
	loss_policy: 0.25188
	loss_value: 0.49116
	loss_reward: 0.02923
[2025-05-11 16:55:43] nn step 37750, lr: 0.1.
	loss_policy_0: 0.12456
	accuracy_policy_0: 0.94227
	loss_value_0: 0.23628
	loss_policy_1: 0.02464
	accuracy_policy_1: 0.93969
	loss_value_1: 0.04753
	loss_reward_1: 0.00431
	loss_policy_2: 0.02476
	accuracy_policy_2: 0.93824
	loss_value_2: 0.04823
	loss_reward_2: 0.00494
	loss_policy_3: 0.02494
	accuracy_policy_3: 0.93773
	loss_value_3: 0.049
	loss_reward_3: 0.00537
	loss_policy_4: 0.02465
	accuracy_policy_4: 0.94277
	loss_value_4: 0.04974
	loss_reward_4: 0.00642
	loss_policy_5: 0.02491
	accuracy_policy_5: 0.95141
	loss_value_5: 0.05055
	loss_reward_5: 0.0079
	loss_policy: 0.24846
	loss_value: 0.48134
	loss_reward: 0.02894
[2025-05-11 16:55:52] nn step 37800, lr: 0.1.
	loss_policy_0: 0.13387
	accuracy_policy_0: 0.94074
	loss_value_0: 0.25698
	loss_policy_1: 0.02634
	accuracy_policy_1: 0.93539
	loss_value_1: 0.05176
	loss_reward_1: 0.0046
	loss_policy_2: 0.02647
	accuracy_policy_2: 0.93707
	loss_value_2: 0.05241
	loss_reward_2: 0.00538
	loss_policy_3: 0.02682
	accuracy_policy_3: 0.94098
	loss_value_3: 0.05336
	loss_reward_3: 0.00603
	loss_policy_4: 0.02669
	accuracy_policy_4: 0.94246
	loss_value_4: 0.05434
	loss_reward_4: 0.00708
	loss_policy_5: 0.02658
	accuracy_policy_5: 0.94969
	loss_value_5: 0.05548
	loss_reward_5: 0.00854
	loss_policy: 0.26677
	loss_value: 0.52433
	loss_reward: 0.03164
Optimization_Done 37800
[2025-05-11 16:57:28] [command] train weight_iter_37800.pkl 171 190
[2025-05-11 16:57:36] nn step 37850, lr: 0.1.
	loss_policy_0: 0.12918
	accuracy_policy_0: 0.94207
	loss_value_0: 0.25909
	loss_policy_1: 0.02541
	accuracy_policy_1: 0.9407
	loss_value_1: 0.05189
	loss_reward_1: 0.00468
	loss_policy_2: 0.02609
	accuracy_policy_2: 0.94039
	loss_value_2: 0.05234
	loss_reward_2: 0.00531
	loss_policy_3: 0.02569
	accuracy_policy_3: 0.94246
	loss_value_3: 0.05342
	loss_reward_3: 0.00579
	loss_policy_4: 0.02557
	accuracy_policy_4: 0.94418
	loss_value_4: 0.05421
	loss_reward_4: 0.00727
	loss_policy_5: 0.02598
	accuracy_policy_5: 0.95105
	loss_value_5: 0.05531
	loss_reward_5: 0.00864
	loss_policy: 0.25792
	loss_value: 0.52626
	loss_reward: 0.03168
[2025-05-11 16:57:44] nn step 37900, lr: 0.1.
	loss_policy_0: 0.12645
	accuracy_policy_0: 0.94133
	loss_value_0: 0.24404
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.9434
	loss_value_1: 0.04872
	loss_reward_1: 0.00457
	loss_policy_2: 0.02457
	accuracy_policy_2: 0.94023
	loss_value_2: 0.04948
	loss_reward_2: 0.00509
	loss_policy_3: 0.02487
	accuracy_policy_3: 0.94254
	loss_value_3: 0.05005
	loss_reward_3: 0.00561
	loss_policy_4: 0.0247
	accuracy_policy_4: 0.94609
	loss_value_4: 0.05085
	loss_reward_4: 0.00681
	loss_policy_5: 0.0246
	accuracy_policy_5: 0.95352
	loss_value_5: 0.05195
	loss_reward_5: 0.00782
	loss_policy: 0.24992
	loss_value: 0.49509
	loss_reward: 0.02989
[2025-05-11 16:57:52] nn step 37950, lr: 0.1.
	loss_policy_0: 0.11881
	accuracy_policy_0: 0.94484
	loss_value_0: 0.23117
	loss_policy_1: 0.0239
	accuracy_policy_1: 0.94094
	loss_value_1: 0.04632
	loss_reward_1: 0.00438
	loss_policy_2: 0.02385
	accuracy_policy_2: 0.94195
	loss_value_2: 0.04678
	loss_reward_2: 0.00497
	loss_policy_3: 0.02381
	accuracy_policy_3: 0.94273
	loss_value_3: 0.04751
	loss_reward_3: 0.00554
	loss_policy_4: 0.02389
	accuracy_policy_4: 0.94406
	loss_value_4: 0.0486
	loss_reward_4: 0.00653
	loss_policy_5: 0.02372
	accuracy_policy_5: 0.95562
	loss_value_5: 0.04932
	loss_reward_5: 0.00763
	loss_policy: 0.23798
	loss_value: 0.4697
	loss_reward: 0.02905
[2025-05-11 16:57:59] nn step 38000, lr: 0.1.
	loss_policy_0: 0.12155
	accuracy_policy_0: 0.94414
	loss_value_0: 0.2331
	loss_policy_1: 0.02418
	accuracy_policy_1: 0.94332
	loss_value_1: 0.04665
	loss_reward_1: 0.0043
	loss_policy_2: 0.02429
	accuracy_policy_2: 0.94082
	loss_value_2: 0.04739
	loss_reward_2: 0.00494
	loss_policy_3: 0.02439
	accuracy_policy_3: 0.94312
	loss_value_3: 0.04793
	loss_reward_3: 0.00513
	loss_policy_4: 0.02451
	accuracy_policy_4: 0.94352
	loss_value_4: 0.04889
	loss_reward_4: 0.00644
	loss_policy_5: 0.0242
	accuracy_policy_5: 0.9552
	loss_value_5: 0.05011
	loss_reward_5: 0.00775
	loss_policy: 0.24311
	loss_value: 0.47407
	loss_reward: 0.02856
Optimization_Done 38000
[2025-05-11 16:59:37] [command] train weight_iter_38000.pkl 172 191
[2025-05-11 16:59:46] nn step 38050, lr: 0.1.
	loss_policy_0: 0.12326
	accuracy_policy_0: 0.94281
	loss_value_0: 0.23753
	loss_policy_1: 0.02422
	accuracy_policy_1: 0.9402
	loss_value_1: 0.04749
	loss_reward_1: 0.00413
	loss_policy_2: 0.0243
	accuracy_policy_2: 0.94207
	loss_value_2: 0.04833
	loss_reward_2: 0.00471
	loss_policy_3: 0.0246
	accuracy_policy_3: 0.94227
	loss_value_3: 0.04876
	loss_reward_3: 0.00529
	loss_policy_4: 0.02441
	accuracy_policy_4: 0.94441
	loss_value_4: 0.04961
	loss_reward_4: 0.0064
	loss_policy_5: 0.0244
	accuracy_policy_5: 0.95301
	loss_value_5: 0.05066
	loss_reward_5: 0.00734
	loss_policy: 0.24519
	loss_value: 0.48238
	loss_reward: 0.02788
[2025-05-11 16:59:53] nn step 38100, lr: 0.1.
	loss_policy_0: 0.12356
	accuracy_policy_0: 0.94223
	loss_value_0: 0.23699
	loss_policy_1: 0.02408
	accuracy_policy_1: 0.94168
	loss_value_1: 0.04707
	loss_reward_1: 0.00436
	loss_policy_2: 0.02476
	accuracy_policy_2: 0.94363
	loss_value_2: 0.04775
	loss_reward_2: 0.00484
	loss_policy_3: 0.02426
	accuracy_policy_3: 0.94227
	loss_value_3: 0.04861
	loss_reward_3: 0.00546
	loss_policy_4: 0.02425
	accuracy_policy_4: 0.94383
	loss_value_4: 0.0493
	loss_reward_4: 0.00621
	loss_policy_5: 0.02417
	accuracy_policy_5: 0.9548
	loss_value_5: 0.05032
	loss_reward_5: 0.00756
	loss_policy: 0.24506
	loss_value: 0.48004
	loss_reward: 0.02843
[2025-05-11 17:00:02] nn step 38150, lr: 0.1.
	loss_policy_0: 0.12533
	accuracy_policy_0: 0.94316
	loss_value_0: 0.24099
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.94219
	loss_value_1: 0.04794
	loss_reward_1: 0.00444
	loss_policy_2: 0.02489
	accuracy_policy_2: 0.9409
	loss_value_2: 0.04862
	loss_reward_2: 0.00492
	loss_policy_3: 0.02488
	accuracy_policy_3: 0.945
	loss_value_3: 0.04931
	loss_reward_3: 0.00551
	loss_policy_4: 0.02503
	accuracy_policy_4: 0.94629
	loss_value_4: 0.05015
	loss_reward_4: 0.00673
	loss_policy_5: 0.0247
	accuracy_policy_5: 0.95531
	loss_value_5: 0.05128
	loss_reward_5: 0.00773
	loss_policy: 0.24957
	loss_value: 0.48828
	loss_reward: 0.02933
[2025-05-11 17:00:10] nn step 38200, lr: 0.1.
	loss_policy_0: 0.13106
	accuracy_policy_0: 0.94062
	loss_value_0: 0.2494
	loss_policy_1: 0.02576
	accuracy_policy_1: 0.93906
	loss_value_1: 0.04993
	loss_reward_1: 0.00448
	loss_policy_2: 0.02603
	accuracy_policy_2: 0.94512
	loss_value_2: 0.05058
	loss_reward_2: 0.00512
	loss_policy_3: 0.02596
	accuracy_policy_3: 0.94398
	loss_value_3: 0.05137
	loss_reward_3: 0.00597
	loss_policy_4: 0.026
	accuracy_policy_4: 0.945
	loss_value_4: 0.05236
	loss_reward_4: 0.00714
	loss_policy_5: 0.02605
	accuracy_policy_5: 0.95512
	loss_value_5: 0.05326
	loss_reward_5: 0.00807
	loss_policy: 0.26086
	loss_value: 0.50691
	loss_reward: 0.03078
Optimization_Done 38200
[2025-05-11 17:01:44] [command] train weight_iter_38200.pkl 173 192
[2025-05-11 17:01:54] nn step 38250, lr: 0.1.
	loss_policy_0: 0.11628
	accuracy_policy_0: 0.94367
	loss_value_0: 0.22889
	loss_policy_1: 0.02317
	accuracy_policy_1: 0.94051
	loss_value_1: 0.04575
	loss_reward_1: 0.00415
	loss_policy_2: 0.02301
	accuracy_policy_2: 0.94508
	loss_value_2: 0.04649
	loss_reward_2: 0.00461
	loss_policy_3: 0.02283
	accuracy_policy_3: 0.94332
	loss_value_3: 0.04705
	loss_reward_3: 0.00491
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.9473
	loss_value_4: 0.0477
	loss_reward_4: 0.00642
	loss_policy_5: 0.02296
	accuracy_policy_5: 0.9548
	loss_value_5: 0.04862
	loss_reward_5: 0.00747
	loss_policy: 0.23132
	loss_value: 0.46449
	loss_reward: 0.02757
[2025-05-11 17:02:02] nn step 38300, lr: 0.1.
	loss_policy_0: 0.12452
	accuracy_policy_0: 0.94156
	loss_value_0: 0.23959
	loss_policy_1: 0.02469
	accuracy_policy_1: 0.94047
	loss_value_1: 0.04808
	loss_reward_1: 0.00433
	loss_policy_2: 0.02475
	accuracy_policy_2: 0.94094
	loss_value_2: 0.04832
	loss_reward_2: 0.00499
	loss_policy_3: 0.02425
	accuracy_policy_3: 0.94379
	loss_value_3: 0.04886
	loss_reward_3: 0.00541
	loss_policy_4: 0.02455
	accuracy_policy_4: 0.94676
	loss_value_4: 0.04944
	loss_reward_4: 0.00662
	loss_policy_5: 0.02455
	accuracy_policy_5: 0.95523
	loss_value_5: 0.05054
	loss_reward_5: 0.00781
	loss_policy: 0.2473
	loss_value: 0.48483
	loss_reward: 0.02915
[2025-05-11 17:02:11] nn step 38350, lr: 0.1.
	loss_policy_0: 0.11899
	accuracy_policy_0: 0.94027
	loss_value_0: 0.22755
	loss_policy_1: 0.02382
	accuracy_policy_1: 0.93793
	loss_value_1: 0.04571
	loss_reward_1: 0.00406
	loss_policy_2: 0.02365
	accuracy_policy_2: 0.9418
	loss_value_2: 0.0463
	loss_reward_2: 0.00481
	loss_policy_3: 0.02372
	accuracy_policy_3: 0.9443
	loss_value_3: 0.04686
	loss_reward_3: 0.00536
	loss_policy_4: 0.02369
	accuracy_policy_4: 0.94516
	loss_value_4: 0.04756
	loss_reward_4: 0.0062
	loss_policy_5: 0.0233
	accuracy_policy_5: 0.95594
	loss_value_5: 0.04833
	loss_reward_5: 0.00765
	loss_policy: 0.23718
	loss_value: 0.46232
	loss_reward: 0.02808
[2025-05-11 17:02:18] nn step 38400, lr: 0.1.
	loss_policy_0: 0.12022
	accuracy_policy_0: 0.94227
	loss_value_0: 0.23171
	loss_policy_1: 0.02369
	accuracy_policy_1: 0.94008
	loss_value_1: 0.04667
	loss_reward_1: 0.00425
	loss_policy_2: 0.02377
	accuracy_policy_2: 0.94402
	loss_value_2: 0.04728
	loss_reward_2: 0.00481
	loss_policy_3: 0.02368
	accuracy_policy_3: 0.945
	loss_value_3: 0.04797
	loss_reward_3: 0.00521
	loss_policy_4: 0.02392
	accuracy_policy_4: 0.94617
	loss_value_4: 0.04895
	loss_reward_4: 0.00639
	loss_policy_5: 0.02366
	accuracy_policy_5: 0.95461
	loss_value_5: 0.04974
	loss_reward_5: 0.00749
	loss_policy: 0.23895
	loss_value: 0.47233
	loss_reward: 0.02815
Optimization_Done 38400
[2025-05-11 17:03:58] [command] train weight_iter_38400.pkl 174 193
[2025-05-11 17:04:06] nn step 38450, lr: 0.1.
	loss_policy_0: 0.11685
	accuracy_policy_0: 0.93949
	loss_value_0: 0.2556
	loss_policy_1: 0.02268
	accuracy_policy_1: 0.93555
	loss_value_1: 0.05055
	loss_reward_1: 0.0043
	loss_policy_2: 0.02292
	accuracy_policy_2: 0.94105
	loss_value_2: 0.05169
	loss_reward_2: 0.00461
	loss_policy_3: 0.02289
	accuracy_policy_3: 0.93988
	loss_value_3: 0.05205
	loss_reward_3: 0.00548
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.9448
	loss_value_4: 0.0528
	loss_reward_4: 0.00668
	loss_policy_5: 0.02289
	accuracy_policy_5: 0.95133
	loss_value_5: 0.05368
	loss_reward_5: 0.00727
	loss_policy: 0.23124
	loss_value: 0.51637
	loss_reward: 0.02834
[2025-05-11 17:04:15] nn step 38500, lr: 0.1.
	loss_policy_0: 0.11931
	accuracy_policy_0: 0.94164
	loss_value_0: 0.25256
	loss_policy_1: 0.02385
	accuracy_policy_1: 0.93559
	loss_value_1: 0.05025
	loss_reward_1: 0.00444
	loss_policy_2: 0.02394
	accuracy_policy_2: 0.93977
	loss_value_2: 0.05086
	loss_reward_2: 0.00485
	loss_policy_3: 0.024
	accuracy_policy_3: 0.9393
	loss_value_3: 0.05137
	loss_reward_3: 0.00535
	loss_policy_4: 0.02383
	accuracy_policy_4: 0.94402
	loss_value_4: 0.05241
	loss_reward_4: 0.00657
	loss_policy_5: 0.02345
	accuracy_policy_5: 0.95074
	loss_value_5: 0.05325
	loss_reward_5: 0.00787
	loss_policy: 0.23839
	loss_value: 0.51071
	loss_reward: 0.02908
[2025-05-11 17:04:23] nn step 38550, lr: 0.1.
	loss_policy_0: 0.11963
	accuracy_policy_0: 0.94324
	loss_value_0: 0.2473
	loss_policy_1: 0.02363
	accuracy_policy_1: 0.94004
	loss_value_1: 0.04928
	loss_reward_1: 0.00418
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.94168
	loss_value_2: 0.04978
	loss_reward_2: 0.0047
	loss_policy_3: 0.02389
	accuracy_policy_3: 0.94234
	loss_value_3: 0.05068
	loss_reward_3: 0.00557
	loss_policy_4: 0.02379
	accuracy_policy_4: 0.94086
	loss_value_4: 0.05119
	loss_reward_4: 0.00664
	loss_policy_5: 0.02377
	accuracy_policy_5: 0.95191
	loss_value_5: 0.05208
	loss_reward_5: 0.00788
	loss_policy: 0.2387
	loss_value: 0.5003
	loss_reward: 0.02896
[2025-05-11 17:04:32] nn step 38600, lr: 0.1.
	loss_policy_0: 0.11699
	accuracy_policy_0: 0.94145
	loss_value_0: 0.23878
	loss_policy_1: 0.02338
	accuracy_policy_1: 0.94066
	loss_value_1: 0.04752
	loss_reward_1: 0.00433
	loss_policy_2: 0.02347
	accuracy_policy_2: 0.94027
	loss_value_2: 0.04826
	loss_reward_2: 0.00467
	loss_policy_3: 0.02338
	accuracy_policy_3: 0.93734
	loss_value_3: 0.04885
	loss_reward_3: 0.00532
	loss_policy_4: 0.02343
	accuracy_policy_4: 0.94203
	loss_value_4: 0.04961
	loss_reward_4: 0.00655
	loss_policy_5: 0.0234
	accuracy_policy_5: 0.95199
	loss_value_5: 0.05052
	loss_reward_5: 0.00751
	loss_policy: 0.23407
	loss_value: 0.48354
	loss_reward: 0.02838
Optimization_Done 38600
[2025-05-11 17:06:03] [command] train weight_iter_38600.pkl 175 194
[2025-05-11 17:06:13] nn step 38650, lr: 0.1.
	loss_policy_0: 0.11956
	accuracy_policy_0: 0.94324
	loss_value_0: 0.25694
	loss_policy_1: 0.02363
	accuracy_policy_1: 0.94152
	loss_value_1: 0.05093
	loss_reward_1: 0.00444
	loss_policy_2: 0.02372
	accuracy_policy_2: 0.93871
	loss_value_2: 0.0514
	loss_reward_2: 0.00486
	loss_policy_3: 0.02347
	accuracy_policy_3: 0.94316
	loss_value_3: 0.05186
	loss_reward_3: 0.00545
	loss_policy_4: 0.02386
	accuracy_policy_4: 0.94473
	loss_value_4: 0.05256
	loss_reward_4: 0.00648
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.95668
	loss_value_5: 0.05361
	loss_reward_5: 0.0076
	loss_policy: 0.23782
	loss_value: 0.51731
	loss_reward: 0.02883
[2025-05-11 17:06:21] nn step 38700, lr: 0.1.
	loss_policy_0: 0.12009
	accuracy_policy_0: 0.94348
	loss_value_0: 0.25444
	loss_policy_1: 0.0242
	accuracy_policy_1: 0.94105
	loss_value_1: 0.05099
	loss_reward_1: 0.00445
	loss_policy_2: 0.02383
	accuracy_policy_2: 0.94297
	loss_value_2: 0.05153
	loss_reward_2: 0.00502
	loss_policy_3: 0.02421
	accuracy_policy_3: 0.94449
	loss_value_3: 0.05258
	loss_reward_3: 0.00553
	loss_policy_4: 0.02401
	accuracy_policy_4: 0.94598
	loss_value_4: 0.0534
	loss_reward_4: 0.00675
	loss_policy_5: 0.02402
	accuracy_policy_5: 0.95457
	loss_value_5: 0.05436
	loss_reward_5: 0.00807
	loss_policy: 0.24035
	loss_value: 0.5173
	loss_reward: 0.02982
[2025-05-11 17:06:28] nn step 38750, lr: 0.1.
	loss_policy_0: 0.11721
	accuracy_policy_0: 0.9416
	loss_value_0: 0.24274
	loss_policy_1: 0.0234
	accuracy_policy_1: 0.93961
	loss_value_1: 0.04843
	loss_reward_1: 0.00435
	loss_policy_2: 0.02339
	accuracy_policy_2: 0.9418
	loss_value_2: 0.04925
	loss_reward_2: 0.00507
	loss_policy_3: 0.02311
	accuracy_policy_3: 0.94336
	loss_value_3: 0.04969
	loss_reward_3: 0.00574
	loss_policy_4: 0.02324
	accuracy_policy_4: 0.94586
	loss_value_4: 0.05056
	loss_reward_4: 0.00681
	loss_policy_5: 0.02289
	accuracy_policy_5: 0.95555
	loss_value_5: 0.05153
	loss_reward_5: 0.00774
	loss_policy: 0.23323
	loss_value: 0.49221
	loss_reward: 0.02971
[2025-05-11 17:06:37] nn step 38800, lr: 0.1.
	loss_policy_0: 0.12239
	accuracy_policy_0: 0.94289
	loss_value_0: 0.25201
	loss_policy_1: 0.0243
	accuracy_policy_1: 0.94105
	loss_value_1: 0.05044
	loss_reward_1: 0.00445
	loss_policy_2: 0.02443
	accuracy_policy_2: 0.94184
	loss_value_2: 0.0511
	loss_reward_2: 0.00523
	loss_policy_3: 0.0243
	accuracy_policy_3: 0.94059
	loss_value_3: 0.05169
	loss_reward_3: 0.00589
	loss_policy_4: 0.024
	accuracy_policy_4: 0.94738
	loss_value_4: 0.05254
	loss_reward_4: 0.007
	loss_policy_5: 0.02429
	accuracy_policy_5: 0.955
	loss_value_5: 0.05351
	loss_reward_5: 0.00839
	loss_policy: 0.24371
	loss_value: 0.51128
	loss_reward: 0.03097
Optimization_Done 38800
[2025-05-11 17:08:14] [command] train weight_iter_38800.pkl 176 195
[2025-05-11 17:08:21] nn step 38850, lr: 0.1.
	loss_policy_0: 0.1148
	accuracy_policy_0: 0.94676
	loss_value_0: 0.24047
	loss_policy_1: 0.02253
	accuracy_policy_1: 0.94512
	loss_value_1: 0.04784
	loss_reward_1: 0.00418
	loss_policy_2: 0.02278
	accuracy_policy_2: 0.94703
	loss_value_2: 0.04848
	loss_reward_2: 0.0047
	loss_policy_3: 0.02282
	accuracy_policy_3: 0.94594
	loss_value_3: 0.04904
	loss_reward_3: 0.00541
	loss_policy_4: 0.02279
	accuracy_policy_4: 0.9498
	loss_value_4: 0.04975
	loss_reward_4: 0.00624
	loss_policy_5: 0.02269
	accuracy_policy_5: 0.95805
	loss_value_5: 0.05069
	loss_reward_5: 0.00739
	loss_policy: 0.2284
	loss_value: 0.48626
	loss_reward: 0.02792
[2025-05-11 17:08:30] nn step 38900, lr: 0.1.
	loss_policy_0: 0.11128
	accuracy_policy_0: 0.94672
	loss_value_0: 0.22929
	loss_policy_1: 0.02219
	accuracy_policy_1: 0.94219
	loss_value_1: 0.04571
	loss_reward_1: 0.00408
	loss_policy_2: 0.02208
	accuracy_policy_2: 0.94734
	loss_value_2: 0.04627
	loss_reward_2: 0.00443
	loss_policy_3: 0.02238
	accuracy_policy_3: 0.94598
	loss_value_3: 0.04694
	loss_reward_3: 0.00504
	loss_policy_4: 0.02238
	accuracy_policy_4: 0.94578
	loss_value_4: 0.04766
	loss_reward_4: 0.0061
	loss_policy_5: 0.02212
	accuracy_policy_5: 0.95688
	loss_value_5: 0.04843
	loss_reward_5: 0.00713
	loss_policy: 0.22243
	loss_value: 0.4643
	loss_reward: 0.02678
[2025-05-11 17:08:38] nn step 38950, lr: 0.1.
	loss_policy_0: 0.11443
	accuracy_policy_0: 0.94578
	loss_value_0: 0.23112
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.9416
	loss_value_1: 0.04617
	loss_reward_1: 0.004
	loss_policy_2: 0.02267
	accuracy_policy_2: 0.94215
	loss_value_2: 0.04674
	loss_reward_2: 0.00461
	loss_policy_3: 0.02291
	accuracy_policy_3: 0.94355
	loss_value_3: 0.04751
	loss_reward_3: 0.00531
	loss_policy_4: 0.02287
	accuracy_policy_4: 0.94871
	loss_value_4: 0.04834
	loss_reward_4: 0.00623
	loss_policy_5: 0.02274
	accuracy_policy_5: 0.95695
	loss_value_5: 0.04909
	loss_reward_5: 0.0074
	loss_policy: 0.22827
	loss_value: 0.46896
	loss_reward: 0.02755
[2025-05-11 17:08:47] nn step 39000, lr: 0.1.
	loss_policy_0: 0.11382
	accuracy_policy_0: 0.94434
	loss_value_0: 0.2284
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.94254
	loss_value_1: 0.04561
	loss_reward_1: 0.00414
	loss_policy_2: 0.02275
	accuracy_policy_2: 0.94422
	loss_value_2: 0.04644
	loss_reward_2: 0.00444
	loss_policy_3: 0.02273
	accuracy_policy_3: 0.94793
	loss_value_3: 0.04674
	loss_reward_3: 0.00519
	loss_policy_4: 0.0228
	accuracy_policy_4: 0.94848
	loss_value_4: 0.04713
	loss_reward_4: 0.00627
	loss_policy_5: 0.02232
	accuracy_policy_5: 0.95824
	loss_value_5: 0.04801
	loss_reward_5: 0.00731
	loss_policy: 0.22707
	loss_value: 0.46232
	loss_reward: 0.02735
Optimization_Done 39000
[2025-05-11 17:10:20] [command] train weight_iter_39000.pkl 177 196
[2025-05-11 17:10:30] nn step 39050, lr: 0.1.
	loss_policy_0: 0.10971
	accuracy_policy_0: 0.94949
	loss_value_0: 0.22991
	loss_policy_1: 0.02179
	accuracy_policy_1: 0.94598
	loss_value_1: 0.04581
	loss_reward_1: 0.0041
	loss_policy_2: 0.0219
	accuracy_policy_2: 0.94469
	loss_value_2: 0.04646
	loss_reward_2: 0.00452
	loss_policy_3: 0.02199
	accuracy_policy_3: 0.94754
	loss_value_3: 0.04665
	loss_reward_3: 0.00516
	loss_policy_4: 0.02206
	accuracy_policy_4: 0.9491
	loss_value_4: 0.0471
	loss_reward_4: 0.00625
	loss_policy_5: 0.022
	accuracy_policy_5: 0.95926
	loss_value_5: 0.04823
	loss_reward_5: 0.00688
	loss_policy: 0.21945
	loss_value: 0.46416
	loss_reward: 0.02692
[2025-05-11 17:10:39] nn step 39100, lr: 0.1.
	loss_policy_0: 0.11906
	accuracy_policy_0: 0.94488
	loss_value_0: 0.24118
	loss_policy_1: 0.02341
	accuracy_policy_1: 0.94156
	loss_value_1: 0.0481
	loss_reward_1: 0.00418
	loss_policy_2: 0.02315
	accuracy_policy_2: 0.94473
	loss_value_2: 0.04873
	loss_reward_2: 0.00468
	loss_policy_3: 0.02362
	accuracy_policy_3: 0.945
	loss_value_3: 0.04922
	loss_reward_3: 0.00544
	loss_policy_4: 0.02369
	accuracy_policy_4: 0.94676
	loss_value_4: 0.04997
	loss_reward_4: 0.00654
	loss_policy_5: 0.02335
	accuracy_policy_5: 0.95625
	loss_value_5: 0.0507
	loss_reward_5: 0.00758
	loss_policy: 0.23629
	loss_value: 0.48791
	loss_reward: 0.02843
[2025-05-11 17:10:45] nn step 39150, lr: 0.1.
	loss_policy_0: 0.11564
	accuracy_policy_0: 0.94707
	loss_value_0: 0.23983
	loss_policy_1: 0.02309
	accuracy_policy_1: 0.94363
	loss_value_1: 0.04811
	loss_reward_1: 0.00426
	loss_policy_2: 0.0234
	accuracy_policy_2: 0.94266
	loss_value_2: 0.04849
	loss_reward_2: 0.00466
	loss_policy_3: 0.02333
	accuracy_policy_3: 0.94582
	loss_value_3: 0.04908
	loss_reward_3: 0.00546
	loss_policy_4: 0.02333
	accuracy_policy_4: 0.94594
	loss_value_4: 0.05002
	loss_reward_4: 0.00671
	loss_policy_5: 0.02327
	accuracy_policy_5: 0.95789
	loss_value_5: 0.05086
	loss_reward_5: 0.00781
	loss_policy: 0.23206
	loss_value: 0.4864
	loss_reward: 0.02891
[2025-05-11 17:10:54] nn step 39200, lr: 0.1.
	loss_policy_0: 0.11807
	accuracy_policy_0: 0.94383
	loss_value_0: 0.24079
	loss_policy_1: 0.02358
	accuracy_policy_1: 0.9441
	loss_value_1: 0.04859
	loss_reward_1: 0.00448
	loss_policy_2: 0.02372
	accuracy_policy_2: 0.94109
	loss_value_2: 0.04903
	loss_reward_2: 0.00499
	loss_policy_3: 0.02362
	accuracy_policy_3: 0.94805
	loss_value_3: 0.04955
	loss_reward_3: 0.00569
	loss_policy_4: 0.02365
	accuracy_policy_4: 0.94828
	loss_value_4: 0.05053
	loss_reward_4: 0.00707
	loss_policy_5: 0.02346
	accuracy_policy_5: 0.95762
	loss_value_5: 0.05142
	loss_reward_5: 0.00806
	loss_policy: 0.2361
	loss_value: 0.4899
	loss_reward: 0.03029
Optimization_Done 39200
[2025-05-11 17:12:29] [command] train weight_iter_39200.pkl 178 197
[2025-05-11 17:12:38] nn step 39250, lr: 0.1.
	loss_policy_0: 0.11647
	accuracy_policy_0: 0.94871
	loss_value_0: 0.24286
	loss_policy_1: 0.02303
	accuracy_policy_1: 0.94746
	loss_value_1: 0.04837
	loss_reward_1: 0.00421
	loss_policy_2: 0.02311
	accuracy_policy_2: 0.94723
	loss_value_2: 0.04884
	loss_reward_2: 0.0049
	loss_policy_3: 0.02318
	accuracy_policy_3: 0.94602
	loss_value_3: 0.0495
	loss_reward_3: 0.00549
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.94996
	loss_value_4: 0.05007
	loss_reward_4: 0.00632
	loss_policy_5: 0.02266
	accuracy_policy_5: 0.95965
	loss_value_5: 0.05084
	loss_reward_5: 0.00732
	loss_policy: 0.23146
	loss_value: 0.49047
	loss_reward: 0.02824
[2025-05-11 17:12:45] nn step 39300, lr: 0.1.
	loss_policy_0: 0.1249
	accuracy_policy_0: 0.94914
	loss_value_0: 0.25606
	loss_policy_1: 0.02501
	accuracy_policy_1: 0.94566
	loss_value_1: 0.05114
	loss_reward_1: 0.0047
	loss_policy_2: 0.02498
	accuracy_policy_2: 0.94984
	loss_value_2: 0.05167
	loss_reward_2: 0.00529
	loss_policy_3: 0.02484
	accuracy_policy_3: 0.9518
	loss_value_3: 0.05241
	loss_reward_3: 0.00585
	loss_policy_4: 0.02511
	accuracy_policy_4: 0.95336
	loss_value_4: 0.05326
	loss_reward_4: 0.00708
	loss_policy_5: 0.02453
	accuracy_policy_5: 0.96074
	loss_value_5: 0.05401
	loss_reward_5: 0.00807
	loss_policy: 0.24936
	loss_value: 0.51855
	loss_reward: 0.03099
[2025-05-11 17:12:54] nn step 39350, lr: 0.1.
	loss_policy_0: 0.12111
	accuracy_policy_0: 0.94945
	loss_value_0: 0.24726
	loss_policy_1: 0.0244
	accuracy_policy_1: 0.94867
	loss_value_1: 0.04955
	loss_reward_1: 0.00446
	loss_policy_2: 0.02428
	accuracy_policy_2: 0.94812
	loss_value_2: 0.05046
	loss_reward_2: 0.0051
	loss_policy_3: 0.02473
	accuracy_policy_3: 0.94891
	loss_value_3: 0.05121
	loss_reward_3: 0.00585
	loss_policy_4: 0.02442
	accuracy_policy_4: 0.95145
	loss_value_4: 0.05211
	loss_reward_4: 0.00705
	loss_policy_5: 0.02439
	accuracy_policy_5: 0.9609
	loss_value_5: 0.05272
	loss_reward_5: 0.0078
	loss_policy: 0.24332
	loss_value: 0.50331
	loss_reward: 0.03026
[2025-05-11 17:13:03] nn step 39400, lr: 0.1.
	loss_policy_0: 0.12031
	accuracy_policy_0: 0.94871
	loss_value_0: 0.24491
	loss_policy_1: 0.02411
	accuracy_policy_1: 0.94262
	loss_value_1: 0.04892
	loss_reward_1: 0.00435
	loss_policy_2: 0.02404
	accuracy_policy_2: 0.94539
	loss_value_2: 0.04953
	loss_reward_2: 0.00491
	loss_policy_3: 0.02398
	accuracy_policy_3: 0.94715
	loss_value_3: 0.05004
	loss_reward_3: 0.0057
	loss_policy_4: 0.0241
	accuracy_policy_4: 0.94945
	loss_value_4: 0.05072
	loss_reward_4: 0.00684
	loss_policy_5: 0.02414
	accuracy_policy_5: 0.95879
	loss_value_5: 0.05171
	loss_reward_5: 0.00763
	loss_policy: 0.24068
	loss_value: 0.49582
	loss_reward: 0.02943
Optimization_Done 39400
[2025-05-11 17:14:37] [command] train weight_iter_39400.pkl 179 198
[2025-05-11 17:14:46] nn step 39450, lr: 0.1.
	loss_policy_0: 0.11452
	accuracy_policy_0: 0.94961
	loss_value_0: 0.24261
	loss_policy_1: 0.02253
	accuracy_policy_1: 0.94992
	loss_value_1: 0.04868
	loss_reward_1: 0.00426
	loss_policy_2: 0.02256
	accuracy_policy_2: 0.94957
	loss_value_2: 0.04931
	loss_reward_2: 0.00495
	loss_policy_3: 0.02293
	accuracy_policy_3: 0.94906
	loss_value_3: 0.04991
	loss_reward_3: 0.00568
	loss_policy_4: 0.02248
	accuracy_policy_4: 0.95
	loss_value_4: 0.05067
	loss_reward_4: 0.00679
	loss_policy_5: 0.02287
	accuracy_policy_5: 0.95805
	loss_value_5: 0.05138
	loss_reward_5: 0.00762
	loss_policy: 0.2279
	loss_value: 0.49256
	loss_reward: 0.0293
[2025-05-11 17:14:54] nn step 39500, lr: 0.1.
	loss_policy_0: 0.1135
	accuracy_policy_0: 0.94965
	loss_value_0: 0.23603
	loss_policy_1: 0.02268
	accuracy_policy_1: 0.94629
	loss_value_1: 0.04742
	loss_reward_1: 0.00422
	loss_policy_2: 0.02252
	accuracy_policy_2: 0.94746
	loss_value_2: 0.0479
	loss_reward_2: 0.00478
	loss_policy_3: 0.02246
	accuracy_policy_3: 0.95086
	loss_value_3: 0.04825
	loss_reward_3: 0.00514
	loss_policy_4: 0.02255
	accuracy_policy_4: 0.94805
	loss_value_4: 0.04908
	loss_reward_4: 0.00654
	loss_policy_5: 0.02262
	accuracy_policy_5: 0.95973
	loss_value_5: 0.05003
	loss_reward_5: 0.00739
	loss_policy: 0.22631
	loss_value: 0.47871
	loss_reward: 0.02808
[2025-05-11 17:15:01] nn step 39550, lr: 0.1.
	loss_policy_0: 0.11618
	accuracy_policy_0: 0.9509
	loss_value_0: 0.24439
	loss_policy_1: 0.02304
	accuracy_policy_1: 0.94715
	loss_value_1: 0.04877
	loss_reward_1: 0.00427
	loss_policy_2: 0.02317
	accuracy_policy_2: 0.94875
	loss_value_2: 0.04961
	loss_reward_2: 0.00484
	loss_policy_3: 0.02334
	accuracy_policy_3: 0.94867
	loss_value_3: 0.05012
	loss_reward_3: 0.00534
	loss_policy_4: 0.02324
	accuracy_policy_4: 0.95
	loss_value_4: 0.05091
	loss_reward_4: 0.00653
	loss_policy_5: 0.02309
	accuracy_policy_5: 0.96031
	loss_value_5: 0.05189
	loss_reward_5: 0.0077
	loss_policy: 0.23205
	loss_value: 0.49569
	loss_reward: 0.02868
[2025-05-11 17:15:10] nn step 39600, lr: 0.1.
	loss_policy_0: 0.11713
	accuracy_policy_0: 0.94824
	loss_value_0: 0.24079
	loss_policy_1: 0.02311
	accuracy_policy_1: 0.94652
	loss_value_1: 0.04833
	loss_reward_1: 0.00437
	loss_policy_2: 0.0233
	accuracy_policy_2: 0.94656
	loss_value_2: 0.04909
	loss_reward_2: 0.00472
	loss_policy_3: 0.02297
	accuracy_policy_3: 0.9482
	loss_value_3: 0.04935
	loss_reward_3: 0.00531
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.94957
	loss_value_4: 0.05036
	loss_reward_4: 0.00673
	loss_policy_5: 0.02338
	accuracy_policy_5: 0.95875
	loss_value_5: 0.05149
	loss_reward_5: 0.00739
	loss_policy: 0.23291
	loss_value: 0.48942
	loss_reward: 0.02851
Optimization_Done 39600
[2025-05-11 17:16:50] [command] train weight_iter_39600.pkl 180 199
[2025-05-11 17:16:58] nn step 39650, lr: 0.1.
	loss_policy_0: 0.11709
	accuracy_policy_0: 0.95184
	loss_value_0: 0.25138
	loss_policy_1: 0.0232
	accuracy_policy_1: 0.94602
	loss_value_1: 0.05016
	loss_reward_1: 0.00431
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.95031
	loss_value_2: 0.05057
	loss_reward_2: 0.0049
	loss_policy_3: 0.0234
	accuracy_policy_3: 0.95117
	loss_value_3: 0.0511
	loss_reward_3: 0.00534
	loss_policy_4: 0.02349
	accuracy_policy_4: 0.94949
	loss_value_4: 0.05185
	loss_reward_4: 0.00689
	loss_policy_5: 0.0231
	accuracy_policy_5: 0.9591
	loss_value_5: 0.05282
	loss_reward_5: 0.00785
	loss_policy: 0.23379
	loss_value: 0.50788
	loss_reward: 0.02929
[2025-05-11 17:17:07] nn step 39700, lr: 0.1.
	loss_policy_0: 0.12158
	accuracy_policy_0: 0.95098
	loss_value_0: 0.25398
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.94867
	loss_value_1: 0.05062
	loss_reward_1: 0.00433
	loss_policy_2: 0.02375
	accuracy_policy_2: 0.95004
	loss_value_2: 0.05133
	loss_reward_2: 0.00495
	loss_policy_3: 0.02396
	accuracy_policy_3: 0.94789
	loss_value_3: 0.05173
	loss_reward_3: 0.00568
	loss_policy_4: 0.0238
	accuracy_policy_4: 0.95117
	loss_value_4: 0.05261
	loss_reward_4: 0.00673
	loss_policy_5: 0.02368
	accuracy_policy_5: 0.95969
	loss_value_5: 0.05364
	loss_reward_5: 0.00818
	loss_policy: 0.24065
	loss_value: 0.51391
	loss_reward: 0.02987
[2025-05-11 17:17:15] nn step 39750, lr: 0.1.
	loss_policy_0: 0.11511
	accuracy_policy_0: 0.94793
	loss_value_0: 0.24206
	loss_policy_1: 0.02315
	accuracy_policy_1: 0.9459
	loss_value_1: 0.04842
	loss_reward_1: 0.00426
	loss_policy_2: 0.02289
	accuracy_policy_2: 0.94812
	loss_value_2: 0.04903
	loss_reward_2: 0.0048
	loss_policy_3: 0.02287
	accuracy_policy_3: 0.94961
	loss_value_3: 0.04965
	loss_reward_3: 0.00558
	loss_policy_4: 0.02323
	accuracy_policy_4: 0.95074
	loss_value_4: 0.0504
	loss_reward_4: 0.00691
	loss_policy_5: 0.02289
	accuracy_policy_5: 0.95957
	loss_value_5: 0.05103
	loss_reward_5: 0.00743
	loss_policy: 0.23014
	loss_value: 0.49058
	loss_reward: 0.02898
[2025-05-11 17:17:22] nn step 39800, lr: 0.1.
	loss_policy_0: 0.12627
	accuracy_policy_0: 0.94969
	loss_value_0: 0.26613
	loss_policy_1: 0.02485
	accuracy_policy_1: 0.94957
	loss_value_1: 0.05302
	loss_reward_1: 0.00472
	loss_policy_2: 0.02504
	accuracy_policy_2: 0.94898
	loss_value_2: 0.05362
	loss_reward_2: 0.00558
	loss_policy_3: 0.02518
	accuracy_policy_3: 0.94836
	loss_value_3: 0.05406
	loss_reward_3: 0.00592
	loss_policy_4: 0.02504
	accuracy_policy_4: 0.95141
	loss_value_4: 0.05487
	loss_reward_4: 0.00726
	loss_policy_5: 0.02468
	accuracy_policy_5: 0.96117
	loss_value_5: 0.0561
	loss_reward_5: 0.00842
	loss_policy: 0.25107
	loss_value: 0.53779
	loss_reward: 0.0319
Optimization_Done 39800
[2025-05-11 17:18:58] [command] train weight_iter_39800.pkl 181 200
[2025-05-11 17:19:08] nn step 39850, lr: 0.1.
	loss_policy_0: 0.10994
	accuracy_policy_0: 0.95082
	loss_value_0: 0.23664
	loss_policy_1: 0.02204
	accuracy_policy_1: 0.94754
	loss_value_1: 0.04701
	loss_reward_1: 0.0042
	loss_policy_2: 0.02249
	accuracy_policy_2: 0.94918
	loss_value_2: 0.04787
	loss_reward_2: 0.00472
	loss_policy_3: 0.02205
	accuracy_policy_3: 0.9509
	loss_value_3: 0.04852
	loss_reward_3: 0.00541
	loss_policy_4: 0.02197
	accuracy_policy_4: 0.95395
	loss_value_4: 0.04902
	loss_reward_4: 0.00635
	loss_policy_5: 0.02193
	accuracy_policy_5: 0.95996
	loss_value_5: 0.05025
	loss_reward_5: 0.00735
	loss_policy: 0.22042
	loss_value: 0.47931
	loss_reward: 0.02803
[2025-05-11 17:19:16] nn step 39900, lr: 0.1.
	loss_policy_0: 0.11142
	accuracy_policy_0: 0.95125
	loss_value_0: 0.23651
	loss_policy_1: 0.02261
	accuracy_policy_1: 0.94645
	loss_value_1: 0.04713
	loss_reward_1: 0.00431
	loss_policy_2: 0.02259
	accuracy_policy_2: 0.94797
	loss_value_2: 0.04801
	loss_reward_2: 0.00451
	loss_policy_3: 0.02248
	accuracy_policy_3: 0.94777
	loss_value_3: 0.04839
	loss_reward_3: 0.00526
	loss_policy_4: 0.02241
	accuracy_policy_4: 0.95117
	loss_value_4: 0.04907
	loss_reward_4: 0.00655
	loss_policy_5: 0.02227
	accuracy_policy_5: 0.9618
	loss_value_5: 0.05002
	loss_reward_5: 0.00745
	loss_policy: 0.22378
	loss_value: 0.47913
	loss_reward: 0.02809
[2025-05-11 17:19:23] nn step 39950, lr: 0.1.
	loss_policy_0: 0.12096
	accuracy_policy_0: 0.9491
	loss_value_0: 0.24891
	loss_policy_1: 0.02351
	accuracy_policy_1: 0.94703
	loss_value_1: 0.04972
	loss_reward_1: 0.00455
	loss_policy_2: 0.02392
	accuracy_policy_2: 0.94797
	loss_value_2: 0.05022
	loss_reward_2: 0.00473
	loss_policy_3: 0.02357
	accuracy_policy_3: 0.95113
	loss_value_3: 0.0509
	loss_reward_3: 0.0057
	loss_policy_4: 0.02335
	accuracy_policy_4: 0.95402
	loss_value_4: 0.05181
	loss_reward_4: 0.00718
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.96078
	loss_value_5: 0.05263
	loss_reward_5: 0.00789
	loss_policy: 0.23888
	loss_value: 0.5042
	loss_reward: 0.03006
[2025-05-11 17:19:32] nn step 40000, lr: 0.1.
	loss_policy_0: 0.12158
	accuracy_policy_0: 0.95016
	loss_value_0: 0.24909
	loss_policy_1: 0.02406
	accuracy_policy_1: 0.94762
	loss_value_1: 0.04975
	loss_reward_1: 0.00457
	loss_policy_2: 0.02415
	accuracy_policy_2: 0.95039
	loss_value_2: 0.05038
	loss_reward_2: 0.00493
	loss_policy_3: 0.02405
	accuracy_policy_3: 0.95098
	loss_value_3: 0.05113
	loss_reward_3: 0.00574
	loss_policy_4: 0.02413
	accuracy_policy_4: 0.95102
	loss_value_4: 0.05213
	loss_reward_4: 0.00702
	loss_policy_5: 0.02408
	accuracy_policy_5: 0.95996
	loss_value_5: 0.05336
	loss_reward_5: 0.00777
	loss_policy: 0.24204
	loss_value: 0.50583
	loss_reward: 0.03002
Optimization_Done 40000
[2025-05-11 17:21:10] [command] train weight_iter_40000.pkl 182 201
[2025-05-11 17:21:17] nn step 40050, lr: 0.1.
	loss_policy_0: 0.12031
	accuracy_policy_0: 0.95184
	loss_value_0: 0.25678
	loss_policy_1: 0.02411
	accuracy_policy_1: 0.94762
	loss_value_1: 0.0509
	loss_reward_1: 0.00452
	loss_policy_2: 0.02419
	accuracy_policy_2: 0.95098
	loss_value_2: 0.05145
	loss_reward_2: 0.00515
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.95281
	loss_value_3: 0.05224
	loss_reward_3: 0.00565
	loss_policy_4: 0.02391
	accuracy_policy_4: 0.95652
	loss_value_4: 0.05289
	loss_reward_4: 0.00704
	loss_policy_5: 0.02395
	accuracy_policy_5: 0.96184
	loss_value_5: 0.05411
	loss_reward_5: 0.00833
	loss_policy: 0.24034
	loss_value: 0.51838
	loss_reward: 0.03069
[2025-05-11 17:21:26] nn step 40100, lr: 0.1.
	loss_policy_0: 0.11752
	accuracy_policy_0: 0.94867
	loss_value_0: 0.24406
	loss_policy_1: 0.02355
	accuracy_policy_1: 0.94625
	loss_value_1: 0.04856
	loss_reward_1: 0.00433
	loss_policy_2: 0.02323
	accuracy_policy_2: 0.94723
	loss_value_2: 0.04918
	loss_reward_2: 0.00476
	loss_policy_3: 0.02319
	accuracy_policy_3: 0.95133
	loss_value_3: 0.04996
	loss_reward_3: 0.00553
	loss_policy_4: 0.02343
	accuracy_policy_4: 0.95473
	loss_value_4: 0.0507
	loss_reward_4: 0.00679
	loss_policy_5: 0.02324
	accuracy_policy_5: 0.95871
	loss_value_5: 0.0517
	loss_reward_5: 0.00721
	loss_policy: 0.23415
	loss_value: 0.49417
	loss_reward: 0.02862
[2025-05-11 17:21:34] nn step 40150, lr: 0.1.
	loss_policy_0: 0.11791
	accuracy_policy_0: 0.94949
	loss_value_0: 0.24404
	loss_policy_1: 0.02298
	accuracy_policy_1: 0.94668
	loss_value_1: 0.04843
	loss_reward_1: 0.00456
	loss_policy_2: 0.02372
	accuracy_policy_2: 0.94879
	loss_value_2: 0.04899
	loss_reward_2: 0.00479
	loss_policy_3: 0.02373
	accuracy_policy_3: 0.95059
	loss_value_3: 0.04956
	loss_reward_3: 0.00562
	loss_policy_4: 0.0235
	accuracy_policy_4: 0.95344
	loss_value_4: 0.0503
	loss_reward_4: 0.00679
	loss_policy_5: 0.02328
	accuracy_policy_5: 0.96199
	loss_value_5: 0.05124
	loss_reward_5: 0.00774
	loss_policy: 0.23512
	loss_value: 0.49257
	loss_reward: 0.0295
[2025-05-11 17:21:41] nn step 40200, lr: 0.1.
	loss_policy_0: 0.12112
	accuracy_policy_0: 0.94824
	loss_value_0: 0.25476
	loss_policy_1: 0.02427
	accuracy_policy_1: 0.94449
	loss_value_1: 0.05071
	loss_reward_1: 0.00447
	loss_policy_2: 0.02459
	accuracy_policy_2: 0.94781
	loss_value_2: 0.05122
	loss_reward_2: 0.00515
	loss_policy_3: 0.02413
	accuracy_policy_3: 0.95234
	loss_value_3: 0.05202
	loss_reward_3: 0.00579
	loss_policy_4: 0.02439
	accuracy_policy_4: 0.95219
	loss_value_4: 0.05287
	loss_reward_4: 0.00678
	loss_policy_5: 0.0246
	accuracy_policy_5: 0.96008
	loss_value_5: 0.05385
	loss_reward_5: 0.00798
	loss_policy: 0.2431
	loss_value: 0.51543
	loss_reward: 0.03016
Optimization_Done 40200
[2025-05-11 17:23:19] [command] train weight_iter_40200.pkl 183 202
[2025-05-11 17:23:28] nn step 40250, lr: 0.1.
	loss_policy_0: 0.11721
	accuracy_policy_0: 0.95242
	loss_value_0: 0.25414
	loss_policy_1: 0.02322
	accuracy_policy_1: 0.94902
	loss_value_1: 0.05032
	loss_reward_1: 0.00443
	loss_policy_2: 0.02325
	accuracy_policy_2: 0.95207
	loss_value_2: 0.05062
	loss_reward_2: 0.00483
	loss_policy_3: 0.02337
	accuracy_policy_3: 0.95047
	loss_value_3: 0.0513
	loss_reward_3: 0.00558
	loss_policy_4: 0.02314
	accuracy_policy_4: 0.95402
	loss_value_4: 0.05221
	loss_reward_4: 0.00657
	loss_policy_5: 0.02315
	accuracy_policy_5: 0.96125
	loss_value_5: 0.05319
	loss_reward_5: 0.00783
	loss_policy: 0.23333
	loss_value: 0.51178
	loss_reward: 0.02925
[2025-05-11 17:23:35] nn step 40300, lr: 0.1.
	loss_policy_0: 0.11333
	accuracy_policy_0: 0.95312
	loss_value_0: 0.23764
	loss_policy_1: 0.02275
	accuracy_policy_1: 0.94773
	loss_value_1: 0.04716
	loss_reward_1: 0.00429
	loss_policy_2: 0.02276
	accuracy_policy_2: 0.95035
	loss_value_2: 0.04763
	loss_reward_2: 0.00458
	loss_policy_3: 0.02251
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04803
	loss_reward_3: 0.00519
	loss_policy_4: 0.02281
	accuracy_policy_4: 0.95699
	loss_value_4: 0.04883
	loss_reward_4: 0.00636
	loss_policy_5: 0.02228
	accuracy_policy_5: 0.96238
	loss_value_5: 0.04958
	loss_reward_5: 0.00765
	loss_policy: 0.22643
	loss_value: 0.47888
	loss_reward: 0.02807
[2025-05-11 17:23:43] nn step 40350, lr: 0.1.
	loss_policy_0: 0.12029
	accuracy_policy_0: 0.95172
	loss_value_0: 0.24762
	loss_policy_1: 0.02397
	accuracy_policy_1: 0.94945
	loss_value_1: 0.04947
	loss_reward_1: 0.0045
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.94883
	loss_value_2: 0.05035
	loss_reward_2: 0.00511
	loss_policy_3: 0.02397
	accuracy_policy_3: 0.94844
	loss_value_3: 0.05086
	loss_reward_3: 0.00578
	loss_policy_4: 0.02428
	accuracy_policy_4: 0.95402
	loss_value_4: 0.0519
	loss_reward_4: 0.00709
	loss_policy_5: 0.02399
	accuracy_policy_5: 0.95707
	loss_value_5: 0.05275
	loss_reward_5: 0.00788
	loss_policy: 0.2405
	loss_value: 0.50294
	loss_reward: 0.03037
[2025-05-11 17:23:52] nn step 40400, lr: 0.1.
	loss_policy_0: 0.11234
	accuracy_policy_0: 0.9516
	loss_value_0: 0.23206
	loss_policy_1: 0.02247
	accuracy_policy_1: 0.94832
	loss_value_1: 0.04618
	loss_reward_1: 0.00406
	loss_policy_2: 0.02268
	accuracy_policy_2: 0.95078
	loss_value_2: 0.04656
	loss_reward_2: 0.0045
	loss_policy_3: 0.0226
	accuracy_policy_3: 0.95148
	loss_value_3: 0.04732
	loss_reward_3: 0.00549
	loss_policy_4: 0.02253
	accuracy_policy_4: 0.95551
	loss_value_4: 0.04815
	loss_reward_4: 0.00662
	loss_policy_5: 0.02227
	accuracy_policy_5: 0.95988
	loss_value_5: 0.04923
	loss_reward_5: 0.00712
	loss_policy: 0.22488
	loss_value: 0.4695
	loss_reward: 0.02778
Optimization_Done 40400
[2025-05-11 17:25:28] [command] train weight_iter_40400.pkl 184 203
[2025-05-11 17:25:38] nn step 40450, lr: 0.1.
	loss_policy_0: 0.11957
	accuracy_policy_0: 0.95359
	loss_value_0: 0.24365
	loss_policy_1: 0.02352
	accuracy_policy_1: 0.94773
	loss_value_1: 0.04854
	loss_reward_1: 0.00427
	loss_policy_2: 0.02369
	accuracy_policy_2: 0.95035
	loss_value_2: 0.04924
	loss_reward_2: 0.00489
	loss_policy_3: 0.02363
	accuracy_policy_3: 0.94918
	loss_value_3: 0.04979
	loss_reward_3: 0.00556
	loss_policy_4: 0.02383
	accuracy_policy_4: 0.95418
	loss_value_4: 0.05059
	loss_reward_4: 0.00634
	loss_policy_5: 0.0235
	accuracy_policy_5: 0.95762
	loss_value_5: 0.05131
	loss_reward_5: 0.00743
	loss_policy: 0.23774
	loss_value: 0.49312
	loss_reward: 0.02848
[2025-05-11 17:25:47] nn step 40500, lr: 0.1.
	loss_policy_0: 0.12046
	accuracy_policy_0: 0.95004
	loss_value_0: 0.24751
	loss_policy_1: 0.0242
	accuracy_policy_1: 0.94816
	loss_value_1: 0.04917
	loss_reward_1: 0.00433
	loss_policy_2: 0.02429
	accuracy_policy_2: 0.94707
	loss_value_2: 0.04959
	loss_reward_2: 0.00499
	loss_policy_3: 0.0239
	accuracy_policy_3: 0.94996
	loss_value_3: 0.05011
	loss_reward_3: 0.00551
	loss_policy_4: 0.024
	accuracy_policy_4: 0.95352
	loss_value_4: 0.05097
	loss_reward_4: 0.00686
	loss_policy_5: 0.02372
	accuracy_policy_5: 0.95887
	loss_value_5: 0.05169
	loss_reward_5: 0.00779
	loss_policy: 0.24057
	loss_value: 0.49904
	loss_reward: 0.02949
[2025-05-11 17:25:54] nn step 40550, lr: 0.1.
	loss_policy_0: 0.12475
	accuracy_policy_0: 0.94977
	loss_value_0: 0.25158
	loss_policy_1: 0.0248
	accuracy_policy_1: 0.94445
	loss_value_1: 0.0503
	loss_reward_1: 0.00494
	loss_policy_2: 0.02495
	accuracy_policy_2: 0.94555
	loss_value_2: 0.05103
	loss_reward_2: 0.00532
	loss_policy_3: 0.02484
	accuracy_policy_3: 0.94988
	loss_value_3: 0.05124
	loss_reward_3: 0.00568
	loss_policy_4: 0.02493
	accuracy_policy_4: 0.95309
	loss_value_4: 0.05211
	loss_reward_4: 0.00736
	loss_policy_5: 0.02479
	accuracy_policy_5: 0.95824
	loss_value_5: 0.05329
	loss_reward_5: 0.00875
	loss_policy: 0.24906
	loss_value: 0.50955
	loss_reward: 0.03205
[2025-05-11 17:26:02] nn step 40600, lr: 0.1.
	loss_policy_0: 0.11707
	accuracy_policy_0: 0.95172
	loss_value_0: 0.23163
	loss_policy_1: 0.02324
	accuracy_policy_1: 0.94855
	loss_value_1: 0.04636
	loss_reward_1: 0.00433
	loss_policy_2: 0.0236
	accuracy_policy_2: 0.94855
	loss_value_2: 0.04706
	loss_reward_2: 0.00471
	loss_policy_3: 0.02326
	accuracy_policy_3: 0.95117
	loss_value_3: 0.04762
	loss_reward_3: 0.00542
	loss_policy_4: 0.0235
	accuracy_policy_4: 0.95391
	loss_value_4: 0.04843
	loss_reward_4: 0.00661
	loss_policy_5: 0.02323
	accuracy_policy_5: 0.95902
	loss_value_5: 0.04925
	loss_reward_5: 0.00758
	loss_policy: 0.2339
	loss_value: 0.47036
	loss_reward: 0.02865
Optimization_Done 40600
[2025-05-11 17:27:38] [command] train weight_iter_40600.pkl 185 204
[2025-05-11 17:27:48] nn step 40650, lr: 0.1.
	loss_policy_0: 0.12249
	accuracy_policy_0: 0.95148
	loss_value_0: 0.25573
	loss_policy_1: 0.02442
	accuracy_policy_1: 0.94844
	loss_value_1: 0.05092
	loss_reward_1: 0.00461
	loss_policy_2: 0.02412
	accuracy_policy_2: 0.95148
	loss_value_2: 0.05158
	loss_reward_2: 0.00518
	loss_policy_3: 0.0241
	accuracy_policy_3: 0.95285
	loss_value_3: 0.05234
	loss_reward_3: 0.00594
	loss_policy_4: 0.02417
	accuracy_policy_4: 0.95457
	loss_value_4: 0.05287
	loss_reward_4: 0.0071
	loss_policy_5: 0.02405
	accuracy_policy_5: 0.96012
	loss_value_5: 0.05381
	loss_reward_5: 0.00842
	loss_policy: 0.24334
	loss_value: 0.51725
	loss_reward: 0.03125
[2025-05-11 17:27:55] nn step 40700, lr: 0.1.
	loss_policy_0: 0.1125
	accuracy_policy_0: 0.95277
	loss_value_0: 0.23238
	loss_policy_1: 0.02258
	accuracy_policy_1: 0.94625
	loss_value_1: 0.04619
	loss_reward_1: 0.0041
	loss_policy_2: 0.02219
	accuracy_policy_2: 0.94992
	loss_value_2: 0.04694
	loss_reward_2: 0.00476
	loss_policy_3: 0.02221
	accuracy_policy_3: 0.94906
	loss_value_3: 0.04752
	loss_reward_3: 0.00551
	loss_policy_4: 0.02255
	accuracy_policy_4: 0.95488
	loss_value_4: 0.04845
	loss_reward_4: 0.00637
	loss_policy_5: 0.02264
	accuracy_policy_5: 0.95887
	loss_value_5: 0.0493
	loss_reward_5: 0.00734
	loss_policy: 0.22468
	loss_value: 0.47079
	loss_reward: 0.02809
[2025-05-11 17:28:03] nn step 40750, lr: 0.1.
	loss_policy_0: 0.11663
	accuracy_policy_0: 0.95426
	loss_value_0: 0.2376
	loss_policy_1: 0.02302
	accuracy_policy_1: 0.94938
	loss_value_1: 0.0479
	loss_reward_1: 0.00435
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95117
	loss_value_2: 0.04817
	loss_reward_2: 0.00486
	loss_policy_3: 0.02319
	accuracy_policy_3: 0.9523
	loss_value_3: 0.04895
	loss_reward_3: 0.00552
	loss_policy_4: 0.02328
	accuracy_policy_4: 0.95277
	loss_value_4: 0.0495
	loss_reward_4: 0.00666
	loss_policy_5: 0.02313
	accuracy_policy_5: 0.96012
	loss_value_5: 0.05065
	loss_reward_5: 0.00757
	loss_policy: 0.23233
	loss_value: 0.48277
	loss_reward: 0.02896
[2025-05-11 17:28:12] nn step 40800, lr: 0.1.
	loss_policy_0: 0.12337
	accuracy_policy_0: 0.94816
	loss_value_0: 0.25216
	loss_policy_1: 0.02448
	accuracy_policy_1: 0.94867
	loss_value_1: 0.05043
	loss_reward_1: 0.00441
	loss_policy_2: 0.02464
	accuracy_policy_2: 0.94742
	loss_value_2: 0.05132
	loss_reward_2: 0.00521
	loss_policy_3: 0.0246
	accuracy_policy_3: 0.95121
	loss_value_3: 0.05199
	loss_reward_3: 0.00589
	loss_policy_4: 0.02438
	accuracy_policy_4: 0.95406
	loss_value_4: 0.05269
	loss_reward_4: 0.00681
	loss_policy_5: 0.02416
	accuracy_policy_5: 0.95953
	loss_value_5: 0.05356
	loss_reward_5: 0.00825
	loss_policy: 0.24563
	loss_value: 0.51215
	loss_reward: 0.03057
Optimization_Done 40800
[2025-05-11 17:29:48] [command] train weight_iter_40800.pkl 186 205
[2025-05-11 17:29:57] nn step 40850, lr: 0.1.
	loss_policy_0: 0.11005
	accuracy_policy_0: 0.95383
	loss_value_0: 0.24903
	loss_policy_1: 0.02208
	accuracy_policy_1: 0.94816
	loss_value_1: 0.0494
	loss_reward_1: 0.00427
	loss_policy_2: 0.02184
	accuracy_policy_2: 0.95047
	loss_value_2: 0.04999
	loss_reward_2: 0.00439
	loss_policy_3: 0.02179
	accuracy_policy_3: 0.955
	loss_value_3: 0.05077
	loss_reward_3: 0.00525
	loss_policy_4: 0.02191
	accuracy_policy_4: 0.95602
	loss_value_4: 0.05135
	loss_reward_4: 0.00632
	loss_policy_5: 0.02172
	accuracy_policy_5: 0.96168
	loss_value_5: 0.05227
	loss_reward_5: 0.00738
	loss_policy: 0.21938
	loss_value: 0.50281
	loss_reward: 0.02761
[2025-05-11 17:30:06] nn step 40900, lr: 0.1.
	loss_policy_0: 0.12248
	accuracy_policy_0: 0.95293
	loss_value_0: 0.2663
	loss_policy_1: 0.02429
	accuracy_policy_1: 0.94953
	loss_value_1: 0.05288
	loss_reward_1: 0.0047
	loss_policy_2: 0.02437
	accuracy_policy_2: 0.95207
	loss_value_2: 0.0536
	loss_reward_2: 0.00507
	loss_policy_3: 0.02473
	accuracy_policy_3: 0.95168
	loss_value_3: 0.05475
	loss_reward_3: 0.00588
	loss_policy_4: 0.02423
	accuracy_policy_4: 0.95508
	loss_value_4: 0.05537
	loss_reward_4: 0.00702
	loss_policy_5: 0.02431
	accuracy_policy_5: 0.96078
	loss_value_5: 0.0565
	loss_reward_5: 0.00823
	loss_policy: 0.2444
	loss_value: 0.53939
	loss_reward: 0.0309
[2025-05-11 17:30:13] nn step 40950, lr: 0.1.
	loss_policy_0: 0.12083
	accuracy_policy_0: 0.95254
	loss_value_0: 0.25638
	loss_policy_1: 0.02443
	accuracy_policy_1: 0.94602
	loss_value_1: 0.05113
	loss_reward_1: 0.00463
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.94898
	loss_value_2: 0.05174
	loss_reward_2: 0.00504
	loss_policy_3: 0.02424
	accuracy_policy_3: 0.95285
	loss_value_3: 0.05224
	loss_reward_3: 0.00585
	loss_policy_4: 0.02419
	accuracy_policy_4: 0.95176
	loss_value_4: 0.05312
	loss_reward_4: 0.00704
	loss_policy_5: 0.02412
	accuracy_policy_5: 0.95895
	loss_value_5: 0.05417
	loss_reward_5: 0.00791
	loss_policy: 0.2418
	loss_value: 0.51878
	loss_reward: 0.03046
[2025-05-11 17:30:22] nn step 41000, lr: 0.1.
	loss_policy_0: 0.11607
	accuracy_policy_0: 0.95125
	loss_value_0: 0.24932
	loss_policy_1: 0.02335
	accuracy_policy_1: 0.94832
	loss_value_1: 0.04961
	loss_reward_1: 0.00454
	loss_policy_2: 0.02326
	accuracy_policy_2: 0.94996
	loss_value_2: 0.05057
	loss_reward_2: 0.00481
	loss_policy_3: 0.02342
	accuracy_policy_3: 0.95207
	loss_value_3: 0.05139
	loss_reward_3: 0.00545
	loss_policy_4: 0.02342
	accuracy_policy_4: 0.95359
	loss_value_4: 0.05179
	loss_reward_4: 0.00655
	loss_policy_5: 0.02318
	accuracy_policy_5: 0.96195
	loss_value_5: 0.05295
	loss_reward_5: 0.00758
	loss_policy: 0.23272
	loss_value: 0.50563
	loss_reward: 0.02893
Optimization_Done 41000
[2025-05-11 17:31:58] [command] train weight_iter_41000.pkl 187 206
[2025-05-11 17:32:06] nn step 41050, lr: 0.1.
	loss_policy_0: 0.11123
	accuracy_policy_0: 0.95609
	loss_value_0: 0.24568
	loss_policy_1: 0.02243
	accuracy_policy_1: 0.95348
	loss_value_1: 0.04873
	loss_reward_1: 0.0043
	loss_policy_2: 0.02233
	accuracy_policy_2: 0.95383
	loss_value_2: 0.04934
	loss_reward_2: 0.00474
	loss_policy_3: 0.02249
	accuracy_policy_3: 0.95496
	loss_value_3: 0.05039
	loss_reward_3: 0.00561
	loss_policy_4: 0.02227
	accuracy_policy_4: 0.95672
	loss_value_4: 0.05061
	loss_reward_4: 0.00665
	loss_policy_5: 0.02269
	accuracy_policy_5: 0.96121
	loss_value_5: 0.05151
	loss_reward_5: 0.00726
	loss_policy: 0.22345
	loss_value: 0.49626
	loss_reward: 0.02856
[2025-05-11 17:32:15] nn step 41100, lr: 0.1.
	loss_policy_0: 0.11077
	accuracy_policy_0: 0.9543
	loss_value_0: 0.2414
	loss_policy_1: 0.02245
	accuracy_policy_1: 0.9523
	loss_value_1: 0.04806
	loss_reward_1: 0.00431
	loss_policy_2: 0.02243
	accuracy_policy_2: 0.95211
	loss_value_2: 0.04857
	loss_reward_2: 0.00475
	loss_policy_3: 0.0225
	accuracy_policy_3: 0.95297
	loss_value_3: 0.04939
	loss_reward_3: 0.00526
	loss_policy_4: 0.02231
	accuracy_policy_4: 0.9577
	loss_value_4: 0.04996
	loss_reward_4: 0.00649
	loss_policy_5: 0.02227
	accuracy_policy_5: 0.96258
	loss_value_5: 0.05092
	loss_reward_5: 0.00756
	loss_policy: 0.22272
	loss_value: 0.48832
	loss_reward: 0.02837
[2025-05-11 17:32:23] nn step 41150, lr: 0.1.
	loss_policy_0: 0.11277
	accuracy_policy_0: 0.9541
	loss_value_0: 0.23859
	loss_policy_1: 0.0225
	accuracy_policy_1: 0.95207
	loss_value_1: 0.04731
	loss_reward_1: 0.00418
	loss_policy_2: 0.02261
	accuracy_policy_2: 0.95121
	loss_value_2: 0.04805
	loss_reward_2: 0.0046
	loss_policy_3: 0.02266
	accuracy_policy_3: 0.95266
	loss_value_3: 0.04903
	loss_reward_3: 0.00529
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.95602
	loss_value_4: 0.04957
	loss_reward_4: 0.00659
	loss_policy_5: 0.02224
	accuracy_policy_5: 0.9616
	loss_value_5: 0.05065
	loss_reward_5: 0.0075
	loss_policy: 0.2258
	loss_value: 0.4832
	loss_reward: 0.02816
[2025-05-11 17:32:32] nn step 41200, lr: 0.1.
	loss_policy_0: 0.10582
	accuracy_policy_0: 0.95473
	loss_value_0: 0.22168
	loss_policy_1: 0.02083
	accuracy_policy_1: 0.95074
	loss_value_1: 0.04433
	loss_reward_1: 0.0042
	loss_policy_2: 0.02098
	accuracy_policy_2: 0.9516
	loss_value_2: 0.04486
	loss_reward_2: 0.00443
	loss_policy_3: 0.02145
	accuracy_policy_3: 0.9502
	loss_value_3: 0.04521
	loss_reward_3: 0.00502
	loss_policy_4: 0.02087
	accuracy_policy_4: 0.95586
	loss_value_4: 0.0461
	loss_reward_4: 0.0065
	loss_policy_5: 0.02083
	accuracy_policy_5: 0.96234
	loss_value_5: 0.04696
	loss_reward_5: 0.00683
	loss_policy: 0.21077
	loss_value: 0.44913
	loss_reward: 0.02699
Optimization_Done 41200
[2025-05-11 17:34:11] [command] train weight_iter_41200.pkl 188 207
[2025-05-11 17:34:18] nn step 41250, lr: 0.1.
	loss_policy_0: 0.11385
	accuracy_policy_0: 0.95523
	loss_value_0: 0.24548
	loss_policy_1: 0.02234
	accuracy_policy_1: 0.95395
	loss_value_1: 0.04889
	loss_reward_1: 0.00436
	loss_policy_2: 0.02234
	accuracy_policy_2: 0.95402
	loss_value_2: 0.04933
	loss_reward_2: 0.00485
	loss_policy_3: 0.02258
	accuracy_policy_3: 0.95785
	loss_value_3: 0.0498
	loss_reward_3: 0.0055
	loss_policy_4: 0.02256
	accuracy_policy_4: 0.95977
	loss_value_4: 0.05026
	loss_reward_4: 0.00681
	loss_policy_5: 0.02239
	accuracy_policy_5: 0.9625
	loss_value_5: 0.05149
	loss_reward_5: 0.00779
	loss_policy: 0.22605
	loss_value: 0.49525
	loss_reward: 0.02931
[2025-05-11 17:34:26] nn step 41300, lr: 0.1.
	loss_policy_0: 0.11355
	accuracy_policy_0: 0.95602
	loss_value_0: 0.23952
	loss_policy_1: 0.0223
	accuracy_policy_1: 0.95523
	loss_value_1: 0.04777
	loss_reward_1: 0.00432
	loss_policy_2: 0.02257
	accuracy_policy_2: 0.95137
	loss_value_2: 0.04853
	loss_reward_2: 0.00489
	loss_policy_3: 0.02258
	accuracy_policy_3: 0.95398
	loss_value_3: 0.04887
	loss_reward_3: 0.00567
	loss_policy_4: 0.02246
	accuracy_policy_4: 0.95539
	loss_value_4: 0.04981
	loss_reward_4: 0.00657
	loss_policy_5: 0.02254
	accuracy_policy_5: 0.96191
	loss_value_5: 0.05079
	loss_reward_5: 0.00796
	loss_policy: 0.226
	loss_value: 0.48528
	loss_reward: 0.02942
[2025-05-11 17:34:35] nn step 41350, lr: 0.1.
	loss_policy_0: 0.10677
	accuracy_policy_0: 0.95441
	loss_value_0: 0.22374
	loss_policy_1: 0.02144
	accuracy_policy_1: 0.94922
	loss_value_1: 0.04455
	loss_reward_1: 0.0041
	loss_policy_2: 0.02142
	accuracy_policy_2: 0.95172
	loss_value_2: 0.04486
	loss_reward_2: 0.0046
	loss_policy_3: 0.02146
	accuracy_policy_3: 0.95219
	loss_value_3: 0.04522
	loss_reward_3: 0.00529
	loss_policy_4: 0.02142
	accuracy_policy_4: 0.95668
	loss_value_4: 0.04586
	loss_reward_4: 0.00615
	loss_policy_5: 0.02118
	accuracy_policy_5: 0.96379
	loss_value_5: 0.04697
	loss_reward_5: 0.00724
	loss_policy: 0.2137
	loss_value: 0.45119
	loss_reward: 0.02737
[2025-05-11 17:34:44] nn step 41400, lr: 0.1.
	loss_policy_0: 0.11948
	accuracy_policy_0: 0.95375
	loss_value_0: 0.24851
	loss_policy_1: 0.02382
	accuracy_policy_1: 0.95184
	loss_value_1: 0.04957
	loss_reward_1: 0.00465
	loss_policy_2: 0.02406
	accuracy_policy_2: 0.95199
	loss_value_2: 0.05022
	loss_reward_2: 0.00503
	loss_policy_3: 0.02398
	accuracy_policy_3: 0.95258
	loss_value_3: 0.05093
	loss_reward_3: 0.00569
	loss_policy_4: 0.02401
	accuracy_policy_4: 0.95586
	loss_value_4: 0.05154
	loss_reward_4: 0.00688
	loss_policy_5: 0.02384
	accuracy_policy_5: 0.96371
	loss_value_5: 0.05246
	loss_reward_5: 0.00791
	loss_policy: 0.2392
	loss_value: 0.50324
	loss_reward: 0.03017
Optimization_Done 41400
[2025-05-11 17:36:20] [command] train weight_iter_41400.pkl 189 208
[2025-05-11 17:36:30] nn step 41450, lr: 0.1.
	loss_policy_0: 0.10791
	accuracy_policy_0: 0.95777
	loss_value_0: 0.23085
	loss_policy_1: 0.02107
	accuracy_policy_1: 0.95566
	loss_value_1: 0.04594
	loss_reward_1: 0.00403
	loss_policy_2: 0.02106
	accuracy_policy_2: 0.95352
	loss_value_2: 0.04643
	loss_reward_2: 0.00456
	loss_policy_3: 0.0215
	accuracy_policy_3: 0.95535
	loss_value_3: 0.04679
	loss_reward_3: 0.00518
	loss_policy_4: 0.0213
	accuracy_policy_4: 0.95691
	loss_value_4: 0.04703
	loss_reward_4: 0.00614
	loss_policy_5: 0.02096
	accuracy_policy_5: 0.96559
	loss_value_5: 0.04786
	loss_reward_5: 0.0069
	loss_policy: 0.2138
	loss_value: 0.4649
	loss_reward: 0.02681
[2025-05-11 17:36:38] nn step 41500, lr: 0.1.
	loss_policy_0: 0.11445
	accuracy_policy_0: 0.95676
	loss_value_0: 0.23745
	loss_policy_1: 0.0227
	accuracy_policy_1: 0.9532
	loss_value_1: 0.04744
	loss_reward_1: 0.00456
	loss_policy_2: 0.02284
	accuracy_policy_2: 0.95191
	loss_value_2: 0.04779
	loss_reward_2: 0.00464
	loss_policy_3: 0.02281
	accuracy_policy_3: 0.95402
	loss_value_3: 0.04869
	loss_reward_3: 0.00554
	loss_policy_4: 0.02267
	accuracy_policy_4: 0.95816
	loss_value_4: 0.04914
	loss_reward_4: 0.00673
	loss_policy_5: 0.02264
	accuracy_policy_5: 0.96387
	loss_value_5: 0.04999
	loss_reward_5: 0.0076
	loss_policy: 0.22812
	loss_value: 0.4805
	loss_reward: 0.02907
[2025-05-11 17:36:47] nn step 41550, lr: 0.1.
	loss_policy_0: 0.11855
	accuracy_policy_0: 0.95633
	loss_value_0: 0.24669
	loss_policy_1: 0.02338
	accuracy_policy_1: 0.95176
	loss_value_1: 0.04943
	loss_reward_1: 0.00473
	loss_policy_2: 0.02352
	accuracy_policy_2: 0.95371
	loss_value_2: 0.04994
	loss_reward_2: 0.00527
	loss_policy_3: 0.02374
	accuracy_policy_3: 0.95223
	loss_value_3: 0.05074
	loss_reward_3: 0.00586
	loss_policy_4: 0.02372
	accuracy_policy_4: 0.95512
	loss_value_4: 0.05143
	loss_reward_4: 0.00686
	loss_policy_5: 0.02364
	accuracy_policy_5: 0.9632
	loss_value_5: 0.05268
	loss_reward_5: 0.00803
	loss_policy: 0.23656
	loss_value: 0.50091
	loss_reward: 0.03075
[2025-05-11 17:36:54] nn step 41600, lr: 0.1.
	loss_policy_0: 0.11683
	accuracy_policy_0: 0.95496
	loss_value_0: 0.24111
	loss_policy_1: 0.02345
	accuracy_policy_1: 0.95137
	loss_value_1: 0.04799
	loss_reward_1: 0.00436
	loss_policy_2: 0.02306
	accuracy_policy_2: 0.94965
	loss_value_2: 0.04846
	loss_reward_2: 0.00507
	loss_policy_3: 0.02316
	accuracy_policy_3: 0.95344
	loss_value_3: 0.04901
	loss_reward_3: 0.00549
	loss_policy_4: 0.02316
	accuracy_policy_4: 0.9548
	loss_value_4: 0.04971
	loss_reward_4: 0.00664
	loss_policy_5: 0.02294
	accuracy_policy_5: 0.9625
	loss_value_5: 0.05071
	loss_reward_5: 0.00772
	loss_policy: 0.2326
	loss_value: 0.48699
	loss_reward: 0.02927
Optimization_Done 41600
[2025-05-11 17:38:30] [command] train weight_iter_41600.pkl 190 209
[2025-05-11 17:38:40] nn step 41650, lr: 0.1.
	loss_policy_0: 0.10876
	accuracy_policy_0: 0.95828
	loss_value_0: 0.22924
	loss_policy_1: 0.0214
	accuracy_policy_1: 0.95512
	loss_value_1: 0.04582
	loss_reward_1: 0.00424
	loss_policy_2: 0.0215
	accuracy_policy_2: 0.9566
	loss_value_2: 0.0462
	loss_reward_2: 0.00472
	loss_policy_3: 0.02157
	accuracy_policy_3: 0.95555
	loss_value_3: 0.04656
	loss_reward_3: 0.00527
	loss_policy_4: 0.02169
	accuracy_policy_4: 0.9577
	loss_value_4: 0.04714
	loss_reward_4: 0.0061
	loss_policy_5: 0.02139
	accuracy_policy_5: 0.96434
	loss_value_5: 0.0479
	loss_reward_5: 0.0071
	loss_policy: 0.21629
	loss_value: 0.46285
	loss_reward: 0.02743
[2025-05-11 17:38:47] nn step 41700, lr: 0.1.
	loss_policy_0: 0.11802
	accuracy_policy_0: 0.95723
	loss_value_0: 0.24645
	loss_policy_1: 0.02365
	accuracy_policy_1: 0.95109
	loss_value_1: 0.04905
	loss_reward_1: 0.00452
	loss_policy_2: 0.02355
	accuracy_policy_2: 0.95406
	loss_value_2: 0.04968
	loss_reward_2: 0.00498
	loss_policy_3: 0.02369
	accuracy_policy_3: 0.95461
	loss_value_3: 0.05007
	loss_reward_3: 0.00557
	loss_policy_4: 0.0235
	accuracy_policy_4: 0.95637
	loss_value_4: 0.05053
	loss_reward_4: 0.00709
	loss_policy_5: 0.02381
	accuracy_policy_5: 0.96109
	loss_value_5: 0.05143
	loss_reward_5: 0.00788
	loss_policy: 0.23622
	loss_value: 0.4972
	loss_reward: 0.03003
[2025-05-11 17:38:56] nn step 41750, lr: 0.1.
	loss_policy_0: 0.11905
	accuracy_policy_0: 0.95719
	loss_value_0: 0.24114
	loss_policy_1: 0.0236
	accuracy_policy_1: 0.95344
	loss_value_1: 0.04783
	loss_reward_1: 0.0046
	loss_policy_2: 0.02352
	accuracy_policy_2: 0.9534
	loss_value_2: 0.04848
	loss_reward_2: 0.00495
	loss_policy_3: 0.02349
	accuracy_policy_3: 0.9532
	loss_value_3: 0.04913
	loss_reward_3: 0.00578
	loss_policy_4: 0.02404
	accuracy_policy_4: 0.95586
	loss_value_4: 0.04973
	loss_reward_4: 0.00702
	loss_policy_5: 0.02353
	accuracy_policy_5: 0.96445
	loss_value_5: 0.05075
	loss_reward_5: 0.00772
	loss_policy: 0.23723
	loss_value: 0.48706
	loss_reward: 0.03007
[2025-05-11 17:39:05] nn step 41800, lr: 0.1.
	loss_policy_0: 0.12041
	accuracy_policy_0: 0.95738
	loss_value_0: 0.24462
	loss_policy_1: 0.02383
	accuracy_policy_1: 0.95316
	loss_value_1: 0.04892
	loss_reward_1: 0.00464
	loss_policy_2: 0.02386
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04981
	loss_reward_2: 0.00545
	loss_policy_3: 0.02425
	accuracy_policy_3: 0.9518
	loss_value_3: 0.05022
	loss_reward_3: 0.0058
	loss_policy_4: 0.0239
	accuracy_policy_4: 0.95582
	loss_value_4: 0.05098
	loss_reward_4: 0.00705
	loss_policy_5: 0.02381
	accuracy_policy_5: 0.96359
	loss_value_5: 0.05169
	loss_reward_5: 0.00831
	loss_policy: 0.24005
	loss_value: 0.49623
	loss_reward: 0.03126
Optimization_Done 41800
[2025-05-11 17:40:39] [command] train weight_iter_41800.pkl 191 210
[2025-05-11 17:40:49] nn step 41850, lr: 0.1.
	loss_policy_0: 0.11552
	accuracy_policy_0: 0.95777
	loss_value_0: 0.24126
	loss_policy_1: 0.02249
	accuracy_policy_1: 0.95375
	loss_value_1: 0.0478
	loss_reward_1: 0.00436
	loss_policy_2: 0.02284
	accuracy_policy_2: 0.95422
	loss_value_2: 0.04843
	loss_reward_2: 0.00492
	loss_policy_3: 0.02309
	accuracy_policy_3: 0.95438
	loss_value_3: 0.04882
	loss_reward_3: 0.00547
	loss_policy_4: 0.02314
	accuracy_policy_4: 0.95473
	loss_value_4: 0.04949
	loss_reward_4: 0.0067
	loss_policy_5: 0.02293
	accuracy_policy_5: 0.96426
	loss_value_5: 0.04997
	loss_reward_5: 0.00774
	loss_policy: 0.23001
	loss_value: 0.48577
	loss_reward: 0.02919
[2025-05-11 17:40:58] nn step 41900, lr: 0.1.
	loss_policy_0: 0.11304
	accuracy_policy_0: 0.95684
	loss_value_0: 0.23175
	loss_policy_1: 0.02248
	accuracy_policy_1: 0.95406
	loss_value_1: 0.04627
	loss_reward_1: 0.00438
	loss_policy_2: 0.02265
	accuracy_policy_2: 0.95273
	loss_value_2: 0.0464
	loss_reward_2: 0.00472
	loss_policy_3: 0.02249
	accuracy_policy_3: 0.95609
	loss_value_3: 0.04715
	loss_reward_3: 0.00536
	loss_policy_4: 0.02251
	accuracy_policy_4: 0.95734
	loss_value_4: 0.04786
	loss_reward_4: 0.00632
	loss_policy_5: 0.02211
	accuracy_policy_5: 0.96223
	loss_value_5: 0.04821
	loss_reward_5: 0.0073
	loss_policy: 0.22529
	loss_value: 0.46764
	loss_reward: 0.02807
[2025-05-11 17:41:05] nn step 41950, lr: 0.1.
	loss_policy_0: 0.11326
	accuracy_policy_0: 0.95688
	loss_value_0: 0.23055
	loss_policy_1: 0.02234
	accuracy_policy_1: 0.95445
	loss_value_1: 0.04608
	loss_reward_1: 0.00432
	loss_policy_2: 0.02234
	accuracy_policy_2: 0.95312
	loss_value_2: 0.04636
	loss_reward_2: 0.00483
	loss_policy_3: 0.02244
	accuracy_policy_3: 0.95465
	loss_value_3: 0.04723
	loss_reward_3: 0.00545
	loss_policy_4: 0.02224
	accuracy_policy_4: 0.95797
	loss_value_4: 0.04778
	loss_reward_4: 0.00633
	loss_policy_5: 0.02225
	accuracy_policy_5: 0.96395
	loss_value_5: 0.0485
	loss_reward_5: 0.00721
	loss_policy: 0.22487
	loss_value: 0.46651
	loss_reward: 0.02815
[2025-05-11 17:41:13] nn step 42000, lr: 0.1.
	loss_policy_0: 0.11522
	accuracy_policy_0: 0.95863
	loss_value_0: 0.23334
	loss_policy_1: 0.02281
	accuracy_policy_1: 0.95453
	loss_value_1: 0.04652
	loss_reward_1: 0.00452
	loss_policy_2: 0.02316
	accuracy_policy_2: 0.95254
	loss_value_2: 0.04667
	loss_reward_2: 0.00484
	loss_policy_3: 0.02297
	accuracy_policy_3: 0.95367
	loss_value_3: 0.04729
	loss_reward_3: 0.00571
	loss_policy_4: 0.02303
	accuracy_policy_4: 0.95668
	loss_value_4: 0.04784
	loss_reward_4: 0.00671
	loss_policy_5: 0.02285
	accuracy_policy_5: 0.96387
	loss_value_5: 0.04863
	loss_reward_5: 0.00762
	loss_policy: 0.23004
	loss_value: 0.47029
	loss_reward: 0.0294
Optimization_Done 42000
[2025-05-11 17:42:50] [command] train weight_iter_42000.pkl 192 211
[2025-05-11 17:42:58] nn step 42050, lr: 0.1.
	loss_policy_0: 0.1136
	accuracy_policy_0: 0.95926
	loss_value_0: 0.23732
	loss_policy_1: 0.02233
	accuracy_policy_1: 0.95605
	loss_value_1: 0.04708
	loss_reward_1: 0.00425
	loss_policy_2: 0.02248
	accuracy_policy_2: 0.9568
	loss_value_2: 0.04774
	loss_reward_2: 0.00493
	loss_policy_3: 0.02261
	accuracy_policy_3: 0.95602
	loss_value_3: 0.04816
	loss_reward_3: 0.00551
	loss_policy_4: 0.02281
	accuracy_policy_4: 0.96027
	loss_value_4: 0.04869
	loss_reward_4: 0.00622
	loss_policy_5: 0.02249
	accuracy_policy_5: 0.96332
	loss_value_5: 0.04963
	loss_reward_5: 0.0071
	loss_policy: 0.22632
	loss_value: 0.47862
	loss_reward: 0.028
[2025-05-11 17:43:07] nn step 42100, lr: 0.1.
	loss_policy_0: 0.1112
	accuracy_policy_0: 0.96008
	loss_value_0: 0.22721
	loss_policy_1: 0.02224
	accuracy_policy_1: 0.95453
	loss_value_1: 0.04529
	loss_reward_1: 0.00415
	loss_policy_2: 0.0221
	accuracy_policy_2: 0.95301
	loss_value_2: 0.04564
	loss_reward_2: 0.00463
	loss_policy_3: 0.02228
	accuracy_policy_3: 0.95363
	loss_value_3: 0.046
	loss_reward_3: 0.00533
	loss_policy_4: 0.02203
	accuracy_policy_4: 0.95664
	loss_value_4: 0.04641
	loss_reward_4: 0.00625
	loss_policy_5: 0.02185
	accuracy_policy_5: 0.96562
	loss_value_5: 0.04732
	loss_reward_5: 0.0071
	loss_policy: 0.22168
	loss_value: 0.45788
	loss_reward: 0.02746
[2025-05-11 17:43:15] nn step 42150, lr: 0.1.
	loss_policy_0: 0.11244
	accuracy_policy_0: 0.95902
	loss_value_0: 0.22748
	loss_policy_1: 0.02216
	accuracy_policy_1: 0.95461
	loss_value_1: 0.0458
	loss_reward_1: 0.0044
	loss_policy_2: 0.02252
	accuracy_policy_2: 0.95355
	loss_value_2: 0.04638
	loss_reward_2: 0.00499
	loss_policy_3: 0.02254
	accuracy_policy_3: 0.95305
	loss_value_3: 0.04695
	loss_reward_3: 0.00569
	loss_policy_4: 0.0222
	accuracy_policy_4: 0.95734
	loss_value_4: 0.04752
	loss_reward_4: 0.0066
	loss_policy_5: 0.02189
	accuracy_policy_5: 0.96629
	loss_value_5: 0.04818
	loss_reward_5: 0.00732
	loss_policy: 0.22377
	loss_value: 0.46231
	loss_reward: 0.02899
[2025-05-11 17:43:24] nn step 42200, lr: 0.1.
	loss_policy_0: 0.11483
	accuracy_policy_0: 0.95926
	loss_value_0: 0.23176
	loss_policy_1: 0.02315
	accuracy_policy_1: 0.95301
	loss_value_1: 0.04613
	loss_reward_1: 0.00451
	loss_policy_2: 0.02292
	accuracy_policy_2: 0.95699
	loss_value_2: 0.04693
	loss_reward_2: 0.00491
	loss_policy_3: 0.02299
	accuracy_policy_3: 0.95566
	loss_value_3: 0.04734
	loss_reward_3: 0.00554
	loss_policy_4: 0.02289
	accuracy_policy_4: 0.95852
	loss_value_4: 0.04802
	loss_reward_4: 0.00659
	loss_policy_5: 0.02313
	accuracy_policy_5: 0.96383
	loss_value_5: 0.04919
	loss_reward_5: 0.00759
	loss_policy: 0.22991
	loss_value: 0.46936
	loss_reward: 0.02913
Optimization_Done 42200
[2025-05-11 17:45:02] [command] train weight_iter_42200.pkl 193 212
[2025-05-11 17:45:12] nn step 42250, lr: 0.1.
	loss_policy_0: 0.112
	accuracy_policy_0: 0.95848
	loss_value_0: 0.23025
	loss_policy_1: 0.02226
	accuracy_policy_1: 0.95562
	loss_value_1: 0.04571
	loss_reward_1: 0.00429
	loss_policy_2: 0.02213
	accuracy_policy_2: 0.95391
	loss_value_2: 0.04629
	loss_reward_2: 0.00479
	loss_policy_3: 0.02242
	accuracy_policy_3: 0.95535
	loss_value_3: 0.04687
	loss_reward_3: 0.00534
	loss_policy_4: 0.02254
	accuracy_policy_4: 0.9582
	loss_value_4: 0.04742
	loss_reward_4: 0.00622
	loss_policy_5: 0.02254
	accuracy_policy_5: 0.9659
	loss_value_5: 0.04814
	loss_reward_5: 0.00747
	loss_policy: 0.22391
	loss_value: 0.46469
	loss_reward: 0.02812
[2025-05-11 17:45:19] nn step 42300, lr: 0.1.
	loss_policy_0: 0.1125
	accuracy_policy_0: 0.95805
	loss_value_0: 0.23132
	loss_policy_1: 0.02245
	accuracy_policy_1: 0.9543
	loss_value_1: 0.04621
	loss_reward_1: 0.00435
	loss_policy_2: 0.02263
	accuracy_policy_2: 0.95324
	loss_value_2: 0.04661
	loss_reward_2: 0.00482
	loss_policy_3: 0.02281
	accuracy_policy_3: 0.95457
	loss_value_3: 0.04714
	loss_reward_3: 0.00566
	loss_policy_4: 0.02262
	accuracy_policy_4: 0.95684
	loss_value_4: 0.04765
	loss_reward_4: 0.00657
	loss_policy_5: 0.02256
	accuracy_policy_5: 0.96527
	loss_value_5: 0.04874
	loss_reward_5: 0.00766
	loss_policy: 0.22557
	loss_value: 0.46767
	loss_reward: 0.02907
[2025-05-11 17:45:28] nn step 42350, lr: 0.1.
	loss_policy_0: 0.11725
	accuracy_policy_0: 0.95953
	loss_value_0: 0.23416
	loss_policy_1: 0.02323
	accuracy_policy_1: 0.9543
	loss_value_1: 0.04676
	loss_reward_1: 0.00442
	loss_policy_2: 0.02296
	accuracy_policy_2: 0.95391
	loss_value_2: 0.04731
	loss_reward_2: 0.00473
	loss_policy_3: 0.02338
	accuracy_policy_3: 0.95578
	loss_value_3: 0.04799
	loss_reward_3: 0.00543
	loss_policy_4: 0.02312
	accuracy_policy_4: 0.95918
	loss_value_4: 0.04861
	loss_reward_4: 0.00643
	loss_policy_5: 0.02298
	accuracy_policy_5: 0.96352
	loss_value_5: 0.04946
	loss_reward_5: 0.00738
	loss_policy: 0.23292
	loss_value: 0.47429
	loss_reward: 0.02839
[2025-05-11 17:45:36] nn step 42400, lr: 0.1.
	loss_policy_0: 0.12311
	accuracy_policy_0: 0.95961
	loss_value_0: 0.24757
	loss_policy_1: 0.02445
	accuracy_policy_1: 0.95504
	loss_value_1: 0.04916
	loss_reward_1: 0.00487
	loss_policy_2: 0.0246
	accuracy_policy_2: 0.95547
	loss_value_2: 0.05014
	loss_reward_2: 0.00526
	loss_policy_3: 0.02461
	accuracy_policy_3: 0.95383
	loss_value_3: 0.05057
	loss_reward_3: 0.00603
	loss_policy_4: 0.02447
	accuracy_policy_4: 0.95938
	loss_value_4: 0.05174
	loss_reward_4: 0.00707
	loss_policy_5: 0.02458
	accuracy_policy_5: 0.9652
	loss_value_5: 0.05255
	loss_reward_5: 0.00769
	loss_policy: 0.24582
	loss_value: 0.50175
	loss_reward: 0.03093
Optimization_Done 42400
[2025-05-11 17:47:13] [command] train weight_iter_42400.pkl 194 213
[2025-05-11 17:47:21] nn step 42450, lr: 0.1.
	loss_policy_0: 0.11706
	accuracy_policy_0: 0.9577
	loss_value_0: 0.23676
	loss_policy_1: 0.02293
	accuracy_policy_1: 0.95648
	loss_value_1: 0.04712
	loss_reward_1: 0.00438
	loss_policy_2: 0.02341
	accuracy_policy_2: 0.9541
	loss_value_2: 0.04813
	loss_reward_2: 0.00479
	loss_policy_3: 0.02345
	accuracy_policy_3: 0.95535
	loss_value_3: 0.04867
	loss_reward_3: 0.0056
	loss_policy_4: 0.02316
	accuracy_policy_4: 0.95992
	loss_value_4: 0.04919
	loss_reward_4: 0.00644
	loss_policy_5: 0.02355
	accuracy_policy_5: 0.96309
	loss_value_5: 0.04961
	loss_reward_5: 0.00767
	loss_policy: 0.23355
	loss_value: 0.47947
	loss_reward: 0.02888
[2025-05-11 17:47:29] nn step 42500, lr: 0.1.
	loss_policy_0: 0.11497
	accuracy_policy_0: 0.95754
	loss_value_0: 0.22842
	loss_policy_1: 0.02292
	accuracy_policy_1: 0.95359
	loss_value_1: 0.04528
	loss_reward_1: 0.00442
	loss_policy_2: 0.023
	accuracy_policy_2: 0.95543
	loss_value_2: 0.04603
	loss_reward_2: 0.0049
	loss_policy_3: 0.02308
	accuracy_policy_3: 0.95492
	loss_value_3: 0.0467
	loss_reward_3: 0.00559
	loss_policy_4: 0.02272
	accuracy_policy_4: 0.95633
	loss_value_4: 0.04716
	loss_reward_4: 0.00627
	loss_policy_5: 0.02265
	accuracy_policy_5: 0.96316
	loss_value_5: 0.04802
	loss_reward_5: 0.00744
	loss_policy: 0.22935
	loss_value: 0.46162
	loss_reward: 0.02861
[2025-05-11 17:47:38] nn step 42550, lr: 0.1.
	loss_policy_0: 0.11015
	accuracy_policy_0: 0.96238
	loss_value_0: 0.21468
	loss_policy_1: 0.0221
	accuracy_policy_1: 0.95566
	loss_value_1: 0.04308
	loss_reward_1: 0.00422
	loss_policy_2: 0.02212
	accuracy_policy_2: 0.95598
	loss_value_2: 0.04369
	loss_reward_2: 0.00469
	loss_policy_3: 0.02226
	accuracy_policy_3: 0.95457
	loss_value_3: 0.04415
	loss_reward_3: 0.0053
	loss_policy_4: 0.02207
	accuracy_policy_4: 0.95863
	loss_value_4: 0.04482
	loss_reward_4: 0.00607
	loss_policy_5: 0.0221
	accuracy_policy_5: 0.96527
	loss_value_5: 0.04572
	loss_reward_5: 0.00708
	loss_policy: 0.2208
	loss_value: 0.43615
	loss_reward: 0.02737
[2025-05-11 17:47:45] nn step 42600, lr: 0.1.
	loss_policy_0: 0.11278
	accuracy_policy_0: 0.95844
	loss_value_0: 0.21745
	loss_policy_1: 0.02203
	accuracy_policy_1: 0.95535
	loss_value_1: 0.04366
	loss_reward_1: 0.00431
	loss_policy_2: 0.02253
	accuracy_policy_2: 0.95363
	loss_value_2: 0.04419
	loss_reward_2: 0.00474
	loss_policy_3: 0.02222
	accuracy_policy_3: 0.95449
	loss_value_3: 0.04484
	loss_reward_3: 0.00538
	loss_policy_4: 0.02192
	accuracy_policy_4: 0.95727
	loss_value_4: 0.04574
	loss_reward_4: 0.00667
	loss_policy_5: 0.02233
	accuracy_policy_5: 0.96328
	loss_value_5: 0.04645
	loss_reward_5: 0.00736
	loss_policy: 0.22381
	loss_value: 0.44233
	loss_reward: 0.02846
Optimization_Done 42600
[2025-05-11 17:49:23] [command] train weight_iter_42600.pkl 195 214
[2025-05-11 17:49:33] nn step 42650, lr: 0.1.
	loss_policy_0: 0.11399
	accuracy_policy_0: 0.95977
	loss_value_0: 0.22526
	loss_policy_1: 0.02253
	accuracy_policy_1: 0.95367
	loss_value_1: 0.04472
	loss_reward_1: 0.00449
	loss_policy_2: 0.02286
	accuracy_policy_2: 0.95555
	loss_value_2: 0.04553
	loss_reward_2: 0.00486
	loss_policy_3: 0.02281
	accuracy_policy_3: 0.9568
	loss_value_3: 0.04594
	loss_reward_3: 0.00554
	loss_policy_4: 0.02258
	accuracy_policy_4: 0.95898
	loss_value_4: 0.04673
	loss_reward_4: 0.00649
	loss_policy_5: 0.02264
	accuracy_policy_5: 0.9625
	loss_value_5: 0.04746
	loss_reward_5: 0.00731
	loss_policy: 0.22741
	loss_value: 0.45564
	loss_reward: 0.02869
[2025-05-11 17:49:40] nn step 42700, lr: 0.1.
	loss_policy_0: 0.11726
	accuracy_policy_0: 0.95965
	loss_value_0: 0.23047
	loss_policy_1: 0.02322
	accuracy_policy_1: 0.95684
	loss_value_1: 0.04609
	loss_reward_1: 0.00439
	loss_policy_2: 0.02318
	accuracy_policy_2: 0.95633
	loss_value_2: 0.0466
	loss_reward_2: 0.00491
	loss_policy_3: 0.02324
	accuracy_policy_3: 0.95551
	loss_value_3: 0.04717
	loss_reward_3: 0.00568
	loss_policy_4: 0.02318
	accuracy_policy_4: 0.95555
	loss_value_4: 0.04809
	loss_reward_4: 0.00663
	loss_policy_5: 0.02328
	accuracy_policy_5: 0.96438
	loss_value_5: 0.04855
	loss_reward_5: 0.0076
	loss_policy: 0.23337
	loss_value: 0.46697
	loss_reward: 0.02919
[2025-05-11 17:49:48] nn step 42750, lr: 0.1.
	loss_policy_0: 0.12649
	accuracy_policy_0: 0.95738
	loss_value_0: 0.24278
	loss_policy_1: 0.02504
	accuracy_policy_1: 0.95383
	loss_value_1: 0.04859
	loss_reward_1: 0.00482
	loss_policy_2: 0.02462
	accuracy_policy_2: 0.95336
	loss_value_2: 0.04913
	loss_reward_2: 0.00528
	loss_policy_3: 0.02482
	accuracy_policy_3: 0.95281
	loss_value_3: 0.04992
	loss_reward_3: 0.00593
	loss_policy_4: 0.02501
	accuracy_policy_4: 0.95609
	loss_value_4: 0.05078
	loss_reward_4: 0.00709
	loss_policy_5: 0.02465
	accuracy_policy_5: 0.96262
	loss_value_5: 0.05146
	loss_reward_5: 0.00812
	loss_policy: 0.25064
	loss_value: 0.49266
	loss_reward: 0.03124
[2025-05-11 17:49:57] nn step 42800, lr: 0.1.
	loss_policy_0: 0.12986
	accuracy_policy_0: 0.95895
	loss_value_0: 0.25499
	loss_policy_1: 0.02586
	accuracy_policy_1: 0.95363
	loss_value_1: 0.05119
	loss_reward_1: 0.0053
	loss_policy_2: 0.0258
	accuracy_policy_2: 0.95508
	loss_value_2: 0.0516
	loss_reward_2: 0.00567
	loss_policy_3: 0.02591
	accuracy_policy_3: 0.9543
	loss_value_3: 0.05212
	loss_reward_3: 0.00636
	loss_policy_4: 0.02621
	accuracy_policy_4: 0.9577
	loss_value_4: 0.05269
	loss_reward_4: 0.00789
	loss_policy_5: 0.02585
	accuracy_policy_5: 0.96051
	loss_value_5: 0.05342
	loss_reward_5: 0.00891
	loss_policy: 0.25948
	loss_value: 0.51601
	loss_reward: 0.03412
Optimization_Done 42800
[2025-05-11 17:51:34] [command] train weight_iter_42800.pkl 196 215
[2025-05-11 17:51:43] nn step 42850, lr: 0.1.
	loss_policy_0: 0.12195
	accuracy_policy_0: 0.95797
	loss_value_0: 0.24342
	loss_policy_1: 0.02422
	accuracy_policy_1: 0.955
	loss_value_1: 0.04826
	loss_reward_1: 0.00465
	loss_policy_2: 0.02385
	accuracy_policy_2: 0.95254
	loss_value_2: 0.04897
	loss_reward_2: 0.00525
	loss_policy_3: 0.02424
	accuracy_policy_3: 0.95332
	loss_value_3: 0.04915
	loss_reward_3: 0.00603
	loss_policy_4: 0.02426
	accuracy_policy_4: 0.95609
	loss_value_4: 0.04967
	loss_reward_4: 0.00691
	loss_policy_5: 0.02389
	accuracy_policy_5: 0.96117
	loss_value_5: 0.05076
	loss_reward_5: 0.00806
	loss_policy: 0.2424
	loss_value: 0.49023
	loss_reward: 0.03091
[2025-05-11 17:51:51] nn step 42900, lr: 0.1.
	loss_policy_0: 0.11912
	accuracy_policy_0: 0.95898
	loss_value_0: 0.23103
	loss_policy_1: 0.02322
	accuracy_policy_1: 0.95312
	loss_value_1: 0.04626
	loss_reward_1: 0.00442
	loss_policy_2: 0.02352
	accuracy_policy_2: 0.95449
	loss_value_2: 0.04681
	loss_reward_2: 0.00516
	loss_policy_3: 0.02379
	accuracy_policy_3: 0.95336
	loss_value_3: 0.04738
	loss_reward_3: 0.00563
	loss_policy_4: 0.02395
	accuracy_policy_4: 0.95801
	loss_value_4: 0.04808
	loss_reward_4: 0.00664
	loss_policy_5: 0.02343
	accuracy_policy_5: 0.96203
	loss_value_5: 0.04929
	loss_reward_5: 0.00754
	loss_policy: 0.23703
	loss_value: 0.46885
	loss_reward: 0.02939
[2025-05-11 17:51:59] nn step 42950, lr: 0.1.
	loss_policy_0: 0.1238
	accuracy_policy_0: 0.95758
	loss_value_0: 0.2404
	loss_policy_1: 0.02441
	accuracy_policy_1: 0.95453
	loss_value_1: 0.04812
	loss_reward_1: 0.00486
	loss_policy_2: 0.02457
	accuracy_policy_2: 0.95152
	loss_value_2: 0.04882
	loss_reward_2: 0.00503
	loss_policy_3: 0.02469
	accuracy_policy_3: 0.95371
	loss_value_3: 0.04958
	loss_reward_3: 0.00575
	loss_policy_4: 0.02444
	accuracy_policy_4: 0.9568
	loss_value_4: 0.05017
	loss_reward_4: 0.00722
	loss_policy_5: 0.02466
	accuracy_policy_5: 0.96137
	loss_value_5: 0.05127
	loss_reward_5: 0.00781
	loss_policy: 0.24657
	loss_value: 0.48835
	loss_reward: 0.03067
[2025-05-11 17:52:08] nn step 43000, lr: 0.1.
	loss_policy_0: 0.11732
	accuracy_policy_0: 0.95973
	loss_value_0: 0.22551
	loss_policy_1: 0.02354
	accuracy_policy_1: 0.95441
	loss_value_1: 0.04526
	loss_reward_1: 0.0045
	loss_policy_2: 0.02333
	accuracy_policy_2: 0.95254
	loss_value_2: 0.04595
	loss_reward_2: 0.00491
	loss_policy_3: 0.02314
	accuracy_policy_3: 0.95398
	loss_value_3: 0.04666
	loss_reward_3: 0.00572
	loss_policy_4: 0.02372
	accuracy_policy_4: 0.95656
	loss_value_4: 0.04735
	loss_reward_4: 0.00658
	loss_policy_5: 0.02334
	accuracy_policy_5: 0.96059
	loss_value_5: 0.04801
	loss_reward_5: 0.00751
	loss_policy: 0.23439
	loss_value: 0.45874
	loss_reward: 0.02922
Optimization_Done 43000
[2025-05-11 17:53:43] [command] train weight_iter_43000.pkl 197 216
[2025-05-11 17:53:51] nn step 43050, lr: 0.1.
	loss_policy_0: 0.10814
	accuracy_policy_0: 0.9577
	loss_value_0: 0.21139
	loss_policy_1: 0.02144
	accuracy_policy_1: 0.9525
	loss_value_1: 0.04213
	loss_reward_1: 0.00408
	loss_policy_2: 0.02163
	accuracy_policy_2: 0.95094
	loss_value_2: 0.04277
	loss_reward_2: 0.00458
	loss_policy_3: 0.0217
	accuracy_policy_3: 0.95473
	loss_value_3: 0.04303
	loss_reward_3: 0.00518
	loss_policy_4: 0.02146
	accuracy_policy_4: 0.95648
	loss_value_4: 0.04382
	loss_reward_4: 0.00608
	loss_policy_5: 0.02147
	accuracy_policy_5: 0.96109
	loss_value_5: 0.0447
	loss_reward_5: 0.00696
	loss_policy: 0.21585
	loss_value: 0.42784
	loss_reward: 0.02688
[2025-05-11 17:54:00] nn step 43100, lr: 0.1.
	loss_policy_0: 0.11563
	accuracy_policy_0: 0.9557
	loss_value_0: 0.2252
	loss_policy_1: 0.0229
	accuracy_policy_1: 0.95363
	loss_value_1: 0.04475
	loss_reward_1: 0.00443
	loss_policy_2: 0.02274
	accuracy_policy_2: 0.955
	loss_value_2: 0.04572
	loss_reward_2: 0.00479
	loss_policy_3: 0.02295
	accuracy_policy_3: 0.95383
	loss_value_3: 0.04638
	loss_reward_3: 0.0055
	loss_policy_4: 0.02297
	accuracy_policy_4: 0.95621
	loss_value_4: 0.04709
	loss_reward_4: 0.00639
	loss_policy_5: 0.02282
	accuracy_policy_5: 0.96098
	loss_value_5: 0.04782
	loss_reward_5: 0.00744
	loss_policy: 0.23
	loss_value: 0.45696
	loss_reward: 0.02855
[2025-05-11 17:54:08] nn step 43150, lr: 0.1.
	loss_policy_0: 0.11895
	accuracy_policy_0: 0.95723
	loss_value_0: 0.22692
	loss_policy_1: 0.02353
	accuracy_policy_1: 0.95344
	loss_value_1: 0.04533
	loss_reward_1: 0.00448
	loss_policy_2: 0.02375
	accuracy_policy_2: 0.9527
	loss_value_2: 0.04582
	loss_reward_2: 0.00496
	loss_policy_3: 0.02384
	accuracy_policy_3: 0.95434
	loss_value_3: 0.04636
	loss_reward_3: 0.00539
	loss_policy_4: 0.02404
	accuracy_policy_4: 0.95488
	loss_value_4: 0.04704
	loss_reward_4: 0.00616
	loss_policy_5: 0.0237
	accuracy_policy_5: 0.9602
	loss_value_5: 0.04794
	loss_reward_5: 0.00732
	loss_policy: 0.23781
	loss_value: 0.45942
	loss_reward: 0.02832
[2025-05-11 17:54:17] nn step 43200, lr: 0.1.
	loss_policy_0: 0.11928
	accuracy_policy_0: 0.95688
	loss_value_0: 0.2254
	loss_policy_1: 0.0238
	accuracy_policy_1: 0.95395
	loss_value_1: 0.04516
	loss_reward_1: 0.00461
	loss_policy_2: 0.02355
	accuracy_policy_2: 0.95211
	loss_value_2: 0.04562
	loss_reward_2: 0.00506
	loss_policy_3: 0.02369
	accuracy_policy_3: 0.95258
	loss_value_3: 0.04625
	loss_reward_3: 0.00572
	loss_policy_4: 0.02382
	accuracy_policy_4: 0.95504
	loss_value_4: 0.04693
	loss_reward_4: 0.0067
	loss_policy_5: 0.02362
	accuracy_policy_5: 0.96223
	loss_value_5: 0.04805
	loss_reward_5: 0.00789
	loss_policy: 0.23777
	loss_value: 0.4574
	loss_reward: 0.02998
Optimization_Done 43200
[2025-05-11 17:55:52] [command] train weight_iter_43200.pkl 198 217
[2025-05-11 17:56:01] nn step 43250, lr: 0.1.
	loss_policy_0: 0.11492
	accuracy_policy_0: 0.95684
	loss_value_0: 0.22364
	loss_policy_1: 0.02289
	accuracy_policy_1: 0.95164
	loss_value_1: 0.04448
	loss_reward_1: 0.00437
	loss_policy_2: 0.02257
	accuracy_policy_2: 0.95293
	loss_value_2: 0.045
	loss_reward_2: 0.00485
	loss_policy_3: 0.02286
	accuracy_policy_3: 0.95195
	loss_value_3: 0.0452
	loss_reward_3: 0.00532
	loss_policy_4: 0.02298
	accuracy_policy_4: 0.95566
	loss_value_4: 0.04586
	loss_reward_4: 0.00632
	loss_policy_5: 0.02281
	accuracy_policy_5: 0.96039
	loss_value_5: 0.04685
	loss_reward_5: 0.00726
	loss_policy: 0.22902
	loss_value: 0.45104
	loss_reward: 0.02811
[2025-05-11 17:56:10] nn step 43300, lr: 0.1.
	loss_policy_0: 0.1209
	accuracy_policy_0: 0.95758
	loss_value_0: 0.23274
	loss_policy_1: 0.02372
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04638
	loss_reward_1: 0.00464
	loss_policy_2: 0.02408
	accuracy_policy_2: 0.95137
	loss_value_2: 0.04682
	loss_reward_2: 0.00526
	loss_policy_3: 0.02392
	accuracy_policy_3: 0.95383
	loss_value_3: 0.04718
	loss_reward_3: 0.00573
	loss_policy_4: 0.02385
	accuracy_policy_4: 0.95512
	loss_value_4: 0.04807
	loss_reward_4: 0.00705
	loss_policy_5: 0.02379
	accuracy_policy_5: 0.9609
	loss_value_5: 0.04887
	loss_reward_5: 0.00795
	loss_policy: 0.24026
	loss_value: 0.47005
	loss_reward: 0.03063
[2025-05-11 17:56:17] nn step 43350, lr: 0.1.
	loss_policy_0: 0.11605
	accuracy_policy_0: 0.95828
	loss_value_0: 0.22413
	loss_policy_1: 0.02292
	accuracy_policy_1: 0.9552
	loss_value_1: 0.04471
	loss_reward_1: 0.00433
	loss_policy_2: 0.0228
	accuracy_policy_2: 0.95406
	loss_value_2: 0.04528
	loss_reward_2: 0.00483
	loss_policy_3: 0.02321
	accuracy_policy_3: 0.95496
	loss_value_3: 0.04559
	loss_reward_3: 0.00548
	loss_policy_4: 0.02321
	accuracy_policy_4: 0.95715
	loss_value_4: 0.04616
	loss_reward_4: 0.00663
	loss_policy_5: 0.02296
	accuracy_policy_5: 0.96203
	loss_value_5: 0.04724
	loss_reward_5: 0.00741
	loss_policy: 0.23115
	loss_value: 0.4531
	loss_reward: 0.02868
[2025-05-11 17:56:25] nn step 43400, lr: 0.1.
	loss_policy_0: 0.12167
	accuracy_policy_0: 0.95668
	loss_value_0: 0.2335
	loss_policy_1: 0.02401
	accuracy_policy_1: 0.95512
	loss_value_1: 0.0466
	loss_reward_1: 0.00466
	loss_policy_2: 0.02398
	accuracy_policy_2: 0.9516
	loss_value_2: 0.04728
	loss_reward_2: 0.005
	loss_policy_3: 0.02426
	accuracy_policy_3: 0.95211
	loss_value_3: 0.0478
	loss_reward_3: 0.00549
	loss_policy_4: 0.02411
	accuracy_policy_4: 0.95293
	loss_value_4: 0.04862
	loss_reward_4: 0.00662
	loss_policy_5: 0.0239
	accuracy_policy_5: 0.96203
	loss_value_5: 0.04951
	loss_reward_5: 0.00752
	loss_policy: 0.24193
	loss_value: 0.47331
	loss_reward: 0.02927
Optimization_Done 43400
[2025-05-11 17:58:03] [command] train weight_iter_43400.pkl 199 218
[2025-05-11 17:58:12] nn step 43450, lr: 0.1.
	loss_policy_0: 0.11375
	accuracy_policy_0: 0.95961
	loss_value_0: 0.224
	loss_policy_1: 0.02253
	accuracy_policy_1: 0.95812
	loss_value_1: 0.04458
	loss_reward_1: 0.00433
	loss_policy_2: 0.02281
	accuracy_policy_2: 0.95383
	loss_value_2: 0.04523
	loss_reward_2: 0.00456
	loss_policy_3: 0.02268
	accuracy_policy_3: 0.95508
	loss_value_3: 0.04561
	loss_reward_3: 0.00547
	loss_policy_4: 0.0226
	accuracy_policy_4: 0.95777
	loss_value_4: 0.04625
	loss_reward_4: 0.00636
	loss_policy_5: 0.02242
	accuracy_policy_5: 0.96363
	loss_value_5: 0.0468
	loss_reward_5: 0.0075
	loss_policy: 0.22678
	loss_value: 0.45247
	loss_reward: 0.02822
[2025-05-11 17:58:21] nn step 43500, lr: 0.1.
	loss_policy_0: 0.11708
	accuracy_policy_0: 0.95941
	loss_value_0: 0.22631
	loss_policy_1: 0.02348
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04508
	loss_reward_1: 0.0045
	loss_policy_2: 0.02349
	accuracy_policy_2: 0.95445
	loss_value_2: 0.04544
	loss_reward_2: 0.00474
	loss_policy_3: 0.02307
	accuracy_policy_3: 0.95418
	loss_value_3: 0.04591
	loss_reward_3: 0.00555
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.95895
	loss_value_4: 0.04686
	loss_reward_4: 0.00641
	loss_policy_5: 0.02321
	accuracy_policy_5: 0.9623
	loss_value_5: 0.04756
	loss_reward_5: 0.00749
	loss_policy: 0.23371
	loss_value: 0.45717
	loss_reward: 0.02869
[2025-05-11 17:58:29] nn step 43550, lr: 0.1.
	loss_policy_0: 0.12289
	accuracy_policy_0: 0.95578
	loss_value_0: 0.23094
	loss_policy_1: 0.0241
	accuracy_policy_1: 0.95543
	loss_value_1: 0.04613
	loss_reward_1: 0.0047
	loss_policy_2: 0.02454
	accuracy_policy_2: 0.95445
	loss_value_2: 0.04675
	loss_reward_2: 0.00501
	loss_policy_3: 0.02424
	accuracy_policy_3: 0.95301
	loss_value_3: 0.0473
	loss_reward_3: 0.00571
	loss_policy_4: 0.02418
	accuracy_policy_4: 0.95578
	loss_value_4: 0.04798
	loss_reward_4: 0.00667
	loss_policy_5: 0.02432
	accuracy_policy_5: 0.96301
	loss_value_5: 0.04868
	loss_reward_5: 0.00777
	loss_policy: 0.24428
	loss_value: 0.46777
	loss_reward: 0.02985
[2025-05-11 17:58:36] nn step 43600, lr: 0.1.
	loss_policy_0: 0.11876
	accuracy_policy_0: 0.9591
	loss_value_0: 0.22503
	loss_policy_1: 0.02355
	accuracy_policy_1: 0.95621
	loss_value_1: 0.04489
	loss_reward_1: 0.00443
	loss_policy_2: 0.02377
	accuracy_policy_2: 0.9557
	loss_value_2: 0.04573
	loss_reward_2: 0.005
	loss_policy_3: 0.024
	accuracy_policy_3: 0.95695
	loss_value_3: 0.04627
	loss_reward_3: 0.00555
	loss_policy_4: 0.0239
	accuracy_policy_4: 0.9573
	loss_value_4: 0.04698
	loss_reward_4: 0.00637
	loss_policy_5: 0.02345
	accuracy_policy_5: 0.96406
	loss_value_5: 0.0482
	loss_reward_5: 0.00777
	loss_policy: 0.23743
	loss_value: 0.4571
	loss_reward: 0.02912
Optimization_Done 43600
[2025-05-11 18:00:14] [command] train weight_iter_43600.pkl 200 219
[2025-05-11 18:00:23] nn step 43650, lr: 0.1.
	loss_policy_0: 0.11879
	accuracy_policy_0: 0.9623
	loss_value_0: 0.23087
	loss_policy_1: 0.02342
	accuracy_policy_1: 0.95719
	loss_value_1: 0.04591
	loss_reward_1: 0.00448
	loss_policy_2: 0.02341
	accuracy_policy_2: 0.95715
	loss_value_2: 0.04648
	loss_reward_2: 0.00489
	loss_policy_3: 0.02363
	accuracy_policy_3: 0.95605
	loss_value_3: 0.04687
	loss_reward_3: 0.00549
	loss_policy_4: 0.02319
	accuracy_policy_4: 0.96066
	loss_value_4: 0.04745
	loss_reward_4: 0.00635
	loss_policy_5: 0.02322
	accuracy_policy_5: 0.96457
	loss_value_5: 0.04827
	loss_reward_5: 0.00727
	loss_policy: 0.23565
	loss_value: 0.46586
	loss_reward: 0.02847
[2025-05-11 18:00:30] nn step 43700, lr: 0.1.
	loss_policy_0: 0.12444
	accuracy_policy_0: 0.96102
	loss_value_0: 0.23599
	loss_policy_1: 0.02463
	accuracy_policy_1: 0.95688
	loss_value_1: 0.04745
	loss_reward_1: 0.00483
	loss_policy_2: 0.02488
	accuracy_policy_2: 0.95574
	loss_value_2: 0.0482
	loss_reward_2: 0.00531
	loss_policy_3: 0.02477
	accuracy_policy_3: 0.95523
	loss_value_3: 0.04875
	loss_reward_3: 0.00584
	loss_policy_4: 0.02498
	accuracy_policy_4: 0.9591
	loss_value_4: 0.04928
	loss_reward_4: 0.00705
	loss_policy_5: 0.02498
	accuracy_policy_5: 0.96211
	loss_value_5: 0.05017
	loss_reward_5: 0.00801
	loss_policy: 0.24868
	loss_value: 0.47984
	loss_reward: 0.03103
[2025-05-11 18:00:39] nn step 43750, lr: 0.1.
	loss_policy_0: 0.11465
	accuracy_policy_0: 0.96066
	loss_value_0: 0.21449
	loss_policy_1: 0.02274
	accuracy_policy_1: 0.95715
	loss_value_1: 0.04294
	loss_reward_1: 0.00424
	loss_policy_2: 0.02264
	accuracy_policy_2: 0.95625
	loss_value_2: 0.04362
	loss_reward_2: 0.00453
	loss_policy_3: 0.02257
	accuracy_policy_3: 0.95371
	loss_value_3: 0.04416
	loss_reward_3: 0.00525
	loss_policy_4: 0.02258
	accuracy_policy_4: 0.95926
	loss_value_4: 0.04485
	loss_reward_4: 0.00619
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.96191
	loss_value_5: 0.04577
	loss_reward_5: 0.00724
	loss_policy: 0.2278
	loss_value: 0.43582
	loss_reward: 0.02746
[2025-05-11 18:00:47] nn step 43800, lr: 0.1.
	loss_policy_0: 0.12939
	accuracy_policy_0: 0.95852
	loss_value_0: 0.24164
	loss_policy_1: 0.02536
	accuracy_policy_1: 0.95578
	loss_value_1: 0.04826
	loss_reward_1: 0.00478
	loss_policy_2: 0.02534
	accuracy_policy_2: 0.95578
	loss_value_2: 0.0488
	loss_reward_2: 0.00549
	loss_policy_3: 0.02551
	accuracy_policy_3: 0.9575
	loss_value_3: 0.04948
	loss_reward_3: 0.00606
	loss_policy_4: 0.02528
	accuracy_policy_4: 0.95961
	loss_value_4: 0.04996
	loss_reward_4: 0.00706
	loss_policy_5: 0.02524
	accuracy_policy_5: 0.96219
	loss_value_5: 0.05085
	loss_reward_5: 0.00834
	loss_policy: 0.25612
	loss_value: 0.489
	loss_reward: 0.03174
Optimization_Done 43800
[2025-05-11 18:02:21] [command] train weight_iter_43800.pkl 201 220
[2025-05-11 18:02:29] nn step 43850, lr: 0.1.
	loss_policy_0: 0.11463
	accuracy_policy_0: 0.95977
	loss_value_0: 0.22041
	loss_policy_1: 0.02251
	accuracy_policy_1: 0.95379
	loss_value_1: 0.04388
	loss_reward_1: 0.00424
	loss_policy_2: 0.02252
	accuracy_policy_2: 0.95301
	loss_value_2: 0.04444
	loss_reward_2: 0.00465
	loss_policy_3: 0.02256
	accuracy_policy_3: 0.95621
	loss_value_3: 0.04524
	loss_reward_3: 0.00524
	loss_policy_4: 0.02248
	accuracy_policy_4: 0.95992
	loss_value_4: 0.04544
	loss_reward_4: 0.00609
	loss_policy_5: 0.02239
	accuracy_policy_5: 0.96223
	loss_value_5: 0.04621
	loss_reward_5: 0.0073
	loss_policy: 0.22709
	loss_value: 0.44561
	loss_reward: 0.02752
[2025-05-11 18:02:37] nn step 43900, lr: 0.1.
	loss_policy_0: 0.11749
	accuracy_policy_0: 0.96051
	loss_value_0: 0.22043
	loss_policy_1: 0.02327
	accuracy_policy_1: 0.95547
	loss_value_1: 0.04394
	loss_reward_1: 0.00453
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.95371
	loss_value_2: 0.04489
	loss_reward_2: 0.00486
	loss_policy_3: 0.02342
	accuracy_policy_3: 0.95422
	loss_value_3: 0.04534
	loss_reward_3: 0.0055
	loss_policy_4: 0.0234
	accuracy_policy_4: 0.95625
	loss_value_4: 0.04607
	loss_reward_4: 0.00625
	loss_policy_5: 0.02359
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04706
	loss_reward_5: 0.00732
	loss_policy: 0.23472
	loss_value: 0.44774
	loss_reward: 0.02847
[2025-05-11 18:02:46] nn step 43950, lr: 0.1.
	loss_policy_0: 0.11989
	accuracy_policy_0: 0.95746
	loss_value_0: 0.22342
	loss_policy_1: 0.02359
	accuracy_policy_1: 0.95461
	loss_value_1: 0.04438
	loss_reward_1: 0.00441
	loss_policy_2: 0.02368
	accuracy_policy_2: 0.95195
	loss_value_2: 0.04508
	loss_reward_2: 0.00505
	loss_policy_3: 0.0238
	accuracy_policy_3: 0.95434
	loss_value_3: 0.04564
	loss_reward_3: 0.00553
	loss_policy_4: 0.02323
	accuracy_policy_4: 0.95559
	loss_value_4: 0.04612
	loss_reward_4: 0.00656
	loss_policy_5: 0.02322
	accuracy_policy_5: 0.96223
	loss_value_5: 0.04702
	loss_reward_5: 0.00767
	loss_policy: 0.23741
	loss_value: 0.45166
	loss_reward: 0.02922
[2025-05-11 18:02:53] nn step 44000, lr: 0.1.
	loss_policy_0: 0.11895
	accuracy_policy_0: 0.9593
	loss_value_0: 0.22476
	loss_policy_1: 0.02326
	accuracy_policy_1: 0.95578
	loss_value_1: 0.04511
	loss_reward_1: 0.00458
	loss_policy_2: 0.02354
	accuracy_policy_2: 0.95652
	loss_value_2: 0.04586
	loss_reward_2: 0.0049
	loss_policy_3: 0.02407
	accuracy_policy_3: 0.95547
	loss_value_3: 0.04647
	loss_reward_3: 0.00559
	loss_policy_4: 0.0236
	accuracy_policy_4: 0.95832
	loss_value_4: 0.04738
	loss_reward_4: 0.00645
	loss_policy_5: 0.02365
	accuracy_policy_5: 0.96152
	loss_value_5: 0.04828
	loss_reward_5: 0.00769
	loss_policy: 0.23706
	loss_value: 0.45788
	loss_reward: 0.02921
Optimization_Done 44000
[2025-05-11 18:04:32] [command] train weight_iter_44000.pkl 202 221
[2025-05-11 18:04:42] nn step 44050, lr: 0.1.
	loss_policy_0: 0.1201
	accuracy_policy_0: 0.96031
	loss_value_0: 0.23421
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.95594
	loss_value_1: 0.04656
	loss_reward_1: 0.00451
	loss_policy_2: 0.02358
	accuracy_policy_2: 0.95039
	loss_value_2: 0.04703
	loss_reward_2: 0.00508
	loss_policy_3: 0.02395
	accuracy_policy_3: 0.95238
	loss_value_3: 0.04772
	loss_reward_3: 0.00576
	loss_policy_4: 0.02342
	accuracy_policy_4: 0.95672
	loss_value_4: 0.0484
	loss_reward_4: 0.00658
	loss_policy_5: 0.02372
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04926
	loss_reward_5: 0.00787
	loss_policy: 0.23834
	loss_value: 0.47318
	loss_reward: 0.0298
[2025-05-11 18:04:49] nn step 44100, lr: 0.1.
	loss_policy_0: 0.11152
	accuracy_policy_0: 0.96039
	loss_value_0: 0.21521
	loss_policy_1: 0.02227
	accuracy_policy_1: 0.95648
	loss_value_1: 0.04271
	loss_reward_1: 0.00423
	loss_policy_2: 0.02226
	accuracy_policy_2: 0.95703
	loss_value_2: 0.0433
	loss_reward_2: 0.00464
	loss_policy_3: 0.02198
	accuracy_policy_3: 0.95375
	loss_value_3: 0.04357
	loss_reward_3: 0.0052
	loss_policy_4: 0.02223
	accuracy_policy_4: 0.95984
	loss_value_4: 0.04425
	loss_reward_4: 0.00606
	loss_policy_5: 0.02215
	accuracy_policy_5: 0.96047
	loss_value_5: 0.0451
	loss_reward_5: 0.00725
	loss_policy: 0.22241
	loss_value: 0.43415
	loss_reward: 0.02737
[2025-05-11 18:04:58] nn step 44150, lr: 0.1.
	loss_policy_0: 0.11639
	accuracy_policy_0: 0.96164
	loss_value_0: 0.22146
	loss_policy_1: 0.02339
	accuracy_policy_1: 0.95461
	loss_value_1: 0.04429
	loss_reward_1: 0.00447
	loss_policy_2: 0.02301
	accuracy_policy_2: 0.95508
	loss_value_2: 0.04527
	loss_reward_2: 0.0049
	loss_policy_3: 0.02349
	accuracy_policy_3: 0.95348
	loss_value_3: 0.04572
	loss_reward_3: 0.00543
	loss_policy_4: 0.02314
	accuracy_policy_4: 0.95707
	loss_value_4: 0.04607
	loss_reward_4: 0.00655
	loss_policy_5: 0.02329
	accuracy_policy_5: 0.96246
	loss_value_5: 0.04676
	loss_reward_5: 0.00729
	loss_policy: 0.23272
	loss_value: 0.44958
	loss_reward: 0.02864
[2025-05-11 18:05:06] nn step 44200, lr: 0.1.
	loss_policy_0: 0.12048
	accuracy_policy_0: 0.95883
	loss_value_0: 0.22781
	loss_policy_1: 0.02367
	accuracy_policy_1: 0.95375
	loss_value_1: 0.04571
	loss_reward_1: 0.00451
	loss_policy_2: 0.02379
	accuracy_policy_2: 0.9552
	loss_value_2: 0.04642
	loss_reward_2: 0.00507
	loss_policy_3: 0.02394
	accuracy_policy_3: 0.9541
	loss_value_3: 0.04697
	loss_reward_3: 0.00543
	loss_policy_4: 0.0239
	accuracy_policy_4: 0.95742
	loss_value_4: 0.04745
	loss_reward_4: 0.00667
	loss_policy_5: 0.02368
	accuracy_policy_5: 0.96059
	loss_value_5: 0.04819
	loss_reward_5: 0.00742
	loss_policy: 0.23946
	loss_value: 0.46255
	loss_reward: 0.02909
Optimization_Done 44200
[2025-05-11 18:06:42] [command] train weight_iter_44200.pkl 203 222
[2025-05-11 18:06:52] nn step 44250, lr: 0.1.
	loss_policy_0: 0.11026
	accuracy_policy_0: 0.96051
	loss_value_0: 0.21791
	loss_policy_1: 0.02163
	accuracy_policy_1: 0.95824
	loss_value_1: 0.04287
	loss_reward_1: 0.0042
	loss_policy_2: 0.0214
	accuracy_policy_2: 0.95574
	loss_value_2: 0.04366
	loss_reward_2: 0.00451
	loss_policy_3: 0.02143
	accuracy_policy_3: 0.95922
	loss_value_3: 0.04412
	loss_reward_3: 0.00537
	loss_policy_4: 0.02169
	accuracy_policy_4: 0.9602
	loss_value_4: 0.04463
	loss_reward_4: 0.00626
	loss_policy_5: 0.02169
	accuracy_policy_5: 0.9625
	loss_value_5: 0.04544
	loss_reward_5: 0.00686
	loss_policy: 0.2181
	loss_value: 0.43861
	loss_reward: 0.02719
[2025-05-11 18:07:01] nn step 44300, lr: 0.1.
	loss_policy_0: 0.11765
	accuracy_policy_0: 0.95855
	loss_value_0: 0.22207
	loss_policy_1: 0.02308
	accuracy_policy_1: 0.95711
	loss_value_1: 0.04402
	loss_reward_1: 0.00448
	loss_policy_2: 0.02297
	accuracy_policy_2: 0.95695
	loss_value_2: 0.04454
	loss_reward_2: 0.00477
	loss_policy_3: 0.02298
	accuracy_policy_3: 0.95809
	loss_value_3: 0.04493
	loss_reward_3: 0.0051
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.95848
	loss_value_4: 0.04555
	loss_reward_4: 0.00654
	loss_policy_5: 0.02307
	accuracy_policy_5: 0.96289
	loss_value_5: 0.0465
	loss_reward_5: 0.00725
	loss_policy: 0.23276
	loss_value: 0.4476
	loss_reward: 0.02813
[2025-05-11 18:07:07] nn step 44350, lr: 0.1.
	loss_policy_0: 0.11095
	accuracy_policy_0: 0.96105
	loss_value_0: 0.21026
	loss_policy_1: 0.02186
	accuracy_policy_1: 0.95809
	loss_value_1: 0.04215
	loss_reward_1: 0.00402
	loss_policy_2: 0.02198
	accuracy_policy_2: 0.95676
	loss_value_2: 0.04287
	loss_reward_2: 0.00444
	loss_policy_3: 0.02226
	accuracy_policy_3: 0.95613
	loss_value_3: 0.04345
	loss_reward_3: 0.00522
	loss_policy_4: 0.0222
	accuracy_policy_4: 0.95961
	loss_value_4: 0.04383
	loss_reward_4: 0.0059
	loss_policy_5: 0.02209
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04477
	loss_reward_5: 0.00684
	loss_policy: 0.22134
	loss_value: 0.42732
	loss_reward: 0.02643
[2025-05-11 18:07:16] nn step 44400, lr: 0.1.
	loss_policy_0: 0.12143
	accuracy_policy_0: 0.95988
	loss_value_0: 0.23385
	loss_policy_1: 0.0246
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04663
	loss_reward_1: 0.00469
	loss_policy_2: 0.02454
	accuracy_policy_2: 0.95449
	loss_value_2: 0.04738
	loss_reward_2: 0.00502
	loss_policy_3: 0.02465
	accuracy_policy_3: 0.95609
	loss_value_3: 0.04798
	loss_reward_3: 0.00577
	loss_policy_4: 0.02439
	accuracy_policy_4: 0.95941
	loss_value_4: 0.04864
	loss_reward_4: 0.00675
	loss_policy_5: 0.0245
	accuracy_policy_5: 0.96375
	loss_value_5: 0.04948
	loss_reward_5: 0.00773
	loss_policy: 0.24411
	loss_value: 0.47396
	loss_reward: 0.02997
Optimization_Done 44400
[2025-05-11 18:08:52] [command] train weight_iter_44400.pkl 204 223
[2025-05-11 18:09:00] nn step 44450, lr: 0.1.
	loss_policy_0: 0.10911
	accuracy_policy_0: 0.96141
	loss_value_0: 0.21799
	loss_policy_1: 0.02148
	accuracy_policy_1: 0.95723
	loss_value_1: 0.04297
	loss_reward_1: 0.00419
	loss_policy_2: 0.02192
	accuracy_policy_2: 0.95793
	loss_value_2: 0.04323
	loss_reward_2: 0.00447
	loss_policy_3: 0.02189
	accuracy_policy_3: 0.95648
	loss_value_3: 0.04406
	loss_reward_3: 0.00509
	loss_policy_4: 0.0221
	accuracy_policy_4: 0.9598
	loss_value_4: 0.04453
	loss_reward_4: 0.00616
	loss_policy_5: 0.0215
	accuracy_policy_5: 0.96316
	loss_value_5: 0.0454
	loss_reward_5: 0.00682
	loss_policy: 0.21798
	loss_value: 0.43818
	loss_reward: 0.02673
[2025-05-11 18:09:08] nn step 44500, lr: 0.1.
	loss_policy_0: 0.11758
	accuracy_policy_0: 0.95969
	loss_value_0: 0.22645
	loss_policy_1: 0.02341
	accuracy_policy_1: 0.9568
	loss_value_1: 0.04505
	loss_reward_1: 0.00448
	loss_policy_2: 0.02357
	accuracy_policy_2: 0.95594
	loss_value_2: 0.0457
	loss_reward_2: 0.00485
	loss_policy_3: 0.02359
	accuracy_policy_3: 0.95688
	loss_value_3: 0.04634
	loss_reward_3: 0.00551
	loss_policy_4: 0.023
	accuracy_policy_4: 0.95891
	loss_value_4: 0.04688
	loss_reward_4: 0.00655
	loss_policy_5: 0.02309
	accuracy_policy_5: 0.96457
	loss_value_5: 0.04794
	loss_reward_5: 0.00766
	loss_policy: 0.23424
	loss_value: 0.45836
	loss_reward: 0.02905
[2025-05-11 18:09:17] nn step 44550, lr: 0.1.
	loss_policy_0: 0.12474
	accuracy_policy_0: 0.95965
	loss_value_0: 0.23482
	loss_policy_1: 0.02462
	accuracy_policy_1: 0.95465
	loss_value_1: 0.04701
	loss_reward_1: 0.0048
	loss_policy_2: 0.02473
	accuracy_policy_2: 0.9573
	loss_value_2: 0.04768
	loss_reward_2: 0.00505
	loss_policy_3: 0.02465
	accuracy_policy_3: 0.95734
	loss_value_3: 0.04802
	loss_reward_3: 0.00587
	loss_policy_4: 0.02474
	accuracy_policy_4: 0.95719
	loss_value_4: 0.04906
	loss_reward_4: 0.00679
	loss_policy_5: 0.02455
	accuracy_policy_5: 0.96332
	loss_value_5: 0.04996
	loss_reward_5: 0.00771
	loss_policy: 0.24803
	loss_value: 0.47654
	loss_reward: 0.03021
[2025-05-11 18:09:26] nn step 44600, lr: 0.1.
	loss_policy_0: 0.11691
	accuracy_policy_0: 0.95812
	loss_value_0: 0.21651
	loss_policy_1: 0.02306
	accuracy_policy_1: 0.95676
	loss_value_1: 0.04344
	loss_reward_1: 0.00436
	loss_policy_2: 0.02291
	accuracy_policy_2: 0.95543
	loss_value_2: 0.04442
	loss_reward_2: 0.00471
	loss_policy_3: 0.02314
	accuracy_policy_3: 0.95289
	loss_value_3: 0.04505
	loss_reward_3: 0.0053
	loss_policy_4: 0.02295
	accuracy_policy_4: 0.95801
	loss_value_4: 0.04589
	loss_reward_4: 0.0065
	loss_policy_5: 0.02293
	accuracy_policy_5: 0.96242
	loss_value_5: 0.04657
	loss_reward_5: 0.00772
	loss_policy: 0.2319
	loss_value: 0.44188
	loss_reward: 0.02859
Optimization_Done 44600
[2025-05-11 18:11:03] [command] train weight_iter_44600.pkl 205 224
[2025-05-11 18:11:12] nn step 44650, lr: 0.1.
	loss_policy_0: 0.11676
	accuracy_policy_0: 0.96113
	loss_value_0: 0.23611
	loss_policy_1: 0.02311
	accuracy_policy_1: 0.95895
	loss_value_1: 0.04692
	loss_reward_1: 0.00467
	loss_policy_2: 0.02306
	accuracy_policy_2: 0.95777
	loss_value_2: 0.04711
	loss_reward_2: 0.0051
	loss_policy_3: 0.0232
	accuracy_policy_3: 0.95668
	loss_value_3: 0.04759
	loss_reward_3: 0.00574
	loss_policy_4: 0.02328
	accuracy_policy_4: 0.95902
	loss_value_4: 0.04822
	loss_reward_4: 0.00664
	loss_policy_5: 0.02299
	accuracy_policy_5: 0.96316
	loss_value_5: 0.04896
	loss_reward_5: 0.0078
	loss_policy: 0.2324
	loss_value: 0.47492
	loss_reward: 0.02995
[2025-05-11 18:11:21] nn step 44700, lr: 0.1.
	loss_policy_0: 0.11659
	accuracy_policy_0: 0.9623
	loss_value_0: 0.22755
	loss_policy_1: 0.02306
	accuracy_policy_1: 0.95652
	loss_value_1: 0.04533
	loss_reward_1: 0.00452
	loss_policy_2: 0.02333
	accuracy_policy_2: 0.95414
	loss_value_2: 0.04604
	loss_reward_2: 0.00474
	loss_policy_3: 0.02309
	accuracy_policy_3: 0.95637
	loss_value_3: 0.04684
	loss_reward_3: 0.00545
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.95973
	loss_value_4: 0.04778
	loss_reward_4: 0.00651
	loss_policy_5: 0.02307
	accuracy_policy_5: 0.96387
	loss_value_5: 0.04838
	loss_reward_5: 0.00713
	loss_policy: 0.23215
	loss_value: 0.46192
	loss_reward: 0.02835
[2025-05-11 18:11:28] nn step 44750, lr: 0.1.
	loss_policy_0: 0.12111
	accuracy_policy_0: 0.96094
	loss_value_0: 0.2325
	loss_policy_1: 0.02389
	accuracy_policy_1: 0.95887
	loss_value_1: 0.0464
	loss_reward_1: 0.00458
	loss_policy_2: 0.02406
	accuracy_policy_2: 0.95734
	loss_value_2: 0.04683
	loss_reward_2: 0.00524
	loss_policy_3: 0.02434
	accuracy_policy_3: 0.95754
	loss_value_3: 0.04763
	loss_reward_3: 0.00576
	loss_policy_4: 0.02386
	accuracy_policy_4: 0.95961
	loss_value_4: 0.04835
	loss_reward_4: 0.00678
	loss_policy_5: 0.0239
	accuracy_policy_5: 0.96234
	loss_value_5: 0.04938
	loss_reward_5: 0.00759
	loss_policy: 0.24116
	loss_value: 0.47109
	loss_reward: 0.02996
[2025-05-11 18:11:36] nn step 44800, lr: 0.1.
	loss_policy_0: 0.11984
	accuracy_policy_0: 0.96148
	loss_value_0: 0.22836
	loss_policy_1: 0.02391
	accuracy_policy_1: 0.95832
	loss_value_1: 0.04583
	loss_reward_1: 0.00462
	loss_policy_2: 0.02372
	accuracy_policy_2: 0.95602
	loss_value_2: 0.04649
	loss_reward_2: 0.00485
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.95863
	loss_value_3: 0.04713
	loss_reward_3: 0.00544
	loss_policy_4: 0.02403
	accuracy_policy_4: 0.95973
	loss_value_4: 0.04767
	loss_reward_4: 0.00643
	loss_policy_5: 0.02371
	accuracy_policy_5: 0.96359
	loss_value_5: 0.04848
	loss_reward_5: 0.00774
	loss_policy: 0.23908
	loss_value: 0.46396
	loss_reward: 0.02908
Optimization_Done 44800
[2025-05-11 18:13:14] [command] train weight_iter_44800.pkl 206 225
[2025-05-11 18:13:21] nn step 44850, lr: 0.1.
	loss_policy_0: 0.11256
	accuracy_policy_0: 0.9648
	loss_value_0: 0.22675
	loss_policy_1: 0.02247
	accuracy_policy_1: 0.9582
	loss_value_1: 0.0453
	loss_reward_1: 0.00431
	loss_policy_2: 0.02259
	accuracy_policy_2: 0.95801
	loss_value_2: 0.04609
	loss_reward_2: 0.00463
	loss_policy_3: 0.02273
	accuracy_policy_3: 0.95734
	loss_value_3: 0.04667
	loss_reward_3: 0.00553
	loss_policy_4: 0.02242
	accuracy_policy_4: 0.96105
	loss_value_4: 0.04732
	loss_reward_4: 0.00652
	loss_policy_5: 0.02209
	accuracy_policy_5: 0.96234
	loss_value_5: 0.04819
	loss_reward_5: 0.00714
	loss_policy: 0.22486
	loss_value: 0.46032
	loss_reward: 0.02812
[2025-05-11 18:13:30] nn step 44900, lr: 0.1.
	loss_policy_0: 0.11932
	accuracy_policy_0: 0.96055
	loss_value_0: 0.23822
	loss_policy_1: 0.02371
	accuracy_policy_1: 0.95605
	loss_value_1: 0.04761
	loss_reward_1: 0.0045
	loss_policy_2: 0.0238
	accuracy_policy_2: 0.95664
	loss_value_2: 0.04818
	loss_reward_2: 0.00491
	loss_policy_3: 0.02375
	accuracy_policy_3: 0.95535
	loss_value_3: 0.04878
	loss_reward_3: 0.00573
	loss_policy_4: 0.02352
	accuracy_policy_4: 0.95957
	loss_value_4: 0.0496
	loss_reward_4: 0.00701
	loss_policy_5: 0.02334
	accuracy_policy_5: 0.96336
	loss_value_5: 0.05054
	loss_reward_5: 0.00754
	loss_policy: 0.23744
	loss_value: 0.48292
	loss_reward: 0.02969
[2025-05-11 18:13:38] nn step 44950, lr: 0.1.
	loss_policy_0: 0.11673
	accuracy_policy_0: 0.9575
	loss_value_0: 0.22813
	loss_policy_1: 0.02318
	accuracy_policy_1: 0.95648
	loss_value_1: 0.0457
	loss_reward_1: 0.00426
	loss_policy_2: 0.02333
	accuracy_policy_2: 0.95438
	loss_value_2: 0.04658
	loss_reward_2: 0.0051
	loss_policy_3: 0.02356
	accuracy_policy_3: 0.95488
	loss_value_3: 0.04719
	loss_reward_3: 0.00545
	loss_policy_4: 0.02343
	accuracy_policy_4: 0.95875
	loss_value_4: 0.04797
	loss_reward_4: 0.00661
	loss_policy_5: 0.02341
	accuracy_policy_5: 0.96195
	loss_value_5: 0.04901
	loss_reward_5: 0.0077
	loss_policy: 0.23364
	loss_value: 0.46458
	loss_reward: 0.02912
[2025-05-11 18:13:45] nn step 45000, lr: 0.1.
	loss_policy_0: 0.11529
	accuracy_policy_0: 0.96121
	loss_value_0: 0.22544
	loss_policy_1: 0.02281
	accuracy_policy_1: 0.9584
	loss_value_1: 0.04532
	loss_reward_1: 0.0043
	loss_policy_2: 0.0229
	accuracy_policy_2: 0.95676
	loss_value_2: 0.04588
	loss_reward_2: 0.00502
	loss_policy_3: 0.02284
	accuracy_policy_3: 0.9566
	loss_value_3: 0.04652
	loss_reward_3: 0.00559
	loss_policy_4: 0.02273
	accuracy_policy_4: 0.95922
	loss_value_4: 0.04734
	loss_reward_4: 0.00656
	loss_policy_5: 0.02285
	accuracy_policy_5: 0.96191
	loss_value_5: 0.04847
	loss_reward_5: 0.00778
	loss_policy: 0.22943
	loss_value: 0.45897
	loss_reward: 0.02925
Optimization_Done 45000
[2025-05-11 18:15:22] [command] train weight_iter_45000.pkl 207 226
[2025-05-11 18:15:31] nn step 45050, lr: 0.1.
	loss_policy_0: 0.11623
	accuracy_policy_0: 0.96191
	loss_value_0: 0.23433
	loss_policy_1: 0.0231
	accuracy_policy_1: 0.95805
	loss_value_1: 0.04655
	loss_reward_1: 0.00452
	loss_policy_2: 0.02332
	accuracy_policy_2: 0.95707
	loss_value_2: 0.04718
	loss_reward_2: 0.00486
	loss_policy_3: 0.02309
	accuracy_policy_3: 0.95699
	loss_value_3: 0.0477
	loss_reward_3: 0.00555
	loss_policy_4: 0.02304
	accuracy_policy_4: 0.96199
	loss_value_4: 0.04811
	loss_reward_4: 0.00643
	loss_policy_5: 0.02312
	accuracy_policy_5: 0.96363
	loss_value_5: 0.04915
	loss_reward_5: 0.00728
	loss_policy: 0.2319
	loss_value: 0.47302
	loss_reward: 0.02863
[2025-05-11 18:15:38] nn step 45100, lr: 0.1.
	loss_policy_0: 0.11764
	accuracy_policy_0: 0.95969
	loss_value_0: 0.23049
	loss_policy_1: 0.0234
	accuracy_policy_1: 0.95461
	loss_value_1: 0.04633
	loss_reward_1: 0.00438
	loss_policy_2: 0.02324
	accuracy_policy_2: 0.9577
	loss_value_2: 0.04682
	loss_reward_2: 0.00492
	loss_policy_3: 0.02327
	accuracy_policy_3: 0.95898
	loss_value_3: 0.04769
	loss_reward_3: 0.00542
	loss_policy_4: 0.02307
	accuracy_policy_4: 0.96238
	loss_value_4: 0.0484
	loss_reward_4: 0.00653
	loss_policy_5: 0.02332
	accuracy_policy_5: 0.96508
	loss_value_5: 0.04918
	loss_reward_5: 0.00749
	loss_policy: 0.23394
	loss_value: 0.46892
	loss_reward: 0.02875
[2025-05-11 18:15:47] nn step 45150, lr: 0.1.
	loss_policy_0: 0.12315
	accuracy_policy_0: 0.95809
	loss_value_0: 0.23974
	loss_policy_1: 0.02426
	accuracy_policy_1: 0.95754
	loss_value_1: 0.04769
	loss_reward_1: 0.00457
	loss_policy_2: 0.02418
	accuracy_policy_2: 0.95496
	loss_value_2: 0.04855
	loss_reward_2: 0.00523
	loss_policy_3: 0.02414
	accuracy_policy_3: 0.95484
	loss_value_3: 0.04895
	loss_reward_3: 0.00574
	loss_policy_4: 0.02414
	accuracy_policy_4: 0.95766
	loss_value_4: 0.04967
	loss_reward_4: 0.0065
	loss_policy_5: 0.0241
	accuracy_policy_5: 0.96309
	loss_value_5: 0.05045
	loss_reward_5: 0.00785
	loss_policy: 0.24397
	loss_value: 0.48505
	loss_reward: 0.02989
[2025-05-11 18:15:55] nn step 45200, lr: 0.1.
	loss_policy_0: 0.11995
	accuracy_policy_0: 0.96004
	loss_value_0: 0.23132
	loss_policy_1: 0.02353
	accuracy_policy_1: 0.9566
	loss_value_1: 0.04615
	loss_reward_1: 0.00429
	loss_policy_2: 0.02347
	accuracy_policy_2: 0.95898
	loss_value_2: 0.04702
	loss_reward_2: 0.00487
	loss_policy_3: 0.0241
	accuracy_policy_3: 0.95539
	loss_value_3: 0.04785
	loss_reward_3: 0.00553
	loss_policy_4: 0.02371
	accuracy_policy_4: 0.95992
	loss_value_4: 0.04886
	loss_reward_4: 0.00628
	loss_policy_5: 0.02379
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04958
	loss_reward_5: 0.00767
	loss_policy: 0.23856
	loss_value: 0.47077
	loss_reward: 0.02863
Optimization_Done 45200
[2025-05-11 18:17:31] [command] train weight_iter_45200.pkl 208 227
[2025-05-11 18:17:40] nn step 45250, lr: 0.1.
	loss_policy_0: 0.11488
	accuracy_policy_0: 0.96031
	loss_value_0: 0.23552
	loss_policy_1: 0.02296
	accuracy_policy_1: 0.9577
	loss_value_1: 0.04688
	loss_reward_1: 0.00428
	loss_policy_2: 0.02249
	accuracy_policy_2: 0.95562
	loss_value_2: 0.04747
	loss_reward_2: 0.00482
	loss_policy_3: 0.02306
	accuracy_policy_3: 0.95781
	loss_value_3: 0.04822
	loss_reward_3: 0.00517
	loss_policy_4: 0.0227
	accuracy_policy_4: 0.95941
	loss_value_4: 0.04854
	loss_reward_4: 0.00637
	loss_policy_5: 0.02301
	accuracy_policy_5: 0.96375
	loss_value_5: 0.04963
	loss_reward_5: 0.00733
	loss_policy: 0.22911
	loss_value: 0.47627
	loss_reward: 0.02797
[2025-05-11 18:17:49] nn step 45300, lr: 0.1.
	loss_policy_0: 0.11264
	accuracy_policy_0: 0.95852
	loss_value_0: 0.21931
	loss_policy_1: 0.02222
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04385
	loss_reward_1: 0.00398
	loss_policy_2: 0.02195
	accuracy_policy_2: 0.95492
	loss_value_2: 0.0442
	loss_reward_2: 0.00462
	loss_policy_3: 0.02231
	accuracy_policy_3: 0.95598
	loss_value_3: 0.04472
	loss_reward_3: 0.00524
	loss_policy_4: 0.02214
	accuracy_policy_4: 0.96148
	loss_value_4: 0.04562
	loss_reward_4: 0.00594
	loss_policy_5: 0.02224
	accuracy_policy_5: 0.9657
	loss_value_5: 0.04639
	loss_reward_5: 0.00714
	loss_policy: 0.22349
	loss_value: 0.4441
	loss_reward: 0.02693
[2025-05-11 18:17:58] nn step 45350, lr: 0.1.
	loss_policy_0: 0.12135
	accuracy_policy_0: 0.95812
	loss_value_0: 0.23652
	loss_policy_1: 0.02392
	accuracy_policy_1: 0.95645
	loss_value_1: 0.0471
	loss_reward_1: 0.00461
	loss_policy_2: 0.0243
	accuracy_policy_2: 0.95402
	loss_value_2: 0.04794
	loss_reward_2: 0.00499
	loss_policy_3: 0.02416
	accuracy_policy_3: 0.9534
	loss_value_3: 0.04865
	loss_reward_3: 0.00554
	loss_policy_4: 0.02414
	accuracy_policy_4: 0.95719
	loss_value_4: 0.04942
	loss_reward_4: 0.00652
	loss_policy_5: 0.02443
	accuracy_policy_5: 0.96473
	loss_value_5: 0.05036
	loss_reward_5: 0.00775
	loss_policy: 0.24229
	loss_value: 0.47999
	loss_reward: 0.02941
[2025-05-11 18:18:05] nn step 45400, lr: 0.1.
	loss_policy_0: 0.11884
	accuracy_policy_0: 0.9593
	loss_value_0: 0.23287
	loss_policy_1: 0.02364
	accuracy_policy_1: 0.95504
	loss_value_1: 0.04651
	loss_reward_1: 0.00457
	loss_policy_2: 0.02358
	accuracy_policy_2: 0.95691
	loss_value_2: 0.04713
	loss_reward_2: 0.00508
	loss_policy_3: 0.02405
	accuracy_policy_3: 0.955
	loss_value_3: 0.04772
	loss_reward_3: 0.00538
	loss_policy_4: 0.0237
	accuracy_policy_4: 0.9573
	loss_value_4: 0.04837
	loss_reward_4: 0.00663
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.96281
	loss_value_5: 0.0493
	loss_reward_5: 0.00785
	loss_policy: 0.23739
	loss_value: 0.47189
	loss_reward: 0.02951
Optimization_Done 45400
[2025-05-11 18:19:39] [command] train weight_iter_45400.pkl 209 228
[2025-05-11 18:19:49] nn step 45450, lr: 0.1.
	loss_policy_0: 0.11346
	accuracy_policy_0: 0.9602
	loss_value_0: 0.22682
	loss_policy_1: 0.02235
	accuracy_policy_1: 0.95758
	loss_value_1: 0.04528
	loss_reward_1: 0.00422
	loss_policy_2: 0.02232
	accuracy_policy_2: 0.9557
	loss_value_2: 0.04577
	loss_reward_2: 0.00466
	loss_policy_3: 0.0224
	accuracy_policy_3: 0.9566
	loss_value_3: 0.04653
	loss_reward_3: 0.00521
	loss_policy_4: 0.02252
	accuracy_policy_4: 0.95848
	loss_value_4: 0.04688
	loss_reward_4: 0.00624
	loss_policy_5: 0.02217
	accuracy_policy_5: 0.9625
	loss_value_5: 0.04775
	loss_reward_5: 0.00751
	loss_policy: 0.22522
	loss_value: 0.45903
	loss_reward: 0.02784
[2025-05-11 18:19:56] nn step 45500, lr: 0.1.
	loss_policy_0: 0.11461
	accuracy_policy_0: 0.96125
	loss_value_0: 0.2273
	loss_policy_1: 0.02313
	accuracy_policy_1: 0.95699
	loss_value_1: 0.04534
	loss_reward_1: 0.00445
	loss_policy_2: 0.02327
	accuracy_policy_2: 0.95508
	loss_value_2: 0.04593
	loss_reward_2: 0.00506
	loss_policy_3: 0.02326
	accuracy_policy_3: 0.95824
	loss_value_3: 0.04704
	loss_reward_3: 0.00546
	loss_policy_4: 0.02299
	accuracy_policy_4: 0.95738
	loss_value_4: 0.04771
	loss_reward_4: 0.00654
	loss_policy_5: 0.02307
	accuracy_policy_5: 0.96281
	loss_value_5: 0.04869
	loss_reward_5: 0.00778
	loss_policy: 0.23032
	loss_value: 0.46201
	loss_reward: 0.02929
[2025-05-11 18:20:04] nn step 45550, lr: 0.1.
	loss_policy_0: 0.11924
	accuracy_policy_0: 0.96004
	loss_value_0: 0.23442
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.95703
	loss_value_1: 0.04688
	loss_reward_1: 0.00439
	loss_policy_2: 0.02405
	accuracy_policy_2: 0.95723
	loss_value_2: 0.04765
	loss_reward_2: 0.00525
	loss_policy_3: 0.02382
	accuracy_policy_3: 0.95527
	loss_value_3: 0.04847
	loss_reward_3: 0.00556
	loss_policy_4: 0.02418
	accuracy_policy_4: 0.95824
	loss_value_4: 0.04944
	loss_reward_4: 0.00674
	loss_policy_5: 0.02387
	accuracy_policy_5: 0.96133
	loss_value_5: 0.05024
	loss_reward_5: 0.00801
	loss_policy: 0.23902
	loss_value: 0.4771
	loss_reward: 0.02995
[2025-05-11 18:20:13] nn step 45600, lr: 0.1.
	loss_policy_0: 0.12875
	accuracy_policy_0: 0.95727
	loss_value_0: 0.24972
	loss_policy_1: 0.02511
	accuracy_policy_1: 0.9534
	loss_value_1: 0.05014
	loss_reward_1: 0.00485
	loss_policy_2: 0.02523
	accuracy_policy_2: 0.95113
	loss_value_2: 0.05095
	loss_reward_2: 0.00556
	loss_policy_3: 0.02529
	accuracy_policy_3: 0.95336
	loss_value_3: 0.05188
	loss_reward_3: 0.00606
	loss_policy_4: 0.02535
	accuracy_policy_4: 0.95672
	loss_value_4: 0.05261
	loss_reward_4: 0.00719
	loss_policy_5: 0.02508
	accuracy_policy_5: 0.96141
	loss_value_5: 0.0534
	loss_reward_5: 0.00856
	loss_policy: 0.2548
	loss_value: 0.50871
	loss_reward: 0.03223
Optimization_Done 45600
[2025-05-11 18:21:48] [command] train weight_iter_45600.pkl 210 229
[2025-05-11 18:21:58] nn step 45650, lr: 0.1.
	loss_policy_0: 0.11611
	accuracy_policy_0: 0.95383
	loss_value_0: 0.23118
	loss_policy_1: 0.02289
	accuracy_policy_1: 0.9518
	loss_value_1: 0.04599
	loss_reward_1: 0.00417
	loss_policy_2: 0.0228
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04644
	loss_reward_2: 0.00478
	loss_policy_3: 0.02295
	accuracy_policy_3: 0.95344
	loss_value_3: 0.04683
	loss_reward_3: 0.00533
	loss_policy_4: 0.02283
	accuracy_policy_4: 0.95438
	loss_value_4: 0.04764
	loss_reward_4: 0.00625
	loss_policy_5: 0.02298
	accuracy_policy_5: 0.96094
	loss_value_5: 0.04857
	loss_reward_5: 0.00713
	loss_policy: 0.23056
	loss_value: 0.46665
	loss_reward: 0.02765
[2025-05-11 18:22:06] nn step 45700, lr: 0.1.
	loss_policy_0: 0.11729
	accuracy_policy_0: 0.95754
	loss_value_0: 0.22831
	loss_policy_1: 0.02316
	accuracy_policy_1: 0.95355
	loss_value_1: 0.04545
	loss_reward_1: 0.00433
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95547
	loss_value_2: 0.04598
	loss_reward_2: 0.00478
	loss_policy_3: 0.0232
	accuracy_policy_3: 0.95359
	loss_value_3: 0.04702
	loss_reward_3: 0.00532
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.95746
	loss_value_4: 0.04778
	loss_reward_4: 0.0064
	loss_policy_5: 0.02292
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04871
	loss_reward_5: 0.0072
	loss_policy: 0.23272
	loss_value: 0.46324
	loss_reward: 0.02803
[2025-05-11 18:22:15] nn step 45750, lr: 0.1.
	loss_policy_0: 0.11959
	accuracy_policy_0: 0.95512
	loss_value_0: 0.23184
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.95262
	loss_value_1: 0.04612
	loss_reward_1: 0.00435
	loss_policy_2: 0.0237
	accuracy_policy_2: 0.95094
	loss_value_2: 0.04689
	loss_reward_2: 0.00501
	loss_policy_3: 0.02362
	accuracy_policy_3: 0.95254
	loss_value_3: 0.04749
	loss_reward_3: 0.00544
	loss_policy_4: 0.02349
	accuracy_policy_4: 0.95648
	loss_value_4: 0.04803
	loss_reward_4: 0.00632
	loss_policy_5: 0.0235
	accuracy_policy_5: 0.96184
	loss_value_5: 0.0489
	loss_reward_5: 0.00749
	loss_policy: 0.23777
	loss_value: 0.46927
	loss_reward: 0.02861
[2025-05-11 18:22:22] nn step 45800, lr: 0.1.
	loss_policy_0: 0.11916
	accuracy_policy_0: 0.95582
	loss_value_0: 0.22731
	loss_policy_1: 0.02342
	accuracy_policy_1: 0.95297
	loss_value_1: 0.04545
	loss_reward_1: 0.00427
	loss_policy_2: 0.02326
	accuracy_policy_2: 0.95188
	loss_value_2: 0.04626
	loss_reward_2: 0.0051
	loss_policy_3: 0.02341
	accuracy_policy_3: 0.95316
	loss_value_3: 0.04687
	loss_reward_3: 0.00555
	loss_policy_4: 0.02335
	accuracy_policy_4: 0.95648
	loss_value_4: 0.04783
	loss_reward_4: 0.00629
	loss_policy_5: 0.02309
	accuracy_policy_5: 0.96359
	loss_value_5: 0.04894
	loss_reward_5: 0.00771
	loss_policy: 0.23569
	loss_value: 0.46266
	loss_reward: 0.02892
Optimization_Done 45800
[2025-05-11 18:23:56] [command] train weight_iter_45800.pkl 211 230
[2025-05-11 18:24:06] nn step 45850, lr: 0.1.
	loss_policy_0: 0.12134
	accuracy_policy_0: 0.95516
	loss_value_0: 0.25202
	loss_policy_1: 0.02413
	accuracy_policy_1: 0.95328
	loss_value_1: 0.04988
	loss_reward_1: 0.00445
	loss_policy_2: 0.02447
	accuracy_policy_2: 0.95203
	loss_value_2: 0.05049
	loss_reward_2: 0.00522
	loss_policy_3: 0.02423
	accuracy_policy_3: 0.9548
	loss_value_3: 0.05123
	loss_reward_3: 0.00584
	loss_policy_4: 0.02443
	accuracy_policy_4: 0.95449
	loss_value_4: 0.05228
	loss_reward_4: 0.00702
	loss_policy_5: 0.02412
	accuracy_policy_5: 0.95996
	loss_value_5: 0.05319
	loss_reward_5: 0.00796
	loss_policy: 0.24272
	loss_value: 0.50908
	loss_reward: 0.03049
[2025-05-11 18:24:12] nn step 45900, lr: 0.1.
	loss_policy_0: 0.12555
	accuracy_policy_0: 0.9543
	loss_value_0: 0.25397
	loss_policy_1: 0.025
	accuracy_policy_1: 0.94992
	loss_value_1: 0.05056
	loss_reward_1: 0.00476
	loss_policy_2: 0.02491
	accuracy_policy_2: 0.9502
	loss_value_2: 0.05092
	loss_reward_2: 0.0053
	loss_policy_3: 0.02505
	accuracy_policy_3: 0.95168
	loss_value_3: 0.05197
	loss_reward_3: 0.00577
	loss_policy_4: 0.02515
	accuracy_policy_4: 0.95527
	loss_value_4: 0.05264
	loss_reward_4: 0.00715
	loss_policy_5: 0.02456
	accuracy_policy_5: 0.96238
	loss_value_5: 0.05342
	loss_reward_5: 0.00847
	loss_policy: 0.25022
	loss_value: 0.51349
	loss_reward: 0.03145
[2025-05-11 18:24:21] nn step 45950, lr: 0.1.
	loss_policy_0: 0.11979
	accuracy_policy_0: 0.95566
	loss_value_0: 0.24274
	loss_policy_1: 0.02379
	accuracy_policy_1: 0.94957
	loss_value_1: 0.04835
	loss_reward_1: 0.00453
	loss_policy_2: 0.02413
	accuracy_policy_2: 0.95141
	loss_value_2: 0.04904
	loss_reward_2: 0.00504
	loss_policy_3: 0.02404
	accuracy_policy_3: 0.95109
	loss_value_3: 0.04963
	loss_reward_3: 0.00541
	loss_policy_4: 0.02355
	accuracy_policy_4: 0.95312
	loss_value_4: 0.0505
	loss_reward_4: 0.00684
	loss_policy_5: 0.02371
	accuracy_policy_5: 0.96145
	loss_value_5: 0.05124
	loss_reward_5: 0.00791
	loss_policy: 0.23901
	loss_value: 0.49149
	loss_reward: 0.02972
[2025-05-11 18:24:30] nn step 46000, lr: 0.1.
	loss_policy_0: 0.11901
	accuracy_policy_0: 0.95672
	loss_value_0: 0.23758
	loss_policy_1: 0.02358
	accuracy_policy_1: 0.95199
	loss_value_1: 0.04766
	loss_reward_1: 0.00453
	loss_policy_2: 0.02411
	accuracy_policy_2: 0.95238
	loss_value_2: 0.0481
	loss_reward_2: 0.0051
	loss_policy_3: 0.02405
	accuracy_policy_3: 0.95047
	loss_value_3: 0.04879
	loss_reward_3: 0.00566
	loss_policy_4: 0.02366
	accuracy_policy_4: 0.95301
	loss_value_4: 0.0496
	loss_reward_4: 0.0063
	loss_policy_5: 0.0234
	accuracy_policy_5: 0.96203
	loss_value_5: 0.0506
	loss_reward_5: 0.00778
	loss_policy: 0.23781
	loss_value: 0.48234
	loss_reward: 0.02936
Optimization_Done 46000
[2025-05-11 18:26:06] [command] train weight_iter_46000.pkl 212 231
[2025-05-11 18:26:16] nn step 46050, lr: 0.1.
	loss_policy_0: 0.11109
	accuracy_policy_0: 0.95566
	loss_value_0: 0.23135
	loss_policy_1: 0.02221
	accuracy_policy_1: 0.95133
	loss_value_1: 0.04613
	loss_reward_1: 0.00437
	loss_policy_2: 0.02231
	accuracy_policy_2: 0.95102
	loss_value_2: 0.04649
	loss_reward_2: 0.00468
	loss_policy_3: 0.02235
	accuracy_policy_3: 0.95113
	loss_value_3: 0.04722
	loss_reward_3: 0.00536
	loss_policy_4: 0.02237
	accuracy_policy_4: 0.95219
	loss_value_4: 0.04785
	loss_reward_4: 0.00635
	loss_policy_5: 0.02204
	accuracy_policy_5: 0.96039
	loss_value_5: 0.0487
	loss_reward_5: 0.0072
	loss_policy: 0.22237
	loss_value: 0.46775
	loss_reward: 0.02797
[2025-05-11 18:26:24] nn step 46100, lr: 0.1.
	loss_policy_0: 0.1117
	accuracy_policy_0: 0.95535
	loss_value_0: 0.22292
	loss_policy_1: 0.02202
	accuracy_policy_1: 0.9507
	loss_value_1: 0.04443
	loss_reward_1: 0.00403
	loss_policy_2: 0.0218
	accuracy_policy_2: 0.95238
	loss_value_2: 0.045
	loss_reward_2: 0.00451
	loss_policy_3: 0.02215
	accuracy_policy_3: 0.9527
	loss_value_3: 0.04587
	loss_reward_3: 0.00511
	loss_policy_4: 0.02162
	accuracy_policy_4: 0.95598
	loss_value_4: 0.04666
	loss_reward_4: 0.00608
	loss_policy_5: 0.0215
	accuracy_policy_5: 0.96285
	loss_value_5: 0.04749
	loss_reward_5: 0.00708
	loss_policy: 0.22079
	loss_value: 0.45237
	loss_reward: 0.02681
[2025-05-11 18:26:31] nn step 46150, lr: 0.1.
	loss_policy_0: 0.11839
	accuracy_policy_0: 0.95605
	loss_value_0: 0.23829
	loss_policy_1: 0.02326
	accuracy_policy_1: 0.95051
	loss_value_1: 0.04762
	loss_reward_1: 0.00442
	loss_policy_2: 0.02347
	accuracy_policy_2: 0.94949
	loss_value_2: 0.04825
	loss_reward_2: 0.00485
	loss_policy_3: 0.02351
	accuracy_policy_3: 0.95383
	loss_value_3: 0.04884
	loss_reward_3: 0.00551
	loss_policy_4: 0.02341
	accuracy_policy_4: 0.955
	loss_value_4: 0.04961
	loss_reward_4: 0.00637
	loss_policy_5: 0.02311
	accuracy_policy_5: 0.96262
	loss_value_5: 0.05061
	loss_reward_5: 0.00771
	loss_policy: 0.23514
	loss_value: 0.48322
	loss_reward: 0.02886
[2025-05-11 18:26:40] nn step 46200, lr: 0.1.
	loss_policy_0: 0.11205
	accuracy_policy_0: 0.95508
	loss_value_0: 0.22034
	loss_policy_1: 0.02232
	accuracy_policy_1: 0.95055
	loss_value_1: 0.04405
	loss_reward_1: 0.00436
	loss_policy_2: 0.02234
	accuracy_policy_2: 0.95234
	loss_value_2: 0.04479
	loss_reward_2: 0.00466
	loss_policy_3: 0.02227
	accuracy_policy_3: 0.95418
	loss_value_3: 0.04546
	loss_reward_3: 0.00497
	loss_policy_4: 0.02217
	accuracy_policy_4: 0.95379
	loss_value_4: 0.04641
	loss_reward_4: 0.00646
	loss_policy_5: 0.02212
	accuracy_policy_5: 0.95863
	loss_value_5: 0.04746
	loss_reward_5: 0.0075
	loss_policy: 0.22327
	loss_value: 0.44851
	loss_reward: 0.02794
Optimization_Done 46200
[2025-05-11 18:28:16] [command] train weight_iter_46200.pkl 213 232
[2025-05-11 18:28:24] nn step 46250, lr: 0.1.
	loss_policy_0: 0.11611
	accuracy_policy_0: 0.95785
	loss_value_0: 0.24837
	loss_policy_1: 0.02243
	accuracy_policy_1: 0.95461
	loss_value_1: 0.04942
	loss_reward_1: 0.00415
	loss_policy_2: 0.02281
	accuracy_policy_2: 0.95066
	loss_value_2: 0.0501
	loss_reward_2: 0.00502
	loss_policy_3: 0.02273
	accuracy_policy_3: 0.9498
	loss_value_3: 0.05076
	loss_reward_3: 0.0058
	loss_policy_4: 0.02278
	accuracy_policy_4: 0.95277
	loss_value_4: 0.05173
	loss_reward_4: 0.00678
	loss_policy_5: 0.02262
	accuracy_policy_5: 0.9607
	loss_value_5: 0.05258
	loss_reward_5: 0.00772
	loss_policy: 0.22947
	loss_value: 0.50295
	loss_reward: 0.02946
[2025-05-11 18:28:32] nn step 46300, lr: 0.1.
	loss_policy_0: 0.11628
	accuracy_policy_0: 0.95629
	loss_value_0: 0.24081
	loss_policy_1: 0.02337
	accuracy_policy_1: 0.95074
	loss_value_1: 0.04789
	loss_reward_1: 0.00439
	loss_policy_2: 0.02305
	accuracy_policy_2: 0.95387
	loss_value_2: 0.04834
	loss_reward_2: 0.00484
	loss_policy_3: 0.02321
	accuracy_policy_3: 0.95449
	loss_value_3: 0.04886
	loss_reward_3: 0.00539
	loss_policy_4: 0.02334
	accuracy_policy_4: 0.955
	loss_value_4: 0.0497
	loss_reward_4: 0.00608
	loss_policy_5: 0.02297
	accuracy_policy_5: 0.96109
	loss_value_5: 0.05088
	loss_reward_5: 0.00785
	loss_policy: 0.23223
	loss_value: 0.48648
	loss_reward: 0.02855
[2025-05-11 18:28:41] nn step 46350, lr: 0.1.
	loss_policy_0: 0.1112
	accuracy_policy_0: 0.95477
	loss_value_0: 0.22405
	loss_policy_1: 0.02201
	accuracy_policy_1: 0.95191
	loss_value_1: 0.04459
	loss_reward_1: 0.00419
	loss_policy_2: 0.02168
	accuracy_policy_2: 0.95164
	loss_value_2: 0.04521
	loss_reward_2: 0.00467
	loss_policy_3: 0.02168
	accuracy_policy_3: 0.95328
	loss_value_3: 0.04571
	loss_reward_3: 0.00522
	loss_policy_4: 0.02177
	accuracy_policy_4: 0.95504
	loss_value_4: 0.04634
	loss_reward_4: 0.0061
	loss_policy_5: 0.02155
	accuracy_policy_5: 0.96293
	loss_value_5: 0.04741
	loss_reward_5: 0.00722
	loss_policy: 0.21989
	loss_value: 0.45331
	loss_reward: 0.02739
[2025-05-11 18:28:49] nn step 46400, lr: 0.1.
	loss_policy_0: 0.11692
	accuracy_policy_0: 0.95594
	loss_value_0: 0.2394
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.95211
	loss_value_1: 0.04809
	loss_reward_1: 0.00453
	loss_policy_2: 0.02327
	accuracy_policy_2: 0.95266
	loss_value_2: 0.04859
	loss_reward_2: 0.00493
	loss_policy_3: 0.02376
	accuracy_policy_3: 0.95156
	loss_value_3: 0.0493
	loss_reward_3: 0.00562
	loss_policy_4: 0.02379
	accuracy_policy_4: 0.95309
	loss_value_4: 0.05002
	loss_reward_4: 0.00658
	loss_policy_5: 0.02325
	accuracy_policy_5: 0.96195
	loss_value_5: 0.05119
	loss_reward_5: 0.00779
	loss_policy: 0.23455
	loss_value: 0.48658
	loss_reward: 0.02943
Optimization_Done 46400
[2025-05-11 18:30:23] [command] train weight_iter_46400.pkl 214 233
[2025-05-11 18:30:33] nn step 46450, lr: 0.1.
	loss_policy_0: 0.10781
	accuracy_policy_0: 0.95781
	loss_value_0: 0.24049
	loss_policy_1: 0.02166
	accuracy_policy_1: 0.95184
	loss_value_1: 0.04772
	loss_reward_1: 0.00397
	loss_policy_2: 0.02149
	accuracy_policy_2: 0.9543
	loss_value_2: 0.04844
	loss_reward_2: 0.00446
	loss_policy_3: 0.02122
	accuracy_policy_3: 0.95504
	loss_value_3: 0.04896
	loss_reward_3: 0.00515
	loss_policy_4: 0.02134
	accuracy_policy_4: 0.95637
	loss_value_4: 0.04949
	loss_reward_4: 0.00625
	loss_policy_5: 0.02083
	accuracy_policy_5: 0.96246
	loss_value_5: 0.05033
	loss_reward_5: 0.00738
	loss_policy: 0.21434
	loss_value: 0.48543
	loss_reward: 0.02721
[2025-05-11 18:30:41] nn step 46500, lr: 0.1.
	loss_policy_0: 0.11487
	accuracy_policy_0: 0.95652
	loss_value_0: 0.24368
	loss_policy_1: 0.0231
	accuracy_policy_1: 0.95289
	loss_value_1: 0.04865
	loss_reward_1: 0.00463
	loss_policy_2: 0.023
	accuracy_policy_2: 0.95297
	loss_value_2: 0.04917
	loss_reward_2: 0.00491
	loss_policy_3: 0.02296
	accuracy_policy_3: 0.95332
	loss_value_3: 0.0499
	loss_reward_3: 0.00551
	loss_policy_4: 0.02309
	accuracy_policy_4: 0.95668
	loss_value_4: 0.05079
	loss_reward_4: 0.0069
	loss_policy_5: 0.02299
	accuracy_policy_5: 0.96375
	loss_value_5: 0.05166
	loss_reward_5: 0.00773
	loss_policy: 0.23
	loss_value: 0.49385
	loss_reward: 0.02968
[2025-05-11 18:30:48] nn step 46550, lr: 0.1.
	loss_policy_0: 0.11522
	accuracy_policy_0: 0.95531
	loss_value_0: 0.23759
	loss_policy_1: 0.02289
	accuracy_policy_1: 0.95246
	loss_value_1: 0.04785
	loss_reward_1: 0.0044
	loss_policy_2: 0.023
	accuracy_policy_2: 0.9507
	loss_value_2: 0.04853
	loss_reward_2: 0.0049
	loss_policy_3: 0.02279
	accuracy_policy_3: 0.9493
	loss_value_3: 0.0492
	loss_reward_3: 0.00537
	loss_policy_4: 0.02287
	accuracy_policy_4: 0.955
	loss_value_4: 0.05034
	loss_reward_4: 0.00655
	loss_policy_5: 0.02286
	accuracy_policy_5: 0.96055
	loss_value_5: 0.05153
	loss_reward_5: 0.00802
	loss_policy: 0.22963
	loss_value: 0.48504
	loss_reward: 0.02925
[2025-05-11 18:30:57] nn step 46600, lr: 0.1.
	loss_policy_0: 0.1104
	accuracy_policy_0: 0.95316
	loss_value_0: 0.22973
	loss_policy_1: 0.02219
	accuracy_policy_1: 0.94809
	loss_value_1: 0.04586
	loss_reward_1: 0.00425
	loss_policy_2: 0.02227
	accuracy_policy_2: 0.95148
	loss_value_2: 0.04627
	loss_reward_2: 0.00493
	loss_policy_3: 0.02207
	accuracy_policy_3: 0.95195
	loss_value_3: 0.0468
	loss_reward_3: 0.00538
	loss_policy_4: 0.02259
	accuracy_policy_4: 0.95102
	loss_value_4: 0.04767
	loss_reward_4: 0.00654
	loss_policy_5: 0.02195
	accuracy_policy_5: 0.96352
	loss_value_5: 0.04868
	loss_reward_5: 0.00774
	loss_policy: 0.22147
	loss_value: 0.46502
	loss_reward: 0.02884
Optimization_Done 46600
[2025-05-11 18:32:31] [command] train weight_iter_46600.pkl 215 234
[2025-05-11 18:32:39] nn step 46650, lr: 0.1.
	loss_policy_0: 0.1126
	accuracy_policy_0: 0.95504
	loss_value_0: 0.24826
	loss_policy_1: 0.02243
	accuracy_policy_1: 0.95301
	loss_value_1: 0.04975
	loss_reward_1: 0.00439
	loss_policy_2: 0.02262
	accuracy_policy_2: 0.9518
	loss_value_2: 0.05003
	loss_reward_2: 0.00475
	loss_policy_3: 0.02287
	accuracy_policy_3: 0.9527
	loss_value_3: 0.05066
	loss_reward_3: 0.00543
	loss_policy_4: 0.02244
	accuracy_policy_4: 0.9541
	loss_value_4: 0.05137
	loss_reward_4: 0.00662
	loss_policy_5: 0.02227
	accuracy_policy_5: 0.96445
	loss_value_5: 0.05245
	loss_reward_5: 0.00773
	loss_policy: 0.22524
	loss_value: 0.50251
	loss_reward: 0.02892
[2025-05-11 18:32:48] nn step 46700, lr: 0.1.
	loss_policy_0: 0.11287
	accuracy_policy_0: 0.95441
	loss_value_0: 0.24145
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.94938
	loss_value_1: 0.0481
	loss_reward_1: 0.0045
	loss_policy_2: 0.0227
	accuracy_policy_2: 0.95262
	loss_value_2: 0.04883
	loss_reward_2: 0.00501
	loss_policy_3: 0.02266
	accuracy_policy_3: 0.9516
	loss_value_3: 0.04965
	loss_reward_3: 0.00545
	loss_policy_4: 0.0228
	accuracy_policy_4: 0.95328
	loss_value_4: 0.05043
	loss_reward_4: 0.00648
	loss_policy_5: 0.02238
	accuracy_policy_5: 0.9627
	loss_value_5: 0.0516
	loss_reward_5: 0.00792
	loss_policy: 0.22607
	loss_value: 0.49004
	loss_reward: 0.02937
[2025-05-11 18:32:56] nn step 46750, lr: 0.1.
	loss_policy_0: 0.11463
	accuracy_policy_0: 0.95582
	loss_value_0: 0.23947
	loss_policy_1: 0.02302
	accuracy_policy_1: 0.9516
	loss_value_1: 0.04777
	loss_reward_1: 0.00469
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95094
	loss_value_2: 0.04852
	loss_reward_2: 0.00499
	loss_policy_3: 0.02279
	accuracy_policy_3: 0.95262
	loss_value_3: 0.04942
	loss_reward_3: 0.00556
	loss_policy_4: 0.02276
	accuracy_policy_4: 0.95426
	loss_value_4: 0.05031
	loss_reward_4: 0.00691
	loss_policy_5: 0.02253
	accuracy_policy_5: 0.96309
	loss_value_5: 0.05114
	loss_reward_5: 0.00777
	loss_policy: 0.22883
	loss_value: 0.48663
	loss_reward: 0.02991
[2025-05-11 18:33:05] nn step 46800, lr: 0.1.
	loss_policy_0: 0.11248
	accuracy_policy_0: 0.9543
	loss_value_0: 0.23817
	loss_policy_1: 0.02263
	accuracy_policy_1: 0.9493
	loss_value_1: 0.04769
	loss_reward_1: 0.00452
	loss_policy_2: 0.02257
	accuracy_policy_2: 0.95051
	loss_value_2: 0.04809
	loss_reward_2: 0.00514
	loss_policy_3: 0.02277
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04885
	loss_reward_3: 0.0059
	loss_policy_4: 0.02267
	accuracy_policy_4: 0.95449
	loss_value_4: 0.04933
	loss_reward_4: 0.00715
	loss_policy_5: 0.02272
	accuracy_policy_5: 0.96008
	loss_value_5: 0.05055
	loss_reward_5: 0.00805
	loss_policy: 0.22584
	loss_value: 0.48267
	loss_reward: 0.03076
Optimization_Done 46800
[2025-05-11 18:34:44] [command] train weight_iter_46800.pkl 216 235
[2025-05-11 18:34:53] nn step 46850, lr: 0.1.
	loss_policy_0: 0.10524
	accuracy_policy_0: 0.95617
	loss_value_0: 0.2463
	loss_policy_1: 0.02097
	accuracy_policy_1: 0.95211
	loss_value_1: 0.04887
	loss_reward_1: 0.00416
	loss_policy_2: 0.02122
	accuracy_policy_2: 0.95074
	loss_value_2: 0.0491
	loss_reward_2: 0.00454
	loss_policy_3: 0.02108
	accuracy_policy_3: 0.95098
	loss_value_3: 0.04984
	loss_reward_3: 0.00525
	loss_policy_4: 0.02097
	accuracy_policy_4: 0.95473
	loss_value_4: 0.05031
	loss_reward_4: 0.00646
	loss_policy_5: 0.02092
	accuracy_policy_5: 0.95973
	loss_value_5: 0.05121
	loss_reward_5: 0.00717
	loss_policy: 0.2104
	loss_value: 0.49562
	loss_reward: 0.02758
[2025-05-11 18:35:02] nn step 46900, lr: 0.1.
	loss_policy_0: 0.1129
	accuracy_policy_0: 0.95215
	loss_value_0: 0.24426
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.94883
	loss_value_1: 0.04859
	loss_reward_1: 0.00448
	loss_policy_2: 0.0226
	accuracy_policy_2: 0.9484
	loss_value_2: 0.04939
	loss_reward_2: 0.00518
	loss_policy_3: 0.02302
	accuracy_policy_3: 0.9484
	loss_value_3: 0.04994
	loss_reward_3: 0.00593
	loss_policy_4: 0.02281
	accuracy_policy_4: 0.95277
	loss_value_4: 0.05102
	loss_reward_4: 0.0069
	loss_policy_5: 0.02228
	accuracy_policy_5: 0.96211
	loss_value_5: 0.05227
	loss_reward_5: 0.00809
	loss_policy: 0.22626
	loss_value: 0.49547
	loss_reward: 0.03058
[2025-05-11 18:35:09] nn step 46950, lr: 0.1.
	loss_policy_0: 0.11384
	accuracy_policy_0: 0.95051
	loss_value_0: 0.24033
	loss_policy_1: 0.02279
	accuracy_policy_1: 0.95023
	loss_value_1: 0.04795
	loss_reward_1: 0.00458
	loss_policy_2: 0.02284
	accuracy_policy_2: 0.95043
	loss_value_2: 0.04848
	loss_reward_2: 0.00516
	loss_policy_3: 0.02289
	accuracy_policy_3: 0.9525
	loss_value_3: 0.04938
	loss_reward_3: 0.00566
	loss_policy_4: 0.02278
	accuracy_policy_4: 0.95137
	loss_value_4: 0.04994
	loss_reward_4: 0.00672
	loss_policy_5: 0.02254
	accuracy_policy_5: 0.96258
	loss_value_5: 0.05082
	loss_reward_5: 0.00781
	loss_policy: 0.22768
	loss_value: 0.4869
	loss_reward: 0.02993
[2025-05-11 18:35:17] nn step 47000, lr: 0.1.
	loss_policy_0: 0.10821
	accuracy_policy_0: 0.95398
	loss_value_0: 0.22344
	loss_policy_1: 0.02139
	accuracy_policy_1: 0.94891
	loss_value_1: 0.04447
	loss_reward_1: 0.00422
	loss_policy_2: 0.02179
	accuracy_policy_2: 0.9493
	loss_value_2: 0.04493
	loss_reward_2: 0.00458
	loss_policy_3: 0.02185
	accuracy_policy_3: 0.94895
	loss_value_3: 0.0457
	loss_reward_3: 0.00523
	loss_policy_4: 0.02183
	accuracy_policy_4: 0.9516
	loss_value_4: 0.04662
	loss_reward_4: 0.00636
	loss_policy_5: 0.02151
	accuracy_policy_5: 0.96277
	loss_value_5: 0.04763
	loss_reward_5: 0.00704
	loss_policy: 0.21658
	loss_value: 0.45279
	loss_reward: 0.02743
Optimization_Done 47000
[2025-05-11 18:36:52] [command] train weight_iter_47000.pkl 217 236
[2025-05-11 18:37:00] nn step 47050, lr: 0.1.
	loss_policy_0: 0.10644
	accuracy_policy_0: 0.95074
	loss_value_0: 0.22784
	loss_policy_1: 0.02148
	accuracy_policy_1: 0.95059
	loss_value_1: 0.04541
	loss_reward_1: 0.0042
	loss_policy_2: 0.02164
	accuracy_policy_2: 0.95039
	loss_value_2: 0.04579
	loss_reward_2: 0.00463
	loss_policy_3: 0.02136
	accuracy_policy_3: 0.95301
	loss_value_3: 0.04678
	loss_reward_3: 0.00499
	loss_policy_4: 0.02137
	accuracy_policy_4: 0.95457
	loss_value_4: 0.04706
	loss_reward_4: 0.00606
	loss_policy_5: 0.02138
	accuracy_policy_5: 0.95832
	loss_value_5: 0.04791
	loss_reward_5: 0.00694
	loss_policy: 0.21366
	loss_value: 0.46078
	loss_reward: 0.02683
[2025-05-11 18:37:08] nn step 47100, lr: 0.1.
	loss_policy_0: 0.11142
	accuracy_policy_0: 0.95203
	loss_value_0: 0.2296
	loss_policy_1: 0.02238
	accuracy_policy_1: 0.94691
	loss_value_1: 0.04592
	loss_reward_1: 0.00426
	loss_policy_2: 0.0226
	accuracy_policy_2: 0.95109
	loss_value_2: 0.0464
	loss_reward_2: 0.00492
	loss_policy_3: 0.02234
	accuracy_policy_3: 0.95117
	loss_value_3: 0.0471
	loss_reward_3: 0.00528
	loss_policy_4: 0.0224
	accuracy_policy_4: 0.95203
	loss_value_4: 0.04759
	loss_reward_4: 0.00644
	loss_policy_5: 0.02221
	accuracy_policy_5: 0.96074
	loss_value_5: 0.04855
	loss_reward_5: 0.00767
	loss_policy: 0.22335
	loss_value: 0.46516
	loss_reward: 0.02858
[2025-05-11 18:37:17] nn step 47150, lr: 0.1.
	loss_policy_0: 0.11242
	accuracy_policy_0: 0.95285
	loss_value_0: 0.23209
	loss_policy_1: 0.02259
	accuracy_policy_1: 0.9482
	loss_value_1: 0.04644
	loss_reward_1: 0.00443
	loss_policy_2: 0.02289
	accuracy_policy_2: 0.94723
	loss_value_2: 0.04689
	loss_reward_2: 0.00483
	loss_policy_3: 0.02308
	accuracy_policy_3: 0.94879
	loss_value_3: 0.04764
	loss_reward_3: 0.00518
	loss_policy_4: 0.0229
	accuracy_policy_4: 0.95039
	loss_value_4: 0.04788
	loss_reward_4: 0.00654
	loss_policy_5: 0.02245
	accuracy_policy_5: 0.95977
	loss_value_5: 0.04892
	loss_reward_5: 0.00767
	loss_policy: 0.22633
	loss_value: 0.46986
	loss_reward: 0.02865
[2025-05-11 18:37:24] nn step 47200, lr: 0.1.
	loss_policy_0: 0.11539
	accuracy_policy_0: 0.95219
	loss_value_0: 0.23596
	loss_policy_1: 0.02299
	accuracy_policy_1: 0.94855
	loss_value_1: 0.04704
	loss_reward_1: 0.00449
	loss_policy_2: 0.02276
	accuracy_policy_2: 0.94957
	loss_value_2: 0.04777
	loss_reward_2: 0.0048
	loss_policy_3: 0.023
	accuracy_policy_3: 0.94941
	loss_value_3: 0.04815
	loss_reward_3: 0.00522
	loss_policy_4: 0.0231
	accuracy_policy_4: 0.95168
	loss_value_4: 0.04903
	loss_reward_4: 0.00654
	loss_policy_5: 0.023
	accuracy_policy_5: 0.95949
	loss_value_5: 0.05019
	loss_reward_5: 0.00758
	loss_policy: 0.23025
	loss_value: 0.47814
	loss_reward: 0.02864
Optimization_Done 47200
[2025-05-11 18:39:02] [command] train weight_iter_47200.pkl 218 237
[2025-05-11 18:39:12] nn step 47250, lr: 0.1.
	loss_policy_0: 0.11434
	accuracy_policy_0: 0.95043
	loss_value_0: 0.23879
	loss_policy_1: 0.02251
	accuracy_policy_1: 0.95016
	loss_value_1: 0.0476
	loss_reward_1: 0.0043
	loss_policy_2: 0.02276
	accuracy_policy_2: 0.94652
	loss_value_2: 0.04824
	loss_reward_2: 0.00499
	loss_policy_3: 0.02259
	accuracy_policy_3: 0.94984
	loss_value_3: 0.04871
	loss_reward_3: 0.00551
	loss_policy_4: 0.02266
	accuracy_policy_4: 0.95145
	loss_value_4: 0.0493
	loss_reward_4: 0.00658
	loss_policy_5: 0.0228
	accuracy_policy_5: 0.96047
	loss_value_5: 0.05057
	loss_reward_5: 0.00764
	loss_policy: 0.22766
	loss_value: 0.48321
	loss_reward: 0.02902
[2025-05-11 18:39:19] nn step 47300, lr: 0.1.
	loss_policy_0: 0.11424
	accuracy_policy_0: 0.95285
	loss_value_0: 0.23412
	loss_policy_1: 0.02322
	accuracy_policy_1: 0.94648
	loss_value_1: 0.04687
	loss_reward_1: 0.00444
	loss_policy_2: 0.02286
	accuracy_policy_2: 0.94949
	loss_value_2: 0.04765
	loss_reward_2: 0.00486
	loss_policy_3: 0.02307
	accuracy_policy_3: 0.94965
	loss_value_3: 0.048
	loss_reward_3: 0.00552
	loss_policy_4: 0.02307
	accuracy_policy_4: 0.95324
	loss_value_4: 0.049
	loss_reward_4: 0.00653
	loss_policy_5: 0.02311
	accuracy_policy_5: 0.96316
	loss_value_5: 0.05001
	loss_reward_5: 0.00765
	loss_policy: 0.22956
	loss_value: 0.47566
	loss_reward: 0.029
[2025-05-11 18:39:27] nn step 47350, lr: 0.1.
	loss_policy_0: 0.10878
	accuracy_policy_0: 0.95258
	loss_value_0: 0.22082
	loss_policy_1: 0.02172
	accuracy_policy_1: 0.95027
	loss_value_1: 0.04412
	loss_reward_1: 0.00415
	loss_policy_2: 0.02162
	accuracy_policy_2: 0.94941
	loss_value_2: 0.04448
	loss_reward_2: 0.00479
	loss_policy_3: 0.02189
	accuracy_policy_3: 0.95074
	loss_value_3: 0.04508
	loss_reward_3: 0.00546
	loss_policy_4: 0.02186
	accuracy_policy_4: 0.9523
	loss_value_4: 0.04589
	loss_reward_4: 0.00615
	loss_policy_5: 0.02145
	accuracy_policy_5: 0.96117
	loss_value_5: 0.04689
	loss_reward_5: 0.00714
	loss_policy: 0.21731
	loss_value: 0.44727
	loss_reward: 0.02769
[2025-05-11 18:39:36] nn step 47400, lr: 0.1.
	loss_policy_0: 0.121
	accuracy_policy_0: 0.95297
	loss_value_0: 0.24932
	loss_policy_1: 0.02412
	accuracy_policy_1: 0.95172
	loss_value_1: 0.04979
	loss_reward_1: 0.00487
	loss_policy_2: 0.02449
	accuracy_policy_2: 0.94973
	loss_value_2: 0.05058
	loss_reward_2: 0.00543
	loss_policy_3: 0.0244
	accuracy_policy_3: 0.95043
	loss_value_3: 0.05145
	loss_reward_3: 0.00593
	loss_policy_4: 0.02445
	accuracy_policy_4: 0.95285
	loss_value_4: 0.05224
	loss_reward_4: 0.00723
	loss_policy_5: 0.02401
	accuracy_policy_5: 0.96223
	loss_value_5: 0.05294
	loss_reward_5: 0.00868
	loss_policy: 0.24247
	loss_value: 0.50631
	loss_reward: 0.03214
Optimization_Done 47400
[2025-05-11 18:41:11] [command] train weight_iter_47400.pkl 219 238
[2025-05-11 18:41:20] nn step 47450, lr: 0.1.
	loss_policy_0: 0.10493
	accuracy_policy_0: 0.95547
	loss_value_0: 0.22416
	loss_policy_1: 0.02102
	accuracy_policy_1: 0.95078
	loss_value_1: 0.04473
	loss_reward_1: 0.00425
	loss_policy_2: 0.02109
	accuracy_policy_2: 0.95227
	loss_value_2: 0.045
	loss_reward_2: 0.00452
	loss_policy_3: 0.02108
	accuracy_policy_3: 0.95492
	loss_value_3: 0.04559
	loss_reward_3: 0.00505
	loss_policy_4: 0.02106
	accuracy_policy_4: 0.95859
	loss_value_4: 0.04621
	loss_reward_4: 0.00619
	loss_policy_5: 0.0208
	accuracy_policy_5: 0.96379
	loss_value_5: 0.04707
	loss_reward_5: 0.0069
	loss_policy: 0.20999
	loss_value: 0.45276
	loss_reward: 0.02691
[2025-05-11 18:41:29] nn step 47500, lr: 0.1.
	loss_policy_0: 0.11224
	accuracy_policy_0: 0.95305
	loss_value_0: 0.23639
	loss_policy_1: 0.02277
	accuracy_policy_1: 0.94707
	loss_value_1: 0.04734
	loss_reward_1: 0.0044
	loss_policy_2: 0.02273
	accuracy_policy_2: 0.95
	loss_value_2: 0.04782
	loss_reward_2: 0.00488
	loss_policy_3: 0.02246
	accuracy_policy_3: 0.95477
	loss_value_3: 0.04819
	loss_reward_3: 0.00541
	loss_policy_4: 0.02237
	accuracy_policy_4: 0.95289
	loss_value_4: 0.04902
	loss_reward_4: 0.00678
	loss_policy_5: 0.02235
	accuracy_policy_5: 0.96113
	loss_value_5: 0.04959
	loss_reward_5: 0.00785
	loss_policy: 0.22492
	loss_value: 0.47836
	loss_reward: 0.02933
[2025-05-11 18:41:38] nn step 47550, lr: 0.1.
	loss_policy_0: 0.11309
	accuracy_policy_0: 0.95438
	loss_value_0: 0.23416
	loss_policy_1: 0.02257
	accuracy_policy_1: 0.95055
	loss_value_1: 0.04678
	loss_reward_1: 0.00448
	loss_policy_2: 0.02249
	accuracy_policy_2: 0.9498
	loss_value_2: 0.04742
	loss_reward_2: 0.00493
	loss_policy_3: 0.02258
	accuracy_policy_3: 0.95121
	loss_value_3: 0.04787
	loss_reward_3: 0.0053
	loss_policy_4: 0.02253
	accuracy_policy_4: 0.9541
	loss_value_4: 0.04861
	loss_reward_4: 0.00648
	loss_policy_5: 0.02237
	accuracy_policy_5: 0.9607
	loss_value_5: 0.04953
	loss_reward_5: 0.00782
	loss_policy: 0.22562
	loss_value: 0.47438
	loss_reward: 0.02902
[2025-05-11 18:41:45] nn step 47600, lr: 0.1.
	loss_policy_0: 0.10762
	accuracy_policy_0: 0.95254
	loss_value_0: 0.21928
	loss_policy_1: 0.02164
	accuracy_policy_1: 0.95113
	loss_value_1: 0.04382
	loss_reward_1: 0.0042
	loss_policy_2: 0.02169
	accuracy_policy_2: 0.95211
	loss_value_2: 0.04434
	loss_reward_2: 0.00477
	loss_policy_3: 0.02185
	accuracy_policy_3: 0.94926
	loss_value_3: 0.04477
	loss_reward_3: 0.00518
	loss_policy_4: 0.02174
	accuracy_policy_4: 0.95309
	loss_value_4: 0.04538
	loss_reward_4: 0.00629
	loss_policy_5: 0.02166
	accuracy_policy_5: 0.96148
	loss_value_5: 0.04619
	loss_reward_5: 0.00725
	loss_policy: 0.2162
	loss_value: 0.44378
	loss_reward: 0.0277
Optimization_Done 47600
[2025-05-11 18:43:20] [command] train weight_iter_47600.pkl 220 239
[2025-05-11 18:43:30] nn step 47650, lr: 0.1.
	loss_policy_0: 0.11934
	accuracy_policy_0: 0.95461
	loss_value_0: 0.25317
	loss_policy_1: 0.02392
	accuracy_policy_1: 0.95078
	loss_value_1: 0.05051
	loss_reward_1: 0.00462
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.95016
	loss_value_2: 0.05106
	loss_reward_2: 0.00519
	loss_policy_3: 0.02403
	accuracy_policy_3: 0.95316
	loss_value_3: 0.05168
	loss_reward_3: 0.00578
	loss_policy_4: 0.02392
	accuracy_policy_4: 0.95242
	loss_value_4: 0.05232
	loss_reward_4: 0.00719
	loss_policy_5: 0.02382
	accuracy_policy_5: 0.96102
	loss_value_5: 0.05334
	loss_reward_5: 0.00798
	loss_policy: 0.2392
	loss_value: 0.51207
	loss_reward: 0.03076
[2025-05-11 18:43:37] nn step 47700, lr: 0.1.
	loss_policy_0: 0.11002
	accuracy_policy_0: 0.95586
	loss_value_0: 0.22841
	loss_policy_1: 0.02194
	accuracy_policy_1: 0.94988
	loss_value_1: 0.04551
	loss_reward_1: 0.00435
	loss_policy_2: 0.02234
	accuracy_policy_2: 0.94934
	loss_value_2: 0.04582
	loss_reward_2: 0.00474
	loss_policy_3: 0.02187
	accuracy_policy_3: 0.94977
	loss_value_3: 0.04656
	loss_reward_3: 0.00529
	loss_policy_4: 0.02237
	accuracy_policy_4: 0.9507
	loss_value_4: 0.04727
	loss_reward_4: 0.00626
	loss_policy_5: 0.02169
	accuracy_policy_5: 0.96422
	loss_value_5: 0.04838
	loss_reward_5: 0.00725
	loss_policy: 0.22022
	loss_value: 0.46195
	loss_reward: 0.0279
[2025-05-11 18:43:46] nn step 47750, lr: 0.1.
	loss_policy_0: 0.11858
	accuracy_policy_0: 0.95219
	loss_value_0: 0.24225
	loss_policy_1: 0.02359
	accuracy_policy_1: 0.94828
	loss_value_1: 0.04833
	loss_reward_1: 0.00463
	loss_policy_2: 0.02357
	accuracy_policy_2: 0.94859
	loss_value_2: 0.04891
	loss_reward_2: 0.00498
	loss_policy_3: 0.02389
	accuracy_policy_3: 0.94973
	loss_value_3: 0.04962
	loss_reward_3: 0.00559
	loss_policy_4: 0.02392
	accuracy_policy_4: 0.95164
	loss_value_4: 0.05018
	loss_reward_4: 0.0069
	loss_policy_5: 0.02373
	accuracy_policy_5: 0.96008
	loss_value_5: 0.05108
	loss_reward_5: 0.00773
	loss_policy: 0.23728
	loss_value: 0.49038
	loss_reward: 0.02983
[2025-05-11 18:43:54] nn step 47800, lr: 0.1.
	loss_policy_0: 0.11535
	accuracy_policy_0: 0.95594
	loss_value_0: 0.23968
	loss_policy_1: 0.02328
	accuracy_policy_1: 0.94965
	loss_value_1: 0.04783
	loss_reward_1: 0.00463
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.94914
	loss_value_2: 0.04866
	loss_reward_2: 0.00508
	loss_policy_3: 0.02363
	accuracy_policy_3: 0.95199
	loss_value_3: 0.04911
	loss_reward_3: 0.00556
	loss_policy_4: 0.02357
	accuracy_policy_4: 0.95371
	loss_value_4: 0.04978
	loss_reward_4: 0.00681
	loss_policy_5: 0.02335
	accuracy_policy_5: 0.96117
	loss_value_5: 0.05105
	loss_reward_5: 0.00795
	loss_policy: 0.23269
	loss_value: 0.48611
	loss_reward: 0.03004
Optimization_Done 47800
[2025-05-11 18:45:28] [command] train weight_iter_47800.pkl 221 240
[2025-05-11 18:45:37] nn step 47850, lr: 0.1.
	loss_policy_0: 0.11619
	accuracy_policy_0: 0.95656
	loss_value_0: 0.245
	loss_policy_1: 0.02278
	accuracy_policy_1: 0.95258
	loss_value_1: 0.04868
	loss_reward_1: 0.00457
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95062
	loss_value_2: 0.04916
	loss_reward_2: 0.00509
	loss_policy_3: 0.02327
	accuracy_policy_3: 0.95082
	loss_value_3: 0.04969
	loss_reward_3: 0.00566
	loss_policy_4: 0.02318
	accuracy_policy_4: 0.95273
	loss_value_4: 0.05025
	loss_reward_4: 0.00668
	loss_policy_5: 0.02315
	accuracy_policy_5: 0.96188
	loss_value_5: 0.05117
	loss_reward_5: 0.00798
	loss_policy: 0.23166
	loss_value: 0.49395
	loss_reward: 0.02998
[2025-05-11 18:45:46] nn step 47900, lr: 0.1.
	loss_policy_0: 0.12536
	accuracy_policy_0: 0.95496
	loss_value_0: 0.2564
	loss_policy_1: 0.02448
	accuracy_policy_1: 0.95145
	loss_value_1: 0.05089
	loss_reward_1: 0.00494
	loss_policy_2: 0.02473
	accuracy_policy_2: 0.95109
	loss_value_2: 0.05189
	loss_reward_2: 0.0054
	loss_policy_3: 0.02439
	accuracy_policy_3: 0.95309
	loss_value_3: 0.05252
	loss_reward_3: 0.00606
	loss_policy_4: 0.02444
	accuracy_policy_4: 0.95438
	loss_value_4: 0.05294
	loss_reward_4: 0.00751
	loss_policy_5: 0.02427
	accuracy_policy_5: 0.96152
	loss_value_5: 0.05388
	loss_reward_5: 0.00875
	loss_policy: 0.24768
	loss_value: 0.51853
	loss_reward: 0.03265
[2025-05-11 18:45:55] nn step 47950, lr: 0.1.
	loss_policy_0: 0.11645
	accuracy_policy_0: 0.9541
	loss_value_0: 0.23595
	loss_policy_1: 0.02298
	accuracy_policy_1: 0.95004
	loss_value_1: 0.04694
	loss_reward_1: 0.00461
	loss_policy_2: 0.02307
	accuracy_policy_2: 0.94992
	loss_value_2: 0.04741
	loss_reward_2: 0.00513
	loss_policy_3: 0.0232
	accuracy_policy_3: 0.95223
	loss_value_3: 0.04796
	loss_reward_3: 0.00571
	loss_policy_4: 0.02283
	accuracy_policy_4: 0.95461
	loss_value_4: 0.0485
	loss_reward_4: 0.00673
	loss_policy_5: 0.0227
	accuracy_policy_5: 0.96316
	loss_value_5: 0.04972
	loss_reward_5: 0.00786
	loss_policy: 0.23123
	loss_value: 0.47648
	loss_reward: 0.03005
[2025-05-11 18:46:01] nn step 48000, lr: 0.1.
	loss_policy_0: 0.1184
	accuracy_policy_0: 0.95387
	loss_value_0: 0.24372
	loss_policy_1: 0.0235
	accuracy_policy_1: 0.95145
	loss_value_1: 0.04862
	loss_reward_1: 0.00471
	loss_policy_2: 0.02352
	accuracy_policy_2: 0.95074
	loss_value_2: 0.04916
	loss_reward_2: 0.00524
	loss_policy_3: 0.02386
	accuracy_policy_3: 0.95
	loss_value_3: 0.04995
	loss_reward_3: 0.00555
	loss_policy_4: 0.02358
	accuracy_policy_4: 0.95227
	loss_value_4: 0.0504
	loss_reward_4: 0.00694
	loss_policy_5: 0.0233
	accuracy_policy_5: 0.96055
	loss_value_5: 0.05113
	loss_reward_5: 0.0078
	loss_policy: 0.23616
	loss_value: 0.49298
	loss_reward: 0.03024
Optimization_Done 48000
[2025-05-11 18:47:38] [command] train weight_iter_48000.pkl 222 241
[2025-05-11 18:47:48] nn step 48050, lr: 0.1.
	loss_policy_0: 0.11201
	accuracy_policy_0: 0.95363
	loss_value_0: 0.23343
	loss_policy_1: 0.02241
	accuracy_policy_1: 0.95191
	loss_value_1: 0.04649
	loss_reward_1: 0.00439
	loss_policy_2: 0.02245
	accuracy_policy_2: 0.9527
	loss_value_2: 0.04723
	loss_reward_2: 0.00488
	loss_policy_3: 0.02237
	accuracy_policy_3: 0.95453
	loss_value_3: 0.0476
	loss_reward_3: 0.00529
	loss_policy_4: 0.02225
	accuracy_policy_4: 0.95469
	loss_value_4: 0.04829
	loss_reward_4: 0.0063
	loss_policy_5: 0.02231
	accuracy_policy_5: 0.9627
	loss_value_5: 0.04927
	loss_reward_5: 0.00776
	loss_policy: 0.2238
	loss_value: 0.47231
	loss_reward: 0.02862
[2025-05-11 18:47:55] nn step 48100, lr: 0.1.
	loss_policy_0: 0.10906
	accuracy_policy_0: 0.95562
	loss_value_0: 0.21926
	loss_policy_1: 0.02146
	accuracy_policy_1: 0.95117
	loss_value_1: 0.0439
	loss_reward_1: 0.00434
	loss_policy_2: 0.02146
	accuracy_policy_2: 0.94949
	loss_value_2: 0.04448
	loss_reward_2: 0.00471
	loss_policy_3: 0.02149
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04495
	loss_reward_3: 0.00509
	loss_policy_4: 0.02167
	accuracy_policy_4: 0.95594
	loss_value_4: 0.04567
	loss_reward_4: 0.00604
	loss_policy_5: 0.02153
	accuracy_policy_5: 0.96344
	loss_value_5: 0.04628
	loss_reward_5: 0.00717
	loss_policy: 0.21667
	loss_value: 0.44453
	loss_reward: 0.02734
[2025-05-11 18:48:03] nn step 48150, lr: 0.1.
	loss_policy_0: 0.11383
	accuracy_policy_0: 0.95699
	loss_value_0: 0.23504
	loss_policy_1: 0.02297
	accuracy_policy_1: 0.94988
	loss_value_1: 0.04695
	loss_reward_1: 0.00435
	loss_policy_2: 0.0229
	accuracy_policy_2: 0.95133
	loss_value_2: 0.04756
	loss_reward_2: 0.00492
	loss_policy_3: 0.02303
	accuracy_policy_3: 0.95305
	loss_value_3: 0.04831
	loss_reward_3: 0.0052
	loss_policy_4: 0.02302
	accuracy_policy_4: 0.95551
	loss_value_4: 0.04898
	loss_reward_4: 0.00642
	loss_policy_5: 0.02286
	accuracy_policy_5: 0.96383
	loss_value_5: 0.04978
	loss_reward_5: 0.00768
	loss_policy: 0.22861
	loss_value: 0.47662
	loss_reward: 0.02856
[2025-05-11 18:48:12] nn step 48200, lr: 0.1.
	loss_policy_0: 0.11289
	accuracy_policy_0: 0.95656
	loss_value_0: 0.2296
	loss_policy_1: 0.02266
	accuracy_policy_1: 0.95227
	loss_value_1: 0.04572
	loss_reward_1: 0.00438
	loss_policy_2: 0.02284
	accuracy_policy_2: 0.95207
	loss_value_2: 0.04609
	loss_reward_2: 0.00474
	loss_policy_3: 0.02282
	accuracy_policy_3: 0.95191
	loss_value_3: 0.0468
	loss_reward_3: 0.00575
	loss_policy_4: 0.02274
	accuracy_policy_4: 0.9518
	loss_value_4: 0.04756
	loss_reward_4: 0.00657
	loss_policy_5: 0.02273
	accuracy_policy_5: 0.96199
	loss_value_5: 0.04843
	loss_reward_5: 0.00745
	loss_policy: 0.22668
	loss_value: 0.4642
	loss_reward: 0.02888
Optimization_Done 48200
[2025-05-11 18:49:51] [command] train weight_iter_48200.pkl 223 242
[2025-05-11 18:50:01] nn step 48250, lr: 0.1.
	loss_policy_0: 0.11505
	accuracy_policy_0: 0.95621
	loss_value_0: 0.24242
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.9493
	loss_value_1: 0.04846
	loss_reward_1: 0.00447
	loss_policy_2: 0.02287
	accuracy_policy_2: 0.95266
	loss_value_2: 0.04874
	loss_reward_2: 0.00513
	loss_policy_3: 0.02293
	accuracy_policy_3: 0.95531
	loss_value_3: 0.04915
	loss_reward_3: 0.00572
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.95551
	loss_value_4: 0.04989
	loss_reward_4: 0.00695
	loss_policy_5: 0.0225
	accuracy_policy_5: 0.96258
	loss_value_5: 0.05066
	loss_reward_5: 0.00788
	loss_policy: 0.22906
	loss_value: 0.48932
	loss_reward: 0.03015
[2025-05-11 18:50:09] nn step 48300, lr: 0.1.
	loss_policy_0: 0.11765
	accuracy_policy_0: 0.95391
	loss_value_0: 0.24654
	loss_policy_1: 0.0236
	accuracy_policy_1: 0.9491
	loss_value_1: 0.04898
	loss_reward_1: 0.00465
	loss_policy_2: 0.02358
	accuracy_policy_2: 0.94996
	loss_value_2: 0.0494
	loss_reward_2: 0.0053
	loss_policy_3: 0.02368
	accuracy_policy_3: 0.95277
	loss_value_3: 0.04984
	loss_reward_3: 0.00553
	loss_policy_4: 0.02357
	accuracy_policy_4: 0.95711
	loss_value_4: 0.05024
	loss_reward_4: 0.00681
	loss_policy_5: 0.02349
	accuracy_policy_5: 0.96273
	loss_value_5: 0.0511
	loss_reward_5: 0.00806
	loss_policy: 0.23557
	loss_value: 0.4961
	loss_reward: 0.03034
[2025-05-11 18:50:16] nn step 48350, lr: 0.1.
	loss_policy_0: 0.11202
	accuracy_policy_0: 0.95566
	loss_value_0: 0.23045
	loss_policy_1: 0.02207
	accuracy_policy_1: 0.94879
	loss_value_1: 0.04586
	loss_reward_1: 0.00433
	loss_policy_2: 0.02222
	accuracy_policy_2: 0.95031
	loss_value_2: 0.04635
	loss_reward_2: 0.00494
	loss_policy_3: 0.0225
	accuracy_policy_3: 0.95348
	loss_value_3: 0.04695
	loss_reward_3: 0.00554
	loss_policy_4: 0.02243
	accuracy_policy_4: 0.95348
	loss_value_4: 0.04743
	loss_reward_4: 0.00671
	loss_policy_5: 0.02239
	accuracy_policy_5: 0.96203
	loss_value_5: 0.04848
	loss_reward_5: 0.00764
	loss_policy: 0.22363
	loss_value: 0.46551
	loss_reward: 0.02916
[2025-05-11 18:50:25] nn step 48400, lr: 0.1.
	loss_policy_0: 0.11648
	accuracy_policy_0: 0.95508
	loss_value_0: 0.23822
	loss_policy_1: 0.02304
	accuracy_policy_1: 0.95316
	loss_value_1: 0.04772
	loss_reward_1: 0.00471
	loss_policy_2: 0.02359
	accuracy_policy_2: 0.9527
	loss_value_2: 0.04834
	loss_reward_2: 0.00521
	loss_policy_3: 0.02316
	accuracy_policy_3: 0.95418
	loss_value_3: 0.04902
	loss_reward_3: 0.00538
	loss_policy_4: 0.0235
	accuracy_policy_4: 0.9548
	loss_value_4: 0.04974
	loss_reward_4: 0.00666
	loss_policy_5: 0.0231
	accuracy_policy_5: 0.96414
	loss_value_5: 0.05054
	loss_reward_5: 0.00801
	loss_policy: 0.23289
	loss_value: 0.48358
	loss_reward: 0.02997
Optimization_Done 48400
[2025-05-11 18:52:04] [command] train weight_iter_48400.pkl 224 243
[2025-05-11 18:52:12] nn step 48450, lr: 0.1.
	loss_policy_0: 0.11394
	accuracy_policy_0: 0.95617
	loss_value_0: 0.23893
	loss_policy_1: 0.02266
	accuracy_policy_1: 0.9534
	loss_value_1: 0.04748
	loss_reward_1: 0.00459
	loss_policy_2: 0.02282
	accuracy_policy_2: 0.95234
	loss_value_2: 0.04811
	loss_reward_2: 0.00502
	loss_policy_3: 0.02285
	accuracy_policy_3: 0.95215
	loss_value_3: 0.04877
	loss_reward_3: 0.00554
	loss_policy_4: 0.02269
	accuracy_policy_4: 0.95324
	loss_value_4: 0.04941
	loss_reward_4: 0.00684
	loss_policy_5: 0.02265
	accuracy_policy_5: 0.96102
	loss_value_5: 0.05011
	loss_reward_5: 0.00795
	loss_policy: 0.22761
	loss_value: 0.48281
	loss_reward: 0.02995
[2025-05-11 18:52:21] nn step 48500, lr: 0.1.
	loss_policy_0: 0.11888
	accuracy_policy_0: 0.95449
	loss_value_0: 0.241
	loss_policy_1: 0.02378
	accuracy_policy_1: 0.9502
	loss_value_1: 0.04812
	loss_reward_1: 0.00455
	loss_policy_2: 0.02346
	accuracy_policy_2: 0.95109
	loss_value_2: 0.04851
	loss_reward_2: 0.00515
	loss_policy_3: 0.0232
	accuracy_policy_3: 0.95207
	loss_value_3: 0.04927
	loss_reward_3: 0.00575
	loss_policy_4: 0.02334
	accuracy_policy_4: 0.95379
	loss_value_4: 0.04988
	loss_reward_4: 0.00673
	loss_policy_5: 0.02331
	accuracy_policy_5: 0.96129
	loss_value_5: 0.05076
	loss_reward_5: 0.00819
	loss_policy: 0.23598
	loss_value: 0.48755
	loss_reward: 0.03037
[2025-05-11 18:52:30] nn step 48550, lr: 0.1.
	loss_policy_0: 0.11955
	accuracy_policy_0: 0.95324
	loss_value_0: 0.24636
	loss_policy_1: 0.02381
	accuracy_policy_1: 0.9502
	loss_value_1: 0.049
	loss_reward_1: 0.00476
	loss_policy_2: 0.02377
	accuracy_policy_2: 0.95105
	loss_value_2: 0.04934
	loss_reward_2: 0.00539
	loss_policy_3: 0.02365
	accuracy_policy_3: 0.95023
	loss_value_3: 0.0502
	loss_reward_3: 0.00582
	loss_policy_4: 0.02403
	accuracy_policy_4: 0.95289
	loss_value_4: 0.05119
	loss_reward_4: 0.00698
	loss_policy_5: 0.02356
	accuracy_policy_5: 0.96336
	loss_value_5: 0.05195
	loss_reward_5: 0.00828
	loss_policy: 0.23838
	loss_value: 0.49803
	loss_reward: 0.03122
[2025-05-11 18:52:36] nn step 48600, lr: 0.1.
	loss_policy_0: 0.11212
	accuracy_policy_0: 0.95402
	loss_value_0: 0.22673
	loss_policy_1: 0.02217
	accuracy_policy_1: 0.95105
	loss_value_1: 0.04517
	loss_reward_1: 0.00439
	loss_policy_2: 0.02227
	accuracy_policy_2: 0.95078
	loss_value_2: 0.04584
	loss_reward_2: 0.00473
	loss_policy_3: 0.02227
	accuracy_policy_3: 0.95043
	loss_value_3: 0.0464
	loss_reward_3: 0.00547
	loss_policy_4: 0.0224
	accuracy_policy_4: 0.95371
	loss_value_4: 0.04699
	loss_reward_4: 0.00636
	loss_policy_5: 0.02217
	accuracy_policy_5: 0.96336
	loss_value_5: 0.0478
	loss_reward_5: 0.00738
	loss_policy: 0.2234
	loss_value: 0.45892
	loss_reward: 0.02834
Optimization_Done 48600
[2025-05-11 18:54:12] [command] train weight_iter_48600.pkl 225 244
[2025-05-11 18:54:22] nn step 48650, lr: 0.1.
	loss_policy_0: 0.11345
	accuracy_policy_0: 0.95777
	loss_value_0: 0.23784
	loss_policy_1: 0.02254
	accuracy_policy_1: 0.95582
	loss_value_1: 0.04735
	loss_reward_1: 0.00438
	loss_policy_2: 0.02238
	accuracy_policy_2: 0.95508
	loss_value_2: 0.04769
	loss_reward_2: 0.00513
	loss_policy_3: 0.0224
	accuracy_policy_3: 0.95758
	loss_value_3: 0.04836
	loss_reward_3: 0.00539
	loss_policy_4: 0.02254
	accuracy_policy_4: 0.95727
	loss_value_4: 0.04896
	loss_reward_4: 0.00649
	loss_policy_5: 0.02248
	accuracy_policy_5: 0.96555
	loss_value_5: 0.04994
	loss_reward_5: 0.00767
	loss_policy: 0.2258
	loss_value: 0.48015
	loss_reward: 0.02907
[2025-05-11 18:54:29] nn step 48700, lr: 0.1.
	loss_policy_0: 0.11948
	accuracy_policy_0: 0.95582
	loss_value_0: 0.24081
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.95344
	loss_value_1: 0.04832
	loss_reward_1: 0.0046
	loss_policy_2: 0.02366
	accuracy_policy_2: 0.95227
	loss_value_2: 0.04853
	loss_reward_2: 0.0052
	loss_policy_3: 0.02355
	accuracy_policy_3: 0.95352
	loss_value_3: 0.0491
	loss_reward_3: 0.00569
	loss_policy_4: 0.02362
	accuracy_policy_4: 0.95512
	loss_value_4: 0.04978
	loss_reward_4: 0.00667
	loss_policy_5: 0.02378
	accuracy_policy_5: 0.96117
	loss_value_5: 0.05063
	loss_reward_5: 0.00796
	loss_policy: 0.23766
	loss_value: 0.48716
	loss_reward: 0.03012
[2025-05-11 18:54:37] nn step 48750, lr: 0.1.
	loss_policy_0: 0.11275
	accuracy_policy_0: 0.95629
	loss_value_0: 0.22738
	loss_policy_1: 0.02239
	accuracy_policy_1: 0.95527
	loss_value_1: 0.04533
	loss_reward_1: 0.00448
	loss_policy_2: 0.02225
	accuracy_policy_2: 0.95301
	loss_value_2: 0.04609
	loss_reward_2: 0.00474
	loss_policy_3: 0.02228
	accuracy_policy_3: 0.9534
	loss_value_3: 0.0467
	loss_reward_3: 0.00549
	loss_policy_4: 0.0225
	accuracy_policy_4: 0.95484
	loss_value_4: 0.04721
	loss_reward_4: 0.00673
	loss_policy_5: 0.02244
	accuracy_policy_5: 0.9641
	loss_value_5: 0.04824
	loss_reward_5: 0.00757
	loss_policy: 0.22462
	loss_value: 0.46095
	loss_reward: 0.02901
[2025-05-11 18:54:46] nn step 48800, lr: 0.1.
	loss_policy_0: 0.11527
	accuracy_policy_0: 0.95883
	loss_value_0: 0.23242
	loss_policy_1: 0.02324
	accuracy_policy_1: 0.95266
	loss_value_1: 0.04687
	loss_reward_1: 0.00449
	loss_policy_2: 0.02283
	accuracy_policy_2: 0.95207
	loss_value_2: 0.04738
	loss_reward_2: 0.00508
	loss_policy_3: 0.02279
	accuracy_policy_3: 0.95441
	loss_value_3: 0.04778
	loss_reward_3: 0.00556
	loss_policy_4: 0.02304
	accuracy_policy_4: 0.95363
	loss_value_4: 0.04852
	loss_reward_4: 0.00671
	loss_policy_5: 0.02284
	accuracy_policy_5: 0.9643
	loss_value_5: 0.04924
	loss_reward_5: 0.00772
	loss_policy: 0.23
	loss_value: 0.47221
	loss_reward: 0.02957
Optimization_Done 48800
[2025-05-11 18:56:22] [command] train weight_iter_48800.pkl 226 245
[2025-05-11 18:56:32] nn step 48850, lr: 0.1.
	loss_policy_0: 0.1227
	accuracy_policy_0: 0.95656
	loss_value_0: 0.25266
	loss_policy_1: 0.02421
	accuracy_policy_1: 0.95395
	loss_value_1: 0.0499
	loss_reward_1: 0.00465
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.95473
	loss_value_2: 0.05068
	loss_reward_2: 0.00536
	loss_policy_3: 0.02426
	accuracy_policy_3: 0.95422
	loss_value_3: 0.0511
	loss_reward_3: 0.0061
	loss_policy_4: 0.0242
	accuracy_policy_4: 0.95539
	loss_value_4: 0.0517
	loss_reward_4: 0.00707
	loss_policy_5: 0.02441
	accuracy_policy_5: 0.96078
	loss_value_5: 0.05253
	loss_reward_5: 0.00799
	loss_policy: 0.24396
	loss_value: 0.50858
	loss_reward: 0.03118
[2025-05-11 18:56:41] nn step 48900, lr: 0.1.
	loss_policy_0: 0.11452
	accuracy_policy_0: 0.95656
	loss_value_0: 0.23175
	loss_policy_1: 0.02278
	accuracy_policy_1: 0.9532
	loss_value_1: 0.04623
	loss_reward_1: 0.00445
	loss_policy_2: 0.02292
	accuracy_policy_2: 0.95297
	loss_value_2: 0.04662
	loss_reward_2: 0.00481
	loss_policy_3: 0.02289
	accuracy_policy_3: 0.95441
	loss_value_3: 0.04742
	loss_reward_3: 0.00554
	loss_policy_4: 0.02292
	accuracy_policy_4: 0.95289
	loss_value_4: 0.04847
	loss_reward_4: 0.00662
	loss_policy_5: 0.0229
	accuracy_policy_5: 0.95863
	loss_value_5: 0.04944
	loss_reward_5: 0.00806
	loss_policy: 0.22893
	loss_value: 0.46993
	loss_reward: 0.02948
[2025-05-11 18:56:48] nn step 48950, lr: 0.1.
	loss_policy_0: 0.1169
	accuracy_policy_0: 0.95691
	loss_value_0: 0.23557
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.95289
	loss_value_1: 0.04693
	loss_reward_1: 0.00476
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.95184
	loss_value_2: 0.0479
	loss_reward_2: 0.00513
	loss_policy_3: 0.02358
	accuracy_policy_3: 0.95402
	loss_value_3: 0.04855
	loss_reward_3: 0.00576
	loss_policy_4: 0.02315
	accuracy_policy_4: 0.95477
	loss_value_4: 0.04919
	loss_reward_4: 0.00695
	loss_policy_5: 0.02304
	accuracy_policy_5: 0.96383
	loss_value_5: 0.05032
	loss_reward_5: 0.0079
	loss_policy: 0.23379
	loss_value: 0.47846
	loss_reward: 0.0305
[2025-05-11 18:56:56] nn step 49000, lr: 0.1.
	loss_policy_0: 0.11364
	accuracy_policy_0: 0.95426
	loss_value_0: 0.22652
	loss_policy_1: 0.02285
	accuracy_policy_1: 0.95211
	loss_value_1: 0.04527
	loss_reward_1: 0.00455
	loss_policy_2: 0.02268
	accuracy_policy_2: 0.95145
	loss_value_2: 0.04592
	loss_reward_2: 0.00494
	loss_policy_3: 0.02264
	accuracy_policy_3: 0.95199
	loss_value_3: 0.04659
	loss_reward_3: 0.00545
	loss_policy_4: 0.02265
	accuracy_policy_4: 0.95461
	loss_value_4: 0.04743
	loss_reward_4: 0.00684
	loss_policy_5: 0.02239
	accuracy_policy_5: 0.96105
	loss_value_5: 0.0482
	loss_reward_5: 0.00784
	loss_policy: 0.22684
	loss_value: 0.45992
	loss_reward: 0.02962
Optimization_Done 49000
[2025-05-11 18:58:34] [command] train weight_iter_49000.pkl 227 246
[2025-05-11 18:58:42] nn step 49050, lr: 0.1.
	loss_policy_0: 0.11612
	accuracy_policy_0: 0.95812
	loss_value_0: 0.23415
	loss_policy_1: 0.02268
	accuracy_policy_1: 0.95203
	loss_value_1: 0.04624
	loss_reward_1: 0.00435
	loss_policy_2: 0.02283
	accuracy_policy_2: 0.95438
	loss_value_2: 0.04674
	loss_reward_2: 0.00481
	loss_policy_3: 0.02275
	accuracy_policy_3: 0.95105
	loss_value_3: 0.04718
	loss_reward_3: 0.00553
	loss_policy_4: 0.02266
	accuracy_policy_4: 0.95441
	loss_value_4: 0.04774
	loss_reward_4: 0.00633
	loss_policy_5: 0.0227
	accuracy_policy_5: 0.96145
	loss_value_5: 0.04855
	loss_reward_5: 0.00719
	loss_policy: 0.22975
	loss_value: 0.47059
	loss_reward: 0.02822
[2025-05-11 18:58:50] nn step 49100, lr: 0.1.
	loss_policy_0: 0.11026
	accuracy_policy_0: 0.9575
	loss_value_0: 0.22552
	loss_policy_1: 0.02206
	accuracy_policy_1: 0.95227
	loss_value_1: 0.04476
	loss_reward_1: 0.00437
	loss_policy_2: 0.02225
	accuracy_policy_2: 0.9498
	loss_value_2: 0.04535
	loss_reward_2: 0.0047
	loss_policy_3: 0.022
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04591
	loss_reward_3: 0.00514
	loss_policy_4: 0.02214
	accuracy_policy_4: 0.95418
	loss_value_4: 0.0465
	loss_reward_4: 0.00637
	loss_policy_5: 0.0223
	accuracy_policy_5: 0.96293
	loss_value_5: 0.04742
	loss_reward_5: 0.00751
	loss_policy: 0.22101
	loss_value: 0.45546
	loss_reward: 0.02808
[2025-05-11 18:58:59] nn step 49150, lr: 0.1.
	loss_policy_0: 0.11936
	accuracy_policy_0: 0.95539
	loss_value_0: 0.23469
	loss_policy_1: 0.02302
	accuracy_policy_1: 0.95043
	loss_value_1: 0.04694
	loss_reward_1: 0.00468
	loss_policy_2: 0.02357
	accuracy_policy_2: 0.94984
	loss_value_2: 0.04761
	loss_reward_2: 0.00481
	loss_policy_3: 0.02349
	accuracy_policy_3: 0.95137
	loss_value_3: 0.04799
	loss_reward_3: 0.00572
	loss_policy_4: 0.02344
	accuracy_policy_4: 0.95512
	loss_value_4: 0.04891
	loss_reward_4: 0.0068
	loss_policy_5: 0.02337
	accuracy_policy_5: 0.95895
	loss_value_5: 0.04984
	loss_reward_5: 0.00774
	loss_policy: 0.23626
	loss_value: 0.47599
	loss_reward: 0.02975
[2025-05-11 18:59:07] nn step 49200, lr: 0.1.
	loss_policy_0: 0.12124
	accuracy_policy_0: 0.95152
	loss_value_0: 0.23722
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.95047
	loss_value_1: 0.04755
	loss_reward_1: 0.00467
	loss_policy_2: 0.02397
	accuracy_policy_2: 0.95109
	loss_value_2: 0.04826
	loss_reward_2: 0.00504
	loss_policy_3: 0.02423
	accuracy_policy_3: 0.95172
	loss_value_3: 0.04875
	loss_reward_3: 0.00547
	loss_policy_4: 0.02391
	accuracy_policy_4: 0.95305
	loss_value_4: 0.04912
	loss_reward_4: 0.00658
	loss_policy_5: 0.02395
	accuracy_policy_5: 0.95922
	loss_value_5: 0.05027
	loss_reward_5: 0.00744
	loss_policy: 0.24125
	loss_value: 0.48117
	loss_reward: 0.02919
Optimization_Done 49200
[2025-05-11 19:00:46] [command] train weight_iter_49200.pkl 228 247
[2025-05-11 19:00:57] nn step 49250, lr: 0.1.
	loss_policy_0: 0.11494
	accuracy_policy_0: 0.9591
	loss_value_0: 0.23695
	loss_policy_1: 0.02273
	accuracy_policy_1: 0.95746
	loss_value_1: 0.04708
	loss_reward_1: 0.00452
	loss_policy_2: 0.02266
	accuracy_policy_2: 0.95723
	loss_value_2: 0.04785
	loss_reward_2: 0.00478
	loss_policy_3: 0.02313
	accuracy_policy_3: 0.95516
	loss_value_3: 0.04857
	loss_reward_3: 0.00554
	loss_policy_4: 0.02257
	accuracy_policy_4: 0.95875
	loss_value_4: 0.04921
	loss_reward_4: 0.00659
	loss_policy_5: 0.02282
	accuracy_policy_5: 0.9632
	loss_value_5: 0.05007
	loss_reward_5: 0.00753
	loss_policy: 0.22886
	loss_value: 0.47974
	loss_reward: 0.02896
[2025-05-11 19:01:04] nn step 49300, lr: 0.1.
	loss_policy_0: 0.1222
	accuracy_policy_0: 0.95578
	loss_value_0: 0.24504
	loss_policy_1: 0.02424
	accuracy_policy_1: 0.95441
	loss_value_1: 0.04895
	loss_reward_1: 0.00489
	loss_policy_2: 0.02385
	accuracy_policy_2: 0.95148
	loss_value_2: 0.04925
	loss_reward_2: 0.00507
	loss_policy_3: 0.02406
	accuracy_policy_3: 0.95391
	loss_value_3: 0.04965
	loss_reward_3: 0.00581
	loss_policy_4: 0.0242
	accuracy_policy_4: 0.95336
	loss_value_4: 0.0504
	loss_reward_4: 0.00693
	loss_policy_5: 0.02387
	accuracy_policy_5: 0.96348
	loss_value_5: 0.05109
	loss_reward_5: 0.00802
	loss_policy: 0.24241
	loss_value: 0.49437
	loss_reward: 0.03072
[2025-05-11 19:01:13] nn step 49350, lr: 0.1.
	loss_policy_0: 0.11736
	accuracy_policy_0: 0.95605
	loss_value_0: 0.23601
	loss_policy_1: 0.02307
	accuracy_policy_1: 0.95293
	loss_value_1: 0.04692
	loss_reward_1: 0.00472
	loss_policy_2: 0.02305
	accuracy_policy_2: 0.95375
	loss_value_2: 0.04775
	loss_reward_2: 0.00499
	loss_policy_3: 0.02331
	accuracy_policy_3: 0.95191
	loss_value_3: 0.0482
	loss_reward_3: 0.00572
	loss_policy_4: 0.02312
	accuracy_policy_4: 0.95707
	loss_value_4: 0.04887
	loss_reward_4: 0.00672
	loss_policy_5: 0.0232
	accuracy_policy_5: 0.96289
	loss_value_5: 0.04965
	loss_reward_5: 0.00792
	loss_policy: 0.2331
	loss_value: 0.47741
	loss_reward: 0.03007
[2025-05-11 19:01:21] nn step 49400, lr: 0.1.
	loss_policy_0: 0.10915
	accuracy_policy_0: 0.95605
	loss_value_0: 0.21703
	loss_policy_1: 0.02182
	accuracy_policy_1: 0.95121
	loss_value_1: 0.04339
	loss_reward_1: 0.00429
	loss_policy_2: 0.02189
	accuracy_policy_2: 0.95148
	loss_value_2: 0.04389
	loss_reward_2: 0.00455
	loss_policy_3: 0.02198
	accuracy_policy_3: 0.95309
	loss_value_3: 0.04446
	loss_reward_3: 0.00505
	loss_policy_4: 0.02146
	accuracy_policy_4: 0.95211
	loss_value_4: 0.04506
	loss_reward_4: 0.00627
	loss_policy_5: 0.0216
	accuracy_policy_5: 0.96281
	loss_value_5: 0.04594
	loss_reward_5: 0.00725
	loss_policy: 0.21791
	loss_value: 0.43976
	loss_reward: 0.02741
Optimization_Done 49400
[2025-05-11 19:02:57] [command] train weight_iter_49400.pkl 229 248
[2025-05-11 19:03:06] nn step 49450, lr: 0.1.
	loss_policy_0: 0.10944
	accuracy_policy_0: 0.9541
	loss_value_0: 0.22039
	loss_policy_1: 0.02124
	accuracy_policy_1: 0.95227
	loss_value_1: 0.04373
	loss_reward_1: 0.00412
	loss_policy_2: 0.02142
	accuracy_policy_2: 0.9509
	loss_value_2: 0.04401
	loss_reward_2: 0.00456
	loss_policy_3: 0.02147
	accuracy_policy_3: 0.9527
	loss_value_3: 0.04455
	loss_reward_3: 0.00498
	loss_policy_4: 0.0215
	accuracy_policy_4: 0.95684
	loss_value_4: 0.04529
	loss_reward_4: 0.00594
	loss_policy_5: 0.02138
	accuracy_policy_5: 0.96285
	loss_value_5: 0.04605
	loss_reward_5: 0.00716
	loss_policy: 0.21647
	loss_value: 0.44403
	loss_reward: 0.02675
[2025-05-11 19:03:14] nn step 49500, lr: 0.1.
	loss_policy_0: 0.1197
	accuracy_policy_0: 0.95562
	loss_value_0: 0.24174
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.95309
	loss_value_1: 0.04814
	loss_reward_1: 0.00462
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.95352
	loss_value_2: 0.04875
	loss_reward_2: 0.00508
	loss_policy_3: 0.02404
	accuracy_policy_3: 0.95246
	loss_value_3: 0.04938
	loss_reward_3: 0.00576
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.95441
	loss_value_4: 0.05
	loss_reward_4: 0.00672
	loss_policy_5: 0.02374
	accuracy_policy_5: 0.96301
	loss_value_5: 0.05083
	loss_reward_5: 0.0076
	loss_policy: 0.23871
	loss_value: 0.48884
	loss_reward: 0.02978
[2025-05-11 19:03:23] nn step 49550, lr: 0.1.
	loss_policy_0: 0.12034
	accuracy_policy_0: 0.95566
	loss_value_0: 0.23608
	loss_policy_1: 0.02356
	accuracy_policy_1: 0.95305
	loss_value_1: 0.04738
	loss_reward_1: 0.00456
	loss_policy_2: 0.02368
	accuracy_policy_2: 0.95227
	loss_value_2: 0.04812
	loss_reward_2: 0.00526
	loss_policy_3: 0.02397
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04854
	loss_reward_3: 0.00572
	loss_policy_4: 0.0238
	accuracy_policy_4: 0.95348
	loss_value_4: 0.0492
	loss_reward_4: 0.00669
	loss_policy_5: 0.02378
	accuracy_policy_5: 0.96094
	loss_value_5: 0.04997
	loss_reward_5: 0.00842
	loss_policy: 0.23913
	loss_value: 0.4793
	loss_reward: 0.03065
[2025-05-11 19:03:30] nn step 49600, lr: 0.1.
	loss_policy_0: 0.1152
	accuracy_policy_0: 0.95434
	loss_value_0: 0.23081
	loss_policy_1: 0.02265
	accuracy_policy_1: 0.94949
	loss_value_1: 0.04624
	loss_reward_1: 0.00476
	loss_policy_2: 0.02266
	accuracy_policy_2: 0.94926
	loss_value_2: 0.0467
	loss_reward_2: 0.00486
	loss_policy_3: 0.02281
	accuracy_policy_3: 0.95141
	loss_value_3: 0.04724
	loss_reward_3: 0.00545
	loss_policy_4: 0.02267
	accuracy_policy_4: 0.95207
	loss_value_4: 0.04766
	loss_reward_4: 0.00667
	loss_policy_5: 0.02255
	accuracy_policy_5: 0.96156
	loss_value_5: 0.04884
	loss_reward_5: 0.00768
	loss_policy: 0.22853
	loss_value: 0.46749
	loss_reward: 0.02943
Optimization_Done 49600
[2025-05-11 19:05:07] [command] train weight_iter_49600.pkl 230 249
[2025-05-11 19:05:16] nn step 49650, lr: 0.1.
	loss_policy_0: 0.11147
	accuracy_policy_0: 0.95637
	loss_value_0: 0.22982
	loss_policy_1: 0.02206
	accuracy_policy_1: 0.95496
	loss_value_1: 0.04577
	loss_reward_1: 0.00434
	loss_policy_2: 0.02203
	accuracy_policy_2: 0.95297
	loss_value_2: 0.04625
	loss_reward_2: 0.00465
	loss_policy_3: 0.02199
	accuracy_policy_3: 0.95645
	loss_value_3: 0.04663
	loss_reward_3: 0.0053
	loss_policy_4: 0.02199
	accuracy_policy_4: 0.95574
	loss_value_4: 0.04742
	loss_reward_4: 0.00631
	loss_policy_5: 0.02208
	accuracy_policy_5: 0.96184
	loss_value_5: 0.0482
	loss_reward_5: 0.00707
	loss_policy: 0.22162
	loss_value: 0.46408
	loss_reward: 0.02767
[2025-05-11 19:05:23] nn step 49700, lr: 0.1.
	loss_policy_0: 0.11712
	accuracy_policy_0: 0.95844
	loss_value_0: 0.23418
	loss_policy_1: 0.02307
	accuracy_policy_1: 0.95496
	loss_value_1: 0.04659
	loss_reward_1: 0.00463
	loss_policy_2: 0.02336
	accuracy_policy_2: 0.95328
	loss_value_2: 0.04702
	loss_reward_2: 0.00521
	loss_policy_3: 0.02302
	accuracy_policy_3: 0.95527
	loss_value_3: 0.04745
	loss_reward_3: 0.00591
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.95684
	loss_value_4: 0.04836
	loss_reward_4: 0.0066
	loss_policy_5: 0.02308
	accuracy_policy_5: 0.96238
	loss_value_5: 0.04934
	loss_reward_5: 0.00774
	loss_policy: 0.23271
	loss_value: 0.47293
	loss_reward: 0.03008
[2025-05-11 19:05:32] nn step 49750, lr: 0.1.
	loss_policy_0: 0.1162
	accuracy_policy_0: 0.95461
	loss_value_0: 0.23179
	loss_policy_1: 0.02306
	accuracy_policy_1: 0.95391
	loss_value_1: 0.04646
	loss_reward_1: 0.00464
	loss_policy_2: 0.02308
	accuracy_policy_2: 0.95105
	loss_value_2: 0.04709
	loss_reward_2: 0.00493
	loss_policy_3: 0.02312
	accuracy_policy_3: 0.95504
	loss_value_3: 0.04755
	loss_reward_3: 0.00538
	loss_policy_4: 0.02316
	accuracy_policy_4: 0.95668
	loss_value_4: 0.04825
	loss_reward_4: 0.00649
	loss_policy_5: 0.02258
	accuracy_policy_5: 0.96309
	loss_value_5: 0.04902
	loss_reward_5: 0.00771
	loss_policy: 0.23121
	loss_value: 0.47015
	loss_reward: 0.02916
[2025-05-11 19:05:41] nn step 49800, lr: 0.1.
	loss_policy_0: 0.11894
	accuracy_policy_0: 0.95664
	loss_value_0: 0.23561
	loss_policy_1: 0.02373
	accuracy_policy_1: 0.95188
	loss_value_1: 0.04716
	loss_reward_1: 0.00457
	loss_policy_2: 0.02382
	accuracy_policy_2: 0.95281
	loss_value_2: 0.04795
	loss_reward_2: 0.00512
	loss_policy_3: 0.02383
	accuracy_policy_3: 0.95332
	loss_value_3: 0.04841
	loss_reward_3: 0.0057
	loss_policy_4: 0.02371
	accuracy_policy_4: 0.95484
	loss_value_4: 0.0492
	loss_reward_4: 0.00681
	loss_policy_5: 0.02345
	accuracy_policy_5: 0.96328
	loss_value_5: 0.05009
	loss_reward_5: 0.00789
	loss_policy: 0.23748
	loss_value: 0.47843
	loss_reward: 0.03008
Optimization_Done 49800
[2025-05-11 19:07:16] [command] train weight_iter_49800.pkl 231 250
[2025-05-11 19:07:25] nn step 49850, lr: 0.1.
	loss_policy_0: 0.1235
	accuracy_policy_0: 0.95953
	loss_value_0: 0.25197
	loss_policy_1: 0.0243
	accuracy_policy_1: 0.9582
	loss_value_1: 0.04986
	loss_reward_1: 0.00473
	loss_policy_2: 0.02455
	accuracy_policy_2: 0.95711
	loss_value_2: 0.05062
	loss_reward_2: 0.00524
	loss_policy_3: 0.02458
	accuracy_policy_3: 0.95594
	loss_value_3: 0.05131
	loss_reward_3: 0.00572
	loss_policy_4: 0.02451
	accuracy_policy_4: 0.9559
	loss_value_4: 0.05189
	loss_reward_4: 0.00671
	loss_policy_5: 0.02426
	accuracy_policy_5: 0.96273
	loss_value_5: 0.05299
	loss_reward_5: 0.0081
	loss_policy: 0.2457
	loss_value: 0.50865
	loss_reward: 0.03049
[2025-05-11 19:07:34] nn step 49900, lr: 0.1.
	loss_policy_0: 0.11489
	accuracy_policy_0: 0.95879
	loss_value_0: 0.23017
	loss_policy_1: 0.02286
	accuracy_policy_1: 0.95578
	loss_value_1: 0.04582
	loss_reward_1: 0.00449
	loss_policy_2: 0.02297
	accuracy_policy_2: 0.9548
	loss_value_2: 0.04687
	loss_reward_2: 0.00492
	loss_policy_3: 0.02298
	accuracy_policy_3: 0.95605
	loss_value_3: 0.04742
	loss_reward_3: 0.00535
	loss_policy_4: 0.02291
	accuracy_policy_4: 0.95652
	loss_value_4: 0.04793
	loss_reward_4: 0.00646
	loss_policy_5: 0.02286
	accuracy_policy_5: 0.9609
	loss_value_5: 0.04907
	loss_reward_5: 0.00755
	loss_policy: 0.22946
	loss_value: 0.46728
	loss_reward: 0.02877
[2025-05-11 19:07:41] nn step 49950, lr: 0.1.
	loss_policy_0: 0.11723
	accuracy_policy_0: 0.95656
	loss_value_0: 0.23256
	loss_policy_1: 0.02344
	accuracy_policy_1: 0.95578
	loss_value_1: 0.04614
	loss_reward_1: 0.00463
	loss_policy_2: 0.02336
	accuracy_policy_2: 0.95414
	loss_value_2: 0.04683
	loss_reward_2: 0.00481
	loss_policy_3: 0.02351
	accuracy_policy_3: 0.95336
	loss_value_3: 0.04748
	loss_reward_3: 0.00559
	loss_policy_4: 0.02325
	accuracy_policy_4: 0.95867
	loss_value_4: 0.04834
	loss_reward_4: 0.00649
	loss_policy_5: 0.02314
	accuracy_policy_5: 0.96391
	loss_value_5: 0.0491
	loss_reward_5: 0.00757
	loss_policy: 0.23393
	loss_value: 0.47045
	loss_reward: 0.0291
[2025-05-11 19:07:50] nn step 50000, lr: 0.1.
	loss_policy_0: 0.11776
	accuracy_policy_0: 0.95637
	loss_value_0: 0.22876
	loss_policy_1: 0.02319
	accuracy_policy_1: 0.95535
	loss_value_1: 0.0459
	loss_reward_1: 0.00437
	loss_policy_2: 0.02342
	accuracy_policy_2: 0.95289
	loss_value_2: 0.04689
	loss_reward_2: 0.00513
	loss_policy_3: 0.02346
	accuracy_policy_3: 0.95309
	loss_value_3: 0.0473
	loss_reward_3: 0.00566
	loss_policy_4: 0.02323
	accuracy_policy_4: 0.95457
	loss_value_4: 0.04794
	loss_reward_4: 0.00665
	loss_policy_5: 0.02329
	accuracy_policy_5: 0.9609
	loss_value_5: 0.04909
	loss_reward_5: 0.00753
	loss_policy: 0.23435
	loss_value: 0.46589
	loss_reward: 0.02933
Optimization_Done 50000
[2025-05-11 19:09:26] [command] train weight_iter_50000.pkl 232 251
[2025-05-11 19:09:35] nn step 50050, lr: 0.1.
	loss_policy_0: 0.11324
	accuracy_policy_0: 0.95875
	loss_value_0: 0.2244
	loss_policy_1: 0.02215
	accuracy_policy_1: 0.95512
	loss_value_1: 0.04442
	loss_reward_1: 0.0043
	loss_policy_2: 0.02212
	accuracy_policy_2: 0.95609
	loss_value_2: 0.0455
	loss_reward_2: 0.00473
	loss_policy_3: 0.02221
	accuracy_policy_3: 0.95488
	loss_value_3: 0.04589
	loss_reward_3: 0.00507
	loss_policy_4: 0.02206
	accuracy_policy_4: 0.9577
	loss_value_4: 0.04655
	loss_reward_4: 0.00621
	loss_policy_5: 0.02226
	accuracy_policy_5: 0.96227
	loss_value_5: 0.04767
	loss_reward_5: 0.00735
	loss_policy: 0.22404
	loss_value: 0.45443
	loss_reward: 0.02766
[2025-05-11 19:09:42] nn step 50100, lr: 0.1.
	loss_policy_0: 0.11512
	accuracy_policy_0: 0.95641
	loss_value_0: 0.22169
	loss_policy_1: 0.02261
	accuracy_policy_1: 0.95555
	loss_value_1: 0.04433
	loss_reward_1: 0.00426
	loss_policy_2: 0.02274
	accuracy_policy_2: 0.95289
	loss_value_2: 0.04505
	loss_reward_2: 0.00473
	loss_policy_3: 0.02236
	accuracy_policy_3: 0.95535
	loss_value_3: 0.04558
	loss_reward_3: 0.00517
	loss_policy_4: 0.02274
	accuracy_policy_4: 0.95738
	loss_value_4: 0.04634
	loss_reward_4: 0.00623
	loss_policy_5: 0.0225
	accuracy_policy_5: 0.96117
	loss_value_5: 0.04708
	loss_reward_5: 0.00723
	loss_policy: 0.22807
	loss_value: 0.45006
	loss_reward: 0.02763
[2025-05-11 19:09:51] nn step 50150, lr: 0.1.
	loss_policy_0: 0.11093
	accuracy_policy_0: 0.95582
	loss_value_0: 0.21307
	loss_policy_1: 0.0219
	accuracy_policy_1: 0.95543
	loss_value_1: 0.04243
	loss_reward_1: 0.00412
	loss_policy_2: 0.02178
	accuracy_policy_2: 0.95434
	loss_value_2: 0.04311
	loss_reward_2: 0.00461
	loss_policy_3: 0.02197
	accuracy_policy_3: 0.95418
	loss_value_3: 0.04393
	loss_reward_3: 0.00494
	loss_policy_4: 0.02192
	accuracy_policy_4: 0.95711
	loss_value_4: 0.04451
	loss_reward_4: 0.00588
	loss_policy_5: 0.0219
	accuracy_policy_5: 0.96172
	loss_value_5: 0.04545
	loss_reward_5: 0.00707
	loss_policy: 0.2204
	loss_value: 0.4325
	loss_reward: 0.02663
[2025-05-11 19:10:00] nn step 50200, lr: 0.1.
	loss_policy_0: 0.12342
	accuracy_policy_0: 0.95754
	loss_value_0: 0.23865
	loss_policy_1: 0.02449
	accuracy_policy_1: 0.95348
	loss_value_1: 0.04787
	loss_reward_1: 0.00458
	loss_policy_2: 0.02487
	accuracy_policy_2: 0.95188
	loss_value_2: 0.04833
	loss_reward_2: 0.00501
	loss_policy_3: 0.02501
	accuracy_policy_3: 0.95359
	loss_value_3: 0.0492
	loss_reward_3: 0.0057
	loss_policy_4: 0.02465
	accuracy_policy_4: 0.95496
	loss_value_4: 0.05012
	loss_reward_4: 0.0067
	loss_policy_5: 0.02437
	accuracy_policy_5: 0.96113
	loss_value_5: 0.0512
	loss_reward_5: 0.00761
	loss_policy: 0.24682
	loss_value: 0.48536
	loss_reward: 0.0296
Optimization_Done 50200
[2025-05-11 19:11:38] [command] train weight_iter_50200.pkl 233 252
[2025-05-11 19:11:48] nn step 50250, lr: 0.1.
	loss_policy_0: 0.12739
	accuracy_policy_0: 0.95449
	loss_value_0: 0.25109
	loss_policy_1: 0.02513
	accuracy_policy_1: 0.95215
	loss_value_1: 0.05007
	loss_reward_1: 0.00455
	loss_policy_2: 0.02504
	accuracy_policy_2: 0.9502
	loss_value_2: 0.05088
	loss_reward_2: 0.00521
	loss_policy_3: 0.0252
	accuracy_policy_3: 0.9523
	loss_value_3: 0.05161
	loss_reward_3: 0.00565
	loss_policy_4: 0.02509
	accuracy_policy_4: 0.95383
	loss_value_4: 0.05243
	loss_reward_4: 0.0068
	loss_policy_5: 0.0251
	accuracy_policy_5: 0.95695
	loss_value_5: 0.05361
	loss_reward_5: 0.00809
	loss_policy: 0.25295
	loss_value: 0.50969
	loss_reward: 0.03029
[2025-05-11 19:11:56] nn step 50300, lr: 0.1.
	loss_policy_0: 0.12021
	accuracy_policy_0: 0.95516
	loss_value_0: 0.23523
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.95238
	loss_value_1: 0.04686
	loss_reward_1: 0.00461
	loss_policy_2: 0.02393
	accuracy_policy_2: 0.95086
	loss_value_2: 0.04758
	loss_reward_2: 0.0048
	loss_policy_3: 0.0241
	accuracy_policy_3: 0.95285
	loss_value_3: 0.04826
	loss_reward_3: 0.00553
	loss_policy_4: 0.02391
	accuracy_policy_4: 0.95672
	loss_value_4: 0.04913
	loss_reward_4: 0.00659
	loss_policy_5: 0.02405
	accuracy_policy_5: 0.96164
	loss_value_5: 0.0502
	loss_reward_5: 0.00772
	loss_policy: 0.24008
	loss_value: 0.47726
	loss_reward: 0.02926
[2025-05-11 19:12:03] nn step 50350, lr: 0.1.
	loss_policy_0: 0.11552
	accuracy_policy_0: 0.95262
	loss_value_0: 0.21937
	loss_policy_1: 0.02283
	accuracy_policy_1: 0.94883
	loss_value_1: 0.04378
	loss_reward_1: 0.00421
	loss_policy_2: 0.02298
	accuracy_policy_2: 0.94949
	loss_value_2: 0.04428
	loss_reward_2: 0.00464
	loss_policy_3: 0.02267
	accuracy_policy_3: 0.95016
	loss_value_3: 0.04489
	loss_reward_3: 0.00521
	loss_policy_4: 0.02307
	accuracy_policy_4: 0.95195
	loss_value_4: 0.04565
	loss_reward_4: 0.00606
	loss_policy_5: 0.02275
	accuracy_policy_5: 0.95707
	loss_value_5: 0.04673
	loss_reward_5: 0.00703
	loss_policy: 0.22982
	loss_value: 0.4447
	loss_reward: 0.02714
[2025-05-11 19:12:12] nn step 50400, lr: 0.1.
	loss_policy_0: 0.12195
	accuracy_policy_0: 0.95504
	loss_value_0: 0.22988
	loss_policy_1: 0.02456
	accuracy_policy_1: 0.95398
	loss_value_1: 0.0457
	loss_reward_1: 0.00447
	loss_policy_2: 0.02442
	accuracy_policy_2: 0.95398
	loss_value_2: 0.0467
	loss_reward_2: 0.00514
	loss_policy_3: 0.02418
	accuracy_policy_3: 0.95289
	loss_value_3: 0.04731
	loss_reward_3: 0.00546
	loss_policy_4: 0.02447
	accuracy_policy_4: 0.95512
	loss_value_4: 0.04826
	loss_reward_4: 0.00635
	loss_policy_5: 0.02408
	accuracy_policy_5: 0.95891
	loss_value_5: 0.04934
	loss_reward_5: 0.00746
	loss_policy: 0.24365
	loss_value: 0.4672
	loss_reward: 0.02888
Optimization_Done 50400
[2025-05-11 19:13:47] [command] train weight_iter_50400.pkl 234 253
[2025-05-11 19:13:55] nn step 50450, lr: 0.1.
	loss_policy_0: 0.11754
	accuracy_policy_0: 0.95781
	loss_value_0: 0.22928
	loss_policy_1: 0.02338
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04572
	loss_reward_1: 0.00438
	loss_policy_2: 0.02362
	accuracy_policy_2: 0.95191
	loss_value_2: 0.04648
	loss_reward_2: 0.00487
	loss_policy_3: 0.02373
	accuracy_policy_3: 0.95129
	loss_value_3: 0.047
	loss_reward_3: 0.00531
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.95301
	loss_value_4: 0.04757
	loss_reward_4: 0.00644
	loss_policy_5: 0.02351
	accuracy_policy_5: 0.95652
	loss_value_5: 0.04848
	loss_reward_5: 0.00747
	loss_policy: 0.23518
	loss_value: 0.46454
	loss_reward: 0.02848
[2025-05-11 19:14:04] nn step 50500, lr: 0.1.
	loss_policy_0: 0.11986
	accuracy_policy_0: 0.95715
	loss_value_0: 0.22651
	loss_policy_1: 0.02369
	accuracy_policy_1: 0.95605
	loss_value_1: 0.04534
	loss_reward_1: 0.0043
	loss_policy_2: 0.02377
	accuracy_policy_2: 0.95551
	loss_value_2: 0.04606
	loss_reward_2: 0.00498
	loss_policy_3: 0.0243
	accuracy_policy_3: 0.9534
	loss_value_3: 0.04664
	loss_reward_3: 0.00546
	loss_policy_4: 0.02398
	accuracy_policy_4: 0.95602
	loss_value_4: 0.04745
	loss_reward_4: 0.0065
	loss_policy_5: 0.02377
	accuracy_policy_5: 0.96
	loss_value_5: 0.04829
	loss_reward_5: 0.00782
	loss_policy: 0.23937
	loss_value: 0.46029
	loss_reward: 0.02908
[2025-05-11 19:14:13] nn step 50550, lr: 0.1.
	loss_policy_0: 0.12127
	accuracy_policy_0: 0.95664
	loss_value_0: 0.22729
	loss_policy_1: 0.02413
	accuracy_policy_1: 0.95414
	loss_value_1: 0.04547
	loss_reward_1: 0.00447
	loss_policy_2: 0.02389
	accuracy_policy_2: 0.95352
	loss_value_2: 0.04634
	loss_reward_2: 0.005
	loss_policy_3: 0.0242
	accuracy_policy_3: 0.95148
	loss_value_3: 0.04705
	loss_reward_3: 0.00551
	loss_policy_4: 0.02409
	accuracy_policy_4: 0.9518
	loss_value_4: 0.04754
	loss_reward_4: 0.00655
	loss_policy_5: 0.0239
	accuracy_policy_5: 0.95746
	loss_value_5: 0.04856
	loss_reward_5: 0.00752
	loss_policy: 0.24149
	loss_value: 0.46225
	loss_reward: 0.02905
[2025-05-11 19:14:20] nn step 50600, lr: 0.1.
	loss_policy_0: 0.12638
	accuracy_policy_0: 0.9552
	loss_value_0: 0.23367
	loss_policy_1: 0.02491
	accuracy_policy_1: 0.95449
	loss_value_1: 0.0469
	loss_reward_1: 0.0046
	loss_policy_2: 0.02483
	accuracy_policy_2: 0.95359
	loss_value_2: 0.04755
	loss_reward_2: 0.00523
	loss_policy_3: 0.02542
	accuracy_policy_3: 0.95012
	loss_value_3: 0.04857
	loss_reward_3: 0.00556
	loss_policy_4: 0.02508
	accuracy_policy_4: 0.95551
	loss_value_4: 0.04956
	loss_reward_4: 0.00659
	loss_policy_5: 0.02501
	accuracy_policy_5: 0.95875
	loss_value_5: 0.05071
	loss_reward_5: 0.00802
	loss_policy: 0.25163
	loss_value: 0.47696
	loss_reward: 0.03001
Optimization_Done 50600
[2025-05-11 19:15:58] [command] train weight_iter_50600.pkl 235 254
[2025-05-11 19:16:07] nn step 50650, lr: 0.1.
	loss_policy_0: 0.12214
	accuracy_policy_0: 0.95191
	loss_value_0: 0.23137
	loss_policy_1: 0.02408
	accuracy_policy_1: 0.95398
	loss_value_1: 0.04585
	loss_reward_1: 0.00459
	loss_policy_2: 0.02386
	accuracy_policy_2: 0.95148
	loss_value_2: 0.04663
	loss_reward_2: 0.0049
	loss_policy_3: 0.02435
	accuracy_policy_3: 0.95184
	loss_value_3: 0.04693
	loss_reward_3: 0.00537
	loss_policy_4: 0.02441
	accuracy_policy_4: 0.95352
	loss_value_4: 0.04777
	loss_reward_4: 0.00642
	loss_policy_5: 0.02407
	accuracy_policy_5: 0.95816
	loss_value_5: 0.04854
	loss_reward_5: 0.00732
	loss_policy: 0.24291
	loss_value: 0.46708
	loss_reward: 0.02859
[2025-05-11 19:16:14] nn step 50700, lr: 0.1.
	loss_policy_0: 0.11716
	accuracy_policy_0: 0.95531
	loss_value_0: 0.21758
	loss_policy_1: 0.02303
	accuracy_policy_1: 0.95395
	loss_value_1: 0.04366
	loss_reward_1: 0.00449
	loss_policy_2: 0.02329
	accuracy_policy_2: 0.95176
	loss_value_2: 0.04446
	loss_reward_2: 0.00492
	loss_policy_3: 0.02348
	accuracy_policy_3: 0.95121
	loss_value_3: 0.04513
	loss_reward_3: 0.00575
	loss_policy_4: 0.02323
	accuracy_policy_4: 0.95523
	loss_value_4: 0.04598
	loss_reward_4: 0.0067
	loss_policy_5: 0.02301
	accuracy_policy_5: 0.95766
	loss_value_5: 0.04719
	loss_reward_5: 0.00788
	loss_policy: 0.23321
	loss_value: 0.44399
	loss_reward: 0.02974
[2025-05-11 19:16:23] nn step 50750, lr: 0.1.
	loss_policy_0: 0.12939
	accuracy_policy_0: 0.95473
	loss_value_0: 0.23809
	loss_policy_1: 0.0257
	accuracy_policy_1: 0.95355
	loss_value_1: 0.04775
	loss_reward_1: 0.00473
	loss_policy_2: 0.02597
	accuracy_policy_2: 0.95145
	loss_value_2: 0.04848
	loss_reward_2: 0.0051
	loss_policy_3: 0.02589
	accuracy_policy_3: 0.95219
	loss_value_3: 0.04931
	loss_reward_3: 0.00568
	loss_policy_4: 0.02596
	accuracy_policy_4: 0.95363
	loss_value_4: 0.0501
	loss_reward_4: 0.00708
	loss_policy_5: 0.02581
	accuracy_policy_5: 0.95512
	loss_value_5: 0.05086
	loss_reward_5: 0.00772
	loss_policy: 0.25872
	loss_value: 0.48459
	loss_reward: 0.0303
[2025-05-11 19:16:31] nn step 50800, lr: 0.1.
	loss_policy_0: 0.13642
	accuracy_policy_0: 0.95363
	loss_value_0: 0.25089
	loss_policy_1: 0.02673
	accuracy_policy_1: 0.9532
	loss_value_1: 0.05059
	loss_reward_1: 0.00507
	loss_policy_2: 0.02715
	accuracy_policy_2: 0.95113
	loss_value_2: 0.05132
	loss_reward_2: 0.00545
	loss_policy_3: 0.02695
	accuracy_policy_3: 0.95125
	loss_value_3: 0.05218
	loss_reward_3: 0.00629
	loss_policy_4: 0.02705
	accuracy_policy_4: 0.95414
	loss_value_4: 0.05322
	loss_reward_4: 0.00711
	loss_policy_5: 0.02696
	accuracy_policy_5: 0.95762
	loss_value_5: 0.0542
	loss_reward_5: 0.00859
	loss_policy: 0.27127
	loss_value: 0.5124
	loss_reward: 0.03251
Optimization_Done 50800
[2025-05-11 19:18:07] [command] train weight_iter_50800.pkl 236 255
[2025-05-11 19:18:17] nn step 50850, lr: 0.1.
	loss_policy_0: 0.12461
	accuracy_policy_0: 0.95922
	loss_value_0: 0.2475
	loss_policy_1: 0.02464
	accuracy_policy_1: 0.95926
	loss_value_1: 0.04915
	loss_reward_1: 0.00465
	loss_policy_2: 0.02471
	accuracy_policy_2: 0.95758
	loss_value_2: 0.05013
	loss_reward_2: 0.00514
	loss_policy_3: 0.02491
	accuracy_policy_3: 0.95723
	loss_value_3: 0.05083
	loss_reward_3: 0.00573
	loss_policy_4: 0.02485
	accuracy_policy_4: 0.95809
	loss_value_4: 0.05164
	loss_reward_4: 0.00674
	loss_policy_5: 0.02478
	accuracy_policy_5: 0.96137
	loss_value_5: 0.05245
	loss_reward_5: 0.00835
	loss_policy: 0.2485
	loss_value: 0.50171
	loss_reward: 0.0306
[2025-05-11 19:18:26] nn step 50900, lr: 0.1.
	loss_policy_0: 0.12046
	accuracy_policy_0: 0.95961
	loss_value_0: 0.22493
	loss_policy_1: 0.02385
	accuracy_policy_1: 0.95723
	loss_value_1: 0.04535
	loss_reward_1: 0.00433
	loss_policy_2: 0.02398
	accuracy_policy_2: 0.95461
	loss_value_2: 0.04626
	loss_reward_2: 0.0049
	loss_policy_3: 0.02393
	accuracy_policy_3: 0.9557
	loss_value_3: 0.047
	loss_reward_3: 0.00539
	loss_policy_4: 0.02375
	accuracy_policy_4: 0.95773
	loss_value_4: 0.04769
	loss_reward_4: 0.00645
	loss_policy_5: 0.02372
	accuracy_policy_5: 0.96004
	loss_value_5: 0.04891
	loss_reward_5: 0.00731
	loss_policy: 0.23969
	loss_value: 0.46015
	loss_reward: 0.02838
[2025-05-11 19:18:34] nn step 50950, lr: 0.1.
	loss_policy_0: 0.12651
	accuracy_policy_0: 0.95703
	loss_value_0: 0.23391
	loss_policy_1: 0.025
	accuracy_policy_1: 0.95547
	loss_value_1: 0.04699
	loss_reward_1: 0.00446
	loss_policy_2: 0.02518
	accuracy_policy_2: 0.95422
	loss_value_2: 0.04769
	loss_reward_2: 0.00516
	loss_policy_3: 0.02497
	accuracy_policy_3: 0.95609
	loss_value_3: 0.04852
	loss_reward_3: 0.00574
	loss_policy_4: 0.02501
	accuracy_policy_4: 0.95895
	loss_value_4: 0.0492
	loss_reward_4: 0.00666
	loss_policy_5: 0.02458
	accuracy_policy_5: 0.96191
	loss_value_5: 0.05035
	loss_reward_5: 0.00773
	loss_policy: 0.25126
	loss_value: 0.47666
	loss_reward: 0.02976
[2025-05-11 19:18:42] nn step 51000, lr: 0.1.
	loss_policy_0: 0.12734
	accuracy_policy_0: 0.9559
	loss_value_0: 0.23531
	loss_policy_1: 0.02549
	accuracy_policy_1: 0.95219
	loss_value_1: 0.04719
	loss_reward_1: 0.0047
	loss_policy_2: 0.02554
	accuracy_policy_2: 0.95246
	loss_value_2: 0.04792
	loss_reward_2: 0.00509
	loss_policy_3: 0.02544
	accuracy_policy_3: 0.95422
	loss_value_3: 0.04875
	loss_reward_3: 0.00568
	loss_policy_4: 0.02532
	accuracy_policy_4: 0.95637
	loss_value_4: 0.04963
	loss_reward_4: 0.00679
	loss_policy_5: 0.02533
	accuracy_policy_5: 0.9593
	loss_value_5: 0.05075
	loss_reward_5: 0.00795
	loss_policy: 0.25445
	loss_value: 0.47955
	loss_reward: 0.03022
Optimization_Done 51000
[2025-05-11 19:20:19] [command] train weight_iter_51000.pkl 237 256
[2025-05-11 19:20:28] nn step 51050, lr: 0.1.
	loss_policy_0: 0.11015
	accuracy_policy_0: 0.96074
	loss_value_0: 0.22746
	loss_policy_1: 0.02204
	accuracy_policy_1: 0.95941
	loss_value_1: 0.04518
	loss_reward_1: 0.00418
	loss_policy_2: 0.02205
	accuracy_policy_2: 0.95637
	loss_value_2: 0.04569
	loss_reward_2: 0.00457
	loss_policy_3: 0.02199
	accuracy_policy_3: 0.95758
	loss_value_3: 0.04665
	loss_reward_3: 0.00516
	loss_policy_4: 0.02187
	accuracy_policy_4: 0.96117
	loss_value_4: 0.04732
	loss_reward_4: 0.0058
	loss_policy_5: 0.02156
	accuracy_policy_5: 0.96207
	loss_value_5: 0.04822
	loss_reward_5: 0.00701
	loss_policy: 0.21966
	loss_value: 0.4605
	loss_reward: 0.02672
[2025-05-11 19:20:36] nn step 51100, lr: 0.1.
	loss_policy_0: 0.12078
	accuracy_policy_0: 0.9575
	loss_value_0: 0.22816
	loss_policy_1: 0.02355
	accuracy_policy_1: 0.9591
	loss_value_1: 0.04543
	loss_reward_1: 0.00464
	loss_policy_2: 0.02397
	accuracy_policy_2: 0.95633
	loss_value_2: 0.0459
	loss_reward_2: 0.00513
	loss_policy_3: 0.02389
	accuracy_policy_3: 0.95969
	loss_value_3: 0.04671
	loss_reward_3: 0.00556
	loss_policy_4: 0.0238
	accuracy_policy_4: 0.95965
	loss_value_4: 0.04775
	loss_reward_4: 0.00657
	loss_policy_5: 0.02398
	accuracy_policy_5: 0.9623
	loss_value_5: 0.04883
	loss_reward_5: 0.00717
	loss_policy: 0.23997
	loss_value: 0.46277
	loss_reward: 0.02907
[2025-05-11 19:20:45] nn step 51150, lr: 0.1.
	loss_policy_0: 0.12201
	accuracy_policy_0: 0.9573
	loss_value_0: 0.22576
	loss_policy_1: 0.02417
	accuracy_policy_1: 0.95582
	loss_value_1: 0.04532
	loss_reward_1: 0.00438
	loss_policy_2: 0.02431
	accuracy_policy_2: 0.9566
	loss_value_2: 0.04607
	loss_reward_2: 0.00484
	loss_policy_3: 0.0243
	accuracy_policy_3: 0.95762
	loss_value_3: 0.04705
	loss_reward_3: 0.00533
	loss_policy_4: 0.02427
	accuracy_policy_4: 0.95723
	loss_value_4: 0.04756
	loss_reward_4: 0.00625
	loss_policy_5: 0.02379
	accuracy_policy_5: 0.96074
	loss_value_5: 0.04852
	loss_reward_5: 0.00731
	loss_policy: 0.24284
	loss_value: 0.46028
	loss_reward: 0.0281
[2025-05-11 19:20:54] nn step 51200, lr: 0.1.
	loss_policy_0: 0.12905
	accuracy_policy_0: 0.95695
	loss_value_0: 0.23908
	loss_policy_1: 0.02565
	accuracy_policy_1: 0.95566
	loss_value_1: 0.04788
	loss_reward_1: 0.00478
	loss_policy_2: 0.0256
	accuracy_policy_2: 0.95305
	loss_value_2: 0.04893
	loss_reward_2: 0.00522
	loss_policy_3: 0.0257
	accuracy_policy_3: 0.95617
	loss_value_3: 0.04996
	loss_reward_3: 0.00589
	loss_policy_4: 0.02577
	accuracy_policy_4: 0.95867
	loss_value_4: 0.05057
	loss_reward_4: 0.00667
	loss_policy_5: 0.02565
	accuracy_policy_5: 0.9616
	loss_value_5: 0.05158
	loss_reward_5: 0.00802
	loss_policy: 0.25743
	loss_value: 0.48802
	loss_reward: 0.03057
Optimization_Done 51200
[2025-05-11 19:22:26] [command] train weight_iter_51200.pkl 238 257
[2025-05-11 19:22:36] nn step 51250, lr: 0.1.
	loss_policy_0: 0.12017
	accuracy_policy_0: 0.96125
	loss_value_0: 0.23902
	loss_policy_1: 0.02374
	accuracy_policy_1: 0.95957
	loss_value_1: 0.04718
	loss_reward_1: 0.00449
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.95586
	loss_value_2: 0.04817
	loss_reward_2: 0.00488
	loss_policy_3: 0.02422
	accuracy_policy_3: 0.95613
	loss_value_3: 0.04906
	loss_reward_3: 0.00536
	loss_policy_4: 0.02415
	accuracy_policy_4: 0.95988
	loss_value_4: 0.04971
	loss_reward_4: 0.00633
	loss_policy_5: 0.02421
	accuracy_policy_5: 0.96184
	loss_value_5: 0.05055
	loss_reward_5: 0.00773
	loss_policy: 0.24066
	loss_value: 0.48368
	loss_reward: 0.0288
[2025-05-11 19:22:45] nn step 51300, lr: 0.1.
	loss_policy_0: 0.12379
	accuracy_policy_0: 0.95531
	loss_value_0: 0.23318
	loss_policy_1: 0.02433
	accuracy_policy_1: 0.95535
	loss_value_1: 0.04642
	loss_reward_1: 0.00444
	loss_policy_2: 0.0245
	accuracy_policy_2: 0.95547
	loss_value_2: 0.04721
	loss_reward_2: 0.00483
	loss_policy_3: 0.02436
	accuracy_policy_3: 0.9559
	loss_value_3: 0.04798
	loss_reward_3: 0.00546
	loss_policy_4: 0.02432
	accuracy_policy_4: 0.95945
	loss_value_4: 0.04866
	loss_reward_4: 0.00646
	loss_policy_5: 0.02443
	accuracy_policy_5: 0.95965
	loss_value_5: 0.04964
	loss_reward_5: 0.00753
	loss_policy: 0.24573
	loss_value: 0.47309
	loss_reward: 0.02871
[2025-05-11 19:22:52] nn step 51350, lr: 0.1.
	loss_policy_0: 0.1219
	accuracy_policy_0: 0.9591
	loss_value_0: 0.22504
	loss_policy_1: 0.02411
	accuracy_policy_1: 0.95676
	loss_value_1: 0.04515
	loss_reward_1: 0.00431
	loss_policy_2: 0.02389
	accuracy_policy_2: 0.95715
	loss_value_2: 0.04595
	loss_reward_2: 0.00501
	loss_policy_3: 0.02404
	accuracy_policy_3: 0.95637
	loss_value_3: 0.04648
	loss_reward_3: 0.00563
	loss_policy_4: 0.02411
	accuracy_policy_4: 0.95754
	loss_value_4: 0.0473
	loss_reward_4: 0.00622
	loss_policy_5: 0.02369
	accuracy_policy_5: 0.96141
	loss_value_5: 0.04843
	loss_reward_5: 0.00744
	loss_policy: 0.24173
	loss_value: 0.45836
	loss_reward: 0.02861
[2025-05-11 19:23:00] nn step 51400, lr: 0.1.
	loss_policy_0: 0.12582
	accuracy_policy_0: 0.95754
	loss_value_0: 0.23552
	loss_policy_1: 0.02534
	accuracy_policy_1: 0.95434
	loss_value_1: 0.04726
	loss_reward_1: 0.00461
	loss_policy_2: 0.02484
	accuracy_policy_2: 0.95277
	loss_value_2: 0.04792
	loss_reward_2: 0.00485
	loss_policy_3: 0.02538
	accuracy_policy_3: 0.95613
	loss_value_3: 0.04837
	loss_reward_3: 0.00556
	loss_policy_4: 0.0253
	accuracy_policy_4: 0.95586
	loss_value_4: 0.04931
	loss_reward_4: 0.00691
	loss_policy_5: 0.02492
	accuracy_policy_5: 0.9598
	loss_value_5: 0.0504
	loss_reward_5: 0.00791
	loss_policy: 0.2516
	loss_value: 0.47877
	loss_reward: 0.02985
Optimization_Done 51400
[2025-05-11 19:24:37] [command] train weight_iter_51400.pkl 239 258
[2025-05-11 19:24:46] nn step 51450, lr: 0.1.
	loss_policy_0: 0.11707
	accuracy_policy_0: 0.95676
	loss_value_0: 0.25928
	loss_policy_1: 0.02321
	accuracy_policy_1: 0.95484
	loss_value_1: 0.05167
	loss_reward_1: 0.0043
	loss_policy_2: 0.02331
	accuracy_policy_2: 0.95434
	loss_value_2: 0.0527
	loss_reward_2: 0.00484
	loss_policy_3: 0.02333
	accuracy_policy_3: 0.95293
	loss_value_3: 0.05357
	loss_reward_3: 0.00537
	loss_policy_4: 0.02371
	accuracy_policy_4: 0.9552
	loss_value_4: 0.05415
	loss_reward_4: 0.0065
	loss_policy_5: 0.02318
	accuracy_policy_5: 0.95539
	loss_value_5: 0.05521
	loss_reward_5: 0.00785
	loss_policy: 0.23381
	loss_value: 0.52657
	loss_reward: 0.02886
[2025-05-11 19:24:55] nn step 51500, lr: 0.1.
	loss_policy_0: 0.12285
	accuracy_policy_0: 0.95691
	loss_value_0: 0.24623
	loss_policy_1: 0.0241
	accuracy_policy_1: 0.95359
	loss_value_1: 0.0488
	loss_reward_1: 0.00444
	loss_policy_2: 0.02427
	accuracy_policy_2: 0.95383
	loss_value_2: 0.04956
	loss_reward_2: 0.00491
	loss_policy_3: 0.02427
	accuracy_policy_3: 0.955
	loss_value_3: 0.05046
	loss_reward_3: 0.00582
	loss_policy_4: 0.02413
	accuracy_policy_4: 0.955
	loss_value_4: 0.05124
	loss_reward_4: 0.00673
	loss_policy_5: 0.02416
	accuracy_policy_5: 0.95738
	loss_value_5: 0.05196
	loss_reward_5: 0.00781
	loss_policy: 0.24377
	loss_value: 0.49825
	loss_reward: 0.02971
[2025-05-11 19:25:03] nn step 51550, lr: 0.1.
	loss_policy_0: 0.12848
	accuracy_policy_0: 0.9559
	loss_value_0: 0.25093
	loss_policy_1: 0.02525
	accuracy_policy_1: 0.95262
	loss_value_1: 0.05019
	loss_reward_1: 0.0047
	loss_policy_2: 0.0252
	accuracy_policy_2: 0.95156
	loss_value_2: 0.0508
	loss_reward_2: 0.00535
	loss_policy_3: 0.02563
	accuracy_policy_3: 0.95047
	loss_value_3: 0.0516
	loss_reward_3: 0.00583
	loss_policy_4: 0.02583
	accuracy_policy_4: 0.95211
	loss_value_4: 0.0525
	loss_reward_4: 0.00675
	loss_policy_5: 0.02554
	accuracy_policy_5: 0.95777
	loss_value_5: 0.05369
	loss_reward_5: 0.0081
	loss_policy: 0.25592
	loss_value: 0.50971
	loss_reward: 0.03073
[2025-05-11 19:25:12] nn step 51600, lr: 0.1.
	loss_policy_0: 0.12054
	accuracy_policy_0: 0.95512
	loss_value_0: 0.23506
	loss_policy_1: 0.02369
	accuracy_policy_1: 0.95359
	loss_value_1: 0.04656
	loss_reward_1: 0.00445
	loss_policy_2: 0.02366
	accuracy_policy_2: 0.95273
	loss_value_2: 0.04757
	loss_reward_2: 0.00472
	loss_policy_3: 0.02395
	accuracy_policy_3: 0.95199
	loss_value_3: 0.0482
	loss_reward_3: 0.00544
	loss_policy_4: 0.02422
	accuracy_policy_4: 0.95414
	loss_value_4: 0.04883
	loss_reward_4: 0.00649
	loss_policy_5: 0.02399
	accuracy_policy_5: 0.9552
	loss_value_5: 0.0498
	loss_reward_5: 0.00742
	loss_policy: 0.24005
	loss_value: 0.47602
	loss_reward: 0.02852
Optimization_Done 51600
[2025-05-11 19:26:47] [command] train weight_iter_51600.pkl 240 259
[2025-05-11 19:26:57] nn step 51650, lr: 0.1.
	loss_policy_0: 0.11648
	accuracy_policy_0: 0.9584
	loss_value_0: 0.2438
	loss_policy_1: 0.02334
	accuracy_policy_1: 0.95316
	loss_value_1: 0.04823
	loss_reward_1: 0.00422
	loss_policy_2: 0.02345
	accuracy_policy_2: 0.95477
	loss_value_2: 0.04905
	loss_reward_2: 0.00488
	loss_policy_3: 0.02343
	accuracy_policy_3: 0.95414
	loss_value_3: 0.04974
	loss_reward_3: 0.00532
	loss_policy_4: 0.02349
	accuracy_policy_4: 0.9573
	loss_value_4: 0.05053
	loss_reward_4: 0.00618
	loss_policy_5: 0.02308
	accuracy_policy_5: 0.95918
	loss_value_5: 0.05171
	loss_reward_5: 0.00716
	loss_policy: 0.23327
	loss_value: 0.49307
	loss_reward: 0.02775
[2025-05-11 19:27:06] nn step 51700, lr: 0.1.
	loss_policy_0: 0.11166
	accuracy_policy_0: 0.95648
	loss_value_0: 0.22109
	loss_policy_1: 0.02245
	accuracy_policy_1: 0.95203
	loss_value_1: 0.04387
	loss_reward_1: 0.00409
	loss_policy_2: 0.02272
	accuracy_policy_2: 0.95219
	loss_value_2: 0.04463
	loss_reward_2: 0.00451
	loss_policy_3: 0.02263
	accuracy_policy_3: 0.95355
	loss_value_3: 0.0455
	loss_reward_3: 0.00512
	loss_policy_4: 0.0225
	accuracy_policy_4: 0.95406
	loss_value_4: 0.04627
	loss_reward_4: 0.00587
	loss_policy_5: 0.02246
	accuracy_policy_5: 0.95754
	loss_value_5: 0.04704
	loss_reward_5: 0.00702
	loss_policy: 0.22442
	loss_value: 0.44839
	loss_reward: 0.02661
[2025-05-11 19:27:12] nn step 51750, lr: 0.1.
	loss_policy_0: 0.11922
	accuracy_policy_0: 0.95469
	loss_value_0: 0.23548
	loss_policy_1: 0.02399
	accuracy_policy_1: 0.9543
	loss_value_1: 0.04705
	loss_reward_1: 0.00442
	loss_policy_2: 0.02392
	accuracy_policy_2: 0.95223
	loss_value_2: 0.04814
	loss_reward_2: 0.005
	loss_policy_3: 0.02404
	accuracy_policy_3: 0.95016
	loss_value_3: 0.04841
	loss_reward_3: 0.00544
	loss_policy_4: 0.02412
	accuracy_policy_4: 0.95164
	loss_value_4: 0.04921
	loss_reward_4: 0.00618
	loss_policy_5: 0.0237
	accuracy_policy_5: 0.95906
	loss_value_5: 0.05028
	loss_reward_5: 0.00756
	loss_policy: 0.239
	loss_value: 0.47857
	loss_reward: 0.02859
[2025-05-11 19:27:21] nn step 51800, lr: 0.1.
	loss_policy_0: 0.11638
	accuracy_policy_0: 0.95449
	loss_value_0: 0.22582
	loss_policy_1: 0.02312
	accuracy_policy_1: 0.95375
	loss_value_1: 0.04508
	loss_reward_1: 0.00432
	loss_policy_2: 0.02362
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04581
	loss_reward_2: 0.00481
	loss_policy_3: 0.0234
	accuracy_policy_3: 0.95113
	loss_value_3: 0.04639
	loss_reward_3: 0.0054
	loss_policy_4: 0.0233
	accuracy_policy_4: 0.95305
	loss_value_4: 0.04718
	loss_reward_4: 0.00605
	loss_policy_5: 0.02328
	accuracy_policy_5: 0.95746
	loss_value_5: 0.04828
	loss_reward_5: 0.00736
	loss_policy: 0.2331
	loss_value: 0.45855
	loss_reward: 0.02794
Optimization_Done 51800
[2025-05-11 19:29:00] [command] train weight_iter_51800.pkl 241 260
[2025-05-11 19:29:10] nn step 51850, lr: 0.1.
	loss_policy_0: 0.11572
	accuracy_policy_0: 0.95668
	loss_value_0: 0.22734
	loss_policy_1: 0.02281
	accuracy_policy_1: 0.95461
	loss_value_1: 0.0452
	loss_reward_1: 0.00428
	loss_policy_2: 0.02306
	accuracy_policy_2: 0.95285
	loss_value_2: 0.04579
	loss_reward_2: 0.00467
	loss_policy_3: 0.02276
	accuracy_policy_3: 0.95371
	loss_value_3: 0.04647
	loss_reward_3: 0.00527
	loss_policy_4: 0.02316
	accuracy_policy_4: 0.95445
	loss_value_4: 0.04721
	loss_reward_4: 0.00594
	loss_policy_5: 0.02286
	accuracy_policy_5: 0.95719
	loss_value_5: 0.04797
	loss_reward_5: 0.00709
	loss_policy: 0.23036
	loss_value: 0.45999
	loss_reward: 0.02725
[2025-05-11 19:29:19] nn step 51900, lr: 0.1.
	loss_policy_0: 0.12069
	accuracy_policy_0: 0.95555
	loss_value_0: 0.23161
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.95242
	loss_value_1: 0.0465
	loss_reward_1: 0.00437
	loss_policy_2: 0.02395
	accuracy_policy_2: 0.95223
	loss_value_2: 0.04732
	loss_reward_2: 0.00498
	loss_policy_3: 0.02361
	accuracy_policy_3: 0.95168
	loss_value_3: 0.04807
	loss_reward_3: 0.00556
	loss_policy_4: 0.02375
	accuracy_policy_4: 0.95477
	loss_value_4: 0.04889
	loss_reward_4: 0.00633
	loss_policy_5: 0.0239
	accuracy_policy_5: 0.95641
	loss_value_5: 0.04997
	loss_reward_5: 0.00764
	loss_policy: 0.23978
	loss_value: 0.47236
	loss_reward: 0.02888
[2025-05-11 19:29:28] nn step 51950, lr: 0.1.
	loss_policy_0: 0.12628
	accuracy_policy_0: 0.95441
	loss_value_0: 0.2431
	loss_policy_1: 0.02499
	accuracy_policy_1: 0.95117
	loss_value_1: 0.04881
	loss_reward_1: 0.00454
	loss_policy_2: 0.02487
	accuracy_policy_2: 0.9516
	loss_value_2: 0.04972
	loss_reward_2: 0.00506
	loss_policy_3: 0.02514
	accuracy_policy_3: 0.95324
	loss_value_3: 0.05067
	loss_reward_3: 0.00577
	loss_policy_4: 0.02502
	accuracy_policy_4: 0.95406
	loss_value_4: 0.05138
	loss_reward_4: 0.00658
	loss_policy_5: 0.02466
	accuracy_policy_5: 0.9575
	loss_value_5: 0.05205
	loss_reward_5: 0.00796
	loss_policy: 0.25096
	loss_value: 0.49572
	loss_reward: 0.02992
[2025-05-11 19:29:35] nn step 52000, lr: 0.1.
	loss_policy_0: 0.12261
	accuracy_policy_0: 0.95492
	loss_value_0: 0.23614
	loss_policy_1: 0.02447
	accuracy_policy_1: 0.95102
	loss_value_1: 0.04693
	loss_reward_1: 0.00452
	loss_policy_2: 0.02461
	accuracy_policy_2: 0.95328
	loss_value_2: 0.04767
	loss_reward_2: 0.00499
	loss_policy_3: 0.02465
	accuracy_policy_3: 0.94953
	loss_value_3: 0.04848
	loss_reward_3: 0.00564
	loss_policy_4: 0.02442
	accuracy_policy_4: 0.95363
	loss_value_4: 0.04879
	loss_reward_4: 0.00674
	loss_policy_5: 0.0244
	accuracy_policy_5: 0.95742
	loss_value_5: 0.04993
	loss_reward_5: 0.00765
	loss_policy: 0.24517
	loss_value: 0.47793
	loss_reward: 0.02955
Optimization_Done 52000
[2025-05-11 19:31:09] [command] train weight_iter_52000.pkl 242 261
[2025-05-11 19:31:19] nn step 52050, lr: 0.1.
	loss_policy_0: 0.11612
	accuracy_policy_0: 0.95285
	loss_value_0: 0.2279
	loss_policy_1: 0.02308
	accuracy_policy_1: 0.95129
	loss_value_1: 0.04552
	loss_reward_1: 0.00427
	loss_policy_2: 0.02288
	accuracy_policy_2: 0.95102
	loss_value_2: 0.04618
	loss_reward_2: 0.00471
	loss_policy_3: 0.02292
	accuracy_policy_3: 0.9491
	loss_value_3: 0.04668
	loss_reward_3: 0.00503
	loss_policy_4: 0.02301
	accuracy_policy_4: 0.94961
	loss_value_4: 0.04737
	loss_reward_4: 0.00595
	loss_policy_5: 0.02275
	accuracy_policy_5: 0.95375
	loss_value_5: 0.04831
	loss_reward_5: 0.00706
	loss_policy: 0.23077
	loss_value: 0.46195
	loss_reward: 0.02703
[2025-05-11 19:31:26] nn step 52100, lr: 0.1.
	loss_policy_0: 0.12318
	accuracy_policy_0: 0.95117
	loss_value_0: 0.23712
	loss_policy_1: 0.02391
	accuracy_policy_1: 0.95242
	loss_value_1: 0.04731
	loss_reward_1: 0.0044
	loss_policy_2: 0.02395
	accuracy_policy_2: 0.95168
	loss_value_2: 0.04793
	loss_reward_2: 0.00503
	loss_policy_3: 0.02425
	accuracy_policy_3: 0.94918
	loss_value_3: 0.04849
	loss_reward_3: 0.00559
	loss_policy_4: 0.02426
	accuracy_policy_4: 0.95383
	loss_value_4: 0.04927
	loss_reward_4: 0.00634
	loss_policy_5: 0.02399
	accuracy_policy_5: 0.95848
	loss_value_5: 0.05039
	loss_reward_5: 0.00745
	loss_policy: 0.24354
	loss_value: 0.4805
	loss_reward: 0.02881
[2025-05-11 19:31:35] nn step 52150, lr: 0.1.
	loss_policy_0: 0.12705
	accuracy_policy_0: 0.95148
	loss_value_0: 0.24736
	loss_policy_1: 0.02516
	accuracy_policy_1: 0.94645
	loss_value_1: 0.04931
	loss_reward_1: 0.00473
	loss_policy_2: 0.02496
	accuracy_policy_2: 0.94879
	loss_value_2: 0.05018
	loss_reward_2: 0.00522
	loss_policy_3: 0.02526
	accuracy_policy_3: 0.94629
	loss_value_3: 0.05065
	loss_reward_3: 0.0058
	loss_policy_4: 0.02521
	accuracy_policy_4: 0.94832
	loss_value_4: 0.05124
	loss_reward_4: 0.00688
	loss_policy_5: 0.02485
	accuracy_policy_5: 0.95535
	loss_value_5: 0.05262
	loss_reward_5: 0.00846
	loss_policy: 0.25249
	loss_value: 0.50136
	loss_reward: 0.03109
[2025-05-11 19:31:44] nn step 52200, lr: 0.1.
	loss_policy_0: 0.11782
	accuracy_policy_0: 0.95141
	loss_value_0: 0.22368
	loss_policy_1: 0.02343
	accuracy_policy_1: 0.95152
	loss_value_1: 0.04485
	loss_reward_1: 0.00429
	loss_policy_2: 0.02373
	accuracy_policy_2: 0.95055
	loss_value_2: 0.0457
	loss_reward_2: 0.00481
	loss_policy_3: 0.02382
	accuracy_policy_3: 0.95043
	loss_value_3: 0.0463
	loss_reward_3: 0.00526
	loss_policy_4: 0.02327
	accuracy_policy_4: 0.95188
	loss_value_4: 0.04728
	loss_reward_4: 0.00633
	loss_policy_5: 0.02327
	accuracy_policy_5: 0.95484
	loss_value_5: 0.04837
	loss_reward_5: 0.00763
	loss_policy: 0.23536
	loss_value: 0.45618
	loss_reward: 0.02831
Optimization_Done 52200
[2025-05-11 19:33:21] [command] train weight_iter_52200.pkl 243 262
[2025-05-11 19:33:31] nn step 52250, lr: 0.1.
	loss_policy_0: 0.12019
	accuracy_policy_0: 0.95145
	loss_value_0: 0.23892
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.94945
	loss_value_1: 0.04756
	loss_reward_1: 0.00433
	loss_policy_2: 0.02391
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04839
	loss_reward_2: 0.00472
	loss_policy_3: 0.02401
	accuracy_policy_3: 0.94875
	loss_value_3: 0.04908
	loss_reward_3: 0.00561
	loss_policy_4: 0.02365
	accuracy_policy_4: 0.9525
	loss_value_4: 0.04959
	loss_reward_4: 0.00648
	loss_policy_5: 0.02356
	accuracy_policy_5: 0.95504
	loss_value_5: 0.05059
	loss_reward_5: 0.00723
	loss_policy: 0.23925
	loss_value: 0.48414
	loss_reward: 0.02837
[2025-05-11 19:33:39] nn step 52300, lr: 0.1.
	loss_policy_0: 0.12451
	accuracy_policy_0: 0.95148
	loss_value_0: 0.24275
	loss_policy_1: 0.02441
	accuracy_policy_1: 0.95082
	loss_value_1: 0.04865
	loss_reward_1: 0.00446
	loss_policy_2: 0.02432
	accuracy_policy_2: 0.95117
	loss_value_2: 0.04928
	loss_reward_2: 0.0051
	loss_policy_3: 0.02471
	accuracy_policy_3: 0.95031
	loss_value_3: 0.05005
	loss_reward_3: 0.00543
	loss_policy_4: 0.02461
	accuracy_policy_4: 0.95129
	loss_value_4: 0.05093
	loss_reward_4: 0.00676
	loss_policy_5: 0.02417
	accuracy_policy_5: 0.95766
	loss_value_5: 0.0516
	loss_reward_5: 0.00803
	loss_policy: 0.24673
	loss_value: 0.49325
	loss_reward: 0.02979
[2025-05-11 19:33:47] nn step 52350, lr: 0.1.
	loss_policy_0: 0.11402
	accuracy_policy_0: 0.95242
	loss_value_0: 0.22306
	loss_policy_1: 0.02272
	accuracy_policy_1: 0.95082
	loss_value_1: 0.04463
	loss_reward_1: 0.0042
	loss_policy_2: 0.02282
	accuracy_policy_2: 0.9493
	loss_value_2: 0.04526
	loss_reward_2: 0.00468
	loss_policy_3: 0.02284
	accuracy_policy_3: 0.9477
	loss_value_3: 0.0456
	loss_reward_3: 0.00502
	loss_policy_4: 0.02291
	accuracy_policy_4: 0.95012
	loss_value_4: 0.04641
	loss_reward_4: 0.00583
	loss_policy_5: 0.02283
	accuracy_policy_5: 0.95355
	loss_value_5: 0.04763
	loss_reward_5: 0.00725
	loss_policy: 0.22814
	loss_value: 0.4526
	loss_reward: 0.02698
[2025-05-11 19:33:55] nn step 52400, lr: 0.1.
	loss_policy_0: 0.12156
	accuracy_policy_0: 0.95156
	loss_value_0: 0.23369
	loss_policy_1: 0.0241
	accuracy_policy_1: 0.95035
	loss_value_1: 0.04663
	loss_reward_1: 0.00445
	loss_policy_2: 0.02425
	accuracy_policy_2: 0.94777
	loss_value_2: 0.04751
	loss_reward_2: 0.00508
	loss_policy_3: 0.02411
	accuracy_policy_3: 0.94762
	loss_value_3: 0.04809
	loss_reward_3: 0.00561
	loss_policy_4: 0.02395
	accuracy_policy_4: 0.94891
	loss_value_4: 0.04917
	loss_reward_4: 0.00643
	loss_policy_5: 0.02397
	accuracy_policy_5: 0.9559
	loss_value_5: 0.04985
	loss_reward_5: 0.00769
	loss_policy: 0.24194
	loss_value: 0.47494
	loss_reward: 0.02926
Optimization_Done 52400
[2025-05-11 19:35:31] [command] train weight_iter_52400.pkl 244 263
[2025-05-11 19:35:39] nn step 52450, lr: 0.1.
	loss_policy_0: 0.10973
	accuracy_policy_0: 0.95398
	loss_value_0: 0.21959
	loss_policy_1: 0.0215
	accuracy_policy_1: 0.95203
	loss_value_1: 0.04385
	loss_reward_1: 0.00396
	loss_policy_2: 0.02153
	accuracy_policy_2: 0.94891
	loss_value_2: 0.04441
	loss_reward_2: 0.00458
	loss_policy_3: 0.02159
	accuracy_policy_3: 0.95273
	loss_value_3: 0.045
	loss_reward_3: 0.0051
	loss_policy_4: 0.02157
	accuracy_policy_4: 0.95258
	loss_value_4: 0.04563
	loss_reward_4: 0.00594
	loss_policy_5: 0.0213
	accuracy_policy_5: 0.95957
	loss_value_5: 0.04616
	loss_reward_5: 0.00675
	loss_policy: 0.21721
	loss_value: 0.44465
	loss_reward: 0.02632
[2025-05-11 19:35:48] nn step 52500, lr: 0.1.
	loss_policy_0: 0.12137
	accuracy_policy_0: 0.95336
	loss_value_0: 0.23995
	loss_policy_1: 0.02438
	accuracy_policy_1: 0.94984
	loss_value_1: 0.04779
	loss_reward_1: 0.00447
	loss_policy_2: 0.02426
	accuracy_policy_2: 0.94938
	loss_value_2: 0.0484
	loss_reward_2: 0.00492
	loss_policy_3: 0.02423
	accuracy_policy_3: 0.94832
	loss_value_3: 0.04927
	loss_reward_3: 0.0056
	loss_policy_4: 0.02424
	accuracy_policy_4: 0.9516
	loss_value_4: 0.05007
	loss_reward_4: 0.00673
	loss_policy_5: 0.02417
	accuracy_policy_5: 0.95766
	loss_value_5: 0.05088
	loss_reward_5: 0.00772
	loss_policy: 0.24265
	loss_value: 0.48637
	loss_reward: 0.02942
[2025-05-11 19:35:56] nn step 52550, lr: 0.1.
	loss_policy_0: 0.11175
	accuracy_policy_0: 0.95219
	loss_value_0: 0.2184
	loss_policy_1: 0.02236
	accuracy_policy_1: 0.94852
	loss_value_1: 0.04398
	loss_reward_1: 0.00401
	loss_policy_2: 0.02204
	accuracy_policy_2: 0.95031
	loss_value_2: 0.04445
	loss_reward_2: 0.00466
	loss_policy_3: 0.02255
	accuracy_policy_3: 0.95086
	loss_value_3: 0.04498
	loss_reward_3: 0.00509
	loss_policy_4: 0.0224
	accuracy_policy_4: 0.95164
	loss_value_4: 0.04539
	loss_reward_4: 0.00582
	loss_policy_5: 0.0222
	accuracy_policy_5: 0.95586
	loss_value_5: 0.04642
	loss_reward_5: 0.00699
	loss_policy: 0.22329
	loss_value: 0.44363
	loss_reward: 0.02657
[2025-05-11 19:36:05] nn step 52600, lr: 0.1.
	loss_policy_0: 0.11718
	accuracy_policy_0: 0.95477
	loss_value_0: 0.22453
	loss_policy_1: 0.02316
	accuracy_policy_1: 0.95125
	loss_value_1: 0.04483
	loss_reward_1: 0.00421
	loss_policy_2: 0.02341
	accuracy_policy_2: 0.95023
	loss_value_2: 0.04581
	loss_reward_2: 0.00486
	loss_policy_3: 0.02328
	accuracy_policy_3: 0.94828
	loss_value_3: 0.04658
	loss_reward_3: 0.00508
	loss_policy_4: 0.02343
	accuracy_policy_4: 0.95055
	loss_value_4: 0.04703
	loss_reward_4: 0.00611
	loss_policy_5: 0.02291
	accuracy_policy_5: 0.9568
	loss_value_5: 0.04811
	loss_reward_5: 0.00738
	loss_policy: 0.23337
	loss_value: 0.45689
	loss_reward: 0.02765
Optimization_Done 52600
[2025-05-11 19:37:41] [command] train weight_iter_52600.pkl 245 264
[2025-05-11 19:37:50] nn step 52650, lr: 0.1.
	loss_policy_0: 0.11845
	accuracy_policy_0: 0.95211
	loss_value_0: 0.23864
	loss_policy_1: 0.02348
	accuracy_policy_1: 0.95168
	loss_value_1: 0.04732
	loss_reward_1: 0.00435
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.95316
	loss_value_2: 0.04795
	loss_reward_2: 0.00511
	loss_policy_3: 0.02358
	accuracy_policy_3: 0.94973
	loss_value_3: 0.0488
	loss_reward_3: 0.00558
	loss_policy_4: 0.02325
	accuracy_policy_4: 0.95078
	loss_value_4: 0.04957
	loss_reward_4: 0.00642
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.95441
	loss_value_5: 0.05025
	loss_reward_5: 0.00763
	loss_policy: 0.23588
	loss_value: 0.48252
	loss_reward: 0.02908
[2025-05-11 19:37:59] nn step 52700, lr: 0.1.
	loss_policy_0: 0.1255
	accuracy_policy_0: 0.95176
	loss_value_0: 0.24456
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.94996
	loss_value_1: 0.04886
	loss_reward_1: 0.0047
	loss_policy_2: 0.02498
	accuracy_policy_2: 0.95129
	loss_value_2: 0.04957
	loss_reward_2: 0.00527
	loss_policy_3: 0.02476
	accuracy_policy_3: 0.95004
	loss_value_3: 0.05002
	loss_reward_3: 0.00577
	loss_policy_4: 0.02445
	accuracy_policy_4: 0.9527
	loss_value_4: 0.05097
	loss_reward_4: 0.00675
	loss_policy_5: 0.02447
	accuracy_policy_5: 0.95699
	loss_value_5: 0.05202
	loss_reward_5: 0.00811
	loss_policy: 0.24889
	loss_value: 0.496
	loss_reward: 0.0306
[2025-05-11 19:38:06] nn step 52750, lr: 0.1.
	loss_policy_0: 0.11657
	accuracy_policy_0: 0.9523
	loss_value_0: 0.22719
	loss_policy_1: 0.02311
	accuracy_policy_1: 0.94977
	loss_value_1: 0.04538
	loss_reward_1: 0.00415
	loss_policy_2: 0.02301
	accuracy_policy_2: 0.94973
	loss_value_2: 0.04615
	loss_reward_2: 0.00489
	loss_policy_3: 0.02315
	accuracy_policy_3: 0.94781
	loss_value_3: 0.04673
	loss_reward_3: 0.00543
	loss_policy_4: 0.02332
	accuracy_policy_4: 0.95223
	loss_value_4: 0.0473
	loss_reward_4: 0.0064
	loss_policy_5: 0.02293
	accuracy_policy_5: 0.95551
	loss_value_5: 0.04809
	loss_reward_5: 0.00736
	loss_policy: 0.23209
	loss_value: 0.46083
	loss_reward: 0.02822
[2025-05-11 19:38:15] nn step 52800, lr: 0.1.
	loss_policy_0: 0.12103
	accuracy_policy_0: 0.95082
	loss_value_0: 0.23445
	loss_policy_1: 0.0238
	accuracy_policy_1: 0.95035
	loss_value_1: 0.04706
	loss_reward_1: 0.00439
	loss_policy_2: 0.02394
	accuracy_policy_2: 0.94941
	loss_value_2: 0.04764
	loss_reward_2: 0.00505
	loss_policy_3: 0.02375
	accuracy_policy_3: 0.95062
	loss_value_3: 0.04852
	loss_reward_3: 0.00549
	loss_policy_4: 0.02362
	accuracy_policy_4: 0.9541
	loss_value_4: 0.04946
	loss_reward_4: 0.00631
	loss_policy_5: 0.02371
	accuracy_policy_5: 0.95664
	loss_value_5: 0.04997
	loss_reward_5: 0.00786
	loss_policy: 0.23985
	loss_value: 0.47711
	loss_reward: 0.0291
Optimization_Done 52800
[2025-05-11 19:39:52] [command] train weight_iter_52800.pkl 246 265
[2025-05-11 19:40:00] nn step 52850, lr: 0.1.
	loss_policy_0: 0.11443
	accuracy_policy_0: 0.95578
	loss_value_0: 0.23263
	loss_policy_1: 0.02245
	accuracy_policy_1: 0.94984
	loss_value_1: 0.04629
	loss_reward_1: 0.00419
	loss_policy_2: 0.0224
	accuracy_policy_2: 0.94828
	loss_value_2: 0.04702
	loss_reward_2: 0.00485
	loss_policy_3: 0.02283
	accuracy_policy_3: 0.95125
	loss_value_3: 0.04742
	loss_reward_3: 0.00534
	loss_policy_4: 0.02295
	accuracy_policy_4: 0.94945
	loss_value_4: 0.04804
	loss_reward_4: 0.00646
	loss_policy_5: 0.02284
	accuracy_policy_5: 0.95367
	loss_value_5: 0.04886
	loss_reward_5: 0.00728
	loss_policy: 0.22791
	loss_value: 0.47025
	loss_reward: 0.02812
[2025-05-11 19:40:08] nn step 52900, lr: 0.1.
	loss_policy_0: 0.11526
	accuracy_policy_0: 0.95492
	loss_value_0: 0.23014
	loss_policy_1: 0.02324
	accuracy_policy_1: 0.9507
	loss_value_1: 0.04595
	loss_reward_1: 0.00396
	loss_policy_2: 0.02282
	accuracy_policy_2: 0.95086
	loss_value_2: 0.04668
	loss_reward_2: 0.00477
	loss_policy_3: 0.02316
	accuracy_policy_3: 0.94859
	loss_value_3: 0.04745
	loss_reward_3: 0.00526
	loss_policy_4: 0.02274
	accuracy_policy_4: 0.95027
	loss_value_4: 0.04791
	loss_reward_4: 0.00611
	loss_policy_5: 0.0228
	accuracy_policy_5: 0.95457
	loss_value_5: 0.04874
	loss_reward_5: 0.00716
	loss_policy: 0.23001
	loss_value: 0.46687
	loss_reward: 0.02725
[2025-05-11 19:40:17] nn step 52950, lr: 0.1.
	loss_policy_0: 0.12788
	accuracy_policy_0: 0.95387
	loss_value_0: 0.25412
	loss_policy_1: 0.02526
	accuracy_policy_1: 0.94758
	loss_value_1: 0.05094
	loss_reward_1: 0.00471
	loss_policy_2: 0.02551
	accuracy_policy_2: 0.95055
	loss_value_2: 0.05176
	loss_reward_2: 0.00554
	loss_policy_3: 0.02524
	accuracy_policy_3: 0.95039
	loss_value_3: 0.05258
	loss_reward_3: 0.00602
	loss_policy_4: 0.02534
	accuracy_policy_4: 0.95348
	loss_value_4: 0.0534
	loss_reward_4: 0.00699
	loss_policy_5: 0.02577
	accuracy_policy_5: 0.95383
	loss_value_5: 0.05461
	loss_reward_5: 0.00836
	loss_policy: 0.25499
	loss_value: 0.51742
	loss_reward: 0.03163
[2025-05-11 19:40:24] nn step 53000, lr: 0.1.
	loss_policy_0: 0.12297
	accuracy_policy_0: 0.95422
	loss_value_0: 0.24002
	loss_policy_1: 0.02423
	accuracy_policy_1: 0.95059
	loss_value_1: 0.04825
	loss_reward_1: 0.00453
	loss_policy_2: 0.02467
	accuracy_policy_2: 0.9502
	loss_value_2: 0.04908
	loss_reward_2: 0.00521
	loss_policy_3: 0.02444
	accuracy_policy_3: 0.94973
	loss_value_3: 0.04963
	loss_reward_3: 0.00568
	loss_policy_4: 0.02451
	accuracy_policy_4: 0.95176
	loss_value_4: 0.05026
	loss_reward_4: 0.00662
	loss_policy_5: 0.02377
	accuracy_policy_5: 0.95672
	loss_value_5: 0.05121
	loss_reward_5: 0.0079
	loss_policy: 0.2446
	loss_value: 0.48843
	loss_reward: 0.02994
Optimization_Done 53000
[2025-05-11 19:42:04] [command] train weight_iter_53000.pkl 247 266
[2025-05-11 19:42:14] nn step 53050, lr: 0.1.
	loss_policy_0: 0.12017
	accuracy_policy_0: 0.95195
	loss_value_0: 0.24145
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.94828
	loss_value_1: 0.04806
	loss_reward_1: 0.00445
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.94961
	loss_value_2: 0.04867
	loss_reward_2: 0.00501
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.94941
	loss_value_3: 0.04914
	loss_reward_3: 0.00535
	loss_policy_4: 0.02369
	accuracy_policy_4: 0.95273
	loss_value_4: 0.04976
	loss_reward_4: 0.00659
	loss_policy_5: 0.02373
	accuracy_policy_5: 0.95668
	loss_value_5: 0.05091
	loss_reward_5: 0.00781
	loss_policy: 0.23888
	loss_value: 0.488
	loss_reward: 0.0292
[2025-05-11 19:42:21] nn step 53100, lr: 0.1.
	loss_policy_0: 0.116
	accuracy_policy_0: 0.95309
	loss_value_0: 0.22975
	loss_policy_1: 0.02298
	accuracy_policy_1: 0.95078
	loss_value_1: 0.04578
	loss_reward_1: 0.00401
	loss_policy_2: 0.02301
	accuracy_policy_2: 0.94816
	loss_value_2: 0.0464
	loss_reward_2: 0.0047
	loss_policy_3: 0.02288
	accuracy_policy_3: 0.94844
	loss_value_3: 0.04677
	loss_reward_3: 0.00525
	loss_policy_4: 0.02286
	accuracy_policy_4: 0.95168
	loss_value_4: 0.0474
	loss_reward_4: 0.00612
	loss_policy_5: 0.02312
	accuracy_policy_5: 0.9568
	loss_value_5: 0.04823
	loss_reward_5: 0.00743
	loss_policy: 0.23085
	loss_value: 0.46433
	loss_reward: 0.02751
[2025-05-11 19:42:30] nn step 53150, lr: 0.1.
	loss_policy_0: 0.1167
	accuracy_policy_0: 0.95117
	loss_value_0: 0.2289
	loss_policy_1: 0.02338
	accuracy_policy_1: 0.94852
	loss_value_1: 0.04593
	loss_reward_1: 0.00424
	loss_policy_2: 0.02313
	accuracy_policy_2: 0.95043
	loss_value_2: 0.04645
	loss_reward_2: 0.00487
	loss_policy_3: 0.02352
	accuracy_policy_3: 0.94711
	loss_value_3: 0.04718
	loss_reward_3: 0.00513
	loss_policy_4: 0.02315
	accuracy_policy_4: 0.95039
	loss_value_4: 0.04777
	loss_reward_4: 0.00597
	loss_policy_5: 0.02317
	accuracy_policy_5: 0.95691
	loss_value_5: 0.04866
	loss_reward_5: 0.00733
	loss_policy: 0.23305
	loss_value: 0.4649
	loss_reward: 0.02753
[2025-05-11 19:42:38] nn step 53200, lr: 0.1.
	loss_policy_0: 0.11132
	accuracy_policy_0: 0.95098
	loss_value_0: 0.22071
	loss_policy_1: 0.02224
	accuracy_policy_1: 0.95082
	loss_value_1: 0.04434
	loss_reward_1: 0.00404
	loss_policy_2: 0.02244
	accuracy_policy_2: 0.94707
	loss_value_2: 0.04503
	loss_reward_2: 0.00459
	loss_policy_3: 0.02271
	accuracy_policy_3: 0.94891
	loss_value_3: 0.04552
	loss_reward_3: 0.00508
	loss_policy_4: 0.02261
	accuracy_policy_4: 0.94789
	loss_value_4: 0.04615
	loss_reward_4: 0.00585
	loss_policy_5: 0.02218
	accuracy_policy_5: 0.9552
	loss_value_5: 0.04695
	loss_reward_5: 0.0068
	loss_policy: 0.2235
	loss_value: 0.4487
	loss_reward: 0.02636
Optimization_Done 53200
[2025-05-11 19:44:15] [command] train weight_iter_53200.pkl 248 267
[2025-05-11 19:44:24] nn step 53250, lr: 0.1.
	loss_policy_0: 0.11935
	accuracy_policy_0: 0.95113
	loss_value_0: 0.24361
	loss_policy_1: 0.02385
	accuracy_policy_1: 0.9509
	loss_value_1: 0.04839
	loss_reward_1: 0.00445
	loss_policy_2: 0.02357
	accuracy_policy_2: 0.94777
	loss_value_2: 0.04947
	loss_reward_2: 0.00489
	loss_policy_3: 0.02386
	accuracy_policy_3: 0.95094
	loss_value_3: 0.0499
	loss_reward_3: 0.00558
	loss_policy_4: 0.02392
	accuracy_policy_4: 0.95336
	loss_value_4: 0.0508
	loss_reward_4: 0.00645
	loss_policy_5: 0.02401
	accuracy_policy_5: 0.95691
	loss_value_5: 0.05138
	loss_reward_5: 0.00737
	loss_policy: 0.23856
	loss_value: 0.49355
	loss_reward: 0.02875
[2025-05-11 19:44:33] nn step 53300, lr: 0.1.
	loss_policy_0: 0.11884
	accuracy_policy_0: 0.95434
	loss_value_0: 0.23739
	loss_policy_1: 0.02376
	accuracy_policy_1: 0.94914
	loss_value_1: 0.04713
	loss_reward_1: 0.00434
	loss_policy_2: 0.02365
	accuracy_policy_2: 0.94949
	loss_value_2: 0.04799
	loss_reward_2: 0.00509
	loss_policy_3: 0.02332
	accuracy_policy_3: 0.94934
	loss_value_3: 0.04851
	loss_reward_3: 0.00538
	loss_policy_4: 0.02378
	accuracy_policy_4: 0.9493
	loss_value_4: 0.0493
	loss_reward_4: 0.00644
	loss_policy_5: 0.02324
	accuracy_policy_5: 0.95465
	loss_value_5: 0.05042
	loss_reward_5: 0.00759
	loss_policy: 0.23659
	loss_value: 0.48073
	loss_reward: 0.02884
[2025-05-11 19:44:40] nn step 53350, lr: 0.1.
	loss_policy_0: 0.12093
	accuracy_policy_0: 0.95051
	loss_value_0: 0.23991
	loss_policy_1: 0.02391
	accuracy_policy_1: 0.94867
	loss_value_1: 0.04786
	loss_reward_1: 0.00438
	loss_policy_2: 0.0242
	accuracy_policy_2: 0.94875
	loss_value_2: 0.04855
	loss_reward_2: 0.00517
	loss_policy_3: 0.02372
	accuracy_policy_3: 0.94812
	loss_value_3: 0.04904
	loss_reward_3: 0.00569
	loss_policy_4: 0.02378
	accuracy_policy_4: 0.95078
	loss_value_4: 0.0499
	loss_reward_4: 0.00675
	loss_policy_5: 0.02378
	accuracy_policy_5: 0.95602
	loss_value_5: 0.05095
	loss_reward_5: 0.00785
	loss_policy: 0.24032
	loss_value: 0.4862
	loss_reward: 0.02984
[2025-05-11 19:44:48] nn step 53400, lr: 0.1.
	loss_policy_0: 0.11837
	accuracy_policy_0: 0.95215
	loss_value_0: 0.23751
	loss_policy_1: 0.02376
	accuracy_policy_1: 0.94941
	loss_value_1: 0.04751
	loss_reward_1: 0.00431
	loss_policy_2: 0.02342
	accuracy_policy_2: 0.9484
	loss_value_2: 0.04801
	loss_reward_2: 0.00506
	loss_policy_3: 0.02381
	accuracy_policy_3: 0.94664
	loss_value_3: 0.04862
	loss_reward_3: 0.00562
	loss_policy_4: 0.02364
	accuracy_policy_4: 0.95031
	loss_value_4: 0.0494
	loss_reward_4: 0.00625
	loss_policy_5: 0.02372
	accuracy_policy_5: 0.95566
	loss_value_5: 0.05057
	loss_reward_5: 0.00765
	loss_policy: 0.23672
	loss_value: 0.4816
	loss_reward: 0.0289
Optimization_Done 53400
[2025-05-11 19:46:26] [command] train weight_iter_53400.pkl 249 268
[2025-05-11 19:46:34] nn step 53450, lr: 0.1.
	loss_policy_0: 0.1184
	accuracy_policy_0: 0.95195
	loss_value_0: 0.23892
	loss_policy_1: 0.02313
	accuracy_policy_1: 0.94992
	loss_value_1: 0.04717
	loss_reward_1: 0.00425
	loss_policy_2: 0.02326
	accuracy_policy_2: 0.94977
	loss_value_2: 0.04807
	loss_reward_2: 0.0051
	loss_policy_3: 0.02368
	accuracy_policy_3: 0.94891
	loss_value_3: 0.04845
	loss_reward_3: 0.00569
	loss_policy_4: 0.02344
	accuracy_policy_4: 0.95199
	loss_value_4: 0.04906
	loss_reward_4: 0.0065
	loss_policy_5: 0.02325
	accuracy_policy_5: 0.95574
	loss_value_5: 0.04986
	loss_reward_5: 0.0077
	loss_policy: 0.23516
	loss_value: 0.48152
	loss_reward: 0.02924
[2025-05-11 19:46:42] nn step 53500, lr: 0.1.
	loss_policy_0: 0.12441
	accuracy_policy_0: 0.95145
	loss_value_0: 0.24653
	loss_policy_1: 0.02452
	accuracy_policy_1: 0.94996
	loss_value_1: 0.04903
	loss_reward_1: 0.00457
	loss_policy_2: 0.02492
	accuracy_policy_2: 0.94973
	loss_value_2: 0.04969
	loss_reward_2: 0.00518
	loss_policy_3: 0.02472
	accuracy_policy_3: 0.94926
	loss_value_3: 0.05055
	loss_reward_3: 0.00557
	loss_policy_4: 0.02463
	accuracy_policy_4: 0.95109
	loss_value_4: 0.0512
	loss_reward_4: 0.00655
	loss_policy_5: 0.02489
	accuracy_policy_5: 0.95602
	loss_value_5: 0.05212
	loss_reward_5: 0.00824
	loss_policy: 0.24808
	loss_value: 0.49911
	loss_reward: 0.03011
[2025-05-11 19:46:51] nn step 53550, lr: 0.1.
	loss_policy_0: 0.1197
	accuracy_policy_0: 0.95453
	loss_value_0: 0.23607
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.95074
	loss_value_1: 0.04695
	loss_reward_1: 0.00448
	loss_policy_2: 0.02378
	accuracy_policy_2: 0.9518
	loss_value_2: 0.04775
	loss_reward_2: 0.00497
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.95121
	loss_value_3: 0.04831
	loss_reward_3: 0.00557
	loss_policy_4: 0.02324
	accuracy_policy_4: 0.95531
	loss_value_4: 0.04922
	loss_reward_4: 0.00614
	loss_policy_5: 0.02338
	accuracy_policy_5: 0.95668
	loss_value_5: 0.04999
	loss_reward_5: 0.00772
	loss_policy: 0.23783
	loss_value: 0.47829
	loss_reward: 0.02889
[2025-05-11 19:46:58] nn step 53600, lr: 0.1.
	loss_policy_0: 0.1241
	accuracy_policy_0: 0.95035
	loss_value_0: 0.24074
	loss_policy_1: 0.02422
	accuracy_policy_1: 0.94992
	loss_value_1: 0.04801
	loss_reward_1: 0.00457
	loss_policy_2: 0.02425
	accuracy_policy_2: 0.94859
	loss_value_2: 0.04859
	loss_reward_2: 0.00523
	loss_policy_3: 0.02471
	accuracy_policy_3: 0.94918
	loss_value_3: 0.04907
	loss_reward_3: 0.00569
	loss_policy_4: 0.02453
	accuracy_policy_4: 0.9498
	loss_value_4: 0.04993
	loss_reward_4: 0.00696
	loss_policy_5: 0.02423
	accuracy_policy_5: 0.95859
	loss_value_5: 0.0508
	loss_reward_5: 0.00804
	loss_policy: 0.24604
	loss_value: 0.48713
	loss_reward: 0.03048
Optimization_Done 53600
[2025-05-11 19:48:33] [command] train weight_iter_53600.pkl 250 269
[2025-05-11 19:48:42] nn step 53650, lr: 0.1.
	loss_policy_0: 0.11695
	accuracy_policy_0: 0.95387
	loss_value_0: 0.23435
	loss_policy_1: 0.02286
	accuracy_policy_1: 0.95059
	loss_value_1: 0.04666
	loss_reward_1: 0.00443
	loss_policy_2: 0.02308
	accuracy_policy_2: 0.95184
	loss_value_2: 0.04733
	loss_reward_2: 0.00508
	loss_policy_3: 0.02342
	accuracy_policy_3: 0.95023
	loss_value_3: 0.04813
	loss_reward_3: 0.00548
	loss_policy_4: 0.02286
	accuracy_policy_4: 0.95488
	loss_value_4: 0.04852
	loss_reward_4: 0.00631
	loss_policy_5: 0.02276
	accuracy_policy_5: 0.96145
	loss_value_5: 0.04933
	loss_reward_5: 0.00755
	loss_policy: 0.23193
	loss_value: 0.47432
	loss_reward: 0.02885
[2025-05-11 19:48:51] nn step 53700, lr: 0.1.
	loss_policy_0: 0.12039
	accuracy_policy_0: 0.95398
	loss_value_0: 0.23126
	loss_policy_1: 0.02332
	accuracy_policy_1: 0.94801
	loss_value_1: 0.04647
	loss_reward_1: 0.00478
	loss_policy_2: 0.02382
	accuracy_policy_2: 0.94988
	loss_value_2: 0.04713
	loss_reward_2: 0.00528
	loss_policy_3: 0.02356
	accuracy_policy_3: 0.94902
	loss_value_3: 0.04788
	loss_reward_3: 0.00547
	loss_policy_4: 0.02331
	accuracy_policy_4: 0.95059
	loss_value_4: 0.0487
	loss_reward_4: 0.00651
	loss_policy_5: 0.02322
	accuracy_policy_5: 0.95578
	loss_value_5: 0.04964
	loss_reward_5: 0.00772
	loss_policy: 0.2376
	loss_value: 0.47108
	loss_reward: 0.02977
[2025-05-11 19:48:58] nn step 53750, lr: 0.1.
	loss_policy_0: 0.12553
	accuracy_policy_0: 0.95324
	loss_value_0: 0.24797
	loss_policy_1: 0.02491
	accuracy_policy_1: 0.95074
	loss_value_1: 0.04942
	loss_reward_1: 0.00466
	loss_policy_2: 0.02503
	accuracy_policy_2: 0.94895
	loss_value_2: 0.05016
	loss_reward_2: 0.00542
	loss_policy_3: 0.02536
	accuracy_policy_3: 0.9484
	loss_value_3: 0.05067
	loss_reward_3: 0.00574
	loss_policy_4: 0.02516
	accuracy_policy_4: 0.94965
	loss_value_4: 0.05124
	loss_reward_4: 0.00662
	loss_policy_5: 0.02481
	accuracy_policy_5: 0.95625
	loss_value_5: 0.052
	loss_reward_5: 0.00799
	loss_policy: 0.25081
	loss_value: 0.50147
	loss_reward: 0.03041
[2025-05-11 19:49:06] nn step 53800, lr: 0.1.
	loss_policy_0: 0.12265
	accuracy_policy_0: 0.95109
	loss_value_0: 0.24016
	loss_policy_1: 0.02439
	accuracy_policy_1: 0.94855
	loss_value_1: 0.04799
	loss_reward_1: 0.00447
	loss_policy_2: 0.02435
	accuracy_policy_2: 0.9459
	loss_value_2: 0.04847
	loss_reward_2: 0.00509
	loss_policy_3: 0.02447
	accuracy_policy_3: 0.94578
	loss_value_3: 0.04887
	loss_reward_3: 0.00546
	loss_policy_4: 0.02473
	accuracy_policy_4: 0.94848
	loss_value_4: 0.04954
	loss_reward_4: 0.00649
	loss_policy_5: 0.02396
	accuracy_policy_5: 0.95688
	loss_value_5: 0.05033
	loss_reward_5: 0.00758
	loss_policy: 0.24455
	loss_value: 0.48536
	loss_reward: 0.0291
Optimization_Done 53800
[2025-05-11 19:50:45] [command] train weight_iter_53800.pkl 251 270
[2025-05-11 19:50:53] nn step 53850, lr: 0.1.
	loss_policy_0: 0.11973
	accuracy_policy_0: 0.95234
	loss_value_0: 0.23945
	loss_policy_1: 0.02395
	accuracy_policy_1: 0.94793
	loss_value_1: 0.04769
	loss_reward_1: 0.00455
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.95121
	loss_value_2: 0.04856
	loss_reward_2: 0.00501
	loss_policy_3: 0.0237
	accuracy_policy_3: 0.95102
	loss_value_3: 0.04894
	loss_reward_3: 0.00558
	loss_policy_4: 0.02396
	accuracy_policy_4: 0.94918
	loss_value_4: 0.0497
	loss_reward_4: 0.00702
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.95828
	loss_value_5: 0.05046
	loss_reward_5: 0.0081
	loss_policy: 0.23848
	loss_value: 0.4848
	loss_reward: 0.03026
[2025-05-11 19:51:01] nn step 53900, lr: 0.1.
	loss_policy_0: 0.11623
	accuracy_policy_0: 0.95422
	loss_value_0: 0.23111
	loss_policy_1: 0.02333
	accuracy_policy_1: 0.95195
	loss_value_1: 0.04609
	loss_reward_1: 0.00427
	loss_policy_2: 0.02345
	accuracy_policy_2: 0.94938
	loss_value_2: 0.04663
	loss_reward_2: 0.00469
	loss_policy_3: 0.02311
	accuracy_policy_3: 0.9482
	loss_value_3: 0.04713
	loss_reward_3: 0.00518
	loss_policy_4: 0.02316
	accuracy_policy_4: 0.95316
	loss_value_4: 0.04807
	loss_reward_4: 0.00627
	loss_policy_5: 0.02311
	accuracy_policy_5: 0.95758
	loss_value_5: 0.04891
	loss_reward_5: 0.00743
	loss_policy: 0.23239
	loss_value: 0.46794
	loss_reward: 0.02784
[2025-05-11 19:51:10] nn step 53950, lr: 0.1.
	loss_policy_0: 0.12055
	accuracy_policy_0: 0.95316
	loss_value_0: 0.23696
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.95047
	loss_value_1: 0.04748
	loss_reward_1: 0.00444
	loss_policy_2: 0.02424
	accuracy_policy_2: 0.94984
	loss_value_2: 0.04816
	loss_reward_2: 0.00512
	loss_policy_3: 0.02434
	accuracy_policy_3: 0.94906
	loss_value_3: 0.04915
	loss_reward_3: 0.00555
	loss_policy_4: 0.02397
	accuracy_policy_4: 0.94973
	loss_value_4: 0.0494
	loss_reward_4: 0.00634
	loss_policy_5: 0.02384
	accuracy_policy_5: 0.95789
	loss_value_5: 0.05015
	loss_reward_5: 0.00763
	loss_policy: 0.24089
	loss_value: 0.48131
	loss_reward: 0.02909
[2025-05-11 19:51:18] nn step 54000, lr: 0.1.
	loss_policy_0: 0.11049
	accuracy_policy_0: 0.95371
	loss_value_0: 0.21398
	loss_policy_1: 0.02175
	accuracy_policy_1: 0.94879
	loss_value_1: 0.04238
	loss_reward_1: 0.00401
	loss_policy_2: 0.02182
	accuracy_policy_2: 0.95047
	loss_value_2: 0.04301
	loss_reward_2: 0.00442
	loss_policy_3: 0.02252
	accuracy_policy_3: 0.94801
	loss_value_3: 0.04367
	loss_reward_3: 0.00487
	loss_policy_4: 0.02188
	accuracy_policy_4: 0.95027
	loss_value_4: 0.04435
	loss_reward_4: 0.00572
	loss_policy_5: 0.02139
	accuracy_policy_5: 0.95836
	loss_value_5: 0.04533
	loss_reward_5: 0.00699
	loss_policy: 0.21985
	loss_value: 0.43272
	loss_reward: 0.02601
Optimization_Done 54000
[2025-05-11 19:52:54] [command] train weight_iter_54000.pkl 252 271
[2025-05-11 19:53:03] nn step 54050, lr: 0.1.
	loss_policy_0: 0.11252
	accuracy_policy_0: 0.95289
	loss_value_0: 0.22764
	loss_policy_1: 0.02228
	accuracy_policy_1: 0.95059
	loss_value_1: 0.04485
	loss_reward_1: 0.00399
	loss_policy_2: 0.02252
	accuracy_policy_2: 0.94973
	loss_value_2: 0.04555
	loss_reward_2: 0.00469
	loss_policy_3: 0.02243
	accuracy_policy_3: 0.94965
	loss_value_3: 0.04623
	loss_reward_3: 0.00507
	loss_policy_4: 0.02226
	accuracy_policy_4: 0.95016
	loss_value_4: 0.04684
	loss_reward_4: 0.00588
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.9577
	loss_value_5: 0.04782
	loss_reward_5: 0.00699
	loss_policy: 0.22461
	loss_value: 0.45893
	loss_reward: 0.02664
[2025-05-11 19:53:11] nn step 54100, lr: 0.1.
	loss_policy_0: 0.12197
	accuracy_policy_0: 0.95262
	loss_value_0: 0.24243
	loss_policy_1: 0.02452
	accuracy_policy_1: 0.94816
	loss_value_1: 0.04836
	loss_reward_1: 0.0045
	loss_policy_2: 0.0241
	accuracy_policy_2: 0.95047
	loss_value_2: 0.04904
	loss_reward_2: 0.00508
	loss_policy_3: 0.0244
	accuracy_policy_3: 0.94949
	loss_value_3: 0.0496
	loss_reward_3: 0.00558
	loss_policy_4: 0.02452
	accuracy_policy_4: 0.94816
	loss_value_4: 0.05006
	loss_reward_4: 0.00645
	loss_policy_5: 0.02433
	accuracy_policy_5: 0.95738
	loss_value_5: 0.05067
	loss_reward_5: 0.0078
	loss_policy: 0.24385
	loss_value: 0.49015
	loss_reward: 0.02941
[2025-05-11 19:53:19] nn step 54150, lr: 0.1.
	loss_policy_0: 0.1138
	accuracy_policy_0: 0.95094
	loss_value_0: 0.22145
	loss_policy_1: 0.02236
	accuracy_policy_1: 0.94902
	loss_value_1: 0.04428
	loss_reward_1: 0.00442
	loss_policy_2: 0.02246
	accuracy_policy_2: 0.94781
	loss_value_2: 0.04492
	loss_reward_2: 0.00481
	loss_policy_3: 0.02244
	accuracy_policy_3: 0.94793
	loss_value_3: 0.04548
	loss_reward_3: 0.00531
	loss_policy_4: 0.02257
	accuracy_policy_4: 0.95082
	loss_value_4: 0.04607
	loss_reward_4: 0.00626
	loss_policy_5: 0.02229
	accuracy_policy_5: 0.95898
	loss_value_5: 0.04685
	loss_reward_5: 0.00757
	loss_policy: 0.22593
	loss_value: 0.44906
	loss_reward: 0.02837
[2025-05-11 19:53:28] nn step 54200, lr: 0.1.
	loss_policy_0: 0.11402
	accuracy_policy_0: 0.95523
	loss_value_0: 0.22466
	loss_policy_1: 0.02254
	accuracy_policy_1: 0.95117
	loss_value_1: 0.04494
	loss_reward_1: 0.0043
	loss_policy_2: 0.02297
	accuracy_policy_2: 0.95055
	loss_value_2: 0.04544
	loss_reward_2: 0.00466
	loss_policy_3: 0.02302
	accuracy_policy_3: 0.94848
	loss_value_3: 0.04605
	loss_reward_3: 0.00541
	loss_policy_4: 0.02253
	accuracy_policy_4: 0.9509
	loss_value_4: 0.04695
	loss_reward_4: 0.00625
	loss_policy_5: 0.02245
	accuracy_policy_5: 0.95812
	loss_value_5: 0.04792
	loss_reward_5: 0.0072
	loss_policy: 0.22753
	loss_value: 0.45597
	loss_reward: 0.02781
Optimization_Done 54200
[2025-05-11 19:55:01] [command] train weight_iter_54200.pkl 253 272
[2025-05-11 19:55:11] nn step 54250, lr: 0.1.
	loss_policy_0: 0.10927
	accuracy_policy_0: 0.95422
	loss_value_0: 0.22435
	loss_policy_1: 0.02176
	accuracy_policy_1: 0.95215
	loss_value_1: 0.04457
	loss_reward_1: 0.00403
	loss_policy_2: 0.02186
	accuracy_policy_2: 0.94953
	loss_value_2: 0.0451
	loss_reward_2: 0.00461
	loss_policy_3: 0.02206
	accuracy_policy_3: 0.94902
	loss_value_3: 0.04566
	loss_reward_3: 0.00516
	loss_policy_4: 0.02166
	accuracy_policy_4: 0.95137
	loss_value_4: 0.0464
	loss_reward_4: 0.00594
	loss_policy_5: 0.02177
	accuracy_policy_5: 0.95789
	loss_value_5: 0.0473
	loss_reward_5: 0.00711
	loss_policy: 0.21838
	loss_value: 0.45338
	loss_reward: 0.02684
[2025-05-11 19:55:20] nn step 54300, lr: 0.1.
	loss_policy_0: 0.12059
	accuracy_policy_0: 0.95336
	loss_value_0: 0.23831
	loss_policy_1: 0.02401
	accuracy_policy_1: 0.9509
	loss_value_1: 0.04749
	loss_reward_1: 0.00455
	loss_policy_2: 0.02412
	accuracy_policy_2: 0.94938
	loss_value_2: 0.04824
	loss_reward_2: 0.00503
	loss_policy_3: 0.02397
	accuracy_policy_3: 0.95062
	loss_value_3: 0.04881
	loss_reward_3: 0.00573
	loss_policy_4: 0.0239
	accuracy_policy_4: 0.95121
	loss_value_4: 0.04966
	loss_reward_4: 0.00661
	loss_policy_5: 0.02379
	accuracy_policy_5: 0.95738
	loss_value_5: 0.05042
	loss_reward_5: 0.00754
	loss_policy: 0.24039
	loss_value: 0.48292
	loss_reward: 0.02945
[2025-05-11 19:55:28] nn step 54350, lr: 0.1.
	loss_policy_0: 0.11538
	accuracy_policy_0: 0.95238
	loss_value_0: 0.22612
	loss_policy_1: 0.02262
	accuracy_policy_1: 0.95035
	loss_value_1: 0.04508
	loss_reward_1: 0.00441
	loss_policy_2: 0.02272
	accuracy_policy_2: 0.95121
	loss_value_2: 0.04547
	loss_reward_2: 0.005
	loss_policy_3: 0.02274
	accuracy_policy_3: 0.95078
	loss_value_3: 0.04626
	loss_reward_3: 0.00523
	loss_policy_4: 0.02285
	accuracy_policy_4: 0.95164
	loss_value_4: 0.04718
	loss_reward_4: 0.00617
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.95594
	loss_value_5: 0.04786
	loss_reward_5: 0.00728
	loss_policy: 0.22891
	loss_value: 0.45796
	loss_reward: 0.02809
[2025-05-11 19:55:35] nn step 54400, lr: 0.1.
	loss_policy_0: 0.11977
	accuracy_policy_0: 0.95441
	loss_value_0: 0.23235
	loss_policy_1: 0.02405
	accuracy_policy_1: 0.9525
	loss_value_1: 0.04652
	loss_reward_1: 0.00461
	loss_policy_2: 0.02388
	accuracy_policy_2: 0.95
	loss_value_2: 0.04719
	loss_reward_2: 0.00551
	loss_policy_3: 0.02402
	accuracy_policy_3: 0.94832
	loss_value_3: 0.04782
	loss_reward_3: 0.00574
	loss_policy_4: 0.02396
	accuracy_policy_4: 0.94863
	loss_value_4: 0.04868
	loss_reward_4: 0.00648
	loss_policy_5: 0.02343
	accuracy_policy_5: 0.95781
	loss_value_5: 0.04977
	loss_reward_5: 0.00791
	loss_policy: 0.23911
	loss_value: 0.47233
	loss_reward: 0.03025
Optimization_Done 54400
[2025-05-11 19:57:13] [command] train weight_iter_54400.pkl 254 273
[2025-05-11 19:57:21] nn step 54450, lr: 0.1.
	loss_policy_0: 0.11985
	accuracy_policy_0: 0.9559
	loss_value_0: 0.23792
	loss_policy_1: 0.02372
	accuracy_policy_1: 0.95328
	loss_value_1: 0.04767
	loss_reward_1: 0.00451
	loss_policy_2: 0.02389
	accuracy_policy_2: 0.95141
	loss_value_2: 0.04843
	loss_reward_2: 0.00501
	loss_policy_3: 0.02371
	accuracy_policy_3: 0.9523
	loss_value_3: 0.04902
	loss_reward_3: 0.00539
	loss_policy_4: 0.02371
	accuracy_policy_4: 0.94996
	loss_value_4: 0.04957
	loss_reward_4: 0.00653
	loss_policy_5: 0.02363
	accuracy_policy_5: 0.95934
	loss_value_5: 0.05022
	loss_reward_5: 0.00784
	loss_policy: 0.23851
	loss_value: 0.48284
	loss_reward: 0.02928
[2025-05-11 19:57:30] nn step 54500, lr: 0.1.
	loss_policy_0: 0.11743
	accuracy_policy_0: 0.95352
	loss_value_0: 0.23316
	loss_policy_1: 0.0234
	accuracy_policy_1: 0.95148
	loss_value_1: 0.04645
	loss_reward_1: 0.00471
	loss_policy_2: 0.02347
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04726
	loss_reward_2: 0.005
	loss_policy_3: 0.02356
	accuracy_policy_3: 0.94598
	loss_value_3: 0.04787
	loss_reward_3: 0.00576
	loss_policy_4: 0.02337
	accuracy_policy_4: 0.94941
	loss_value_4: 0.04845
	loss_reward_4: 0.00638
	loss_policy_5: 0.02333
	accuracy_policy_5: 0.95574
	loss_value_5: 0.04959
	loss_reward_5: 0.00773
	loss_policy: 0.23455
	loss_value: 0.47278
	loss_reward: 0.02957
[2025-05-11 19:57:38] nn step 54550, lr: 0.1.
	loss_policy_0: 0.12432
	accuracy_policy_0: 0.95504
	loss_value_0: 0.24346
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.95137
	loss_value_1: 0.04886
	loss_reward_1: 0.00475
	loss_policy_2: 0.02447
	accuracy_policy_2: 0.94953
	loss_value_2: 0.04961
	loss_reward_2: 0.00535
	loss_policy_3: 0.02448
	accuracy_policy_3: 0.94914
	loss_value_3: 0.05013
	loss_reward_3: 0.00586
	loss_policy_4: 0.02469
	accuracy_policy_4: 0.95086
	loss_value_4: 0.05117
	loss_reward_4: 0.00672
	loss_policy_5: 0.02435
	accuracy_policy_5: 0.95668
	loss_value_5: 0.05192
	loss_reward_5: 0.0081
	loss_policy: 0.24705
	loss_value: 0.49514
	loss_reward: 0.03078
[2025-05-11 19:57:47] nn step 54600, lr: 0.1.
	loss_policy_0: 0.12094
	accuracy_policy_0: 0.95379
	loss_value_0: 0.23549
	loss_policy_1: 0.02421
	accuracy_policy_1: 0.94902
	loss_value_1: 0.04714
	loss_reward_1: 0.00465
	loss_policy_2: 0.024
	accuracy_policy_2: 0.9523
	loss_value_2: 0.04784
	loss_reward_2: 0.00527
	loss_policy_3: 0.02432
	accuracy_policy_3: 0.94922
	loss_value_3: 0.04859
	loss_reward_3: 0.00562
	loss_policy_4: 0.024
	accuracy_policy_4: 0.9493
	loss_value_4: 0.04921
	loss_reward_4: 0.00652
	loss_policy_5: 0.02395
	accuracy_policy_5: 0.95809
	loss_value_5: 0.04989
	loss_reward_5: 0.00794
	loss_policy: 0.24144
	loss_value: 0.47816
	loss_reward: 0.03
Optimization_Done 54600
[2025-05-11 19:59:25] [command] train weight_iter_54600.pkl 255 274
[2025-05-11 19:59:35] nn step 54650, lr: 0.1.
	loss_policy_0: 0.1162
	accuracy_policy_0: 0.955
	loss_value_0: 0.23529
	loss_policy_1: 0.02294
	accuracy_policy_1: 0.95199
	loss_value_1: 0.04664
	loss_reward_1: 0.00446
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95094
	loss_value_2: 0.0476
	loss_reward_2: 0.00503
	loss_policy_3: 0.02325
	accuracy_policy_3: 0.95094
	loss_value_3: 0.04806
	loss_reward_3: 0.00534
	loss_policy_4: 0.02276
	accuracy_policy_4: 0.95504
	loss_value_4: 0.04871
	loss_reward_4: 0.00622
	loss_policy_5: 0.02291
	accuracy_policy_5: 0.95852
	loss_value_5: 0.04967
	loss_reward_5: 0.00745
	loss_policy: 0.23114
	loss_value: 0.47595
	loss_reward: 0.0285
[2025-05-11 19:59:44] nn step 54700, lr: 0.1.
	loss_policy_0: 0.11138
	accuracy_policy_0: 0.95523
	loss_value_0: 0.21624
	loss_policy_1: 0.02197
	accuracy_policy_1: 0.95426
	loss_value_1: 0.04303
	loss_reward_1: 0.00406
	loss_policy_2: 0.02221
	accuracy_policy_2: 0.95137
	loss_value_2: 0.04363
	loss_reward_2: 0.00468
	loss_policy_3: 0.02213
	accuracy_policy_3: 0.95078
	loss_value_3: 0.04434
	loss_reward_3: 0.0052
	loss_policy_4: 0.0216
	accuracy_policy_4: 0.95418
	loss_value_4: 0.04491
	loss_reward_4: 0.00603
	loss_policy_5: 0.02189
	accuracy_policy_5: 0.95957
	loss_value_5: 0.04566
	loss_reward_5: 0.00706
	loss_policy: 0.22118
	loss_value: 0.43782
	loss_reward: 0.02703
[2025-05-11 19:59:51] nn step 54750, lr: 0.1.
	loss_policy_0: 0.11361
	accuracy_policy_0: 0.95621
	loss_value_0: 0.22384
	loss_policy_1: 0.02227
	accuracy_policy_1: 0.95289
	loss_value_1: 0.04476
	loss_reward_1: 0.00429
	loss_policy_2: 0.02268
	accuracy_policy_2: 0.95234
	loss_value_2: 0.0453
	loss_reward_2: 0.00486
	loss_policy_3: 0.02282
	accuracy_policy_3: 0.95324
	loss_value_3: 0.04596
	loss_reward_3: 0.00525
	loss_policy_4: 0.02242
	accuracy_policy_4: 0.95434
	loss_value_4: 0.04652
	loss_reward_4: 0.00629
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.95898
	loss_value_5: 0.04725
	loss_reward_5: 0.00721
	loss_policy: 0.22641
	loss_value: 0.45364
	loss_reward: 0.02791
[2025-05-11 19:59:59] nn step 54800, lr: 0.1.
	loss_policy_0: 0.12004
	accuracy_policy_0: 0.95242
	loss_value_0: 0.23217
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.95074
	loss_value_1: 0.04646
	loss_reward_1: 0.00444
	loss_policy_2: 0.02384
	accuracy_policy_2: 0.95156
	loss_value_2: 0.04702
	loss_reward_2: 0.00497
	loss_policy_3: 0.02366
	accuracy_policy_3: 0.9491
	loss_value_3: 0.04785
	loss_reward_3: 0.0057
	loss_policy_4: 0.02387
	accuracy_policy_4: 0.95246
	loss_value_4: 0.04844
	loss_reward_4: 0.00622
	loss_policy_5: 0.02351
	accuracy_policy_5: 0.9598
	loss_value_5: 0.04942
	loss_reward_5: 0.00739
	loss_policy: 0.23878
	loss_value: 0.47136
	loss_reward: 0.02872
Optimization_Done 54800
[2025-05-11 20:01:37] [command] train weight_iter_54800.pkl 256 275
[2025-05-11 20:01:45] nn step 54850, lr: 0.1.
	loss_policy_0: 0.12366
	accuracy_policy_0: 0.95512
	loss_value_0: 0.24682
	loss_policy_1: 0.02419
	accuracy_policy_1: 0.95262
	loss_value_1: 0.04923
	loss_reward_1: 0.00478
	loss_policy_2: 0.02426
	accuracy_policy_2: 0.95297
	loss_value_2: 0.04967
	loss_reward_2: 0.00527
	loss_policy_3: 0.02447
	accuracy_policy_3: 0.95316
	loss_value_3: 0.05042
	loss_reward_3: 0.00571
	loss_policy_4: 0.02435
	accuracy_policy_4: 0.95328
	loss_value_4: 0.05088
	loss_reward_4: 0.00679
	loss_policy_5: 0.02408
	accuracy_policy_5: 0.96152
	loss_value_5: 0.05176
	loss_reward_5: 0.00811
	loss_policy: 0.24501
	loss_value: 0.49879
	loss_reward: 0.03066
[2025-05-11 20:01:54] nn step 54900, lr: 0.1.
	loss_policy_0: 0.11961
	accuracy_policy_0: 0.95664
	loss_value_0: 0.2369
	loss_policy_1: 0.02358
	accuracy_policy_1: 0.95426
	loss_value_1: 0.04741
	loss_reward_1: 0.00454
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.9543
	loss_value_2: 0.04818
	loss_reward_2: 0.0054
	loss_policy_3: 0.02383
	accuracy_policy_3: 0.95062
	loss_value_3: 0.04891
	loss_reward_3: 0.00602
	loss_policy_4: 0.02352
	accuracy_policy_4: 0.95312
	loss_value_4: 0.0492
	loss_reward_4: 0.00675
	loss_policy_5: 0.02388
	accuracy_policy_5: 0.95848
	loss_value_5: 0.0501
	loss_reward_5: 0.008
	loss_policy: 0.23795
	loss_value: 0.48071
	loss_reward: 0.03072
[2025-05-11 20:02:03] nn step 54950, lr: 0.1.
	loss_policy_0: 0.12189
	accuracy_policy_0: 0.95586
	loss_value_0: 0.24012
	loss_policy_1: 0.02432
	accuracy_policy_1: 0.95129
	loss_value_1: 0.04825
	loss_reward_1: 0.00464
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.95121
	loss_value_2: 0.04872
	loss_reward_2: 0.00522
	loss_policy_3: 0.02435
	accuracy_policy_3: 0.94969
	loss_value_3: 0.04935
	loss_reward_3: 0.00562
	loss_policy_4: 0.02418
	accuracy_policy_4: 0.95168
	loss_value_4: 0.05031
	loss_reward_4: 0.00656
	loss_policy_5: 0.02377
	accuracy_policy_5: 0.95801
	loss_value_5: 0.05107
	loss_reward_5: 0.00816
	loss_policy: 0.24268
	loss_value: 0.48783
	loss_reward: 0.03019
[2025-05-11 20:02:10] nn step 55000, lr: 0.1.
	loss_policy_0: 0.11733
	accuracy_policy_0: 0.95609
	loss_value_0: 0.22721
	loss_policy_1: 0.02351
	accuracy_policy_1: 0.95238
	loss_value_1: 0.04542
	loss_reward_1: 0.00431
	loss_policy_2: 0.02337
	accuracy_policy_2: 0.95031
	loss_value_2: 0.04607
	loss_reward_2: 0.00497
	loss_policy_3: 0.02358
	accuracy_policy_3: 0.9518
	loss_value_3: 0.04681
	loss_reward_3: 0.00538
	loss_policy_4: 0.02322
	accuracy_policy_4: 0.95688
	loss_value_4: 0.04741
	loss_reward_4: 0.00624
	loss_policy_5: 0.02279
	accuracy_policy_5: 0.96309
	loss_value_5: 0.04823
	loss_reward_5: 0.00743
	loss_policy: 0.2338
	loss_value: 0.46115
	loss_reward: 0.02833
Optimization_Done 55000
[2025-05-11 20:03:45] [command] train weight_iter_55000.pkl 257 276
[2025-05-11 20:03:55] nn step 55050, lr: 0.1.
	loss_policy_0: 0.11831
	accuracy_policy_0: 0.9557
	loss_value_0: 0.24004
	loss_policy_1: 0.02312
	accuracy_policy_1: 0.95219
	loss_value_1: 0.04799
	loss_reward_1: 0.00436
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.95371
	loss_value_2: 0.04864
	loss_reward_2: 0.00506
	loss_policy_3: 0.02324
	accuracy_policy_3: 0.95133
	loss_value_3: 0.04926
	loss_reward_3: 0.00543
	loss_policy_4: 0.02353
	accuracy_policy_4: 0.95516
	loss_value_4: 0.0497
	loss_reward_4: 0.00632
	loss_policy_5: 0.02327
	accuracy_policy_5: 0.95848
	loss_value_5: 0.05051
	loss_reward_5: 0.00736
	loss_policy: 0.23502
	loss_value: 0.48613
	loss_reward: 0.02853
[2025-05-11 20:04:02] nn step 55100, lr: 0.1.
	loss_policy_0: 0.11355
	accuracy_policy_0: 0.95543
	loss_value_0: 0.22151
	loss_policy_1: 0.02242
	accuracy_policy_1: 0.95188
	loss_value_1: 0.04433
	loss_reward_1: 0.00426
	loss_policy_2: 0.02232
	accuracy_policy_2: 0.95281
	loss_value_2: 0.04492
	loss_reward_2: 0.00474
	loss_policy_3: 0.02224
	accuracy_policy_3: 0.95062
	loss_value_3: 0.04533
	loss_reward_3: 0.00521
	loss_policy_4: 0.02201
	accuracy_policy_4: 0.9541
	loss_value_4: 0.04607
	loss_reward_4: 0.00615
	loss_policy_5: 0.02212
	accuracy_policy_5: 0.96012
	loss_value_5: 0.04695
	loss_reward_5: 0.00711
	loss_policy: 0.22466
	loss_value: 0.44911
	loss_reward: 0.02746
[2025-05-11 20:04:10] nn step 55150, lr: 0.1.
	loss_policy_0: 0.11827
	accuracy_policy_0: 0.94996
	loss_value_0: 0.23326
	loss_policy_1: 0.02371
	accuracy_policy_1: 0.95148
	loss_value_1: 0.04647
	loss_reward_1: 0.00438
	loss_policy_2: 0.02337
	accuracy_policy_2: 0.94895
	loss_value_2: 0.04708
	loss_reward_2: 0.00502
	loss_policy_3: 0.02375
	accuracy_policy_3: 0.94734
	loss_value_3: 0.04755
	loss_reward_3: 0.00556
	loss_policy_4: 0.02382
	accuracy_policy_4: 0.95074
	loss_value_4: 0.04824
	loss_reward_4: 0.00631
	loss_policy_5: 0.02323
	accuracy_policy_5: 0.95734
	loss_value_5: 0.04908
	loss_reward_5: 0.00751
	loss_policy: 0.23615
	loss_value: 0.47169
	loss_reward: 0.02879
[2025-05-11 20:04:19] nn step 55200, lr: 0.1.
	loss_policy_0: 0.11585
	accuracy_policy_0: 0.95137
	loss_value_0: 0.22442
	loss_policy_1: 0.02319
	accuracy_policy_1: 0.95086
	loss_value_1: 0.04477
	loss_reward_1: 0.00436
	loss_policy_2: 0.02315
	accuracy_policy_2: 0.94965
	loss_value_2: 0.0456
	loss_reward_2: 0.00492
	loss_policy_3: 0.02326
	accuracy_policy_3: 0.95023
	loss_value_3: 0.04607
	loss_reward_3: 0.00537
	loss_policy_4: 0.02303
	accuracy_policy_4: 0.95016
	loss_value_4: 0.04685
	loss_reward_4: 0.00614
	loss_policy_5: 0.0229
	accuracy_policy_5: 0.95715
	loss_value_5: 0.04765
	loss_reward_5: 0.00705
	loss_policy: 0.23138
	loss_value: 0.45536
	loss_reward: 0.02784
Optimization_Done 55200
[2025-05-11 20:05:56] [command] train weight_iter_55200.pkl 258 277
[2025-05-11 20:06:05] nn step 55250, lr: 0.1.
	loss_policy_0: 0.11813
	accuracy_policy_0: 0.95422
	loss_value_0: 0.23951
	loss_policy_1: 0.02339
	accuracy_policy_1: 0.95
	loss_value_1: 0.04754
	loss_reward_1: 0.00461
	loss_policy_2: 0.02389
	accuracy_policy_2: 0.94988
	loss_value_2: 0.048
	loss_reward_2: 0.00508
	loss_policy_3: 0.02371
	accuracy_policy_3: 0.94793
	loss_value_3: 0.04875
	loss_reward_3: 0.0056
	loss_policy_4: 0.02353
	accuracy_policy_4: 0.95289
	loss_value_4: 0.04929
	loss_reward_4: 0.00663
	loss_policy_5: 0.02338
	accuracy_policy_5: 0.95984
	loss_value_5: 0.05038
	loss_reward_5: 0.0076
	loss_policy: 0.23605
	loss_value: 0.48347
	loss_reward: 0.02952
[2025-05-11 20:06:14] nn step 55300, lr: 0.1.
	loss_policy_0: 0.11683
	accuracy_policy_0: 0.95617
	loss_value_0: 0.22778
	loss_policy_1: 0.0229
	accuracy_policy_1: 0.95328
	loss_value_1: 0.04552
	loss_reward_1: 0.00443
	loss_policy_2: 0.02309
	accuracy_policy_2: 0.95117
	loss_value_2: 0.04638
	loss_reward_2: 0.00506
	loss_policy_3: 0.02314
	accuracy_policy_3: 0.94828
	loss_value_3: 0.04705
	loss_reward_3: 0.00518
	loss_policy_4: 0.02298
	accuracy_policy_4: 0.95254
	loss_value_4: 0.04776
	loss_reward_4: 0.00586
	loss_policy_5: 0.02256
	accuracy_policy_5: 0.95988
	loss_value_5: 0.04854
	loss_reward_5: 0.00743
	loss_policy: 0.23151
	loss_value: 0.46303
	loss_reward: 0.02797
[2025-05-11 20:06:21] nn step 55350, lr: 0.1.
	loss_policy_0: 0.11517
	accuracy_policy_0: 0.95461
	loss_value_0: 0.22829
	loss_policy_1: 0.02286
	accuracy_policy_1: 0.95199
	loss_value_1: 0.04562
	loss_reward_1: 0.00439
	loss_policy_2: 0.02255
	accuracy_policy_2: 0.9525
	loss_value_2: 0.04618
	loss_reward_2: 0.00491
	loss_policy_3: 0.02294
	accuracy_policy_3: 0.95047
	loss_value_3: 0.04691
	loss_reward_3: 0.00542
	loss_policy_4: 0.02283
	accuracy_policy_4: 0.95156
	loss_value_4: 0.04746
	loss_reward_4: 0.00622
	loss_policy_5: 0.02278
	accuracy_policy_5: 0.95785
	loss_value_5: 0.04837
	loss_reward_5: 0.00763
	loss_policy: 0.22912
	loss_value: 0.46283
	loss_reward: 0.02857
[2025-05-11 20:06:30] nn step 55400, lr: 0.1.
	loss_policy_0: 0.10858
	accuracy_policy_0: 0.95656
	loss_value_0: 0.21431
	loss_policy_1: 0.02163
	accuracy_policy_1: 0.95168
	loss_value_1: 0.04279
	loss_reward_1: 0.00421
	loss_policy_2: 0.02135
	accuracy_policy_2: 0.95406
	loss_value_2: 0.04335
	loss_reward_2: 0.00473
	loss_policy_3: 0.02155
	accuracy_policy_3: 0.95074
	loss_value_3: 0.04394
	loss_reward_3: 0.00506
	loss_policy_4: 0.02157
	accuracy_policy_4: 0.9559
	loss_value_4: 0.04458
	loss_reward_4: 0.00603
	loss_policy_5: 0.0213
	accuracy_policy_5: 0.95836
	loss_value_5: 0.04543
	loss_reward_5: 0.00691
	loss_policy: 0.21597
	loss_value: 0.43439
	loss_reward: 0.02695
Optimization_Done 55400
[2025-05-11 20:08:11] [command] train weight_iter_55400.pkl 259 278
[2025-05-11 20:08:18] nn step 55450, lr: 0.1.
	loss_policy_0: 0.11815
	accuracy_policy_0: 0.9568
	loss_value_0: 0.23384
	loss_policy_1: 0.02356
	accuracy_policy_1: 0.95531
	loss_value_1: 0.04636
	loss_reward_1: 0.00456
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.95645
	loss_value_2: 0.04702
	loss_reward_2: 0.00514
	loss_policy_3: 0.02371
	accuracy_policy_3: 0.95121
	loss_value_3: 0.04759
	loss_reward_3: 0.00544
	loss_policy_4: 0.02376
	accuracy_policy_4: 0.95293
	loss_value_4: 0.04824
	loss_reward_4: 0.00632
	loss_policy_5: 0.02331
	accuracy_policy_5: 0.95926
	loss_value_5: 0.04916
	loss_reward_5: 0.0077
	loss_policy: 0.23602
	loss_value: 0.47221
	loss_reward: 0.02916
[2025-05-11 20:08:27] nn step 55500, lr: 0.1.
	loss_policy_0: 0.11209
	accuracy_policy_0: 0.95684
	loss_value_0: 0.22048
	loss_policy_1: 0.0224
	accuracy_policy_1: 0.95355
	loss_value_1: 0.04393
	loss_reward_1: 0.00447
	loss_policy_2: 0.0226
	accuracy_policy_2: 0.95406
	loss_value_2: 0.04424
	loss_reward_2: 0.00497
	loss_policy_3: 0.02258
	accuracy_policy_3: 0.9523
	loss_value_3: 0.04496
	loss_reward_3: 0.0052
	loss_policy_4: 0.02247
	accuracy_policy_4: 0.95527
	loss_value_4: 0.04558
	loss_reward_4: 0.00604
	loss_policy_5: 0.02241
	accuracy_policy_5: 0.95801
	loss_value_5: 0.04615
	loss_reward_5: 0.00765
	loss_policy: 0.22455
	loss_value: 0.44534
	loss_reward: 0.02833
[2025-05-11 20:08:35] nn step 55550, lr: 0.1.
	loss_policy_0: 0.11641
	accuracy_policy_0: 0.95656
	loss_value_0: 0.22125
	loss_policy_1: 0.02319
	accuracy_policy_1: 0.95391
	loss_value_1: 0.04448
	loss_reward_1: 0.00441
	loss_policy_2: 0.02335
	accuracy_policy_2: 0.95441
	loss_value_2: 0.04523
	loss_reward_2: 0.00496
	loss_policy_3: 0.02334
	accuracy_policy_3: 0.95039
	loss_value_3: 0.04579
	loss_reward_3: 0.00527
	loss_policy_4: 0.02338
	accuracy_policy_4: 0.95344
	loss_value_4: 0.04644
	loss_reward_4: 0.00628
	loss_policy_5: 0.02289
	accuracy_policy_5: 0.96047
	loss_value_5: 0.04752
	loss_reward_5: 0.00716
	loss_policy: 0.23256
	loss_value: 0.45071
	loss_reward: 0.02809
[2025-05-11 20:08:42] nn step 55600, lr: 0.1.
	loss_policy_0: 0.11608
	accuracy_policy_0: 0.95836
	loss_value_0: 0.222
	loss_policy_1: 0.02317
	accuracy_policy_1: 0.95371
	loss_value_1: 0.04452
	loss_reward_1: 0.00445
	loss_policy_2: 0.02297
	accuracy_policy_2: 0.95246
	loss_value_2: 0.04533
	loss_reward_2: 0.005
	loss_policy_3: 0.0231
	accuracy_policy_3: 0.95262
	loss_value_3: 0.0459
	loss_reward_3: 0.00539
	loss_policy_4: 0.02275
	accuracy_policy_4: 0.95414
	loss_value_4: 0.04646
	loss_reward_4: 0.0062
	loss_policy_5: 0.02286
	accuracy_policy_5: 0.96008
	loss_value_5: 0.04729
	loss_reward_5: 0.00761
	loss_policy: 0.23093
	loss_value: 0.45151
	loss_reward: 0.02866
Optimization_Done 55600
[2025-05-11 20:10:18] [command] train weight_iter_55600.pkl 260 279
[2025-05-11 20:10:27] nn step 55650, lr: 0.1.
	loss_policy_0: 0.11959
	accuracy_policy_0: 0.96
	loss_value_0: 0.23531
	loss_policy_1: 0.02358
	accuracy_policy_1: 0.95598
	loss_value_1: 0.04682
	loss_reward_1: 0.00454
	loss_policy_2: 0.02344
	accuracy_policy_2: 0.95473
	loss_value_2: 0.04743
	loss_reward_2: 0.00511
	loss_policy_3: 0.02386
	accuracy_policy_3: 0.95215
	loss_value_3: 0.04804
	loss_reward_3: 0.00574
	loss_policy_4: 0.02403
	accuracy_policy_4: 0.95441
	loss_value_4: 0.0488
	loss_reward_4: 0.0067
	loss_policy_5: 0.02342
	accuracy_policy_5: 0.95891
	loss_value_5: 0.04979
	loss_reward_5: 0.00751
	loss_policy: 0.23791
	loss_value: 0.47619
	loss_reward: 0.02961
[2025-05-11 20:10:36] nn step 55700, lr: 0.1.
	loss_policy_0: 0.12482
	accuracy_policy_0: 0.9552
	loss_value_0: 0.24296
	loss_policy_1: 0.02483
	accuracy_policy_1: 0.95723
	loss_value_1: 0.04871
	loss_reward_1: 0.00487
	loss_policy_2: 0.0245
	accuracy_policy_2: 0.95336
	loss_value_2: 0.04933
	loss_reward_2: 0.00554
	loss_policy_3: 0.02496
	accuracy_policy_3: 0.95375
	loss_value_3: 0.05016
	loss_reward_3: 0.00606
	loss_policy_4: 0.02501
	accuracy_policy_4: 0.95449
	loss_value_4: 0.05086
	loss_reward_4: 0.00714
	loss_policy_5: 0.02474
	accuracy_policy_5: 0.9593
	loss_value_5: 0.05204
	loss_reward_5: 0.00823
	loss_policy: 0.24885
	loss_value: 0.49407
	loss_reward: 0.03184
[2025-05-11 20:10:43] nn step 55750, lr: 0.1.
	loss_policy_0: 0.11126
	accuracy_policy_0: 0.95727
	loss_value_0: 0.21451
	loss_policy_1: 0.02196
	accuracy_policy_1: 0.9534
	loss_value_1: 0.04248
	loss_reward_1: 0.00429
	loss_policy_2: 0.02208
	accuracy_policy_2: 0.95504
	loss_value_2: 0.04333
	loss_reward_2: 0.00486
	loss_policy_3: 0.02204
	accuracy_policy_3: 0.95449
	loss_value_3: 0.04417
	loss_reward_3: 0.00527
	loss_policy_4: 0.0219
	accuracy_policy_4: 0.95266
	loss_value_4: 0.04483
	loss_reward_4: 0.00624
	loss_policy_5: 0.0222
	accuracy_policy_5: 0.95695
	loss_value_5: 0.04568
	loss_reward_5: 0.00722
	loss_policy: 0.22144
	loss_value: 0.435
	loss_reward: 0.02788
[2025-05-11 20:10:52] nn step 55800, lr: 0.1.
	loss_policy_0: 0.11944
	accuracy_policy_0: 0.95707
	loss_value_0: 0.23056
	loss_policy_1: 0.02389
	accuracy_policy_1: 0.95453
	loss_value_1: 0.04654
	loss_reward_1: 0.0045
	loss_policy_2: 0.02422
	accuracy_policy_2: 0.95254
	loss_value_2: 0.04733
	loss_reward_2: 0.0051
	loss_policy_3: 0.02398
	accuracy_policy_3: 0.95504
	loss_value_3: 0.04778
	loss_reward_3: 0.00564
	loss_policy_4: 0.02391
	accuracy_policy_4: 0.95336
	loss_value_4: 0.0486
	loss_reward_4: 0.00653
	loss_policy_5: 0.02404
	accuracy_policy_5: 0.96211
	loss_value_5: 0.04975
	loss_reward_5: 0.00781
	loss_policy: 0.23949
	loss_value: 0.47056
	loss_reward: 0.02957
Optimization_Done 55800
[2025-05-11 20:12:31] [command] train weight_iter_55800.pkl 261 280
[2025-05-11 20:12:41] nn step 55850, lr: 0.1.
	loss_policy_0: 0.11099
	accuracy_policy_0: 0.96117
	loss_value_0: 0.21888
	loss_policy_1: 0.02198
	accuracy_policy_1: 0.95785
	loss_value_1: 0.0433
	loss_reward_1: 0.00426
	loss_policy_2: 0.02193
	accuracy_policy_2: 0.95562
	loss_value_2: 0.04364
	loss_reward_2: 0.00476
	loss_policy_3: 0.02198
	accuracy_policy_3: 0.95562
	loss_value_3: 0.04421
	loss_reward_3: 0.00504
	loss_policy_4: 0.02198
	accuracy_policy_4: 0.95789
	loss_value_4: 0.04482
	loss_reward_4: 0.00579
	loss_policy_5: 0.02171
	accuracy_policy_5: 0.96316
	loss_value_5: 0.04591
	loss_reward_5: 0.00695
	loss_policy: 0.22056
	loss_value: 0.44076
	loss_reward: 0.02681
[2025-05-11 20:12:49] nn step 55900, lr: 0.1.
	loss_policy_0: 0.1262
	accuracy_policy_0: 0.95898
	loss_value_0: 0.23994
	loss_policy_1: 0.02477
	accuracy_policy_1: 0.95594
	loss_value_1: 0.04825
	loss_reward_1: 0.00469
	loss_policy_2: 0.02507
	accuracy_policy_2: 0.95426
	loss_value_2: 0.04898
	loss_reward_2: 0.00525
	loss_policy_3: 0.02514
	accuracy_policy_3: 0.95652
	loss_value_3: 0.04968
	loss_reward_3: 0.00589
	loss_policy_4: 0.0249
	accuracy_policy_4: 0.95609
	loss_value_4: 0.0503
	loss_reward_4: 0.00657
	loss_policy_5: 0.02454
	accuracy_policy_5: 0.96297
	loss_value_5: 0.05125
	loss_reward_5: 0.00794
	loss_policy: 0.25063
	loss_value: 0.4884
	loss_reward: 0.03033
[2025-05-11 20:12:58] nn step 55950, lr: 0.1.
	loss_policy_0: 0.11893
	accuracy_policy_0: 0.9577
	loss_value_0: 0.22621
	loss_policy_1: 0.02328
	accuracy_policy_1: 0.95652
	loss_value_1: 0.04508
	loss_reward_1: 0.00472
	loss_policy_2: 0.02352
	accuracy_policy_2: 0.95297
	loss_value_2: 0.04556
	loss_reward_2: 0.00495
	loss_policy_3: 0.02364
	accuracy_policy_3: 0.95395
	loss_value_3: 0.04627
	loss_reward_3: 0.00529
	loss_policy_4: 0.02329
	accuracy_policy_4: 0.95473
	loss_value_4: 0.04726
	loss_reward_4: 0.00655
	loss_policy_5: 0.02342
	accuracy_policy_5: 0.96254
	loss_value_5: 0.04815
	loss_reward_5: 0.00778
	loss_policy: 0.23608
	loss_value: 0.45853
	loss_reward: 0.02929
[2025-05-11 20:13:05] nn step 56000, lr: 0.1.
	loss_policy_0: 0.11928
	accuracy_policy_0: 0.95867
	loss_value_0: 0.22761
	loss_policy_1: 0.02392
	accuracy_policy_1: 0.95652
	loss_value_1: 0.04575
	loss_reward_1: 0.00463
	loss_policy_2: 0.02381
	accuracy_policy_2: 0.95418
	loss_value_2: 0.04633
	loss_reward_2: 0.00514
	loss_policy_3: 0.02373
	accuracy_policy_3: 0.95422
	loss_value_3: 0.04709
	loss_reward_3: 0.00565
	loss_policy_4: 0.02375
	accuracy_policy_4: 0.95609
	loss_value_4: 0.04795
	loss_reward_4: 0.0066
	loss_policy_5: 0.02369
	accuracy_policy_5: 0.96023
	loss_value_5: 0.04911
	loss_reward_5: 0.00766
	loss_policy: 0.23818
	loss_value: 0.46384
	loss_reward: 0.02969
Optimization_Done 56000
[2025-05-11 20:14:40] [command] train weight_iter_56000.pkl 262 281
[2025-05-11 20:14:50] nn step 56050, lr: 0.1.
	loss_policy_0: 0.119
	accuracy_policy_0: 0.95602
	loss_value_0: 0.23055
	loss_policy_1: 0.02356
	accuracy_policy_1: 0.95383
	loss_value_1: 0.04603
	loss_reward_1: 0.00438
	loss_policy_2: 0.02353
	accuracy_policy_2: 0.95312
	loss_value_2: 0.04672
	loss_reward_2: 0.00501
	loss_policy_3: 0.02327
	accuracy_policy_3: 0.95305
	loss_value_3: 0.04727
	loss_reward_3: 0.00532
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.95465
	loss_value_4: 0.048
	loss_reward_4: 0.00613
	loss_policy_5: 0.02352
	accuracy_policy_5: 0.96223
	loss_value_5: 0.04878
	loss_reward_5: 0.00741
	loss_policy: 0.23626
	loss_value: 0.46735
	loss_reward: 0.02825
[2025-05-11 20:14:57] nn step 56100, lr: 0.1.
	loss_policy_0: 0.1304
	accuracy_policy_0: 0.95551
	loss_value_0: 0.2471
	loss_policy_1: 0.02603
	accuracy_policy_1: 0.95199
	loss_value_1: 0.04939
	loss_reward_1: 0.00491
	loss_policy_2: 0.02586
	accuracy_policy_2: 0.95309
	loss_value_2: 0.04975
	loss_reward_2: 0.00533
	loss_policy_3: 0.02593
	accuracy_policy_3: 0.95105
	loss_value_3: 0.05048
	loss_reward_3: 0.00593
	loss_policy_4: 0.02554
	accuracy_policy_4: 0.95547
	loss_value_4: 0.05143
	loss_reward_4: 0.00705
	loss_policy_5: 0.02557
	accuracy_policy_5: 0.96223
	loss_value_5: 0.05228
	loss_reward_5: 0.00832
	loss_policy: 0.25932
	loss_value: 0.50043
	loss_reward: 0.03153
[2025-05-11 20:15:05] nn step 56150, lr: 0.1.
	loss_policy_0: 0.11739
	accuracy_policy_0: 0.95699
	loss_value_0: 0.22032
	loss_policy_1: 0.02339
	accuracy_policy_1: 0.95461
	loss_value_1: 0.04434
	loss_reward_1: 0.00454
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.95082
	loss_value_2: 0.04486
	loss_reward_2: 0.00513
	loss_policy_3: 0.02341
	accuracy_policy_3: 0.95281
	loss_value_3: 0.04565
	loss_reward_3: 0.00549
	loss_policy_4: 0.02319
	accuracy_policy_4: 0.9534
	loss_value_4: 0.04648
	loss_reward_4: 0.00666
	loss_policy_5: 0.0232
	accuracy_policy_5: 0.95727
	loss_value_5: 0.0472
	loss_reward_5: 0.00772
	loss_policy: 0.23409
	loss_value: 0.44884
	loss_reward: 0.02955
[2025-05-11 20:15:14] nn step 56200, lr: 0.1.
	loss_policy_0: 0.11779
	accuracy_policy_0: 0.95656
	loss_value_0: 0.22391
	loss_policy_1: 0.02317
	accuracy_policy_1: 0.95336
	loss_value_1: 0.04478
	loss_reward_1: 0.00442
	loss_policy_2: 0.02313
	accuracy_policy_2: 0.9507
	loss_value_2: 0.04561
	loss_reward_2: 0.00501
	loss_policy_3: 0.02326
	accuracy_policy_3: 0.95039
	loss_value_3: 0.04638
	loss_reward_3: 0.0055
	loss_policy_4: 0.02354
	accuracy_policy_4: 0.95324
	loss_value_4: 0.04714
	loss_reward_4: 0.00661
	loss_policy_5: 0.0234
	accuracy_policy_5: 0.95934
	loss_value_5: 0.04805
	loss_reward_5: 0.00775
	loss_policy: 0.23429
	loss_value: 0.45588
	loss_reward: 0.0293
Optimization_Done 56200
[2025-05-11 20:16:53] [command] train weight_iter_56200.pkl 263 282
[2025-05-11 20:17:03] nn step 56250, lr: 0.1.
	loss_policy_0: 0.11938
	accuracy_policy_0: 0.9559
	loss_value_0: 0.22958
	loss_policy_1: 0.02407
	accuracy_policy_1: 0.95105
	loss_value_1: 0.04562
	loss_reward_1: 0.00453
	loss_policy_2: 0.02378
	accuracy_policy_2: 0.95438
	loss_value_2: 0.04632
	loss_reward_2: 0.005
	loss_policy_3: 0.02386
	accuracy_policy_3: 0.95316
	loss_value_3: 0.04726
	loss_reward_3: 0.00535
	loss_policy_4: 0.024
	accuracy_policy_4: 0.95621
	loss_value_4: 0.04804
	loss_reward_4: 0.00635
	loss_policy_5: 0.02376
	accuracy_policy_5: 0.95984
	loss_value_5: 0.049
	loss_reward_5: 0.00772
	loss_policy: 0.23885
	loss_value: 0.46581
	loss_reward: 0.02895
[2025-05-11 20:17:12] nn step 56300, lr: 0.1.
	loss_policy_0: 0.11975
	accuracy_policy_0: 0.95594
	loss_value_0: 0.22375
	loss_policy_1: 0.02374
	accuracy_policy_1: 0.95352
	loss_value_1: 0.04456
	loss_reward_1: 0.00441
	loss_policy_2: 0.02358
	accuracy_policy_2: 0.95246
	loss_value_2: 0.04557
	loss_reward_2: 0.00505
	loss_policy_3: 0.02375
	accuracy_policy_3: 0.95453
	loss_value_3: 0.04631
	loss_reward_3: 0.00551
	loss_policy_4: 0.02365
	accuracy_policy_4: 0.9552
	loss_value_4: 0.04696
	loss_reward_4: 0.00625
	loss_policy_5: 0.02397
	accuracy_policy_5: 0.95988
	loss_value_5: 0.04808
	loss_reward_5: 0.0077
	loss_policy: 0.23844
	loss_value: 0.45522
	loss_reward: 0.02892
[2025-05-11 20:17:19] nn step 56350, lr: 0.1.
	loss_policy_0: 0.11163
	accuracy_policy_0: 0.95496
	loss_value_0: 0.20178
	loss_policy_1: 0.02182
	accuracy_policy_1: 0.95203
	loss_value_1: 0.0406
	loss_reward_1: 0.00398
	loss_policy_2: 0.0218
	accuracy_policy_2: 0.95172
	loss_value_2: 0.04126
	loss_reward_2: 0.00442
	loss_policy_3: 0.02218
	accuracy_policy_3: 0.95117
	loss_value_3: 0.04205
	loss_reward_3: 0.00491
	loss_policy_4: 0.02181
	accuracy_policy_4: 0.9532
	loss_value_4: 0.04277
	loss_reward_4: 0.00557
	loss_policy_5: 0.02165
	accuracy_policy_5: 0.95707
	loss_value_5: 0.04344
	loss_reward_5: 0.00665
	loss_policy: 0.22089
	loss_value: 0.41191
	loss_reward: 0.02552
[2025-05-11 20:17:27] nn step 56400, lr: 0.1.
	loss_policy_0: 0.1164
	accuracy_policy_0: 0.95316
	loss_value_0: 0.21401
	loss_policy_1: 0.02346
	accuracy_policy_1: 0.95336
	loss_value_1: 0.04321
	loss_reward_1: 0.00442
	loss_policy_2: 0.02337
	accuracy_policy_2: 0.95246
	loss_value_2: 0.04361
	loss_reward_2: 0.00503
	loss_policy_3: 0.02329
	accuracy_policy_3: 0.9523
	loss_value_3: 0.04432
	loss_reward_3: 0.00524
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.95391
	loss_value_4: 0.04531
	loss_reward_4: 0.00627
	loss_policy_5: 0.02307
	accuracy_policy_5: 0.96016
	loss_value_5: 0.04641
	loss_reward_5: 0.00754
	loss_policy: 0.23298
	loss_value: 0.43687
	loss_reward: 0.0285
Optimization_Done 56400
[2025-05-11 20:19:05] [command] train weight_iter_56400.pkl 264 283
[2025-05-11 20:19:13] nn step 56450, lr: 0.1.
	loss_policy_0: 0.12544
	accuracy_policy_0: 0.95777
	loss_value_0: 0.24262
	loss_policy_1: 0.02469
	accuracy_policy_1: 0.95375
	loss_value_1: 0.04842
	loss_reward_1: 0.00476
	loss_policy_2: 0.02487
	accuracy_policy_2: 0.95559
	loss_value_2: 0.0493
	loss_reward_2: 0.00512
	loss_policy_3: 0.02525
	accuracy_policy_3: 0.95348
	loss_value_3: 0.04998
	loss_reward_3: 0.00554
	loss_policy_4: 0.02499
	accuracy_policy_4: 0.95688
	loss_value_4: 0.05077
	loss_reward_4: 0.00686
	loss_policy_5: 0.025
	accuracy_policy_5: 0.96242
	loss_value_5: 0.05198
	loss_reward_5: 0.00815
	loss_policy: 0.25026
	loss_value: 0.49306
	loss_reward: 0.03043
[2025-05-11 20:19:21] nn step 56500, lr: 0.1.
	loss_policy_0: 0.11803
	accuracy_policy_0: 0.95707
	loss_value_0: 0.21765
	loss_policy_1: 0.02299
	accuracy_policy_1: 0.95594
	loss_value_1: 0.04336
	loss_reward_1: 0.00428
	loss_policy_2: 0.0232
	accuracy_policy_2: 0.95117
	loss_value_2: 0.04405
	loss_reward_2: 0.00486
	loss_policy_3: 0.02322
	accuracy_policy_3: 0.95441
	loss_value_3: 0.04477
	loss_reward_3: 0.00511
	loss_policy_4: 0.02288
	accuracy_policy_4: 0.95766
	loss_value_4: 0.04577
	loss_reward_4: 0.00627
	loss_policy_5: 0.02306
	accuracy_policy_5: 0.96023
	loss_value_5: 0.04676
	loss_reward_5: 0.00723
	loss_policy: 0.23338
	loss_value: 0.44237
	loss_reward: 0.02776
[2025-05-11 20:19:30] nn step 56550, lr: 0.1.
	loss_policy_0: 0.11812
	accuracy_policy_0: 0.95895
	loss_value_0: 0.21994
	loss_policy_1: 0.02352
	accuracy_policy_1: 0.95465
	loss_value_1: 0.04399
	loss_reward_1: 0.00434
	loss_policy_2: 0.0236
	accuracy_policy_2: 0.95438
	loss_value_2: 0.04481
	loss_reward_2: 0.00497
	loss_policy_3: 0.02374
	accuracy_policy_3: 0.95273
	loss_value_3: 0.04545
	loss_reward_3: 0.00559
	loss_policy_4: 0.02354
	accuracy_policy_4: 0.95488
	loss_value_4: 0.04633
	loss_reward_4: 0.00629
	loss_policy_5: 0.02334
	accuracy_policy_5: 0.96172
	loss_value_5: 0.0473
	loss_reward_5: 0.00752
	loss_policy: 0.23586
	loss_value: 0.44782
	loss_reward: 0.0287
[2025-05-11 20:19:37] nn step 56600, lr: 0.1.
	loss_policy_0: 0.12014
	accuracy_policy_0: 0.95891
	loss_value_0: 0.22332
	loss_policy_1: 0.02433
	accuracy_policy_1: 0.95289
	loss_value_1: 0.04485
	loss_reward_1: 0.00448
	loss_policy_2: 0.02404
	accuracy_policy_2: 0.9523
	loss_value_2: 0.0457
	loss_reward_2: 0.00507
	loss_policy_3: 0.02447
	accuracy_policy_3: 0.95371
	loss_value_3: 0.04648
	loss_reward_3: 0.00543
	loss_policy_4: 0.02429
	accuracy_policy_4: 0.95445
	loss_value_4: 0.04721
	loss_reward_4: 0.00646
	loss_policy_5: 0.02395
	accuracy_policy_5: 0.95867
	loss_value_5: 0.04821
	loss_reward_5: 0.00749
	loss_policy: 0.24123
	loss_value: 0.45577
	loss_reward: 0.02892
Optimization_Done 56600
[2025-05-11 20:21:18] [command] train weight_iter_56600.pkl 265 284
[2025-05-11 20:21:27] nn step 56650, lr: 0.1.
	loss_policy_0: 0.11334
	accuracy_policy_0: 0.9543
	loss_value_0: 0.213
	loss_policy_1: 0.02239
	accuracy_policy_1: 0.95258
	loss_value_1: 0.04241
	loss_reward_1: 0.00408
	loss_policy_2: 0.02252
	accuracy_policy_2: 0.95066
	loss_value_2: 0.04315
	loss_reward_2: 0.00458
	loss_policy_3: 0.0222
	accuracy_policy_3: 0.95281
	loss_value_3: 0.04407
	loss_reward_3: 0.00511
	loss_policy_4: 0.02264
	accuracy_policy_4: 0.95078
	loss_value_4: 0.04492
	loss_reward_4: 0.00589
	loss_policy_5: 0.02238
	accuracy_policy_5: 0.95676
	loss_value_5: 0.0458
	loss_reward_5: 0.00688
	loss_policy: 0.22547
	loss_value: 0.43335
	loss_reward: 0.02654
[2025-05-11 20:21:34] nn step 56700, lr: 0.1.
	loss_policy_0: 0.12059
	accuracy_policy_0: 0.95492
	loss_value_0: 0.22398
	loss_policy_1: 0.02387
	accuracy_policy_1: 0.95062
	loss_value_1: 0.04487
	loss_reward_1: 0.00429
	loss_policy_2: 0.0241
	accuracy_policy_2: 0.94688
	loss_value_2: 0.04581
	loss_reward_2: 0.00492
	loss_policy_3: 0.02393
	accuracy_policy_3: 0.95102
	loss_value_3: 0.04632
	loss_reward_3: 0.0053
	loss_policy_4: 0.02411
	accuracy_policy_4: 0.95172
	loss_value_4: 0.04728
	loss_reward_4: 0.00615
	loss_policy_5: 0.02392
	accuracy_policy_5: 0.95598
	loss_value_5: 0.04819
	loss_reward_5: 0.00738
	loss_policy: 0.24051
	loss_value: 0.45645
	loss_reward: 0.02803
[2025-05-11 20:21:43] nn step 56750, lr: 0.1.
	loss_policy_0: 0.12481
	accuracy_policy_0: 0.95297
	loss_value_0: 0.23384
	loss_policy_1: 0.02492
	accuracy_policy_1: 0.94977
	loss_value_1: 0.0467
	loss_reward_1: 0.00469
	loss_policy_2: 0.02515
	accuracy_policy_2: 0.9498
	loss_value_2: 0.04745
	loss_reward_2: 0.00538
	loss_policy_3: 0.02504
	accuracy_policy_3: 0.94957
	loss_value_3: 0.04841
	loss_reward_3: 0.00557
	loss_policy_4: 0.02509
	accuracy_policy_4: 0.95211
	loss_value_4: 0.04918
	loss_reward_4: 0.00663
	loss_policy_5: 0.02461
	accuracy_policy_5: 0.95633
	loss_value_5: 0.05029
	loss_reward_5: 0.00798
	loss_policy: 0.24962
	loss_value: 0.47587
	loss_reward: 0.03026
[2025-05-11 20:21:51] nn step 56800, lr: 0.1.
	loss_policy_0: 0.11876
	accuracy_policy_0: 0.95219
	loss_value_0: 0.21891
	loss_policy_1: 0.02321
	accuracy_policy_1: 0.95219
	loss_value_1: 0.04374
	loss_reward_1: 0.00427
	loss_policy_2: 0.02368
	accuracy_policy_2: 0.95195
	loss_value_2: 0.04456
	loss_reward_2: 0.00515
	loss_policy_3: 0.02356
	accuracy_policy_3: 0.95176
	loss_value_3: 0.04527
	loss_reward_3: 0.00531
	loss_policy_4: 0.02352
	accuracy_policy_4: 0.95121
	loss_value_4: 0.04659
	loss_reward_4: 0.00625
	loss_policy_5: 0.02349
	accuracy_policy_5: 0.9541
	loss_value_5: 0.0476
	loss_reward_5: 0.00772
	loss_policy: 0.23623
	loss_value: 0.44667
	loss_reward: 0.0287
Optimization_Done 56800
[2025-05-11 20:23:28] [command] train weight_iter_56800.pkl 266 285
[2025-05-11 20:23:37] nn step 56850, lr: 0.1.
	loss_policy_0: 0.12002
	accuracy_policy_0: 0.95613
	loss_value_0: 0.22137
	loss_policy_1: 0.0238
	accuracy_policy_1: 0.95613
	loss_value_1: 0.0442
	loss_reward_1: 0.00426
	loss_policy_2: 0.02365
	accuracy_policy_2: 0.9543
	loss_value_2: 0.04499
	loss_reward_2: 0.00495
	loss_policy_3: 0.02378
	accuracy_policy_3: 0.95398
	loss_value_3: 0.04583
	loss_reward_3: 0.0054
	loss_policy_4: 0.02382
	accuracy_policy_4: 0.95418
	loss_value_4: 0.04661
	loss_reward_4: 0.00624
	loss_policy_5: 0.02329
	accuracy_policy_5: 0.95867
	loss_value_5: 0.04732
	loss_reward_5: 0.00733
	loss_policy: 0.23836
	loss_value: 0.45033
	loss_reward: 0.02818
[2025-05-11 20:23:46] nn step 56900, lr: 0.1.
	loss_policy_0: 0.12656
	accuracy_policy_0: 0.9543
	loss_value_0: 0.23016
	loss_policy_1: 0.02481
	accuracy_policy_1: 0.95047
	loss_value_1: 0.04609
	loss_reward_1: 0.00452
	loss_policy_2: 0.02519
	accuracy_policy_2: 0.9502
	loss_value_2: 0.04671
	loss_reward_2: 0.0051
	loss_policy_3: 0.02494
	accuracy_policy_3: 0.95234
	loss_value_3: 0.04761
	loss_reward_3: 0.0056
	loss_policy_4: 0.02478
	accuracy_policy_4: 0.95348
	loss_value_4: 0.0484
	loss_reward_4: 0.00627
	loss_policy_5: 0.02448
	accuracy_policy_5: 0.95754
	loss_value_5: 0.04975
	loss_reward_5: 0.00785
	loss_policy: 0.25075
	loss_value: 0.46871
	loss_reward: 0.02934
[2025-05-11 20:23:52] nn step 56950, lr: 0.1.
	loss_policy_0: 0.12304
	accuracy_policy_0: 0.95461
	loss_value_0: 0.22384
	loss_policy_1: 0.02467
	accuracy_policy_1: 0.95367
	loss_value_1: 0.04498
	loss_reward_1: 0.00454
	loss_policy_2: 0.02447
	accuracy_policy_2: 0.95289
	loss_value_2: 0.04563
	loss_reward_2: 0.00508
	loss_policy_3: 0.02441
	accuracy_policy_3: 0.95363
	loss_value_3: 0.04658
	loss_reward_3: 0.00558
	loss_policy_4: 0.02437
	accuracy_policy_4: 0.95465
	loss_value_4: 0.04748
	loss_reward_4: 0.00662
	loss_policy_5: 0.02472
	accuracy_policy_5: 0.9575
	loss_value_5: 0.04845
	loss_reward_5: 0.00779
	loss_policy: 0.24569
	loss_value: 0.45696
	loss_reward: 0.02961
[2025-05-11 20:24:01] nn step 57000, lr: 0.1.
	loss_policy_0: 0.12279
	accuracy_policy_0: 0.95578
	loss_value_0: 0.22701
	loss_policy_1: 0.02498
	accuracy_policy_1: 0.95188
	loss_value_1: 0.04526
	loss_reward_1: 0.0045
	loss_policy_2: 0.02496
	accuracy_policy_2: 0.94938
	loss_value_2: 0.04609
	loss_reward_2: 0.00519
	loss_policy_3: 0.02492
	accuracy_policy_3: 0.94988
	loss_value_3: 0.04706
	loss_reward_3: 0.00569
	loss_policy_4: 0.0245
	accuracy_policy_4: 0.95441
	loss_value_4: 0.04813
	loss_reward_4: 0.00658
	loss_policy_5: 0.02418
	accuracy_policy_5: 0.95809
	loss_value_5: 0.04927
	loss_reward_5: 0.00788
	loss_policy: 0.24634
	loss_value: 0.46282
	loss_reward: 0.02982
Optimization_Done 57000
[2025-05-11 20:25:39] [command] train weight_iter_57000.pkl 267 286
[2025-05-11 20:25:47] nn step 57050, lr: 0.1.
	loss_policy_0: 0.12864
	accuracy_policy_0: 0.95406
	loss_value_0: 0.23823
	loss_policy_1: 0.02527
	accuracy_policy_1: 0.95305
	loss_value_1: 0.04747
	loss_reward_1: 0.00458
	loss_policy_2: 0.02542
	accuracy_policy_2: 0.95047
	loss_value_2: 0.04825
	loss_reward_2: 0.00505
	loss_policy_3: 0.0254
	accuracy_policy_3: 0.95367
	loss_value_3: 0.04898
	loss_reward_3: 0.00564
	loss_policy_4: 0.0255
	accuracy_policy_4: 0.95152
	loss_value_4: 0.04948
	loss_reward_4: 0.00645
	loss_policy_5: 0.02532
	accuracy_policy_5: 0.95621
	loss_value_5: 0.05066
	loss_reward_5: 0.00753
	loss_policy: 0.25554
	loss_value: 0.48308
	loss_reward: 0.02924
[2025-05-11 20:25:56] nn step 57100, lr: 0.1.
	loss_policy_0: 0.11994
	accuracy_policy_0: 0.95711
	loss_value_0: 0.21817
	loss_policy_1: 0.02363
	accuracy_policy_1: 0.95309
	loss_value_1: 0.04373
	loss_reward_1: 0.00444
	loss_policy_2: 0.02395
	accuracy_policy_2: 0.95238
	loss_value_2: 0.04451
	loss_reward_2: 0.0048
	loss_policy_3: 0.02393
	accuracy_policy_3: 0.95312
	loss_value_3: 0.04523
	loss_reward_3: 0.00526
	loss_policy_4: 0.02419
	accuracy_policy_4: 0.95328
	loss_value_4: 0.04601
	loss_reward_4: 0.00625
	loss_policy_5: 0.02336
	accuracy_policy_5: 0.95812
	loss_value_5: 0.04707
	loss_reward_5: 0.00741
	loss_policy: 0.23899
	loss_value: 0.44472
	loss_reward: 0.02816
[2025-05-11 20:26:04] nn step 57150, lr: 0.1.
	loss_policy_0: 0.12969
	accuracy_policy_0: 0.9523
	loss_value_0: 0.23583
	loss_policy_1: 0.0259
	accuracy_policy_1: 0.94988
	loss_value_1: 0.04768
	loss_reward_1: 0.00463
	loss_policy_2: 0.0259
	accuracy_policy_2: 0.94891
	loss_value_2: 0.04831
	loss_reward_2: 0.00527
	loss_policy_3: 0.026
	accuracy_policy_3: 0.94883
	loss_value_3: 0.04915
	loss_reward_3: 0.00566
	loss_policy_4: 0.02572
	accuracy_policy_4: 0.94906
	loss_value_4: 0.04999
	loss_reward_4: 0.00668
	loss_policy_5: 0.0258
	accuracy_policy_5: 0.9543
	loss_value_5: 0.05109
	loss_reward_5: 0.00793
	loss_policy: 0.25901
	loss_value: 0.48206
	loss_reward: 0.03018
[2025-05-11 20:26:13] nn step 57200, lr: 0.1.
	loss_policy_0: 0.13134
	accuracy_policy_0: 0.9566
	loss_value_0: 0.23107
	loss_policy_1: 0.02592
	accuracy_policy_1: 0.95312
	loss_value_1: 0.04647
	loss_reward_1: 0.00469
	loss_policy_2: 0.02599
	accuracy_policy_2: 0.94969
	loss_value_2: 0.04731
	loss_reward_2: 0.00537
	loss_policy_3: 0.02596
	accuracy_policy_3: 0.95027
	loss_value_3: 0.04843
	loss_reward_3: 0.00553
	loss_policy_4: 0.0259
	accuracy_policy_4: 0.94996
	loss_value_4: 0.0493
	loss_reward_4: 0.00663
	loss_policy_5: 0.02574
	accuracy_policy_5: 0.95539
	loss_value_5: 0.05032
	loss_reward_5: 0.00791
	loss_policy: 0.26084
	loss_value: 0.4729
	loss_reward: 0.03013
Optimization_Done 57200
[2025-05-11 20:27:48] [command] train weight_iter_57200.pkl 268 287
[2025-05-11 20:27:58] nn step 57250, lr: 0.1.
	loss_policy_0: 0.11843
	accuracy_policy_0: 0.95754
	loss_value_0: 0.22142
	loss_policy_1: 0.02338
	accuracy_policy_1: 0.95285
	loss_value_1: 0.0441
	loss_reward_1: 0.00439
	loss_policy_2: 0.02367
	accuracy_policy_2: 0.95266
	loss_value_2: 0.04497
	loss_reward_2: 0.00481
	loss_policy_3: 0.02329
	accuracy_policy_3: 0.95113
	loss_value_3: 0.04577
	loss_reward_3: 0.00542
	loss_policy_4: 0.02362
	accuracy_policy_4: 0.95469
	loss_value_4: 0.04654
	loss_reward_4: 0.0061
	loss_policy_5: 0.02317
	accuracy_policy_5: 0.95707
	loss_value_5: 0.04732
	loss_reward_5: 0.00736
	loss_policy: 0.23555
	loss_value: 0.45012
	loss_reward: 0.02808
[2025-05-11 20:28:06] nn step 57300, lr: 0.1.
	loss_policy_0: 0.117
	accuracy_policy_0: 0.9548
	loss_value_0: 0.2112
	loss_policy_1: 0.02312
	accuracy_policy_1: 0.95102
	loss_value_1: 0.04212
	loss_reward_1: 0.00418
	loss_policy_2: 0.02317
	accuracy_policy_2: 0.9518
	loss_value_2: 0.04274
	loss_reward_2: 0.0046
	loss_policy_3: 0.02304
	accuracy_policy_3: 0.9527
	loss_value_3: 0.04369
	loss_reward_3: 0.00499
	loss_policy_4: 0.02345
	accuracy_policy_4: 0.95176
	loss_value_4: 0.04441
	loss_reward_4: 0.00574
	loss_policy_5: 0.02316
	accuracy_policy_5: 0.95562
	loss_value_5: 0.04529
	loss_reward_5: 0.00692
	loss_policy: 0.23295
	loss_value: 0.42944
	loss_reward: 0.02642
[2025-05-11 20:28:13] nn step 57350, lr: 0.1.
	loss_policy_0: 0.12393
	accuracy_policy_0: 0.95609
	loss_value_0: 0.22497
	loss_policy_1: 0.02498
	accuracy_policy_1: 0.95219
	loss_value_1: 0.04501
	loss_reward_1: 0.00431
	loss_policy_2: 0.02486
	accuracy_policy_2: 0.94898
	loss_value_2: 0.04584
	loss_reward_2: 0.00491
	loss_policy_3: 0.02492
	accuracy_policy_3: 0.94938
	loss_value_3: 0.04673
	loss_reward_3: 0.00562
	loss_policy_4: 0.02487
	accuracy_policy_4: 0.95059
	loss_value_4: 0.04769
	loss_reward_4: 0.00629
	loss_policy_5: 0.02474
	accuracy_policy_5: 0.95512
	loss_value_5: 0.04856
	loss_reward_5: 0.00759
	loss_policy: 0.2483
	loss_value: 0.4588
	loss_reward: 0.02872
[2025-05-11 20:28:22] nn step 57400, lr: 0.1.
	loss_policy_0: 0.12365
	accuracy_policy_0: 0.95551
	loss_value_0: 0.22427
	loss_policy_1: 0.0245
	accuracy_policy_1: 0.95094
	loss_value_1: 0.04478
	loss_reward_1: 0.0045
	loss_policy_2: 0.0252
	accuracy_policy_2: 0.94973
	loss_value_2: 0.04563
	loss_reward_2: 0.00479
	loss_policy_3: 0.02463
	accuracy_policy_3: 0.95016
	loss_value_3: 0.04657
	loss_reward_3: 0.00544
	loss_policy_4: 0.02449
	accuracy_policy_4: 0.95473
	loss_value_4: 0.04737
	loss_reward_4: 0.00637
	loss_policy_5: 0.02451
	accuracy_policy_5: 0.95645
	loss_value_5: 0.04851
	loss_reward_5: 0.00758
	loss_policy: 0.24698
	loss_value: 0.45714
	loss_reward: 0.02868
Optimization_Done 57400
[2025-05-11 20:29:59] [command] train weight_iter_57400.pkl 269 288
[2025-05-11 20:30:07] nn step 57450, lr: 0.1.
	loss_policy_0: 0.11869
	accuracy_policy_0: 0.95672
	loss_value_0: 0.2201
	loss_policy_1: 0.0233
	accuracy_policy_1: 0.95102
	loss_value_1: 0.04364
	loss_reward_1: 0.00428
	loss_policy_2: 0.02346
	accuracy_policy_2: 0.9509
	loss_value_2: 0.04423
	loss_reward_2: 0.00468
	loss_policy_3: 0.02357
	accuracy_policy_3: 0.95199
	loss_value_3: 0.04495
	loss_reward_3: 0.00511
	loss_policy_4: 0.02367
	accuracy_policy_4: 0.95223
	loss_value_4: 0.04581
	loss_reward_4: 0.00591
	loss_policy_5: 0.02322
	accuracy_policy_5: 0.9575
	loss_value_5: 0.04683
	loss_reward_5: 0.00704
	loss_policy: 0.23592
	loss_value: 0.44556
	loss_reward: 0.02702
[2025-05-11 20:30:16] nn step 57500, lr: 0.1.
	loss_policy_0: 0.12441
	accuracy_policy_0: 0.95566
	loss_value_0: 0.22581
	loss_policy_1: 0.02426
	accuracy_policy_1: 0.95375
	loss_value_1: 0.04506
	loss_reward_1: 0.00436
	loss_policy_2: 0.02443
	accuracy_policy_2: 0.95395
	loss_value_2: 0.04591
	loss_reward_2: 0.00487
	loss_policy_3: 0.02442
	accuracy_policy_3: 0.9532
	loss_value_3: 0.04694
	loss_reward_3: 0.00524
	loss_policy_4: 0.02457
	accuracy_policy_4: 0.95391
	loss_value_4: 0.04752
	loss_reward_4: 0.00626
	loss_policy_5: 0.02408
	accuracy_policy_5: 0.96004
	loss_value_5: 0.04842
	loss_reward_5: 0.0073
	loss_policy: 0.24616
	loss_value: 0.45966
	loss_reward: 0.02803
[2025-05-11 20:30:24] nn step 57550, lr: 0.1.
	loss_policy_0: 0.1302
	accuracy_policy_0: 0.95477
	loss_value_0: 0.23309
	loss_policy_1: 0.02572
	accuracy_policy_1: 0.95293
	loss_value_1: 0.04676
	loss_reward_1: 0.00455
	loss_policy_2: 0.02564
	accuracy_policy_2: 0.95055
	loss_value_2: 0.04777
	loss_reward_2: 0.00515
	loss_policy_3: 0.02548
	accuracy_policy_3: 0.95223
	loss_value_3: 0.0483
	loss_reward_3: 0.00573
	loss_policy_4: 0.0254
	accuracy_policy_4: 0.95395
	loss_value_4: 0.04885
	loss_reward_4: 0.00685
	loss_policy_5: 0.02546
	accuracy_policy_5: 0.95949
	loss_value_5: 0.05026
	loss_reward_5: 0.00798
	loss_policy: 0.2579
	loss_value: 0.47502
	loss_reward: 0.03025
[2025-05-11 20:30:31] nn step 57600, lr: 0.1.
	loss_policy_0: 0.12547
	accuracy_policy_0: 0.95305
	loss_value_0: 0.22321
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.9507
	loss_value_1: 0.04498
	loss_reward_1: 0.00445
	loss_policy_2: 0.02495
	accuracy_policy_2: 0.95047
	loss_value_2: 0.04544
	loss_reward_2: 0.00521
	loss_policy_3: 0.0248
	accuracy_policy_3: 0.94895
	loss_value_3: 0.04667
	loss_reward_3: 0.00549
	loss_policy_4: 0.02475
	accuracy_policy_4: 0.9527
	loss_value_4: 0.04764
	loss_reward_4: 0.00653
	loss_policy_5: 0.02466
	accuracy_policy_5: 0.95543
	loss_value_5: 0.04847
	loss_reward_5: 0.00796
	loss_policy: 0.24936
	loss_value: 0.4564
	loss_reward: 0.02963
Optimization_Done 57600
[2025-05-11 20:32:08] [command] train weight_iter_57600.pkl 270 289
[2025-05-11 20:32:18] nn step 57650, lr: 0.1.
	loss_policy_0: 0.12032
	accuracy_policy_0: 0.94895
	loss_value_0: 0.23713
	loss_policy_1: 0.02398
	accuracy_policy_1: 0.94465
	loss_value_1: 0.04703
	loss_reward_1: 0.00425
	loss_policy_2: 0.02407
	accuracy_policy_2: 0.94285
	loss_value_2: 0.04755
	loss_reward_2: 0.00489
	loss_policy_3: 0.02395
	accuracy_policy_3: 0.9448
	loss_value_3: 0.04805
	loss_reward_3: 0.00527
	loss_policy_4: 0.02402
	accuracy_policy_4: 0.94668
	loss_value_4: 0.04911
	loss_reward_4: 0.00627
	loss_policy_5: 0.02367
	accuracy_policy_5: 0.95145
	loss_value_5: 0.05003
	loss_reward_5: 0.00747
	loss_policy: 0.24002
	loss_value: 0.47891
	loss_reward: 0.02815
[2025-05-11 20:32:25] nn step 57700, lr: 0.1.
	loss_policy_0: 0.12332
	accuracy_policy_0: 0.94852
	loss_value_0: 0.23254
	loss_policy_1: 0.02398
	accuracy_policy_1: 0.9441
	loss_value_1: 0.04657
	loss_reward_1: 0.00434
	loss_policy_2: 0.02405
	accuracy_policy_2: 0.94359
	loss_value_2: 0.04705
	loss_reward_2: 0.00475
	loss_policy_3: 0.02432
	accuracy_policy_3: 0.94383
	loss_value_3: 0.04776
	loss_reward_3: 0.00509
	loss_policy_4: 0.02387
	accuracy_policy_4: 0.94637
	loss_value_4: 0.04866
	loss_reward_4: 0.00635
	loss_policy_5: 0.02383
	accuracy_policy_5: 0.94887
	loss_value_5: 0.0497
	loss_reward_5: 0.00732
	loss_policy: 0.24337
	loss_value: 0.47228
	loss_reward: 0.02785
[2025-05-11 20:32:33] nn step 57750, lr: 0.1.
	loss_policy_0: 0.13243
	accuracy_policy_0: 0.94832
	loss_value_0: 0.24361
	loss_policy_1: 0.02595
	accuracy_policy_1: 0.94801
	loss_value_1: 0.04845
	loss_reward_1: 0.00461
	loss_policy_2: 0.02595
	accuracy_policy_2: 0.94598
	loss_value_2: 0.0494
	loss_reward_2: 0.0052
	loss_policy_3: 0.02612
	accuracy_policy_3: 0.94133
	loss_value_3: 0.05042
	loss_reward_3: 0.0057
	loss_policy_4: 0.02583
	accuracy_policy_4: 0.94547
	loss_value_4: 0.05114
	loss_reward_4: 0.00658
	loss_policy_5: 0.02549
	accuracy_policy_5: 0.95121
	loss_value_5: 0.0521
	loss_reward_5: 0.00801
	loss_policy: 0.26177
	loss_value: 0.49512
	loss_reward: 0.03011
[2025-05-11 20:32:42] nn step 57800, lr: 0.1.
	loss_policy_0: 0.11745
	accuracy_policy_0: 0.94949
	loss_value_0: 0.21907
	loss_policy_1: 0.02355
	accuracy_policy_1: 0.94371
	loss_value_1: 0.04388
	loss_reward_1: 0.00414
	loss_policy_2: 0.02318
	accuracy_policy_2: 0.94332
	loss_value_2: 0.04443
	loss_reward_2: 0.00471
	loss_policy_3: 0.0236
	accuracy_policy_3: 0.94426
	loss_value_3: 0.04517
	loss_reward_3: 0.00501
	loss_policy_4: 0.02313
	accuracy_policy_4: 0.94766
	loss_value_4: 0.04584
	loss_reward_4: 0.0058
	loss_policy_5: 0.02304
	accuracy_policy_5: 0.95258
	loss_value_5: 0.04704
	loss_reward_5: 0.00705
	loss_policy: 0.23395
	loss_value: 0.44543
	loss_reward: 0.02671
Optimization_Done 57800
[2025-05-11 20:34:17] [command] train weight_iter_57800.pkl 271 290
[2025-05-11 20:34:27] nn step 57850, lr: 0.1.
	loss_policy_0: 0.11237
	accuracy_policy_0: 0.95141
	loss_value_0: 0.22
	loss_policy_1: 0.02257
	accuracy_policy_1: 0.94422
	loss_value_1: 0.04385
	loss_reward_1: 0.00412
	loss_policy_2: 0.02229
	accuracy_policy_2: 0.94477
	loss_value_2: 0.04416
	loss_reward_2: 0.00469
	loss_policy_3: 0.02232
	accuracy_policy_3: 0.94426
	loss_value_3: 0.04451
	loss_reward_3: 0.00506
	loss_policy_4: 0.02268
	accuracy_policy_4: 0.94711
	loss_value_4: 0.04525
	loss_reward_4: 0.00594
	loss_policy_5: 0.02206
	accuracy_policy_5: 0.95441
	loss_value_5: 0.04605
	loss_reward_5: 0.00691
	loss_policy: 0.22429
	loss_value: 0.44381
	loss_reward: 0.02674
[2025-05-11 20:34:35] nn step 57900, lr: 0.1.
	loss_policy_0: 0.12632
	accuracy_policy_0: 0.95129
	loss_value_0: 0.24092
	loss_policy_1: 0.02526
	accuracy_policy_1: 0.94617
	loss_value_1: 0.04833
	loss_reward_1: 0.00452
	loss_policy_2: 0.02529
	accuracy_policy_2: 0.94617
	loss_value_2: 0.04879
	loss_reward_2: 0.00504
	loss_policy_3: 0.02501
	accuracy_policy_3: 0.94418
	loss_value_3: 0.04959
	loss_reward_3: 0.00559
	loss_policy_4: 0.02505
	accuracy_policy_4: 0.94789
	loss_value_4: 0.05014
	loss_reward_4: 0.00659
	loss_policy_5: 0.02493
	accuracy_policy_5: 0.95387
	loss_value_5: 0.05113
	loss_reward_5: 0.00795
	loss_policy: 0.25186
	loss_value: 0.4889
	loss_reward: 0.02968
[2025-05-11 20:34:44] nn step 57950, lr: 0.1.
	loss_policy_0: 0.12128
	accuracy_policy_0: 0.95043
	loss_value_0: 0.22732
	loss_policy_1: 0.02407
	accuracy_policy_1: 0.94586
	loss_value_1: 0.04564
	loss_reward_1: 0.00435
	loss_policy_2: 0.02385
	accuracy_policy_2: 0.94664
	loss_value_2: 0.04624
	loss_reward_2: 0.00507
	loss_policy_3: 0.02415
	accuracy_policy_3: 0.94508
	loss_value_3: 0.04688
	loss_reward_3: 0.00549
	loss_policy_4: 0.02382
	accuracy_policy_4: 0.94727
	loss_value_4: 0.04752
	loss_reward_4: 0.00669
	loss_policy_5: 0.02399
	accuracy_policy_5: 0.95723
	loss_value_5: 0.04869
	loss_reward_5: 0.00777
	loss_policy: 0.24116
	loss_value: 0.46228
	loss_reward: 0.02937
[2025-05-11 20:34:51] nn step 58000, lr: 0.1.
	loss_policy_0: 0.1168
	accuracy_policy_0: 0.9498
	loss_value_0: 0.22283
	loss_policy_1: 0.02301
	accuracy_policy_1: 0.94477
	loss_value_1: 0.04454
	loss_reward_1: 0.00428
	loss_policy_2: 0.02327
	accuracy_policy_2: 0.94754
	loss_value_2: 0.04528
	loss_reward_2: 0.00477
	loss_policy_3: 0.02343
	accuracy_policy_3: 0.94473
	loss_value_3: 0.04588
	loss_reward_3: 0.00504
	loss_policy_4: 0.02321
	accuracy_policy_4: 0.9482
	loss_value_4: 0.04676
	loss_reward_4: 0.0059
	loss_policy_5: 0.02307
	accuracy_policy_5: 0.95684
	loss_value_5: 0.04775
	loss_reward_5: 0.00737
	loss_policy: 0.2328
	loss_value: 0.45304
	loss_reward: 0.02736
Optimization_Done 58000
[2025-05-11 20:36:33] [command] train weight_iter_58000.pkl 272 291
[2025-05-11 20:36:41] nn step 58050, lr: 0.1.
	loss_policy_0: 0.12302
	accuracy_policy_0: 0.95008
	loss_value_0: 0.24665
	loss_policy_1: 0.02436
	accuracy_policy_1: 0.94105
	loss_value_1: 0.04903
	loss_reward_1: 0.00455
	loss_policy_2: 0.02439
	accuracy_policy_2: 0.94301
	loss_value_2: 0.04957
	loss_reward_2: 0.00483
	loss_policy_3: 0.02478
	accuracy_policy_3: 0.94516
	loss_value_3: 0.05016
	loss_reward_3: 0.00559
	loss_policy_4: 0.0246
	accuracy_policy_4: 0.9484
	loss_value_4: 0.05113
	loss_reward_4: 0.00645
	loss_policy_5: 0.02445
	accuracy_policy_5: 0.95461
	loss_value_5: 0.0518
	loss_reward_5: 0.00762
	loss_policy: 0.2456
	loss_value: 0.49834
	loss_reward: 0.02903
[2025-05-11 20:36:49] nn step 58100, lr: 0.1.
	loss_policy_0: 0.11301
	accuracy_policy_0: 0.94738
	loss_value_0: 0.22023
	loss_policy_1: 0.0224
	accuracy_policy_1: 0.94551
	loss_value_1: 0.04383
	loss_reward_1: 0.00399
	loss_policy_2: 0.02276
	accuracy_policy_2: 0.94297
	loss_value_2: 0.04438
	loss_reward_2: 0.00441
	loss_policy_3: 0.02257
	accuracy_policy_3: 0.94336
	loss_value_3: 0.04485
	loss_reward_3: 0.0049
	loss_policy_4: 0.02277
	accuracy_policy_4: 0.94676
	loss_value_4: 0.04582
	loss_reward_4: 0.00581
	loss_policy_5: 0.02243
	accuracy_policy_5: 0.95367
	loss_value_5: 0.04661
	loss_reward_5: 0.00689
	loss_policy: 0.22594
	loss_value: 0.44573
	loss_reward: 0.026
[2025-05-11 20:36:58] nn step 58150, lr: 0.1.
	loss_policy_0: 0.11764
	accuracy_policy_0: 0.94934
	loss_value_0: 0.22658
	loss_policy_1: 0.02379
	accuracy_policy_1: 0.94602
	loss_value_1: 0.04549
	loss_reward_1: 0.00425
	loss_policy_2: 0.02368
	accuracy_policy_2: 0.94285
	loss_value_2: 0.04597
	loss_reward_2: 0.00495
	loss_policy_3: 0.02355
	accuracy_policy_3: 0.94465
	loss_value_3: 0.04683
	loss_reward_3: 0.00518
	loss_policy_4: 0.02364
	accuracy_policy_4: 0.94699
	loss_value_4: 0.0473
	loss_reward_4: 0.00614
	loss_policy_5: 0.02317
	accuracy_policy_5: 0.95707
	loss_value_5: 0.04827
	loss_reward_5: 0.00741
	loss_policy: 0.23547
	loss_value: 0.46045
	loss_reward: 0.02793
[2025-05-11 20:37:05] nn step 58200, lr: 0.1.
	loss_policy_0: 0.12248
	accuracy_policy_0: 0.94816
	loss_value_0: 0.23449
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.94371
	loss_value_1: 0.04697
	loss_reward_1: 0.0045
	loss_policy_2: 0.02468
	accuracy_policy_2: 0.94277
	loss_value_2: 0.04744
	loss_reward_2: 0.005
	loss_policy_3: 0.02424
	accuracy_policy_3: 0.94645
	loss_value_3: 0.04854
	loss_reward_3: 0.00546
	loss_policy_4: 0.02431
	accuracy_policy_4: 0.94836
	loss_value_4: 0.0492
	loss_reward_4: 0.00612
	loss_policy_5: 0.02415
	accuracy_policy_5: 0.95422
	loss_value_5: 0.04977
	loss_reward_5: 0.00753
	loss_policy: 0.2438
	loss_value: 0.4764
	loss_reward: 0.02861
Optimization_Done 58200
[2025-05-11 20:38:43] [command] train weight_iter_58200.pkl 273 292
[2025-05-11 20:38:52] nn step 58250, lr: 0.1.
	loss_policy_0: 0.11537
	accuracy_policy_0: 0.95145
	loss_value_0: 0.23152
	loss_policy_1: 0.02315
	accuracy_policy_1: 0.94715
	loss_value_1: 0.04599
	loss_reward_1: 0.00418
	loss_policy_2: 0.02299
	accuracy_policy_2: 0.94523
	loss_value_2: 0.04637
	loss_reward_2: 0.00492
	loss_policy_3: 0.02296
	accuracy_policy_3: 0.94656
	loss_value_3: 0.04702
	loss_reward_3: 0.0052
	loss_policy_4: 0.02317
	accuracy_policy_4: 0.9502
	loss_value_4: 0.04754
	loss_reward_4: 0.00598
	loss_policy_5: 0.02263
	accuracy_policy_5: 0.95578
	loss_value_5: 0.04841
	loss_reward_5: 0.00742
	loss_policy: 0.23027
	loss_value: 0.46684
	loss_reward: 0.02769
[2025-05-11 20:39:00] nn step 58300, lr: 0.1.
	loss_policy_0: 0.12861
	accuracy_policy_0: 0.94535
	loss_value_0: 0.24774
	loss_policy_1: 0.02564
	accuracy_policy_1: 0.94352
	loss_value_1: 0.04943
	loss_reward_1: 0.00455
	loss_policy_2: 0.02535
	accuracy_policy_2: 0.94148
	loss_value_2: 0.05002
	loss_reward_2: 0.00517
	loss_policy_3: 0.02529
	accuracy_policy_3: 0.94453
	loss_value_3: 0.05081
	loss_reward_3: 0.00572
	loss_policy_4: 0.02535
	accuracy_policy_4: 0.94781
	loss_value_4: 0.05138
	loss_reward_4: 0.00656
	loss_policy_5: 0.02511
	accuracy_policy_5: 0.95336
	loss_value_5: 0.05262
	loss_reward_5: 0.00814
	loss_policy: 0.25535
	loss_value: 0.502
	loss_reward: 0.03015
[2025-05-11 20:39:08] nn step 58350, lr: 0.1.
	loss_policy_0: 0.11572
	accuracy_policy_0: 0.94945
	loss_value_0: 0.22107
	loss_policy_1: 0.02313
	accuracy_policy_1: 0.94512
	loss_value_1: 0.04424
	loss_reward_1: 0.00425
	loss_policy_2: 0.02289
	accuracy_policy_2: 0.94383
	loss_value_2: 0.04478
	loss_reward_2: 0.0047
	loss_policy_3: 0.02282
	accuracy_policy_3: 0.94543
	loss_value_3: 0.04532
	loss_reward_3: 0.00514
	loss_policy_4: 0.02298
	accuracy_policy_4: 0.94727
	loss_value_4: 0.04627
	loss_reward_4: 0.00601
	loss_policy_5: 0.02268
	accuracy_policy_5: 0.95672
	loss_value_5: 0.047
	loss_reward_5: 0.00702
	loss_policy: 0.23021
	loss_value: 0.44867
	loss_reward: 0.02713
[2025-05-11 20:39:17] nn step 58400, lr: 0.1.
	loss_policy_0: 0.11907
	accuracy_policy_0: 0.94637
	loss_value_0: 0.22718
	loss_policy_1: 0.02393
	accuracy_policy_1: 0.94609
	loss_value_1: 0.04537
	loss_reward_1: 0.00422
	loss_policy_2: 0.02379
	accuracy_policy_2: 0.94211
	loss_value_2: 0.04598
	loss_reward_2: 0.00497
	loss_policy_3: 0.02381
	accuracy_policy_3: 0.94172
	loss_value_3: 0.04662
	loss_reward_3: 0.00546
	loss_policy_4: 0.02358
	accuracy_policy_4: 0.94523
	loss_value_4: 0.04718
	loss_reward_4: 0.00622
	loss_policy_5: 0.02355
	accuracy_policy_5: 0.95574
	loss_value_5: 0.0482
	loss_reward_5: 0.00748
	loss_policy: 0.23774
	loss_value: 0.46052
	loss_reward: 0.02834
Optimization_Done 58400
[2025-05-11 20:40:55] [command] train weight_iter_58400.pkl 274 293
[2025-05-11 20:41:05] nn step 58450, lr: 0.1.
	loss_policy_0: 0.11526
	accuracy_policy_0: 0.95191
	loss_value_0: 0.22543
	loss_policy_1: 0.0227
	accuracy_policy_1: 0.94895
	loss_value_1: 0.04484
	loss_reward_1: 0.00415
	loss_policy_2: 0.02299
	accuracy_policy_2: 0.94531
	loss_value_2: 0.04511
	loss_reward_2: 0.00474
	loss_policy_3: 0.02269
	accuracy_policy_3: 0.9477
	loss_value_3: 0.04579
	loss_reward_3: 0.00517
	loss_policy_4: 0.0229
	accuracy_policy_4: 0.94891
	loss_value_4: 0.04666
	loss_reward_4: 0.00624
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.95734
	loss_value_5: 0.04776
	loss_reward_5: 0.00717
	loss_policy: 0.22914
	loss_value: 0.45559
	loss_reward: 0.02747
[2025-05-11 20:41:14] nn step 58500, lr: 0.1.
	loss_policy_0: 0.10954
	accuracy_policy_0: 0.95082
	loss_value_0: 0.21445
	loss_policy_1: 0.02208
	accuracy_policy_1: 0.94512
	loss_value_1: 0.04268
	loss_reward_1: 0.00412
	loss_policy_2: 0.02222
	accuracy_policy_2: 0.94594
	loss_value_2: 0.04328
	loss_reward_2: 0.00452
	loss_policy_3: 0.0219
	accuracy_policy_3: 0.9457
	loss_value_3: 0.04423
	loss_reward_3: 0.00525
	loss_policy_4: 0.02248
	accuracy_policy_4: 0.9502
	loss_value_4: 0.04482
	loss_reward_4: 0.00604
	loss_policy_5: 0.02208
	accuracy_policy_5: 0.95914
	loss_value_5: 0.04572
	loss_reward_5: 0.00695
	loss_policy: 0.22031
	loss_value: 0.43519
	loss_reward: 0.02689
[2025-05-11 20:41:21] nn step 58550, lr: 0.1.
	loss_policy_0: 0.12161
	accuracy_policy_0: 0.95316
	loss_value_0: 0.23616
	loss_policy_1: 0.02493
	accuracy_policy_1: 0.94766
	loss_value_1: 0.04737
	loss_reward_1: 0.00451
	loss_policy_2: 0.02488
	accuracy_policy_2: 0.9443
	loss_value_2: 0.04809
	loss_reward_2: 0.00513
	loss_policy_3: 0.02501
	accuracy_policy_3: 0.94605
	loss_value_3: 0.04864
	loss_reward_3: 0.00566
	loss_policy_4: 0.02465
	accuracy_policy_4: 0.94949
	loss_value_4: 0.04933
	loss_reward_4: 0.00621
	loss_policy_5: 0.0246
	accuracy_policy_5: 0.95305
	loss_value_5: 0.05051
	loss_reward_5: 0.0078
	loss_policy: 0.24567
	loss_value: 0.4801
	loss_reward: 0.02931
[2025-05-11 20:41:29] nn step 58600, lr: 0.1.
	loss_policy_0: 0.11897
	accuracy_policy_0: 0.95105
	loss_value_0: 0.22694
	loss_policy_1: 0.02397
	accuracy_policy_1: 0.94738
	loss_value_1: 0.04546
	loss_reward_1: 0.00437
	loss_policy_2: 0.02385
	accuracy_policy_2: 0.94535
	loss_value_2: 0.0461
	loss_reward_2: 0.00488
	loss_policy_3: 0.02385
	accuracy_policy_3: 0.94527
	loss_value_3: 0.04668
	loss_reward_3: 0.00547
	loss_policy_4: 0.02378
	accuracy_policy_4: 0.94809
	loss_value_4: 0.04736
	loss_reward_4: 0.00634
	loss_policy_5: 0.02347
	accuracy_policy_5: 0.95801
	loss_value_5: 0.04863
	loss_reward_5: 0.00756
	loss_policy: 0.2379
	loss_value: 0.46117
	loss_reward: 0.02862
Optimization_Done 58600
[2025-05-11 20:43:08] [command] train weight_iter_58600.pkl 275 294
[2025-05-11 20:43:15] nn step 58650, lr: 0.1.
	loss_policy_0: 0.12166
	accuracy_policy_0: 0.94898
	loss_value_0: 0.24498
	loss_policy_1: 0.02391
	accuracy_policy_1: 0.9459
	loss_value_1: 0.04867
	loss_reward_1: 0.00448
	loss_policy_2: 0.02459
	accuracy_policy_2: 0.94238
	loss_value_2: 0.0492
	loss_reward_2: 0.005
	loss_policy_3: 0.02461
	accuracy_policy_3: 0.94219
	loss_value_3: 0.04995
	loss_reward_3: 0.00534
	loss_policy_4: 0.02432
	accuracy_policy_4: 0.94699
	loss_value_4: 0.05042
	loss_reward_4: 0.00637
	loss_policy_5: 0.0241
	accuracy_policy_5: 0.95758
	loss_value_5: 0.0514
	loss_reward_5: 0.00775
	loss_policy: 0.24319
	loss_value: 0.49461
	loss_reward: 0.02893
[2025-05-11 20:43:24] nn step 58700, lr: 0.1.
	loss_policy_0: 0.11072
	accuracy_policy_0: 0.94793
	loss_value_0: 0.21024
	loss_policy_1: 0.0215
	accuracy_policy_1: 0.94531
	loss_value_1: 0.04207
	loss_reward_1: 0.00383
	loss_policy_2: 0.0218
	accuracy_policy_2: 0.94359
	loss_value_2: 0.0426
	loss_reward_2: 0.00464
	loss_policy_3: 0.02166
	accuracy_policy_3: 0.94414
	loss_value_3: 0.04315
	loss_reward_3: 0.00487
	loss_policy_4: 0.02182
	accuracy_policy_4: 0.94672
	loss_value_4: 0.04398
	loss_reward_4: 0.0057
	loss_policy_5: 0.02159
	accuracy_policy_5: 0.95699
	loss_value_5: 0.04479
	loss_reward_5: 0.00706
	loss_policy: 0.21908
	loss_value: 0.42683
	loss_reward: 0.0261
[2025-05-11 20:43:32] nn step 58750, lr: 0.1.
	loss_policy_0: 0.11913
	accuracy_policy_0: 0.94535
	loss_value_0: 0.22629
	loss_policy_1: 0.02371
	accuracy_policy_1: 0.9427
	loss_value_1: 0.04506
	loss_reward_1: 0.00436
	loss_policy_2: 0.0236
	accuracy_policy_2: 0.94453
	loss_value_2: 0.04587
	loss_reward_2: 0.00485
	loss_policy_3: 0.02387
	accuracy_policy_3: 0.9448
	loss_value_3: 0.04666
	loss_reward_3: 0.00541
	loss_policy_4: 0.02398
	accuracy_policy_4: 0.9466
	loss_value_4: 0.04724
	loss_reward_4: 0.00623
	loss_policy_5: 0.02343
	accuracy_policy_5: 0.95605
	loss_value_5: 0.04835
	loss_reward_5: 0.00728
	loss_policy: 0.23772
	loss_value: 0.45946
	loss_reward: 0.02814
[2025-05-11 20:43:41] nn step 58800, lr: 0.1.
	loss_policy_0: 0.11914
	accuracy_policy_0: 0.94809
	loss_value_0: 0.22422
	loss_policy_1: 0.02366
	accuracy_policy_1: 0.94375
	loss_value_1: 0.04525
	loss_reward_1: 0.0043
	loss_policy_2: 0.02403
	accuracy_policy_2: 0.94371
	loss_value_2: 0.04566
	loss_reward_2: 0.00489
	loss_policy_3: 0.02372
	accuracy_policy_3: 0.94387
	loss_value_3: 0.04635
	loss_reward_3: 0.00539
	loss_policy_4: 0.02383
	accuracy_policy_4: 0.94637
	loss_value_4: 0.04705
	loss_reward_4: 0.00611
	loss_policy_5: 0.02357
	accuracy_policy_5: 0.95473
	loss_value_5: 0.04775
	loss_reward_5: 0.0074
	loss_policy: 0.23795
	loss_value: 0.45627
	loss_reward: 0.02808
Optimization_Done 58800
[2025-05-11 20:45:16] [command] train weight_iter_58800.pkl 276 295
[2025-05-11 20:45:25] nn step 58850, lr: 0.1.
	loss_policy_0: 0.11647
	accuracy_policy_0: 0.95242
	loss_value_0: 0.22937
	loss_policy_1: 0.02336
	accuracy_policy_1: 0.94617
	loss_value_1: 0.04554
	loss_reward_1: 0.00425
	loss_policy_2: 0.02337
	accuracy_policy_2: 0.94488
	loss_value_2: 0.04617
	loss_reward_2: 0.00474
	loss_policy_3: 0.02335
	accuracy_policy_3: 0.9466
	loss_value_3: 0.0466
	loss_reward_3: 0.00518
	loss_policy_4: 0.02314
	accuracy_policy_4: 0.95125
	loss_value_4: 0.04735
	loss_reward_4: 0.00608
	loss_policy_5: 0.02321
	accuracy_policy_5: 0.95977
	loss_value_5: 0.04823
	loss_reward_5: 0.00719
	loss_policy: 0.23291
	loss_value: 0.46325
	loss_reward: 0.02743
[2025-05-11 20:45:34] nn step 58900, lr: 0.1.
	loss_policy_0: 0.11749
	accuracy_policy_0: 0.95289
	loss_value_0: 0.23057
	loss_policy_1: 0.02343
	accuracy_policy_1: 0.9457
	loss_value_1: 0.04606
	loss_reward_1: 0.00433
	loss_policy_2: 0.02388
	accuracy_policy_2: 0.94473
	loss_value_2: 0.04663
	loss_reward_2: 0.005
	loss_policy_3: 0.02371
	accuracy_policy_3: 0.94523
	loss_value_3: 0.0474
	loss_reward_3: 0.00538
	loss_policy_4: 0.02376
	accuracy_policy_4: 0.94828
	loss_value_4: 0.04782
	loss_reward_4: 0.00643
	loss_policy_5: 0.02334
	accuracy_policy_5: 0.95875
	loss_value_5: 0.04882
	loss_reward_5: 0.00781
	loss_policy: 0.2356
	loss_value: 0.4673
	loss_reward: 0.02896
[2025-05-11 20:45:41] nn step 58950, lr: 0.1.
	loss_policy_0: 0.11704
	accuracy_policy_0: 0.94812
	loss_value_0: 0.2231
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.94598
	loss_value_1: 0.04462
	loss_reward_1: 0.00438
	loss_policy_2: 0.02339
	accuracy_policy_2: 0.94492
	loss_value_2: 0.04538
	loss_reward_2: 0.00482
	loss_policy_3: 0.02327
	accuracy_policy_3: 0.9468
	loss_value_3: 0.04599
	loss_reward_3: 0.00551
	loss_policy_4: 0.02354
	accuracy_policy_4: 0.9475
	loss_value_4: 0.04673
	loss_reward_4: 0.00622
	loss_policy_5: 0.02321
	accuracy_policy_5: 0.95703
	loss_value_5: 0.04773
	loss_reward_5: 0.00714
	loss_policy: 0.23401
	loss_value: 0.45355
	loss_reward: 0.02807
[2025-05-11 20:45:49] nn step 59000, lr: 0.1.
	loss_policy_0: 0.12357
	accuracy_policy_0: 0.94988
	loss_value_0: 0.23508
	loss_policy_1: 0.02445
	accuracy_policy_1: 0.94617
	loss_value_1: 0.04723
	loss_reward_1: 0.00434
	loss_policy_2: 0.02494
	accuracy_policy_2: 0.94516
	loss_value_2: 0.04834
	loss_reward_2: 0.005
	loss_policy_3: 0.02465
	accuracy_policy_3: 0.94422
	loss_value_3: 0.04906
	loss_reward_3: 0.00547
	loss_policy_4: 0.02458
	accuracy_policy_4: 0.94625
	loss_value_4: 0.04961
	loss_reward_4: 0.0062
	loss_policy_5: 0.0243
	accuracy_policy_5: 0.9575
	loss_value_5: 0.05031
	loss_reward_5: 0.00747
	loss_policy: 0.24648
	loss_value: 0.47963
	loss_reward: 0.02848
Optimization_Done 59000
[2025-05-11 20:47:30] [command] train weight_iter_59000.pkl 277 296
[2025-05-11 20:47:38] nn step 59050, lr: 0.1.
	loss_policy_0: 0.12229
	accuracy_policy_0: 0.95121
	loss_value_0: 0.24025
	loss_policy_1: 0.02418
	accuracy_policy_1: 0.94504
	loss_value_1: 0.04795
	loss_reward_1: 0.00444
	loss_policy_2: 0.02415
	accuracy_policy_2: 0.94355
	loss_value_2: 0.0484
	loss_reward_2: 0.00482
	loss_policy_3: 0.02443
	accuracy_policy_3: 0.94477
	loss_value_3: 0.04899
	loss_reward_3: 0.00553
	loss_policy_4: 0.02393
	accuracy_policy_4: 0.94883
	loss_value_4: 0.04934
	loss_reward_4: 0.00612
	loss_policy_5: 0.02373
	accuracy_policy_5: 0.95922
	loss_value_5: 0.05046
	loss_reward_5: 0.00747
	loss_policy: 0.24272
	loss_value: 0.48539
	loss_reward: 0.02838
[2025-05-11 20:47:46] nn step 59100, lr: 0.1.
	loss_policy_0: 0.113
	accuracy_policy_0: 0.95102
	loss_value_0: 0.21553
	loss_policy_1: 0.02235
	accuracy_policy_1: 0.9443
	loss_value_1: 0.04319
	loss_reward_1: 0.0043
	loss_policy_2: 0.02256
	accuracy_policy_2: 0.94609
	loss_value_2: 0.04361
	loss_reward_2: 0.00466
	loss_policy_3: 0.02266
	accuracy_policy_3: 0.94656
	loss_value_3: 0.04421
	loss_reward_3: 0.00508
	loss_policy_4: 0.02255
	accuracy_policy_4: 0.94988
	loss_value_4: 0.04476
	loss_reward_4: 0.00593
	loss_policy_5: 0.02237
	accuracy_policy_5: 0.95637
	loss_value_5: 0.04549
	loss_reward_5: 0.00678
	loss_policy: 0.22549
	loss_value: 0.43679
	loss_reward: 0.02676
[2025-05-11 20:47:55] nn step 59150, lr: 0.1.
	loss_policy_0: 0.11957
	accuracy_policy_0: 0.94855
	loss_value_0: 0.22552
	loss_policy_1: 0.02359
	accuracy_policy_1: 0.94668
	loss_value_1: 0.04532
	loss_reward_1: 0.00441
	loss_policy_2: 0.02392
	accuracy_policy_2: 0.94406
	loss_value_2: 0.04586
	loss_reward_2: 0.00473
	loss_policy_3: 0.02384
	accuracy_policy_3: 0.94676
	loss_value_3: 0.04625
	loss_reward_3: 0.0054
	loss_policy_4: 0.02382
	accuracy_policy_4: 0.94617
	loss_value_4: 0.04731
	loss_reward_4: 0.00636
	loss_policy_5: 0.02329
	accuracy_policy_5: 0.95781
	loss_value_5: 0.04806
	loss_reward_5: 0.00739
	loss_policy: 0.23803
	loss_value: 0.45832
	loss_reward: 0.02828
[2025-05-11 20:48:01] nn step 59200, lr: 0.1.
	loss_policy_0: 0.122
	accuracy_policy_0: 0.95102
	loss_value_0: 0.22847
	loss_policy_1: 0.02412
	accuracy_policy_1: 0.94391
	loss_value_1: 0.04598
	loss_reward_1: 0.00439
	loss_policy_2: 0.02427
	accuracy_policy_2: 0.94402
	loss_value_2: 0.04653
	loss_reward_2: 0.00506
	loss_policy_3: 0.02416
	accuracy_policy_3: 0.94578
	loss_value_3: 0.0471
	loss_reward_3: 0.00566
	loss_policy_4: 0.02427
	accuracy_policy_4: 0.94832
	loss_value_4: 0.04775
	loss_reward_4: 0.00641
	loss_policy_5: 0.024
	accuracy_policy_5: 0.95703
	loss_value_5: 0.04871
	loss_reward_5: 0.00795
	loss_policy: 0.24282
	loss_value: 0.46453
	loss_reward: 0.02947
Optimization_Done 59200
[2025-05-11 20:49:41] [command] train weight_iter_59200.pkl 278 297
[2025-05-11 20:49:50] nn step 59250, lr: 0.1.
	loss_policy_0: 0.11903
	accuracy_policy_0: 0.94828
	loss_value_0: 0.26197
	loss_policy_1: 0.02345
	accuracy_policy_1: 0.94328
	loss_value_1: 0.0521
	loss_reward_1: 0.00449
	loss_policy_2: 0.02357
	accuracy_policy_2: 0.94375
	loss_value_2: 0.05254
	loss_reward_2: 0.00481
	loss_policy_3: 0.02386
	accuracy_policy_3: 0.94441
	loss_value_3: 0.0533
	loss_reward_3: 0.00529
	loss_policy_4: 0.02392
	accuracy_policy_4: 0.94703
	loss_value_4: 0.054
	loss_reward_4: 0.00629
	loss_policy_5: 0.02349
	accuracy_policy_5: 0.95301
	loss_value_5: 0.05496
	loss_reward_5: 0.0075
	loss_policy: 0.23732
	loss_value: 0.52887
	loss_reward: 0.02838
[2025-05-11 20:49:57] nn step 59300, lr: 0.1.
	loss_policy_0: 0.11148
	accuracy_policy_0: 0.95051
	loss_value_0: 0.22996
	loss_policy_1: 0.02243
	accuracy_policy_1: 0.94445
	loss_value_1: 0.04566
	loss_reward_1: 0.00416
	loss_policy_2: 0.02198
	accuracy_policy_2: 0.94637
	loss_value_2: 0.04602
	loss_reward_2: 0.00474
	loss_policy_3: 0.02248
	accuracy_policy_3: 0.94582
	loss_value_3: 0.04674
	loss_reward_3: 0.0051
	loss_policy_4: 0.02211
	accuracy_policy_4: 0.94848
	loss_value_4: 0.04735
	loss_reward_4: 0.00583
	loss_policy_5: 0.02191
	accuracy_policy_5: 0.955
	loss_value_5: 0.04805
	loss_reward_5: 0.00709
	loss_policy: 0.2224
	loss_value: 0.46377
	loss_reward: 0.02693
[2025-05-11 20:50:06] nn step 59350, lr: 0.1.
	loss_policy_0: 0.11224
	accuracy_policy_0: 0.94777
	loss_value_0: 0.2216
	loss_policy_1: 0.02251
	accuracy_policy_1: 0.94414
	loss_value_1: 0.0442
	loss_reward_1: 0.00397
	loss_policy_2: 0.02257
	accuracy_policy_2: 0.94219
	loss_value_2: 0.04487
	loss_reward_2: 0.0047
	loss_policy_3: 0.02217
	accuracy_policy_3: 0.94395
	loss_value_3: 0.04557
	loss_reward_3: 0.00513
	loss_policy_4: 0.02215
	accuracy_policy_4: 0.94602
	loss_value_4: 0.04652
	loss_reward_4: 0.00596
	loss_policy_5: 0.02206
	accuracy_policy_5: 0.95664
	loss_value_5: 0.04754
	loss_reward_5: 0.00716
	loss_policy: 0.2237
	loss_value: 0.4503
	loss_reward: 0.02693
[2025-05-11 20:50:14] nn step 59400, lr: 0.1.
	loss_policy_0: 0.10888
	accuracy_policy_0: 0.94777
	loss_value_0: 0.21145
	loss_policy_1: 0.02152
	accuracy_policy_1: 0.94172
	loss_value_1: 0.04222
	loss_reward_1: 0.00383
	loss_policy_2: 0.02177
	accuracy_policy_2: 0.94246
	loss_value_2: 0.04257
	loss_reward_2: 0.00438
	loss_policy_3: 0.02188
	accuracy_policy_3: 0.94234
	loss_value_3: 0.0434
	loss_reward_3: 0.00489
	loss_policy_4: 0.02178
	accuracy_policy_4: 0.94523
	loss_value_4: 0.04398
	loss_reward_4: 0.00564
	loss_policy_5: 0.02144
	accuracy_policy_5: 0.95297
	loss_value_5: 0.04489
	loss_reward_5: 0.00675
	loss_policy: 0.21726
	loss_value: 0.4285
	loss_reward: 0.02549
Optimization_Done 59400
[2025-05-11 20:51:52] [command] train weight_iter_59400.pkl 279 298
[2025-05-11 20:52:02] nn step 59450, lr: 0.1.
	loss_policy_0: 0.11249
	accuracy_policy_0: 0.95148
	loss_value_0: 0.22684
	loss_policy_1: 0.02221
	accuracy_policy_1: 0.94594
	loss_value_1: 0.04502
	loss_reward_1: 0.004
	loss_policy_2: 0.02263
	accuracy_policy_2: 0.94445
	loss_value_2: 0.04536
	loss_reward_2: 0.00465
	loss_policy_3: 0.02253
	accuracy_policy_3: 0.94645
	loss_value_3: 0.04621
	loss_reward_3: 0.00493
	loss_policy_4: 0.02217
	accuracy_policy_4: 0.94723
	loss_value_4: 0.04694
	loss_reward_4: 0.00553
	loss_policy_5: 0.02248
	accuracy_policy_5: 0.95336
	loss_value_5: 0.04744
	loss_reward_5: 0.00672
	loss_policy: 0.2245
	loss_value: 0.4578
	loss_reward: 0.02583
[2025-05-11 20:52:10] nn step 59500, lr: 0.1.
	loss_policy_0: 0.11508
	accuracy_policy_0: 0.94828
	loss_value_0: 0.22753
	loss_policy_1: 0.02286
	accuracy_policy_1: 0.94418
	loss_value_1: 0.04516
	loss_reward_1: 0.00409
	loss_policy_2: 0.02308
	accuracy_policy_2: 0.94422
	loss_value_2: 0.0457
	loss_reward_2: 0.00471
	loss_policy_3: 0.02266
	accuracy_policy_3: 0.94473
	loss_value_3: 0.04587
	loss_reward_3: 0.00507
	loss_policy_4: 0.02299
	accuracy_policy_4: 0.94559
	loss_value_4: 0.04679
	loss_reward_4: 0.00593
	loss_policy_5: 0.02261
	accuracy_policy_5: 0.95406
	loss_value_5: 0.04758
	loss_reward_5: 0.00717
	loss_policy: 0.22928
	loss_value: 0.45862
	loss_reward: 0.02696
[2025-05-11 20:52:17] nn step 59550, lr: 0.1.
	loss_policy_0: 0.12181
	accuracy_policy_0: 0.94695
	loss_value_0: 0.23612
	loss_policy_1: 0.02433
	accuracy_policy_1: 0.94293
	loss_value_1: 0.04715
	loss_reward_1: 0.00449
	loss_policy_2: 0.02407
	accuracy_policy_2: 0.93992
	loss_value_2: 0.0476
	loss_reward_2: 0.00493
	loss_policy_3: 0.02417
	accuracy_policy_3: 0.94535
	loss_value_3: 0.04811
	loss_reward_3: 0.00522
	loss_policy_4: 0.02433
	accuracy_policy_4: 0.94453
	loss_value_4: 0.04905
	loss_reward_4: 0.00634
	loss_policy_5: 0.02423
	accuracy_policy_5: 0.95441
	loss_value_5: 0.05009
	loss_reward_5: 0.00747
	loss_policy: 0.24294
	loss_value: 0.47812
	loss_reward: 0.02845
[2025-05-11 20:52:26] nn step 59600, lr: 0.1.
	loss_policy_0: 0.12405
	accuracy_policy_0: 0.94645
	loss_value_0: 0.25438
	loss_policy_1: 0.02463
	accuracy_policy_1: 0.94289
	loss_value_1: 0.05076
	loss_reward_1: 0.00558
	loss_policy_2: 0.02482
	accuracy_policy_2: 0.94168
	loss_value_2: 0.05176
	loss_reward_2: 0.00696
	loss_policy_3: 0.02511
	accuracy_policy_3: 0.94043
	loss_value_3: 0.05231
	loss_reward_3: 0.00742
	loss_policy_4: 0.02488
	accuracy_policy_4: 0.94441
	loss_value_4: 0.05338
	loss_reward_4: 0.00822
	loss_policy_5: 0.02451
	accuracy_policy_5: 0.95
	loss_value_5: 0.05406
	loss_reward_5: 0.00931
	loss_policy: 0.24801
	loss_value: 0.51665
	loss_reward: 0.03749
Optimization_Done 59600
[2025-05-11 20:54:04] [command] train weight_iter_59600.pkl 280 299
[2025-05-11 20:54:11] nn step 59650, lr: 0.1.
	loss_policy_0: 0.11675
	accuracy_policy_0: 0.94617
	loss_value_0: 0.24768
	loss_policy_1: 0.02312
	accuracy_policy_1: 0.93969
	loss_value_1: 0.04912
	loss_reward_1: 0.00449
	loss_policy_2: 0.02317
	accuracy_policy_2: 0.94156
	loss_value_2: 0.04961
	loss_reward_2: 0.00544
	loss_policy_3: 0.02322
	accuracy_policy_3: 0.94281
	loss_value_3: 0.05011
	loss_reward_3: 0.00588
	loss_policy_4: 0.02307
	accuracy_policy_4: 0.94773
	loss_value_4: 0.05094
	loss_reward_4: 0.00697
	loss_policy_5: 0.02288
	accuracy_policy_5: 0.95535
	loss_value_5: 0.05144
	loss_reward_5: 0.008
	loss_policy: 0.2322
	loss_value: 0.49889
	loss_reward: 0.03078
[2025-05-11 20:54:20] nn step 59700, lr: 0.1.
	loss_policy_0: 0.12372
	accuracy_policy_0: 0.9459
	loss_value_0: 0.25614
	loss_policy_1: 0.02487
	accuracy_policy_1: 0.94246
	loss_value_1: 0.05111
	loss_reward_1: 0.00477
	loss_policy_2: 0.02472
	accuracy_policy_2: 0.94324
	loss_value_2: 0.05145
	loss_reward_2: 0.00546
	loss_policy_3: 0.02491
	accuracy_policy_3: 0.94395
	loss_value_3: 0.05204
	loss_reward_3: 0.00564
	loss_policy_4: 0.02461
	accuracy_policy_4: 0.94859
	loss_value_4: 0.05277
	loss_reward_4: 0.00662
	loss_policy_5: 0.02441
	accuracy_policy_5: 0.95691
	loss_value_5: 0.05386
	loss_reward_5: 0.00792
	loss_policy: 0.24725
	loss_value: 0.51737
	loss_reward: 0.03041
[2025-05-11 20:54:29] nn step 59750, lr: 0.1.
	loss_policy_0: 0.12487
	accuracy_policy_0: 0.94844
	loss_value_0: 0.25427
	loss_policy_1: 0.0253
	accuracy_policy_1: 0.94379
	loss_value_1: 0.05034
	loss_reward_1: 0.00463
	loss_policy_2: 0.02501
	accuracy_policy_2: 0.94336
	loss_value_2: 0.051
	loss_reward_2: 0.00526
	loss_policy_3: 0.0252
	accuracy_policy_3: 0.94227
	loss_value_3: 0.05161
	loss_reward_3: 0.00581
	loss_policy_4: 0.02575
	accuracy_policy_4: 0.94652
	loss_value_4: 0.05237
	loss_reward_4: 0.00676
	loss_policy_5: 0.02548
	accuracy_policy_5: 0.95543
	loss_value_5: 0.05323
	loss_reward_5: 0.00777
	loss_policy: 0.2516
	loss_value: 0.51283
	loss_reward: 0.03023
[2025-05-11 20:54:36] nn step 59800, lr: 0.1.
	loss_policy_0: 0.12043
	accuracy_policy_0: 0.94746
	loss_value_0: 0.23652
	loss_policy_1: 0.02392
	accuracy_policy_1: 0.94168
	loss_value_1: 0.04704
	loss_reward_1: 0.00457
	loss_policy_2: 0.02395
	accuracy_policy_2: 0.94152
	loss_value_2: 0.04754
	loss_reward_2: 0.00507
	loss_policy_3: 0.02428
	accuracy_policy_3: 0.94273
	loss_value_3: 0.04818
	loss_reward_3: 0.00544
	loss_policy_4: 0.02401
	accuracy_policy_4: 0.94863
	loss_value_4: 0.04906
	loss_reward_4: 0.00611
	loss_policy_5: 0.02346
	accuracy_policy_5: 0.9557
	loss_value_5: 0.04981
	loss_reward_5: 0.0074
	loss_policy: 0.24005
	loss_value: 0.47815
	loss_reward: 0.02858
Optimization_Done 59800
[2025-05-11 20:56:14] [command] train weight_iter_59800.pkl 281 300
[2025-05-11 20:56:24] nn step 59850, lr: 0.1.
	loss_policy_0: 0.11714
	accuracy_policy_0: 0.94266
	loss_value_0: 0.24357
	loss_policy_1: 0.02342
	accuracy_policy_1: 0.9398
	loss_value_1: 0.04863
	loss_reward_1: 0.0043
	loss_policy_2: 0.02362
	accuracy_policy_2: 0.94285
	loss_value_2: 0.04886
	loss_reward_2: 0.00473
	loss_policy_3: 0.02339
	accuracy_policy_3: 0.94207
	loss_value_3: 0.04952
	loss_reward_3: 0.00555
	loss_policy_4: 0.02374
	accuracy_policy_4: 0.94391
	loss_value_4: 0.05029
	loss_reward_4: 0.00642
	loss_policy_5: 0.0233
	accuracy_policy_5: 0.95316
	loss_value_5: 0.05133
	loss_reward_5: 0.00737
	loss_policy: 0.2346
	loss_value: 0.49221
	loss_reward: 0.02837
[2025-05-11 20:56:30] nn step 59900, lr: 0.1.
	loss_policy_0: 0.11506
	accuracy_policy_0: 0.94496
	loss_value_0: 0.2305
	loss_policy_1: 0.02283
	accuracy_policy_1: 0.94121
	loss_value_1: 0.04595
	loss_reward_1: 0.00425
	loss_policy_2: 0.02303
	accuracy_policy_2: 0.93883
	loss_value_2: 0.0466
	loss_reward_2: 0.00476
	loss_policy_3: 0.0229
	accuracy_policy_3: 0.94129
	loss_value_3: 0.04735
	loss_reward_3: 0.00518
	loss_policy_4: 0.02326
	accuracy_policy_4: 0.94414
	loss_value_4: 0.04797
	loss_reward_4: 0.00624
	loss_policy_5: 0.02272
	accuracy_policy_5: 0.95141
	loss_value_5: 0.04901
	loss_reward_5: 0.00723
	loss_policy: 0.2298
	loss_value: 0.46738
	loss_reward: 0.02766
[2025-05-11 20:56:39] nn step 59950, lr: 0.1.
	loss_policy_0: 0.11811
	accuracy_policy_0: 0.94312
	loss_value_0: 0.24062
	loss_policy_1: 0.02393
	accuracy_policy_1: 0.93898
	loss_value_1: 0.04803
	loss_reward_1: 0.00439
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.93691
	loss_value_2: 0.04855
	loss_reward_2: 0.00493
	loss_policy_3: 0.02427
	accuracy_policy_3: 0.94059
	loss_value_3: 0.04921
	loss_reward_3: 0.0055
	loss_policy_4: 0.02362
	accuracy_policy_4: 0.94359
	loss_value_4: 0.04975
	loss_reward_4: 0.00631
	loss_policy_5: 0.02386
	accuracy_policy_5: 0.95312
	loss_value_5: 0.05095
	loss_reward_5: 0.0073
	loss_policy: 0.23778
	loss_value: 0.48711
	loss_reward: 0.02843
[2025-05-11 20:56:48] nn step 60000, lr: 0.1.
	loss_policy_0: 0.12076
	accuracy_policy_0: 0.94402
	loss_value_0: 0.23673
	loss_policy_1: 0.02422
	accuracy_policy_1: 0.94012
	loss_value_1: 0.04703
	loss_reward_1: 0.00444
	loss_policy_2: 0.02429
	accuracy_policy_2: 0.93879
	loss_value_2: 0.04768
	loss_reward_2: 0.00494
	loss_policy_3: 0.02382
	accuracy_policy_3: 0.94059
	loss_value_3: 0.04828
	loss_reward_3: 0.0053
	loss_policy_4: 0.02427
	accuracy_policy_4: 0.9432
	loss_value_4: 0.0489
	loss_reward_4: 0.00602
	loss_policy_5: 0.02383
	accuracy_policy_5: 0.95266
	loss_value_5: 0.05007
	loss_reward_5: 0.00742
	loss_policy: 0.24118
	loss_value: 0.47868
	loss_reward: 0.02811
Optimization_Done 60000
