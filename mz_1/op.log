A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2025-05-08 04:45:28] [command] train weight_iter_0.pkl 1 1
[2025-05-08 04:45:44] nn step 50, lr: 0.1.
	loss_policy_0: 0.26033
	accuracy_policy_0: 0.11516
	loss_value_0: 0.26949
	loss_policy_1: 0.05266
	accuracy_policy_1: 0.11699
	loss_value_1: 0.05516
	loss_reward_1: 0.03219
	loss_policy_2: 0.05288
	accuracy_policy_2: 0.11434
	loss_value_2: 0.0557
	loss_reward_2: 0.03061
	loss_policy_3: 0.05297
	accuracy_policy_3: 0.11461
	loss_value_3: 0.05695
	loss_reward_3: 0.02905
	loss_policy_4: 0.05307
	accuracy_policy_4: 0.10844
	loss_value_4: 0.05711
	loss_reward_4: 0.03083
	loss_policy_5: 0.05313
	accuracy_policy_5: 0.11219
	loss_value_5: 0.05636
	loss_reward_5: 0.03229
	loss_policy: 0.52504
	loss_value: 0.55078
	loss_reward: 0.15497
[2025-05-08 04:45:50] nn step 100, lr: 0.1.
	loss_policy_0: 0.26878
	accuracy_policy_0: 0.12172
	loss_value_0: 0.19809
	loss_policy_1: 0.05388
	accuracy_policy_1: 0.1232
	loss_value_1: 0.03975
	loss_reward_1: 0.01616
	loss_policy_2: 0.05411
	accuracy_policy_2: 0.12539
	loss_value_2: 0.04012
	loss_reward_2: 0.01478
	loss_policy_3: 0.05433
	accuracy_policy_3: 0.11996
	loss_value_3: 0.04099
	loss_reward_3: 0.01276
	loss_policy_4: 0.05459
	accuracy_policy_4: 0.11535
	loss_value_4: 0.04125
	loss_reward_4: 0.01464
	loss_policy_5: 0.05475
	accuracy_policy_5: 0.11984
	loss_value_5: 0.04072
	loss_reward_5: 0.01628
	loss_policy: 0.54045
	loss_value: 0.40092
	loss_reward: 0.07463
[2025-05-08 04:45:59] nn step 150, lr: 0.1.
	loss_policy_0: 0.27374
	accuracy_policy_0: 0.12965
	loss_value_0: 0.20126
	loss_policy_1: 0.05496
	accuracy_policy_1: 0.12523
	loss_value_1: 0.04039
	loss_reward_1: 0.01624
	loss_policy_2: 0.0552
	accuracy_policy_2: 0.12293
	loss_value_2: 0.04049
	loss_reward_2: 0.01419
	loss_policy_3: 0.05547
	accuracy_policy_3: 0.12328
	loss_value_3: 0.04159
	loss_reward_3: 0.01227
	loss_policy_4: 0.05576
	accuracy_policy_4: 0.11594
	loss_value_4: 0.04163
	loss_reward_4: 0.01432
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.11863
	loss_value_5: 0.04074
	loss_reward_5: 0.01589
	loss_policy: 0.55108
	loss_value: 0.40609
	loss_reward: 0.07292
[2025-05-08 04:46:07] nn step 200, lr: 0.1.
	loss_policy_0: 0.28711
	accuracy_policy_0: 0.12945
	loss_value_0: 0.21223
	loss_policy_1: 0.05763
	accuracy_policy_1: 0.12359
	loss_value_1: 0.04255
	loss_reward_1: 0.01602
	loss_policy_2: 0.05787
	accuracy_policy_2: 0.12285
	loss_value_2: 0.04265
	loss_reward_2: 0.01423
	loss_policy_3: 0.05807
	accuracy_policy_3: 0.12172
	loss_value_3: 0.04379
	loss_reward_3: 0.01236
	loss_policy_4: 0.0583
	accuracy_policy_4: 0.12316
	loss_value_4: 0.04391
	loss_reward_4: 0.01414
	loss_policy_5: 0.05848
	accuracy_policy_5: 0.12539
	loss_value_5: 0.04298
	loss_reward_5: 0.01603
	loss_policy: 0.57746
	loss_value: 0.42812
	loss_reward: 0.07278
Optimization_Done 200
[2025-05-08 04:47:11] [command] train weight_iter_200.pkl 1 2
[2025-05-08 04:47:18] nn step 250, lr: 0.1.
	loss_policy_0: 0.28283
	accuracy_policy_0: 0.13195
	loss_value_0: 0.19422
	loss_policy_1: 0.05696
	accuracy_policy_1: 0.13551
	loss_value_1: 0.04113
	loss_reward_1: 0.01476
	loss_policy_2: 0.0572
	accuracy_policy_2: 0.13914
	loss_value_2: 0.04099
	loss_reward_2: 0.0138
	loss_policy_3: 0.05751
	accuracy_policy_3: 0.13203
	loss_value_3: 0.04203
	loss_reward_3: 0.01204
	loss_policy_4: 0.05762
	accuracy_policy_4: 0.13352
	loss_value_4: 0.04181
	loss_reward_4: 0.01382
	loss_policy_5: 0.05792
	accuracy_policy_5: 0.13297
	loss_value_5: 0.04063
	loss_reward_5: 0.01482
	loss_policy: 0.57004
	loss_value: 0.40081
	loss_reward: 0.06924
[2025-05-08 04:47:26] nn step 300, lr: 0.1.
	loss_policy_0: 0.32577
	accuracy_policy_0: 0.13008
	loss_value_0: 0.22486
	loss_policy_1: 0.06537
	accuracy_policy_1: 0.13289
	loss_value_1: 0.04732
	loss_reward_1: 0.01643
	loss_policy_2: 0.06549
	accuracy_policy_2: 0.13699
	loss_value_2: 0.04748
	loss_reward_2: 0.01489
	loss_policy_3: 0.06567
	accuracy_policy_3: 0.13414
	loss_value_3: 0.04866
	loss_reward_3: 0.01256
	loss_policy_4: 0.06571
	accuracy_policy_4: 0.13633
	loss_value_4: 0.04899
	loss_reward_4: 0.01521
	loss_policy_5: 0.06584
	accuracy_policy_5: 0.12996
	loss_value_5: 0.04845
	loss_reward_5: 0.01683
	loss_policy: 0.65385
	loss_value: 0.46576
	loss_reward: 0.07592
[2025-05-08 04:47:34] nn step 350, lr: 0.1.
	loss_policy_0: 0.31411
	accuracy_policy_0: 0.13297
	loss_value_0: 0.21777
	loss_policy_1: 0.06304
	accuracy_policy_1: 0.13785
	loss_value_1: 0.04646
	loss_reward_1: 0.01494
	loss_policy_2: 0.06311
	accuracy_policy_2: 0.14117
	loss_value_2: 0.04659
	loss_reward_2: 0.01329
	loss_policy_3: 0.06316
	accuracy_policy_3: 0.13461
	loss_value_3: 0.0477
	loss_reward_3: 0.01134
	loss_policy_4: 0.06335
	accuracy_policy_4: 0.1334
	loss_value_4: 0.04835
	loss_reward_4: 0.0136
	loss_policy_5: 0.0635
	accuracy_policy_5: 0.12422
	loss_value_5: 0.0481
	loss_reward_5: 0.01574
	loss_policy: 0.63027
	loss_value: 0.45498
	loss_reward: 0.0689
[2025-05-08 04:47:41] nn step 400, lr: 0.1.
	loss_policy_0: 0.32472
	accuracy_policy_0: 0.13672
	loss_value_0: 0.21912
	loss_policy_1: 0.06522
	accuracy_policy_1: 0.12902
	loss_value_1: 0.04785
	loss_reward_1: 0.0147
	loss_policy_2: 0.06519
	accuracy_policy_2: 0.1425
	loss_value_2: 0.04828
	loss_reward_2: 0.01344
	loss_policy_3: 0.06532
	accuracy_policy_3: 0.1348
	loss_value_3: 0.04946
	loss_reward_3: 0.01063
	loss_policy_4: 0.06538
	accuracy_policy_4: 0.13145
	loss_value_4: 0.05028
	loss_reward_4: 0.01375
	loss_policy_5: 0.06542
	accuracy_policy_5: 0.12883
	loss_value_5: 0.05001
	loss_reward_5: 0.01586
	loss_policy: 0.65125
	loss_value: 0.465
	loss_reward: 0.06838
Optimization_Done 400
[2025-05-08 04:48:46] [command] train weight_iter_400.pkl 1 3
[2025-05-08 04:48:55] nn step 450, lr: 0.1.
	loss_policy_0: 0.34493
	accuracy_policy_0: 0.14914
	loss_value_0: 0.24867
	loss_policy_1: 0.06939
	accuracy_policy_1: 0.12961
	loss_value_1: 0.05224
	loss_reward_1: 0.01399
	loss_policy_2: 0.06946
	accuracy_policy_2: 0.12918
	loss_value_2: 0.05285
	loss_reward_2: 0.01307
	loss_policy_3: 0.06946
	accuracy_policy_3: 0.13379
	loss_value_3: 0.05378
	loss_reward_3: 0.01159
	loss_policy_4: 0.06949
	accuracy_policy_4: 0.13535
	loss_value_4: 0.0546
	loss_reward_4: 0.01379
	loss_policy_5: 0.06944
	accuracy_policy_5: 0.14113
	loss_value_5: 0.05454
	loss_reward_5: 0.01526
	loss_policy: 0.69219
	loss_value: 0.51668
	loss_reward: 0.06769
[2025-05-08 04:49:03] nn step 500, lr: 0.1.
	loss_policy_0: 0.33232
	accuracy_policy_0: 0.14215
	loss_value_0: 0.22292
	loss_policy_1: 0.06664
	accuracy_policy_1: 0.13312
	loss_value_1: 0.04754
	loss_reward_1: 0.01324
	loss_policy_2: 0.06671
	accuracy_policy_2: 0.13793
	loss_value_2: 0.04844
	loss_reward_2: 0.01257
	loss_policy_3: 0.06675
	accuracy_policy_3: 0.13699
	loss_value_3: 0.04977
	loss_reward_3: 0.01068
	loss_policy_4: 0.06681
	accuracy_policy_4: 0.13367
	loss_value_4: 0.05037
	loss_reward_4: 0.01267
	loss_policy_5: 0.06679
	accuracy_policy_5: 0.13652
	loss_value_5: 0.05068
	loss_reward_5: 0.01478
	loss_policy: 0.66603
	loss_value: 0.46972
	loss_reward: 0.06394
[2025-05-08 04:49:10] nn step 550, lr: 0.1.
	loss_policy_0: 0.36102
	accuracy_policy_0: 0.14559
	loss_value_0: 0.23567
	loss_policy_1: 0.07244
	accuracy_policy_1: 0.13477
	loss_value_1: 0.05069
	loss_reward_1: 0.01396
	loss_policy_2: 0.07264
	accuracy_policy_2: 0.13285
	loss_value_2: 0.05153
	loss_reward_2: 0.0134
	loss_policy_3: 0.07258
	accuracy_policy_3: 0.13586
	loss_value_3: 0.05293
	loss_reward_3: 0.01137
	loss_policy_4: 0.07268
	accuracy_policy_4: 0.13582
	loss_value_4: 0.05433
	loss_reward_4: 0.01353
	loss_policy_5: 0.0727
	accuracy_policy_5: 0.13949
	loss_value_5: 0.05458
	loss_reward_5: 0.01533
	loss_policy: 0.72405
	loss_value: 0.49972
	loss_reward: 0.06758
[2025-05-08 04:49:17] nn step 600, lr: 0.1.
	loss_policy_0: 0.36117
	accuracy_policy_0: 0.15098
	loss_value_0: 0.23334
	loss_policy_1: 0.07257
	accuracy_policy_1: 0.13328
	loss_value_1: 0.05013
	loss_reward_1: 0.01333
	loss_policy_2: 0.07261
	accuracy_policy_2: 0.13605
	loss_value_2: 0.051
	loss_reward_2: 0.01315
	loss_policy_3: 0.07264
	accuracy_policy_3: 0.13773
	loss_value_3: 0.05302
	loss_reward_3: 0.01098
	loss_policy_4: 0.07268
	accuracy_policy_4: 0.13656
	loss_value_4: 0.05419
	loss_reward_4: 0.01329
	loss_policy_5: 0.07266
	accuracy_policy_5: 0.13941
	loss_value_5: 0.05472
	loss_reward_5: 0.01561
	loss_policy: 0.72433
	loss_value: 0.49639
	loss_reward: 0.06636
Optimization_Done 600
[2025-05-08 04:50:24] [command] train weight_iter_600.pkl 1 4
[2025-05-08 04:50:33] nn step 650, lr: 0.1.
	loss_policy_0: 0.39008
	accuracy_policy_0: 0.1559
	loss_value_0: 0.26177
	loss_policy_1: 0.07831
	accuracy_policy_1: 0.14727
	loss_value_1: 0.05498
	loss_reward_1: 0.01333
	loss_policy_2: 0.07829
	accuracy_policy_2: 0.15074
	loss_value_2: 0.05581
	loss_reward_2: 0.01332
	loss_policy_3: 0.07841
	accuracy_policy_3: 0.14805
	loss_value_3: 0.05713
	loss_reward_3: 0.01164
	loss_policy_4: 0.07846
	accuracy_policy_4: 0.14547
	loss_value_4: 0.05821
	loss_reward_4: 0.01373
	loss_policy_5: 0.07857
	accuracy_policy_5: 0.14781
	loss_value_5: 0.0586
	loss_reward_5: 0.01552
	loss_policy: 0.78212
	loss_value: 0.5465
	loss_reward: 0.06754
[2025-05-08 04:50:39] nn step 700, lr: 0.1.
	loss_policy_0: 0.36319
	accuracy_policy_0: 0.15363
	loss_value_0: 0.23228
	loss_policy_1: 0.07292
	accuracy_policy_1: 0.14102
	loss_value_1: 0.04921
	loss_reward_1: 0.01235
	loss_policy_2: 0.0729
	accuracy_policy_2: 0.14551
	loss_value_2: 0.0502
	loss_reward_2: 0.01203
	loss_policy_3: 0.07296
	accuracy_policy_3: 0.1448
	loss_value_3: 0.05165
	loss_reward_3: 0.01081
	loss_policy_4: 0.07311
	accuracy_policy_4: 0.1402
	loss_value_4: 0.0527
	loss_reward_4: 0.0126
	loss_policy_5: 0.07309
	accuracy_policy_5: 0.14219
	loss_value_5: 0.05309
	loss_reward_5: 0.0143
	loss_policy: 0.72816
	loss_value: 0.48913
	loss_reward: 0.06209
[2025-05-08 04:50:47] nn step 750, lr: 0.1.
	loss_policy_0: 0.34118
	accuracy_policy_0: 0.15383
	loss_value_0: 0.21545
	loss_policy_1: 0.06857
	accuracy_policy_1: 0.14117
	loss_value_1: 0.04613
	loss_reward_1: 0.01079
	loss_policy_2: 0.0685
	accuracy_policy_2: 0.14266
	loss_value_2: 0.04722
	loss_reward_2: 0.01118
	loss_policy_3: 0.06864
	accuracy_policy_3: 0.1434
	loss_value_3: 0.04855
	loss_reward_3: 0.00966
	loss_policy_4: 0.06871
	accuracy_policy_4: 0.14273
	loss_value_4: 0.04959
	loss_reward_4: 0.01117
	loss_policy_5: 0.06873
	accuracy_policy_5: 0.14918
	loss_value_5: 0.05014
	loss_reward_5: 0.01303
	loss_policy: 0.68433
	loss_value: 0.45707
	loss_reward: 0.05583
[2025-05-08 04:50:54] nn step 800, lr: 0.1.
	loss_policy_0: 0.36055
	accuracy_policy_0: 0.15621
	loss_value_0: 0.22861
	loss_policy_1: 0.07242
	accuracy_policy_1: 0.14316
	loss_value_1: 0.04862
	loss_reward_1: 0.01129
	loss_policy_2: 0.07243
	accuracy_policy_2: 0.14035
	loss_value_2: 0.04966
	loss_reward_2: 0.01134
	loss_policy_3: 0.07244
	accuracy_policy_3: 0.14289
	loss_value_3: 0.05141
	loss_reward_3: 0.01017
	loss_policy_4: 0.07244
	accuracy_policy_4: 0.1443
	loss_value_4: 0.05263
	loss_reward_4: 0.01187
	loss_policy_5: 0.07252
	accuracy_policy_5: 0.14633
	loss_value_5: 0.05327
	loss_reward_5: 0.01368
	loss_policy: 0.72279
	loss_value: 0.4842
	loss_reward: 0.05835
Optimization_Done 800
[2025-05-08 04:51:59] [command] train weight_iter_800.pkl 1 5
[2025-05-08 04:52:06] nn step 850, lr: 0.1.
	loss_policy_0: 0.42516
	accuracy_policy_0: 0.15602
	loss_value_0: 0.27039
	loss_policy_1: 0.08534
	accuracy_policy_1: 0.14477
	loss_value_1: 0.05693
	loss_reward_1: 0.01265
	loss_policy_2: 0.08553
	accuracy_policy_2: 0.14453
	loss_value_2: 0.05818
	loss_reward_2: 0.01323
	loss_policy_3: 0.08547
	accuracy_policy_3: 0.14516
	loss_value_3: 0.05975
	loss_reward_3: 0.01209
	loss_policy_4: 0.08567
	accuracy_policy_4: 0.14598
	loss_value_4: 0.06084
	loss_reward_4: 0.01359
	loss_policy_5: 0.08573
	accuracy_policy_5: 0.14273
	loss_value_5: 0.06162
	loss_reward_5: 0.01578
	loss_policy: 0.85289
	loss_value: 0.56772
	loss_reward: 0.06733
[2025-05-08 04:52:14] nn step 900, lr: 0.1.
	loss_policy_0: 0.37785
	accuracy_policy_0: 0.15699
	loss_value_0: 0.2371
	loss_policy_1: 0.07577
	accuracy_policy_1: 0.14707
	loss_value_1: 0.05024
	loss_reward_1: 0.0106
	loss_policy_2: 0.07588
	accuracy_policy_2: 0.14695
	loss_value_2: 0.05117
	loss_reward_2: 0.01144
	loss_policy_3: 0.07604
	accuracy_policy_3: 0.14359
	loss_value_3: 0.05291
	loss_reward_3: 0.01
	loss_policy_4: 0.07603
	accuracy_policy_4: 0.14176
	loss_value_4: 0.05378
	loss_reward_4: 0.01212
	loss_policy_5: 0.07603
	accuracy_policy_5: 0.14273
	loss_value_5: 0.05443
	loss_reward_5: 0.01331
	loss_policy: 0.75762
	loss_value: 0.49962
	loss_reward: 0.05747
[2025-05-08 04:52:22] nn step 950, lr: 0.1.
	loss_policy_0: 0.40695
	accuracy_policy_0: 0.15668
	loss_value_0: 0.25374
	loss_policy_1: 0.08169
	accuracy_policy_1: 0.14418
	loss_value_1: 0.05403
	loss_reward_1: 0.01148
	loss_policy_2: 0.08177
	accuracy_policy_2: 0.14594
	loss_value_2: 0.05523
	loss_reward_2: 0.01227
	loss_policy_3: 0.08169
	accuracy_policy_3: 0.1473
	loss_value_3: 0.05694
	loss_reward_3: 0.01123
	loss_policy_4: 0.08183
	accuracy_policy_4: 0.14863
	loss_value_4: 0.05796
	loss_reward_4: 0.01272
	loss_policy_5: 0.08188
	accuracy_policy_5: 0.14852
	loss_value_5: 0.05874
	loss_reward_5: 0.01476
	loss_policy: 0.81579
	loss_value: 0.53663
	loss_reward: 0.06246
[2025-05-08 04:52:28] nn step 1000, lr: 0.1.
	loss_policy_0: 0.40658
	accuracy_policy_0: 0.15152
	loss_value_0: 0.25213
	loss_policy_1: 0.08166
	accuracy_policy_1: 0.14676
	loss_value_1: 0.05338
	loss_reward_1: 0.01096
	loss_policy_2: 0.08177
	accuracy_policy_2: 0.14246
	loss_value_2: 0.05488
	loss_reward_2: 0.01186
	loss_policy_3: 0.08181
	accuracy_policy_3: 0.14402
	loss_value_3: 0.05639
	loss_reward_3: 0.01096
	loss_policy_4: 0.08184
	accuracy_policy_4: 0.1466
	loss_value_4: 0.05771
	loss_reward_4: 0.01255
	loss_policy_5: 0.08185
	accuracy_policy_5: 0.14699
	loss_value_5: 0.05853
	loss_reward_5: 0.01431
	loss_policy: 0.81551
	loss_value: 0.53302
	loss_reward: 0.06065
Optimization_Done 1000
[2025-05-08 04:53:38] [command] train weight_iter_1000.pkl 1 6
[2025-05-08 04:53:47] nn step 1050, lr: 0.1.
	loss_policy_0: 0.41044
	accuracy_policy_0: 0.15988
	loss_value_0: 0.2672
	loss_policy_1: 0.08224
	accuracy_policy_1: 0.15078
	loss_value_1: 0.05564
	loss_reward_1: 0.01037
	loss_policy_2: 0.08246
	accuracy_policy_2: 0.14695
	loss_value_2: 0.05741
	loss_reward_2: 0.01166
	loss_policy_3: 0.0825
	accuracy_policy_3: 0.14891
	loss_value_3: 0.05889
	loss_reward_3: 0.01063
	loss_policy_4: 0.08258
	accuracy_policy_4: 0.15289
	loss_value_4: 0.06001
	loss_reward_4: 0.01217
	loss_policy_5: 0.08259
	accuracy_policy_5: 0.15062
	loss_value_5: 0.06112
	loss_reward_5: 0.01417
	loss_policy: 0.82281
	loss_value: 0.56026
	loss_reward: 0.05899
[2025-05-08 04:53:53] nn step 1100, lr: 0.1.
	loss_policy_0: 0.40714
	accuracy_policy_0: 0.15695
	loss_value_0: 0.24928
	loss_policy_1: 0.08157
	accuracy_policy_1: 0.15168
	loss_value_1: 0.05285
	loss_reward_1: 0.00991
	loss_policy_2: 0.08167
	accuracy_policy_2: 0.15012
	loss_value_2: 0.05435
	loss_reward_2: 0.0113
	loss_policy_3: 0.08173
	accuracy_policy_3: 0.15266
	loss_value_3: 0.05588
	loss_reward_3: 0.01071
	loss_policy_4: 0.08184
	accuracy_policy_4: 0.14879
	loss_value_4: 0.05709
	loss_reward_4: 0.01255
	loss_policy_5: 0.0819
	accuracy_policy_5: 0.15273
	loss_value_5: 0.05847
	loss_reward_5: 0.01361
	loss_policy: 0.81585
	loss_value: 0.52793
	loss_reward: 0.05808
[2025-05-08 04:54:01] nn step 1150, lr: 0.1.
	loss_policy_0: 0.44243
	accuracy_policy_0: 0.1577
	loss_value_0: 0.2716
	loss_policy_1: 0.08866
	accuracy_policy_1: 0.15129
	loss_value_1: 0.05744
	loss_reward_1: 0.01119
	loss_policy_2: 0.08879
	accuracy_policy_2: 0.15
	loss_value_2: 0.05918
	loss_reward_2: 0.01251
	loss_policy_3: 0.08903
	accuracy_policy_3: 0.14316
	loss_value_3: 0.06093
	loss_reward_3: 0.01168
	loss_policy_4: 0.08907
	accuracy_policy_4: 0.14559
	loss_value_4: 0.06255
	loss_reward_4: 0.01285
	loss_policy_5: 0.08924
	accuracy_policy_5: 0.14781
	loss_value_5: 0.06379
	loss_reward_5: 0.01525
	loss_policy: 0.88722
	loss_value: 0.5755
	loss_reward: 0.06348
[2025-05-08 04:54:09] nn step 1200, lr: 0.1.
	loss_policy_0: 0.42147
	accuracy_policy_0: 0.15785
	loss_value_0: 0.25613
	loss_policy_1: 0.08445
	accuracy_policy_1: 0.15316
	loss_value_1: 0.0542
	loss_reward_1: 0.00977
	loss_policy_2: 0.0846
	accuracy_policy_2: 0.1502
	loss_value_2: 0.0562
	loss_reward_2: 0.01132
	loss_policy_3: 0.0848
	accuracy_policy_3: 0.14453
	loss_value_3: 0.05797
	loss_reward_3: 0.01094
	loss_policy_4: 0.08479
	accuracy_policy_4: 0.14656
	loss_value_4: 0.05933
	loss_reward_4: 0.01268
	loss_policy_5: 0.08485
	accuracy_policy_5: 0.15234
	loss_value_5: 0.06055
	loss_reward_5: 0.01431
	loss_policy: 0.84496
	loss_value: 0.54439
	loss_reward: 0.05902
Optimization_Done 1200
[2025-05-08 04:55:16] [command] train weight_iter_1200.pkl 1 7
[2025-05-08 04:55:23] nn step 1250, lr: 0.1.
	loss_policy_0: 0.41801
	accuracy_policy_0: 0.16871
	loss_value_0: 0.26718
	loss_policy_1: 0.08393
	accuracy_policy_1: 0.15672
	loss_value_1: 0.05598
	loss_reward_1: 0.00925
	loss_policy_2: 0.08407
	accuracy_policy_2: 0.15336
	loss_value_2: 0.05764
	loss_reward_2: 0.01156
	loss_policy_3: 0.08411
	accuracy_policy_3: 0.15215
	loss_value_3: 0.05909
	loss_reward_3: 0.01073
	loss_policy_4: 0.08418
	accuracy_policy_4: 0.15656
	loss_value_4: 0.06021
	loss_reward_4: 0.01196
	loss_policy_5: 0.08421
	accuracy_policy_5: 0.1507
	loss_value_5: 0.06131
	loss_reward_5: 0.01374
	loss_policy: 0.83851
	loss_value: 0.5614
	loss_reward: 0.05723
[2025-05-08 04:55:31] nn step 1300, lr: 0.1.
	loss_policy_0: 0.45183
	accuracy_policy_0: 0.16465
	loss_value_0: 0.27998
	loss_policy_1: 0.09064
	accuracy_policy_1: 0.15539
	loss_value_1: 0.05925
	loss_reward_1: 0.01024
	loss_policy_2: 0.09083
	accuracy_policy_2: 0.14816
	loss_value_2: 0.06055
	loss_reward_2: 0.01245
	loss_policy_3: 0.09085
	accuracy_policy_3: 0.15613
	loss_value_3: 0.06221
	loss_reward_3: 0.01208
	loss_policy_4: 0.09098
	accuracy_policy_4: 0.14969
	loss_value_4: 0.0638
	loss_reward_4: 0.01316
	loss_policy_5: 0.09105
	accuracy_policy_5: 0.15023
	loss_value_5: 0.06521
	loss_reward_5: 0.01507
	loss_policy: 0.90619
	loss_value: 0.59101
	loss_reward: 0.063
[2025-05-08 04:55:39] nn step 1350, lr: 0.1.
	loss_policy_0: 0.41525
	accuracy_policy_0: 0.16199
	loss_value_0: 0.2523
	loss_policy_1: 0.0834
	accuracy_policy_1: 0.15422
	loss_value_1: 0.05358
	loss_reward_1: 0.00888
	loss_policy_2: 0.08353
	accuracy_policy_2: 0.15254
	loss_value_2: 0.05509
	loss_reward_2: 0.01086
	loss_policy_3: 0.08363
	accuracy_policy_3: 0.15059
	loss_value_3: 0.05678
	loss_reward_3: 0.01056
	loss_policy_4: 0.08364
	accuracy_policy_4: 0.1484
	loss_value_4: 0.05804
	loss_reward_4: 0.01186
	loss_policy_5: 0.08381
	accuracy_policy_5: 0.14461
	loss_value_5: 0.05928
	loss_reward_5: 0.0133
	loss_policy: 0.83327
	loss_value: 0.53507
	loss_reward: 0.05546
[2025-05-08 04:55:45] nn step 1400, lr: 0.1.
	loss_policy_0: 0.41886
	accuracy_policy_0: 0.16379
	loss_value_0: 0.255
	loss_policy_1: 0.08394
	accuracy_policy_1: 0.15719
	loss_value_1: 0.05383
	loss_reward_1: 0.00899
	loss_policy_2: 0.08404
	accuracy_policy_2: 0.15188
	loss_value_2: 0.05519
	loss_reward_2: 0.01086
	loss_policy_3: 0.0842
	accuracy_policy_3: 0.14855
	loss_value_3: 0.05671
	loss_reward_3: 0.01046
	loss_policy_4: 0.08422
	accuracy_policy_4: 0.1457
	loss_value_4: 0.0581
	loss_reward_4: 0.01205
	loss_policy_5: 0.08426
	accuracy_policy_5: 0.14645
	loss_value_5: 0.05961
	loss_reward_5: 0.01382
	loss_policy: 0.83951
	loss_value: 0.53845
	loss_reward: 0.05617
Optimization_Done 1400
[2025-05-08 04:56:54] [command] train weight_iter_1400.pkl 1 8
[2025-05-08 04:57:03] nn step 1450, lr: 0.1.
	loss_policy_0: 0.43349
	accuracy_policy_0: 0.16977
	loss_value_0: 0.27558
	loss_policy_1: 0.08701
	accuracy_policy_1: 0.16344
	loss_value_1: 0.05781
	loss_reward_1: 0.00903
	loss_policy_2: 0.08717
	accuracy_policy_2: 0.16246
	loss_value_2: 0.05918
	loss_reward_2: 0.01204
	loss_policy_3: 0.08717
	accuracy_policy_3: 0.16484
	loss_value_3: 0.06048
	loss_reward_3: 0.01106
	loss_policy_4: 0.08731
	accuracy_policy_4: 0.16141
	loss_value_4: 0.06181
	loss_reward_4: 0.01214
	loss_policy_5: 0.08726
	accuracy_policy_5: 0.15902
	loss_value_5: 0.06267
	loss_reward_5: 0.01429
	loss_policy: 0.86941
	loss_value: 0.57753
	loss_reward: 0.05855
[2025-05-08 04:57:10] nn step 1500, lr: 0.1.
	loss_policy_0: 0.4436
	accuracy_policy_0: 0.16934
	loss_value_0: 0.27586
	loss_policy_1: 0.089
	accuracy_policy_1: 0.16461
	loss_value_1: 0.05798
	loss_reward_1: 0.00847
	loss_policy_2: 0.08904
	accuracy_policy_2: 0.16953
	loss_value_2: 0.05955
	loss_reward_2: 0.01172
	loss_policy_3: 0.08925
	accuracy_policy_3: 0.16082
	loss_value_3: 0.06133
	loss_reward_3: 0.01136
	loss_policy_4: 0.08934
	accuracy_policy_4: 0.15859
	loss_value_4: 0.06319
	loss_reward_4: 0.01276
	loss_policy_5: 0.08946
	accuracy_policy_5: 0.15785
	loss_value_5: 0.06426
	loss_reward_5: 0.01409
	loss_policy: 0.88969
	loss_value: 0.58217
	loss_reward: 0.05841
[2025-05-08 04:57:17] nn step 1550, lr: 0.1.
	loss_policy_0: 0.42169
	accuracy_policy_0: 0.16762
	loss_value_0: 0.25789
	loss_policy_1: 0.08451
	accuracy_policy_1: 0.16211
	loss_value_1: 0.05439
	loss_reward_1: 0.00834
	loss_policy_2: 0.08456
	accuracy_policy_2: 0.16684
	loss_value_2: 0.0562
	loss_reward_2: 0.01086
	loss_policy_3: 0.08476
	accuracy_policy_3: 0.15992
	loss_value_3: 0.05774
	loss_reward_3: 0.0109
	loss_policy_4: 0.08472
	accuracy_policy_4: 0.15844
	loss_value_4: 0.05901
	loss_reward_4: 0.01206
	loss_policy_5: 0.08476
	accuracy_policy_5: 0.16039
	loss_value_5: 0.06024
	loss_reward_5: 0.01352
	loss_policy: 0.845
	loss_value: 0.54548
	loss_reward: 0.05569
[2025-05-08 04:57:25] nn step 1600, lr: 0.1.
	loss_policy_0: 0.4482
	accuracy_policy_0: 0.17391
	loss_value_0: 0.27379
	loss_policy_1: 0.0898
	accuracy_policy_1: 0.16766
	loss_value_1: 0.05785
	loss_reward_1: 0.00879
	loss_policy_2: 0.09008
	accuracy_policy_2: 0.16098
	loss_value_2: 0.0595
	loss_reward_2: 0.01175
	loss_policy_3: 0.09
	accuracy_policy_3: 0.15996
	loss_value_3: 0.06126
	loss_reward_3: 0.01128
	loss_policy_4: 0.09024
	accuracy_policy_4: 0.15711
	loss_value_4: 0.06285
	loss_reward_4: 0.01281
	loss_policy_5: 0.09013
	accuracy_policy_5: 0.16164
	loss_value_5: 0.06427
	loss_reward_5: 0.01436
	loss_policy: 0.89845
	loss_value: 0.57953
	loss_reward: 0.059
Optimization_Done 1600
[2025-05-08 04:58:33] [command] train weight_iter_1600.pkl 1 9
[2025-05-08 04:58:40] nn step 1650, lr: 0.1.
	loss_policy_0: 0.42089
	accuracy_policy_0: 0.19695
	loss_value_0: 0.27013
	loss_policy_1: 0.08458
	accuracy_policy_1: 0.19254
	loss_value_1: 0.05659
	loss_reward_1: 0.00819
	loss_policy_2: 0.08465
	accuracy_policy_2: 0.19074
	loss_value_2: 0.05819
	loss_reward_2: 0.01098
	loss_policy_3: 0.08471
	accuracy_policy_3: 0.18703
	loss_value_3: 0.05947
	loss_reward_3: 0.01062
	loss_policy_4: 0.08484
	accuracy_policy_4: 0.18297
	loss_value_4: 0.06072
	loss_reward_4: 0.01198
	loss_policy_5: 0.08498
	accuracy_policy_5: 0.18277
	loss_value_5: 0.06174
	loss_reward_5: 0.01301
	loss_policy: 0.84466
	loss_value: 0.56684
	loss_reward: 0.05477
[2025-05-08 04:58:48] nn step 1700, lr: 0.1.
	loss_policy_0: 0.42512
	accuracy_policy_0: 0.1909
	loss_value_0: 0.26756
	loss_policy_1: 0.08533
	accuracy_policy_1: 0.18816
	loss_value_1: 0.05606
	loss_reward_1: 0.00806
	loss_policy_2: 0.08534
	accuracy_policy_2: 0.18977
	loss_value_2: 0.05751
	loss_reward_2: 0.011
	loss_policy_3: 0.08543
	accuracy_policy_3: 0.18527
	loss_value_3: 0.05897
	loss_reward_3: 0.01087
	loss_policy_4: 0.08552
	accuracy_policy_4: 0.18363
	loss_value_4: 0.06042
	loss_reward_4: 0.01245
	loss_policy_5: 0.0859
	accuracy_policy_5: 0.1725
	loss_value_5: 0.06152
	loss_reward_5: 0.0134
	loss_policy: 0.85265
	loss_value: 0.56205
	loss_reward: 0.05577
[2025-05-08 04:58:56] nn step 1750, lr: 0.1.
	loss_policy_0: 0.44494
	accuracy_policy_0: 0.18895
	loss_value_0: 0.27168
	loss_policy_1: 0.08925
	accuracy_policy_1: 0.18301
	loss_value_1: 0.05721
	loss_reward_1: 0.00818
	loss_policy_2: 0.08945
	accuracy_policy_2: 0.18129
	loss_value_2: 0.05932
	loss_reward_2: 0.01145
	loss_policy_3: 0.08962
	accuracy_policy_3: 0.17777
	loss_value_3: 0.06106
	loss_reward_3: 0.01148
	loss_policy_4: 0.08979
	accuracy_policy_4: 0.17328
	loss_value_4: 0.06254
	loss_reward_4: 0.01267
	loss_policy_5: 0.08972
	accuracy_policy_5: 0.17895
	loss_value_5: 0.06382
	loss_reward_5: 0.01392
	loss_policy: 0.89278
	loss_value: 0.57562
	loss_reward: 0.05771
[2025-05-08 04:59:02] nn step 1800, lr: 0.1.
	loss_policy_0: 0.44483
	accuracy_policy_0: 0.18895
	loss_value_0: 0.27343
	loss_policy_1: 0.08931
	accuracy_policy_1: 0.18445
	loss_value_1: 0.05768
	loss_reward_1: 0.00814
	loss_policy_2: 0.0895
	accuracy_policy_2: 0.17918
	loss_value_2: 0.05954
	loss_reward_2: 0.01157
	loss_policy_3: 0.0896
	accuracy_policy_3: 0.1732
	loss_value_3: 0.06133
	loss_reward_3: 0.01135
	loss_policy_4: 0.08964
	accuracy_policy_4: 0.1773
	loss_value_4: 0.06286
	loss_reward_4: 0.01304
	loss_policy_5: 0.08973
	accuracy_policy_5: 0.17523
	loss_value_5: 0.06388
	loss_reward_5: 0.01443
	loss_policy: 0.89262
	loss_value: 0.57871
	loss_reward: 0.05853
Optimization_Done 1800
[2025-05-08 05:00:11] [command] train weight_iter_1800.pkl 1 10
[2025-05-08 05:00:20] nn step 1850, lr: 0.1.
	loss_policy_0: 0.43293
	accuracy_policy_0: 0.22156
	loss_value_0: 0.28185
	loss_policy_1: 0.0872
	accuracy_policy_1: 0.20855
	loss_value_1: 0.05887
	loss_reward_1: 0.0076
	loss_policy_2: 0.08727
	accuracy_policy_2: 0.20898
	loss_value_2: 0.06028
	loss_reward_2: 0.01147
	loss_policy_3: 0.08736
	accuracy_policy_3: 0.20715
	loss_value_3: 0.062
	loss_reward_3: 0.01118
	loss_policy_4: 0.0877
	accuracy_policy_4: 0.19953
	loss_value_4: 0.06352
	loss_reward_4: 0.01241
	loss_policy_5: 0.08785
	accuracy_policy_5: 0.2002
	loss_value_5: 0.0649
	loss_reward_5: 0.01415
	loss_policy: 0.87032
	loss_value: 0.59142
	loss_reward: 0.05681
[2025-05-08 05:00:28] nn step 1900, lr: 0.1.
	loss_policy_0: 0.41444
	accuracy_policy_0: 0.22043
	loss_value_0: 0.26311
	loss_policy_1: 0.08334
	accuracy_policy_1: 0.20852
	loss_value_1: 0.05492
	loss_reward_1: 0.00701
	loss_policy_2: 0.0835
	accuracy_policy_2: 0.20871
	loss_value_2: 0.05647
	loss_reward_2: 0.01074
	loss_policy_3: 0.08347
	accuracy_policy_3: 0.20836
	loss_value_3: 0.05794
	loss_reward_3: 0.01054
	loss_policy_4: 0.08378
	accuracy_policy_4: 0.20805
	loss_value_4: 0.05951
	loss_reward_4: 0.01152
	loss_policy_5: 0.08398
	accuracy_policy_5: 0.20168
	loss_value_5: 0.0609
	loss_reward_5: 0.01333
	loss_policy: 0.83252
	loss_value: 0.55285
	loss_reward: 0.05315
[2025-05-08 05:00:34] nn step 1950, lr: 0.1.
	loss_policy_0: 0.44652
	accuracy_policy_0: 0.21422
	loss_value_0: 0.28202
	loss_policy_1: 0.0896
	accuracy_policy_1: 0.20641
	loss_value_1: 0.05894
	loss_reward_1: 0.00774
	loss_policy_2: 0.08989
	accuracy_policy_2: 0.20184
	loss_value_2: 0.06048
	loss_reward_2: 0.01118
	loss_policy_3: 0.09017
	accuracy_policy_3: 0.19969
	loss_value_3: 0.06216
	loss_reward_3: 0.0115
	loss_policy_4: 0.09015
	accuracy_policy_4: 0.20023
	loss_value_4: 0.06352
	loss_reward_4: 0.01246
	loss_policy_5: 0.09011
	accuracy_policy_5: 0.19969
	loss_value_5: 0.06467
	loss_reward_5: 0.01438
	loss_policy: 0.89644
	loss_value: 0.59179
	loss_reward: 0.05725
[2025-05-08 05:00:42] nn step 2000, lr: 0.1.
	loss_policy_0: 0.42828
	accuracy_policy_0: 0.21141
	loss_value_0: 0.26698
	loss_policy_1: 0.08592
	accuracy_policy_1: 0.20797
	loss_value_1: 0.05605
	loss_reward_1: 0.00706
	loss_policy_2: 0.08612
	accuracy_policy_2: 0.20512
	loss_value_2: 0.05795
	loss_reward_2: 0.01068
	loss_policy_3: 0.08657
	accuracy_policy_3: 0.19738
	loss_value_3: 0.0594
	loss_reward_3: 0.01054
	loss_policy_4: 0.08683
	accuracy_policy_4: 0.19289
	loss_value_4: 0.06118
	loss_reward_4: 0.01189
	loss_policy_5: 0.08669
	accuracy_policy_5: 0.19285
	loss_value_5: 0.06251
	loss_reward_5: 0.01377
	loss_policy: 0.86041
	loss_value: 0.56408
	loss_reward: 0.05395
Optimization_Done 2000
[2025-05-08 05:01:48] [command] train weight_iter_2000.pkl 1 11
[2025-05-08 05:01:55] nn step 2050, lr: 0.1.
	loss_policy_0: 0.43485
	accuracy_policy_0: 0.22773
	loss_value_0: 0.27986
	loss_policy_1: 0.08723
	accuracy_policy_1: 0.22707
	loss_value_1: 0.0586
	loss_reward_1: 0.00769
	loss_policy_2: 0.08736
	accuracy_policy_2: 0.22016
	loss_value_2: 0.06028
	loss_reward_2: 0.01068
	loss_policy_3: 0.0877
	accuracy_policy_3: 0.21488
	loss_value_3: 0.06226
	loss_reward_3: 0.01138
	loss_policy_4: 0.08785
	accuracy_policy_4: 0.21672
	loss_value_4: 0.0635
	loss_reward_4: 0.01238
	loss_policy_5: 0.08787
	accuracy_policy_5: 0.21453
	loss_value_5: 0.06464
	loss_reward_5: 0.01433
	loss_policy: 0.87286
	loss_value: 0.58914
	loss_reward: 0.05645
[2025-05-08 05:02:03] nn step 2100, lr: 0.1.
	loss_policy_0: 0.46216
	accuracy_policy_0: 0.2259
	loss_value_0: 0.2932
	loss_policy_1: 0.09245
	accuracy_policy_1: 0.22184
	loss_value_1: 0.06138
	loss_reward_1: 0.0082
	loss_policy_2: 0.09281
	accuracy_policy_2: 0.21609
	loss_value_2: 0.0633
	loss_reward_2: 0.01206
	loss_policy_3: 0.093
	accuracy_policy_3: 0.20988
	loss_value_3: 0.06511
	loss_reward_3: 0.01161
	loss_policy_4: 0.09299
	accuracy_policy_4: 0.21035
	loss_value_4: 0.06659
	loss_reward_4: 0.01344
	loss_policy_5: 0.09341
	accuracy_policy_5: 0.20711
	loss_value_5: 0.06817
	loss_reward_5: 0.0151
	loss_policy: 0.92681
	loss_value: 0.61774
	loss_reward: 0.06042
[2025-05-08 05:02:11] nn step 2150, lr: 0.1.
	loss_policy_0: 0.46117
	accuracy_policy_0: 0.22605
	loss_value_0: 0.29377
	loss_policy_1: 0.09277
	accuracy_policy_1: 0.2202
	loss_value_1: 0.06156
	loss_reward_1: 0.00778
	loss_policy_2: 0.09293
	accuracy_policy_2: 0.21547
	loss_value_2: 0.06348
	loss_reward_2: 0.01149
	loss_policy_3: 0.09312
	accuracy_policy_3: 0.21293
	loss_value_3: 0.06514
	loss_reward_3: 0.01166
	loss_policy_4: 0.09339
	accuracy_policy_4: 0.21055
	loss_value_4: 0.06693
	loss_reward_4: 0.01315
	loss_policy_5: 0.09354
	accuracy_policy_5: 0.20789
	loss_value_5: 0.0681
	loss_reward_5: 0.01488
	loss_policy: 0.92692
	loss_value: 0.61898
	loss_reward: 0.05896
[2025-05-08 05:02:19] nn step 2200, lr: 0.1.
	loss_policy_0: 0.44448
	accuracy_policy_0: 0.2209
	loss_value_0: 0.27889
	loss_policy_1: 0.0892
	accuracy_policy_1: 0.21117
	loss_value_1: 0.05867
	loss_reward_1: 0.00707
	loss_policy_2: 0.08965
	accuracy_policy_2: 0.21098
	loss_value_2: 0.06079
	loss_reward_2: 0.01098
	loss_policy_3: 0.0896
	accuracy_policy_3: 0.20953
	loss_value_3: 0.06239
	loss_reward_3: 0.01107
	loss_policy_4: 0.08993
	accuracy_policy_4: 0.20277
	loss_value_4: 0.06385
	loss_reward_4: 0.01246
	loss_policy_5: 0.08997
	accuracy_policy_5: 0.20367
	loss_value_5: 0.06528
	loss_reward_5: 0.01408
	loss_policy: 0.89283
	loss_value: 0.58987
	loss_reward: 0.05566
Optimization_Done 2200
[2025-05-08 05:03:27] [command] train weight_iter_2200.pkl 1 12
[2025-05-08 05:03:36] nn step 2250, lr: 0.1.
	loss_policy_0: 0.38146
	accuracy_policy_0: 0.29617
	loss_value_0: 0.26406
	loss_policy_1: 0.07658
	accuracy_policy_1: 0.29117
	loss_value_1: 0.05545
	loss_reward_1: 0.00651
	loss_policy_2: 0.07661
	accuracy_policy_2: 0.29422
	loss_value_2: 0.05691
	loss_reward_2: 0.00991
	loss_policy_3: 0.07708
	accuracy_policy_3: 0.28422
	loss_value_3: 0.05838
	loss_reward_3: 0.01004
	loss_policy_4: 0.07714
	accuracy_policy_4: 0.28242
	loss_value_4: 0.05967
	loss_reward_4: 0.01166
	loss_policy_5: 0.07747
	accuracy_policy_5: 0.27922
	loss_value_5: 0.06103
	loss_reward_5: 0.01302
	loss_policy: 0.76634
	loss_value: 0.5555
	loss_reward: 0.05115
[2025-05-08 05:03:44] nn step 2300, lr: 0.1.
	loss_policy_0: 0.38696
	accuracy_policy_0: 0.29109
	loss_value_0: 0.26601
	loss_policy_1: 0.0776
	accuracy_policy_1: 0.28172
	loss_value_1: 0.05568
	loss_reward_1: 0.00651
	loss_policy_2: 0.07776
	accuracy_policy_2: 0.28035
	loss_value_2: 0.05738
	loss_reward_2: 0.00965
	loss_policy_3: 0.07792
	accuracy_policy_3: 0.28105
	loss_value_3: 0.05879
	loss_reward_3: 0.01044
	loss_policy_4: 0.07806
	accuracy_policy_4: 0.27562
	loss_value_4: 0.06011
	loss_reward_4: 0.01101
	loss_policy_5: 0.07838
	accuracy_policy_5: 0.27566
	loss_value_5: 0.06164
	loss_reward_5: 0.01278
	loss_policy: 0.77668
	loss_value: 0.55961
	loss_reward: 0.05038
[2025-05-08 05:03:50] nn step 2350, lr: 0.1.
	loss_policy_0: 0.38954
	accuracy_policy_0: 0.28621
	loss_value_0: 0.26662
	loss_policy_1: 0.07808
	accuracy_policy_1: 0.27816
	loss_value_1: 0.0555
	loss_reward_1: 0.00651
	loss_policy_2: 0.07839
	accuracy_policy_2: 0.27684
	loss_value_2: 0.05711
	loss_reward_2: 0.00987
	loss_policy_3: 0.0786
	accuracy_policy_3: 0.27746
	loss_value_3: 0.05882
	loss_reward_3: 0.01036
	loss_policy_4: 0.07874
	accuracy_policy_4: 0.27297
	loss_value_4: 0.06023
	loss_reward_4: 0.01174
	loss_policy_5: 0.07908
	accuracy_policy_5: 0.26918
	loss_value_5: 0.06165
	loss_reward_5: 0.01283
	loss_policy: 0.78242
	loss_value: 0.55993
	loss_reward: 0.05131
[2025-05-08 05:03:58] nn step 2400, lr: 0.1.
	loss_policy_0: 0.40907
	accuracy_policy_0: 0.28676
	loss_value_0: 0.27973
	loss_policy_1: 0.08225
	accuracy_policy_1: 0.27762
	loss_value_1: 0.05839
	loss_reward_1: 0.00693
	loss_policy_2: 0.08242
	accuracy_policy_2: 0.27227
	loss_value_2: 0.06012
	loss_reward_2: 0.01046
	loss_policy_3: 0.08252
	accuracy_policy_3: 0.27328
	loss_value_3: 0.06176
	loss_reward_3: 0.01113
	loss_policy_4: 0.08253
	accuracy_policy_4: 0.27234
	loss_value_4: 0.06343
	loss_reward_4: 0.01217
	loss_policy_5: 0.08299
	accuracy_policy_5: 0.26336
	loss_value_5: 0.06499
	loss_reward_5: 0.01373
	loss_policy: 0.82179
	loss_value: 0.58843
	loss_reward: 0.05442
Optimization_Done 2400
[2025-05-08 05:05:07] [command] train weight_iter_2400.pkl 1 13
[2025-05-08 05:05:15] nn step 2450, lr: 0.1.
	loss_policy_0: 0.37595
	accuracy_policy_0: 0.35648
	loss_value_0: 0.27779
	loss_policy_1: 0.07536
	accuracy_policy_1: 0.34996
	loss_value_1: 0.05799
	loss_reward_1: 0.00637
	loss_policy_2: 0.07538
	accuracy_policy_2: 0.35086
	loss_value_2: 0.05977
	loss_reward_2: 0.00968
	loss_policy_3: 0.07557
	accuracy_policy_3: 0.35
	loss_value_3: 0.06139
	loss_reward_3: 0.01022
	loss_policy_4: 0.07596
	accuracy_policy_4: 0.34664
	loss_value_4: 0.06284
	loss_reward_4: 0.01166
	loss_policy_5: 0.07603
	accuracy_policy_5: 0.34418
	loss_value_5: 0.06445
	loss_reward_5: 0.01312
	loss_policy: 0.75425
	loss_value: 0.58423
	loss_reward: 0.05104
[2025-05-08 05:05:22] nn step 2500, lr: 0.1.
	loss_policy_0: 0.35954
	accuracy_policy_0: 0.34996
	loss_value_0: 0.26654
	loss_policy_1: 0.07233
	accuracy_policy_1: 0.34609
	loss_value_1: 0.05501
	loss_reward_1: 0.0059
	loss_policy_2: 0.07233
	accuracy_policy_2: 0.34043
	loss_value_2: 0.0564
	loss_reward_2: 0.00927
	loss_policy_3: 0.07262
	accuracy_policy_3: 0.33992
	loss_value_3: 0.05829
	loss_reward_3: 0.00978
	loss_policy_4: 0.07265
	accuracy_policy_4: 0.33727
	loss_value_4: 0.05982
	loss_reward_4: 0.01069
	loss_policy_5: 0.07282
	accuracy_policy_5: 0.33434
	loss_value_5: 0.06119
	loss_reward_5: 0.01206
	loss_policy: 0.72228
	loss_value: 0.55726
	loss_reward: 0.04769
[2025-05-08 05:05:30] nn step 2550, lr: 0.1.
	loss_policy_0: 0.35916
	accuracy_policy_0: 0.34461
	loss_value_0: 0.26368
	loss_policy_1: 0.07217
	accuracy_policy_1: 0.33613
	loss_value_1: 0.05461
	loss_reward_1: 0.00579
	loss_policy_2: 0.07211
	accuracy_policy_2: 0.3359
	loss_value_2: 0.05627
	loss_reward_2: 0.00927
	loss_policy_3: 0.07216
	accuracy_policy_3: 0.3373
	loss_value_3: 0.0579
	loss_reward_3: 0.00938
	loss_policy_4: 0.07259
	accuracy_policy_4: 0.33434
	loss_value_4: 0.05935
	loss_reward_4: 0.0108
	loss_policy_5: 0.0727
	accuracy_policy_5: 0.32496
	loss_value_5: 0.06058
	loss_reward_5: 0.01226
	loss_policy: 0.7209
	loss_value: 0.55239
	loss_reward: 0.04749
[2025-05-08 05:05:38] nn step 2600, lr: 0.1.
	loss_policy_0: 0.39409
	accuracy_policy_0: 0.33816
	loss_value_0: 0.2836
	loss_policy_1: 0.079
	accuracy_policy_1: 0.32859
	loss_value_1: 0.05908
	loss_reward_1: 0.00646
	loss_policy_2: 0.07889
	accuracy_policy_2: 0.3302
	loss_value_2: 0.0611
	loss_reward_2: 0.00996
	loss_policy_3: 0.07936
	accuracy_policy_3: 0.32395
	loss_value_3: 0.06281
	loss_reward_3: 0.01013
	loss_policy_4: 0.07928
	accuracy_policy_4: 0.32367
	loss_value_4: 0.06427
	loss_reward_4: 0.01144
	loss_policy_5: 0.0798
	accuracy_policy_5: 0.32164
	loss_value_5: 0.06546
	loss_reward_5: 0.01329
	loss_policy: 0.79042
	loss_value: 0.59632
	loss_reward: 0.05129
Optimization_Done 2600
[2025-05-08 05:06:46] [command] train weight_iter_2600.pkl 1 14
[2025-05-08 05:06:55] nn step 2650, lr: 0.1.
	loss_policy_0: 0.36607
	accuracy_policy_0: 0.38402
	loss_value_0: 0.27942
	loss_policy_1: 0.07346
	accuracy_policy_1: 0.37836
	loss_value_1: 0.05793
	loss_reward_1: 0.00575
	loss_policy_2: 0.07372
	accuracy_policy_2: 0.37473
	loss_value_2: 0.05979
	loss_reward_2: 0.00898
	loss_policy_3: 0.0737
	accuracy_policy_3: 0.3752
	loss_value_3: 0.06171
	loss_reward_3: 0.01004
	loss_policy_4: 0.07365
	accuracy_policy_4: 0.36996
	loss_value_4: 0.06329
	loss_reward_4: 0.01124
	loss_policy_5: 0.07421
	accuracy_policy_5: 0.36555
	loss_value_5: 0.06469
	loss_reward_5: 0.01235
	loss_policy: 0.7348
	loss_value: 0.58684
	loss_reward: 0.04836
[2025-05-08 05:07:03] nn step 2700, lr: 0.1.
	loss_policy_0: 0.35317
	accuracy_policy_0: 0.37992
	loss_value_0: 0.26603
	loss_policy_1: 0.07087
	accuracy_policy_1: 0.37469
	loss_value_1: 0.0555
	loss_reward_1: 0.00523
	loss_policy_2: 0.07104
	accuracy_policy_2: 0.37086
	loss_value_2: 0.05726
	loss_reward_2: 0.00893
	loss_policy_3: 0.07117
	accuracy_policy_3: 0.37055
	loss_value_3: 0.05892
	loss_reward_3: 0.00965
	loss_policy_4: 0.07108
	accuracy_policy_4: 0.36969
	loss_value_4: 0.06086
	loss_reward_4: 0.01072
	loss_policy_5: 0.07171
	accuracy_policy_5: 0.36488
	loss_value_5: 0.06222
	loss_reward_5: 0.01223
	loss_policy: 0.70904
	loss_value: 0.5608
	loss_reward: 0.04676
[2025-05-08 05:07:09] nn step 2750, lr: 0.1.
	loss_policy_0: 0.33996
	accuracy_policy_0: 0.37602
	loss_value_0: 0.2583
	loss_policy_1: 0.06828
	accuracy_policy_1: 0.3743
	loss_value_1: 0.05359
	loss_reward_1: 0.00527
	loss_policy_2: 0.06846
	accuracy_policy_2: 0.36871
	loss_value_2: 0.05514
	loss_reward_2: 0.00853
	loss_policy_3: 0.06833
	accuracy_policy_3: 0.36879
	loss_value_3: 0.05639
	loss_reward_3: 0.0092
	loss_policy_4: 0.06848
	accuracy_policy_4: 0.37004
	loss_value_4: 0.05791
	loss_reward_4: 0.01025
	loss_policy_5: 0.06875
	accuracy_policy_5: 0.36461
	loss_value_5: 0.05925
	loss_reward_5: 0.01165
	loss_policy: 0.68226
	loss_value: 0.54057
	loss_reward: 0.0449
[2025-05-08 05:07:18] nn step 2800, lr: 0.1.
	loss_policy_0: 0.34341
	accuracy_policy_0: 0.36926
	loss_value_0: 0.25812
	loss_policy_1: 0.06883
	accuracy_policy_1: 0.36562
	loss_value_1: 0.05356
	loss_reward_1: 0.00522
	loss_policy_2: 0.06893
	accuracy_policy_2: 0.36852
	loss_value_2: 0.05527
	loss_reward_2: 0.00883
	loss_policy_3: 0.06914
	accuracy_policy_3: 0.36348
	loss_value_3: 0.05684
	loss_reward_3: 0.00888
	loss_policy_4: 0.06921
	accuracy_policy_4: 0.36312
	loss_value_4: 0.05812
	loss_reward_4: 0.01054
	loss_policy_5: 0.06936
	accuracy_policy_5: 0.35551
	loss_value_5: 0.05947
	loss_reward_5: 0.0114
	loss_policy: 0.68889
	loss_value: 0.54138
	loss_reward: 0.04486
Optimization_Done 2800
[2025-05-08 05:08:28] [command] train weight_iter_2800.pkl 1 15
[2025-05-08 05:08:36] nn step 2850, lr: 0.1.
	loss_policy_0: 0.33644
	accuracy_policy_0: 0.41656
	loss_value_0: 0.26779
	loss_policy_1: 0.06767
	accuracy_policy_1: 0.40812
	loss_value_1: 0.05523
	loss_reward_1: 0.00532
	loss_policy_2: 0.06781
	accuracy_policy_2: 0.40555
	loss_value_2: 0.05681
	loss_reward_2: 0.00847
	loss_policy_3: 0.06778
	accuracy_policy_3: 0.40809
	loss_value_3: 0.05855
	loss_reward_3: 0.00939
	loss_policy_4: 0.06807
	accuracy_policy_4: 0.40363
	loss_value_4: 0.05987
	loss_reward_4: 0.01001
	loss_policy_5: 0.06824
	accuracy_policy_5: 0.39965
	loss_value_5: 0.06118
	loss_reward_5: 0.01173
	loss_policy: 0.67601
	loss_value: 0.55943
	loss_reward: 0.04492
[2025-05-08 05:08:44] nn step 2900, lr: 0.1.
	loss_policy_0: 0.3198
	accuracy_policy_0: 0.40742
	loss_value_0: 0.25086
	loss_policy_1: 0.06437
	accuracy_policy_1: 0.40016
	loss_value_1: 0.05239
	loss_reward_1: 0.00494
	loss_policy_2: 0.06426
	accuracy_policy_2: 0.40277
	loss_value_2: 0.05388
	loss_reward_2: 0.00767
	loss_policy_3: 0.06451
	accuracy_policy_3: 0.39758
	loss_value_3: 0.05525
	loss_reward_3: 0.00865
	loss_policy_4: 0.06481
	accuracy_policy_4: 0.39383
	loss_value_4: 0.05693
	loss_reward_4: 0.00983
	loss_policy_5: 0.0648
	accuracy_policy_5: 0.39215
	loss_value_5: 0.05808
	loss_reward_5: 0.01097
	loss_policy: 0.64256
	loss_value: 0.52741
	loss_reward: 0.04206
[2025-05-08 05:08:52] nn step 2950, lr: 0.1.
	loss_policy_0: 0.33426
	accuracy_policy_0: 0.39891
	loss_value_0: 0.26202
	loss_policy_1: 0.0672
	accuracy_policy_1: 0.3927
	loss_value_1: 0.05422
	loss_reward_1: 0.00515
	loss_policy_2: 0.06703
	accuracy_policy_2: 0.39566
	loss_value_2: 0.05585
	loss_reward_2: 0.00826
	loss_policy_3: 0.06732
	accuracy_policy_3: 0.39008
	loss_value_3: 0.05755
	loss_reward_3: 0.00864
	loss_policy_4: 0.06737
	accuracy_policy_4: 0.38906
	loss_value_4: 0.05913
	loss_reward_4: 0.01011
	loss_policy_5: 0.06769
	accuracy_policy_5: 0.38656
	loss_value_5: 0.06031
	loss_reward_5: 0.01153
	loss_policy: 0.67087
	loss_value: 0.54908
	loss_reward: 0.04368
[2025-05-08 05:08:59] nn step 3000, lr: 0.1.
	loss_policy_0: 0.34787
	accuracy_policy_0: 0.395
	loss_value_0: 0.26775
	loss_policy_1: 0.0698
	accuracy_policy_1: 0.39203
	loss_value_1: 0.05549
	loss_reward_1: 0.0054
	loss_policy_2: 0.06965
	accuracy_policy_2: 0.39402
	loss_value_2: 0.05734
	loss_reward_2: 0.00886
	loss_policy_3: 0.06986
	accuracy_policy_3: 0.39102
	loss_value_3: 0.05908
	loss_reward_3: 0.00895
	loss_policy_4: 0.06999
	accuracy_policy_4: 0.38844
	loss_value_4: 0.06065
	loss_reward_4: 0.01034
	loss_policy_5: 0.07019
	accuracy_policy_5: 0.38609
	loss_value_5: 0.06202
	loss_reward_5: 0.01208
	loss_policy: 0.69735
	loss_value: 0.56233
	loss_reward: 0.04563
Optimization_Done 3000
[2025-05-08 05:10:06] [command] train weight_iter_3000.pkl 1 16
[2025-05-08 05:10:15] nn step 3050, lr: 0.1.
	loss_policy_0: 0.31185
	accuracy_policy_0: 0.43277
	loss_value_0: 0.25335
	loss_policy_1: 0.0624
	accuracy_policy_1: 0.42828
	loss_value_1: 0.05214
	loss_reward_1: 0.00465
	loss_policy_2: 0.06248
	accuracy_policy_2: 0.43098
	loss_value_2: 0.05357
	loss_reward_2: 0.00758
	loss_policy_3: 0.0626
	accuracy_policy_3: 0.42758
	loss_value_3: 0.05534
	loss_reward_3: 0.00815
	loss_policy_4: 0.06275
	accuracy_policy_4: 0.42055
	loss_value_4: 0.05669
	loss_reward_4: 0.00934
	loss_policy_5: 0.06288
	accuracy_policy_5: 0.42289
	loss_value_5: 0.05789
	loss_reward_5: 0.01106
	loss_policy: 0.62496
	loss_value: 0.52899
	loss_reward: 0.04078
[2025-05-08 05:10:22] nn step 3100, lr: 0.1.
	loss_policy_0: 0.34318
	accuracy_policy_0: 0.43238
	loss_value_0: 0.27788
	loss_policy_1: 0.06915
	accuracy_policy_1: 0.42688
	loss_value_1: 0.05803
	loss_reward_1: 0.00517
	loss_policy_2: 0.06914
	accuracy_policy_2: 0.42441
	loss_value_2: 0.05958
	loss_reward_2: 0.0087
	loss_policy_3: 0.06926
	accuracy_policy_3: 0.42086
	loss_value_3: 0.06132
	loss_reward_3: 0.00901
	loss_policy_4: 0.06931
	accuracy_policy_4: 0.42129
	loss_value_4: 0.0629
	loss_reward_4: 0.01009
	loss_policy_5: 0.06948
	accuracy_policy_5: 0.42188
	loss_value_5: 0.06402
	loss_reward_5: 0.01187
	loss_policy: 0.68953
	loss_value: 0.58373
	loss_reward: 0.04483
[2025-05-08 05:10:30] nn step 3150, lr: 0.1.
	loss_policy_0: 0.32004
	accuracy_policy_0: 0.42805
	loss_value_0: 0.26241
	loss_policy_1: 0.06428
	accuracy_policy_1: 0.41906
	loss_value_1: 0.05415
	loss_reward_1: 0.00487
	loss_policy_2: 0.06413
	accuracy_policy_2: 0.42344
	loss_value_2: 0.05565
	loss_reward_2: 0.00801
	loss_policy_3: 0.06445
	accuracy_policy_3: 0.42176
	loss_value_3: 0.05729
	loss_reward_3: 0.00876
	loss_policy_4: 0.0644
	accuracy_policy_4: 0.41777
	loss_value_4: 0.05868
	loss_reward_4: 0.00948
	loss_policy_5: 0.06488
	accuracy_policy_5: 0.41363
	loss_value_5: 0.0599
	loss_reward_5: 0.01108
	loss_policy: 0.64218
	loss_value: 0.54808
	loss_reward: 0.0422
[2025-05-08 05:10:38] nn step 3200, lr: 0.1.
	loss_policy_0: 0.32018
	accuracy_policy_0: 0.42141
	loss_value_0: 0.25724
	loss_policy_1: 0.06427
	accuracy_policy_1: 0.41355
	loss_value_1: 0.05317
	loss_reward_1: 0.00468
	loss_policy_2: 0.0644
	accuracy_policy_2: 0.41617
	loss_value_2: 0.05484
	loss_reward_2: 0.00802
	loss_policy_3: 0.06448
	accuracy_policy_3: 0.41109
	loss_value_3: 0.05628
	loss_reward_3: 0.00862
	loss_policy_4: 0.06458
	accuracy_policy_4: 0.41297
	loss_value_4: 0.05774
	loss_reward_4: 0.00967
	loss_policy_5: 0.06475
	accuracy_policy_5: 0.40957
	loss_value_5: 0.05905
	loss_reward_5: 0.01102
	loss_policy: 0.64265
	loss_value: 0.53833
	loss_reward: 0.04201
Optimization_Done 3200
[2025-05-08 05:11:47] [command] train weight_iter_3200.pkl 1 17
[2025-05-08 05:11:55] nn step 3250, lr: 0.1.
	loss_policy_0: 0.31734
	accuracy_policy_0: 0.45465
	loss_value_0: 0.27111
	loss_policy_1: 0.06357
	accuracy_policy_1: 0.45449
	loss_value_1: 0.05597
	loss_reward_1: 0.00461
	loss_policy_2: 0.06362
	accuracy_policy_2: 0.4532
	loss_value_2: 0.05739
	loss_reward_2: 0.00794
	loss_policy_3: 0.06363
	accuracy_policy_3: 0.4527
	loss_value_3: 0.05863
	loss_reward_3: 0.00846
	loss_policy_4: 0.06385
	accuracy_policy_4: 0.44809
	loss_value_4: 0.06047
	loss_reward_4: 0.00964
	loss_policy_5: 0.06402
	accuracy_policy_5: 0.44574
	loss_value_5: 0.06176
	loss_reward_5: 0.01105
	loss_policy: 0.63603
	loss_value: 0.56534
	loss_reward: 0.04169
[2025-05-08 05:12:03] nn step 3300, lr: 0.1.
	loss_policy_0: 0.31053
	accuracy_policy_0: 0.4518
	loss_value_0: 0.25929
	loss_policy_1: 0.06213
	accuracy_policy_1: 0.45117
	loss_value_1: 0.05367
	loss_reward_1: 0.0046
	loss_policy_2: 0.06235
	accuracy_policy_2: 0.4475
	loss_value_2: 0.05553
	loss_reward_2: 0.00741
	loss_policy_3: 0.0626
	accuracy_policy_3: 0.44277
	loss_value_3: 0.05685
	loss_reward_3: 0.00817
	loss_policy_4: 0.06243
	accuracy_policy_4: 0.44496
	loss_value_4: 0.05802
	loss_reward_4: 0.00943
	loss_policy_5: 0.06261
	accuracy_policy_5: 0.44457
	loss_value_5: 0.05935
	loss_reward_5: 0.01089
	loss_policy: 0.62265
	loss_value: 0.54272
	loss_reward: 0.04049
[2025-05-08 05:12:11] nn step 3350, lr: 0.1.
	loss_policy_0: 0.3072
	accuracy_policy_0: 0.45465
	loss_value_0: 0.25863
	loss_policy_1: 0.06156
	accuracy_policy_1: 0.45102
	loss_value_1: 0.05368
	loss_reward_1: 0.00446
	loss_policy_2: 0.06163
	accuracy_policy_2: 0.44598
	loss_value_2: 0.05474
	loss_reward_2: 0.00751
	loss_policy_3: 0.06164
	accuracy_policy_3: 0.44215
	loss_value_3: 0.0562
	loss_reward_3: 0.0082
	loss_policy_4: 0.0617
	accuracy_policy_4: 0.4473
	loss_value_4: 0.05785
	loss_reward_4: 0.00939
	loss_policy_5: 0.06212
	accuracy_policy_5: 0.44527
	loss_value_5: 0.05916
	loss_reward_5: 0.01067
	loss_policy: 0.61585
	loss_value: 0.54026
	loss_reward: 0.04023
[2025-05-08 05:12:18] nn step 3400, lr: 0.1.
	loss_policy_0: 0.30974
	accuracy_policy_0: 0.4493
	loss_value_0: 0.25738
	loss_policy_1: 0.06192
	accuracy_policy_1: 0.44086
	loss_value_1: 0.05342
	loss_reward_1: 0.00442
	loss_policy_2: 0.06188
	accuracy_policy_2: 0.44023
	loss_value_2: 0.05484
	loss_reward_2: 0.00741
	loss_policy_3: 0.06216
	accuracy_policy_3: 0.43656
	loss_value_3: 0.05617
	loss_reward_3: 0.00825
	loss_policy_4: 0.06204
	accuracy_policy_4: 0.43945
	loss_value_4: 0.05762
	loss_reward_4: 0.00894
	loss_policy_5: 0.0623
	accuracy_policy_5: 0.43605
	loss_value_5: 0.059
	loss_reward_5: 0.01068
	loss_policy: 0.62004
	loss_value: 0.53844
	loss_reward: 0.03971
Optimization_Done 3400
[2025-05-08 05:13:30] [command] train weight_iter_3400.pkl 1 18
[2025-05-08 05:13:39] nn step 3450, lr: 0.1.
	loss_policy_0: 0.33162
	accuracy_policy_0: 0.45051
	loss_value_0: 0.28175
	loss_policy_1: 0.06658
	accuracy_policy_1: 0.44488
	loss_value_1: 0.05829
	loss_reward_1: 0.00499
	loss_policy_2: 0.0666
	accuracy_policy_2: 0.4434
	loss_value_2: 0.05986
	loss_reward_2: 0.00794
	loss_policy_3: 0.06683
	accuracy_policy_3: 0.43914
	loss_value_3: 0.06181
	loss_reward_3: 0.00845
	loss_policy_4: 0.06701
	accuracy_policy_4: 0.43805
	loss_value_4: 0.06317
	loss_reward_4: 0.01002
	loss_policy_5: 0.06715
	accuracy_policy_5: 0.43633
	loss_value_5: 0.06466
	loss_reward_5: 0.01157
	loss_policy: 0.66579
	loss_value: 0.58954
	loss_reward: 0.04297
[2025-05-08 05:13:45] nn step 3500, lr: 0.1.
	loss_policy_0: 0.32662
	accuracy_policy_0: 0.44828
	loss_value_0: 0.27389
	loss_policy_1: 0.06535
	accuracy_policy_1: 0.44574
	loss_value_1: 0.05697
	loss_reward_1: 0.005
	loss_policy_2: 0.06549
	accuracy_policy_2: 0.44277
	loss_value_2: 0.05891
	loss_reward_2: 0.00789
	loss_policy_3: 0.06578
	accuracy_policy_3: 0.43797
	loss_value_3: 0.06048
	loss_reward_3: 0.00876
	loss_policy_4: 0.06578
	accuracy_policy_4: 0.43973
	loss_value_4: 0.06193
	loss_reward_4: 0.00995
	loss_policy_5: 0.06606
	accuracy_policy_5: 0.43633
	loss_value_5: 0.06328
	loss_reward_5: 0.01138
	loss_policy: 0.65507
	loss_value: 0.57546
	loss_reward: 0.04299
[2025-05-08 05:13:53] nn step 3550, lr: 0.1.
	loss_policy_0: 0.30037
	accuracy_policy_0: 0.44973
	loss_value_0: 0.25411
	loss_policy_1: 0.06039
	accuracy_policy_1: 0.44578
	loss_value_1: 0.05244
	loss_reward_1: 0.00425
	loss_policy_2: 0.06045
	accuracy_policy_2: 0.44195
	loss_value_2: 0.05388
	loss_reward_2: 0.00716
	loss_policy_3: 0.06023
	accuracy_policy_3: 0.44281
	loss_value_3: 0.05525
	loss_reward_3: 0.0079
	loss_policy_4: 0.06065
	accuracy_policy_4: 0.43945
	loss_value_4: 0.05671
	loss_reward_4: 0.00889
	loss_policy_5: 0.0609
	accuracy_policy_5: 0.43375
	loss_value_5: 0.05813
	loss_reward_5: 0.01021
	loss_policy: 0.60299
	loss_value: 0.53054
	loss_reward: 0.03841
[2025-05-08 05:14:01] nn step 3600, lr: 0.1.
	loss_policy_0: 0.31344
	accuracy_policy_0: 0.44219
	loss_value_0: 0.26181
	loss_policy_1: 0.06269
	accuracy_policy_1: 0.43832
	loss_value_1: 0.05371
	loss_reward_1: 0.00419
	loss_policy_2: 0.06264
	accuracy_policy_2: 0.43555
	loss_value_2: 0.05512
	loss_reward_2: 0.00741
	loss_policy_3: 0.06294
	accuracy_policy_3: 0.43379
	loss_value_3: 0.05653
	loss_reward_3: 0.00833
	loss_policy_4: 0.063
	accuracy_policy_4: 0.43281
	loss_value_4: 0.05806
	loss_reward_4: 0.00975
	loss_policy_5: 0.06336
	accuracy_policy_5: 0.42805
	loss_value_5: 0.05955
	loss_reward_5: 0.01069
	loss_policy: 0.62806
	loss_value: 0.54479
	loss_reward: 0.04037
Optimization_Done 3600
[2025-05-08 05:15:05] [command] train weight_iter_3600.pkl 1 19
[2025-05-08 05:15:14] nn step 3650, lr: 0.1.
	loss_policy_0: 0.30873
	accuracy_policy_0: 0.46215
	loss_value_0: 0.26647
	loss_policy_1: 0.06203
	accuracy_policy_1: 0.45617
	loss_value_1: 0.05529
	loss_reward_1: 0.00436
	loss_policy_2: 0.06182
	accuracy_policy_2: 0.45738
	loss_value_2: 0.05685
	loss_reward_2: 0.00729
	loss_policy_3: 0.06195
	accuracy_policy_3: 0.45652
	loss_value_3: 0.05817
	loss_reward_3: 0.00836
	loss_policy_4: 0.06213
	accuracy_policy_4: 0.45668
	loss_value_4: 0.05971
	loss_reward_4: 0.00941
	loss_policy_5: 0.06247
	accuracy_policy_5: 0.45168
	loss_value_5: 0.06071
	loss_reward_5: 0.01077
	loss_policy: 0.61914
	loss_value: 0.55719
	loss_reward: 0.04019
[2025-05-08 05:15:22] nn step 3700, lr: 0.1.
	loss_policy_0: 0.30118
	accuracy_policy_0: 0.45984
	loss_value_0: 0.25686
	loss_policy_1: 0.06034
	accuracy_policy_1: 0.45297
	loss_value_1: 0.05308
	loss_reward_1: 0.00413
	loss_policy_2: 0.06024
	accuracy_policy_2: 0.45105
	loss_value_2: 0.0544
	loss_reward_2: 0.00709
	loss_policy_3: 0.06031
	accuracy_policy_3: 0.44887
	loss_value_3: 0.05604
	loss_reward_3: 0.00772
	loss_policy_4: 0.06066
	accuracy_policy_4: 0.45051
	loss_value_4: 0.05766
	loss_reward_4: 0.00881
	loss_policy_5: 0.06088
	accuracy_policy_5: 0.44781
	loss_value_5: 0.05859
	loss_reward_5: 0.01049
	loss_policy: 0.6036
	loss_value: 0.53663
	loss_reward: 0.03824
[2025-05-08 05:15:30] nn step 3750, lr: 0.1.
	loss_policy_0: 0.30547
	accuracy_policy_0: 0.45777
	loss_value_0: 0.26245
	loss_policy_1: 0.06135
	accuracy_policy_1: 0.45031
	loss_value_1: 0.0542
	loss_reward_1: 0.00421
	loss_policy_2: 0.06134
	accuracy_policy_2: 0.45324
	loss_value_2: 0.05561
	loss_reward_2: 0.00723
	loss_policy_3: 0.06146
	accuracy_policy_3: 0.4493
	loss_value_3: 0.05705
	loss_reward_3: 0.00811
	loss_policy_4: 0.06164
	accuracy_policy_4: 0.44777
	loss_value_4: 0.05843
	loss_reward_4: 0.00924
	loss_policy_5: 0.06179
	accuracy_policy_5: 0.44629
	loss_value_5: 0.05956
	loss_reward_5: 0.01088
	loss_policy: 0.61306
	loss_value: 0.5473
	loss_reward: 0.03967
[2025-05-08 05:15:37] nn step 3800, lr: 0.1.
	loss_policy_0: 0.28687
	accuracy_policy_0: 0.46102
	loss_value_0: 0.24783
	loss_policy_1: 0.0575
	accuracy_policy_1: 0.45285
	loss_value_1: 0.05112
	loss_reward_1: 0.00403
	loss_policy_2: 0.0575
	accuracy_policy_2: 0.45367
	loss_value_2: 0.05253
	loss_reward_2: 0.00683
	loss_policy_3: 0.05784
	accuracy_policy_3: 0.45008
	loss_value_3: 0.05372
	loss_reward_3: 0.00756
	loss_policy_4: 0.05786
	accuracy_policy_4: 0.44723
	loss_value_4: 0.05503
	loss_reward_4: 0.00853
	loss_policy_5: 0.05819
	accuracy_policy_5: 0.44465
	loss_value_5: 0.05602
	loss_reward_5: 0.01009
	loss_policy: 0.57577
	loss_value: 0.51625
	loss_reward: 0.03704
Optimization_Done 3800
[2025-05-08 05:16:45] [command] train weight_iter_3800.pkl 1 20
[2025-05-08 05:16:55] nn step 3850, lr: 0.1.
	loss_policy_0: 0.28597
	accuracy_policy_0: 0.47762
	loss_value_0: 0.25197
	loss_policy_1: 0.05736
	accuracy_policy_1: 0.47703
	loss_value_1: 0.05177
	loss_reward_1: 0.00395
	loss_policy_2: 0.05726
	accuracy_policy_2: 0.47363
	loss_value_2: 0.05328
	loss_reward_2: 0.00652
	loss_policy_3: 0.05741
	accuracy_policy_3: 0.47156
	loss_value_3: 0.05431
	loss_reward_3: 0.00746
	loss_policy_4: 0.05751
	accuracy_policy_4: 0.46984
	loss_value_4: 0.05583
	loss_reward_4: 0.00845
	loss_policy_5: 0.05778
	accuracy_policy_5: 0.46957
	loss_value_5: 0.057
	loss_reward_5: 0.00956
	loss_policy: 0.57329
	loss_value: 0.52415
	loss_reward: 0.03593
[2025-05-08 05:17:01] nn step 3900, lr: 0.1.
	loss_policy_0: 0.26506
	accuracy_policy_0: 0.47438
	loss_value_0: 0.23426
	loss_policy_1: 0.05318
	accuracy_policy_1: 0.47203
	loss_value_1: 0.04844
	loss_reward_1: 0.00371
	loss_policy_2: 0.05325
	accuracy_policy_2: 0.46926
	loss_value_2: 0.04966
	loss_reward_2: 0.00643
	loss_policy_3: 0.05328
	accuracy_policy_3: 0.47047
	loss_value_3: 0.05121
	loss_reward_3: 0.00686
	loss_policy_4: 0.05333
	accuracy_policy_4: 0.46816
	loss_value_4: 0.05241
	loss_reward_4: 0.00775
	loss_policy_5: 0.05357
	accuracy_policy_5: 0.46445
	loss_value_5: 0.05361
	loss_reward_5: 0.00922
	loss_policy: 0.53167
	loss_value: 0.48958
	loss_reward: 0.03397
[2025-05-08 05:17:09] nn step 3950, lr: 0.1.
	loss_policy_0: 0.28598
	accuracy_policy_0: 0.47453
	loss_value_0: 0.24964
	loss_policy_1: 0.05755
	accuracy_policy_1: 0.46816
	loss_value_1: 0.05147
	loss_reward_1: 0.00396
	loss_policy_2: 0.05744
	accuracy_policy_2: 0.46707
	loss_value_2: 0.05285
	loss_reward_2: 0.0067
	loss_policy_3: 0.05721
	accuracy_policy_3: 0.46926
	loss_value_3: 0.05425
	loss_reward_3: 0.00738
	loss_policy_4: 0.05754
	accuracy_policy_4: 0.46441
	loss_value_4: 0.0556
	loss_reward_4: 0.00842
	loss_policy_5: 0.05793
	accuracy_policy_5: 0.46379
	loss_value_5: 0.05695
	loss_reward_5: 0.00997
	loss_policy: 0.57366
	loss_value: 0.52076
	loss_reward: 0.03643
[2025-05-08 05:17:17] nn step 4000, lr: 0.1.
	loss_policy_0: 0.2966
	accuracy_policy_0: 0.47258
	loss_value_0: 0.25717
	loss_policy_1: 0.05948
	accuracy_policy_1: 0.46469
	loss_value_1: 0.05313
	loss_reward_1: 0.00409
	loss_policy_2: 0.05955
	accuracy_policy_2: 0.46367
	loss_value_2: 0.05484
	loss_reward_2: 0.0068
	loss_policy_3: 0.05964
	accuracy_policy_3: 0.46234
	loss_value_3: 0.05608
	loss_reward_3: 0.00789
	loss_policy_4: 0.05965
	accuracy_policy_4: 0.46176
	loss_value_4: 0.05722
	loss_reward_4: 0.00903
	loss_policy_5: 0.06018
	accuracy_policy_5: 0.45586
	loss_value_5: 0.05842
	loss_reward_5: 0.01045
	loss_policy: 0.59509
	loss_value: 0.53685
	loss_reward: 0.03826
Optimization_Done 4000
[2025-05-08 05:18:25] [command] train weight_iter_4000.pkl 2 21
[2025-05-08 05:18:34] nn step 4050, lr: 0.1.
	loss_policy_0: 0.28734
	accuracy_policy_0: 0.49867
	loss_value_0: 0.26195
	loss_policy_1: 0.05761
	accuracy_policy_1: 0.49418
	loss_value_1: 0.0539
	loss_reward_1: 0.00403
	loss_policy_2: 0.05748
	accuracy_policy_2: 0.49414
	loss_value_2: 0.05532
	loss_reward_2: 0.00699
	loss_policy_3: 0.05775
	accuracy_policy_3: 0.49406
	loss_value_3: 0.05676
	loss_reward_3: 0.00767
	loss_policy_4: 0.05795
	accuracy_policy_4: 0.49082
	loss_value_4: 0.05806
	loss_reward_4: 0.00824
	loss_policy_5: 0.05822
	accuracy_policy_5: 0.49008
	loss_value_5: 0.0594
	loss_reward_5: 0.01017
	loss_policy: 0.57634
	loss_value: 0.54539
	loss_reward: 0.03711
[2025-05-08 05:18:42] nn step 4100, lr: 0.1.
	loss_policy_0: 0.28186
	accuracy_policy_0: 0.50074
	loss_value_0: 0.2574
	loss_policy_1: 0.05652
	accuracy_policy_1: 0.495
	loss_value_1: 0.05263
	loss_reward_1: 0.00383
	loss_policy_2: 0.0566
	accuracy_policy_2: 0.49359
	loss_value_2: 0.05395
	loss_reward_2: 0.00639
	loss_policy_3: 0.05672
	accuracy_policy_3: 0.48867
	loss_value_3: 0.05514
	loss_reward_3: 0.00724
	loss_policy_4: 0.05698
	accuracy_policy_4: 0.49016
	loss_value_4: 0.05643
	loss_reward_4: 0.00813
	loss_policy_5: 0.05717
	accuracy_policy_5: 0.48328
	loss_value_5: 0.05767
	loss_reward_5: 0.00968
	loss_policy: 0.56586
	loss_value: 0.53323
	loss_reward: 0.03527
[2025-05-08 05:18:50] nn step 4150, lr: 0.1.
	loss_policy_0: 0.31909
	accuracy_policy_0: 0.49703
	loss_value_0: 0.29231
	loss_policy_1: 0.06425
	accuracy_policy_1: 0.4927
	loss_value_1: 0.06006
	loss_reward_1: 0.00427
	loss_policy_2: 0.06425
	accuracy_policy_2: 0.49055
	loss_value_2: 0.06173
	loss_reward_2: 0.00755
	loss_policy_3: 0.06414
	accuracy_policy_3: 0.49258
	loss_value_3: 0.06315
	loss_reward_3: 0.00811
	loss_policy_4: 0.06461
	accuracy_policy_4: 0.49109
	loss_value_4: 0.0642
	loss_reward_4: 0.00955
	loss_policy_5: 0.0652
	accuracy_policy_5: 0.48527
	loss_value_5: 0.06562
	loss_reward_5: 0.01108
	loss_policy: 0.64155
	loss_value: 0.60707
	loss_reward: 0.04057
[2025-05-08 05:18:57] nn step 4200, lr: 0.1.
	loss_policy_0: 0.27403
	accuracy_policy_0: 0.49398
	loss_value_0: 0.24536
	loss_policy_1: 0.05497
	accuracy_policy_1: 0.49043
	loss_value_1: 0.05031
	loss_reward_1: 0.00349
	loss_policy_2: 0.05494
	accuracy_policy_2: 0.49215
	loss_value_2: 0.05157
	loss_reward_2: 0.00606
	loss_policy_3: 0.05506
	accuracy_policy_3: 0.48625
	loss_value_3: 0.05324
	loss_reward_3: 0.00697
	loss_policy_4: 0.05496
	accuracy_policy_4: 0.49062
	loss_value_4: 0.05416
	loss_reward_4: 0.00822
	loss_policy_5: 0.05556
	accuracy_policy_5: 0.47758
	loss_value_5: 0.05532
	loss_reward_5: 0.00953
	loss_policy: 0.54952
	loss_value: 0.50996
	loss_reward: 0.03428
Optimization_Done 4200
[2025-05-08 05:20:05] [command] train weight_iter_4200.pkl 3 22
[2025-05-08 05:20:14] nn step 4250, lr: 0.1.
	loss_policy_0: 0.26692
	accuracy_policy_0: 0.52289
	loss_value_0: 0.25325
	loss_policy_1: 0.05351
	accuracy_policy_1: 0.52262
	loss_value_1: 0.05207
	loss_reward_1: 0.00357
	loss_policy_2: 0.05339
	accuracy_policy_2: 0.52199
	loss_value_2: 0.05333
	loss_reward_2: 0.00613
	loss_policy_3: 0.05343
	accuracy_policy_3: 0.52223
	loss_value_3: 0.05483
	loss_reward_3: 0.00703
	loss_policy_4: 0.05373
	accuracy_policy_4: 0.51859
	loss_value_4: 0.0562
	loss_reward_4: 0.00787
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.51746
	loss_value_5: 0.05706
	loss_reward_5: 0.00943
	loss_policy: 0.53472
	loss_value: 0.52674
	loss_reward: 0.03404
[2025-05-08 05:20:20] nn step 4300, lr: 0.1.
	loss_policy_0: 0.28261
	accuracy_policy_0: 0.5209
	loss_value_0: 0.26366
	loss_policy_1: 0.05628
	accuracy_policy_1: 0.51836
	loss_value_1: 0.05387
	loss_reward_1: 0.00367
	loss_policy_2: 0.05668
	accuracy_policy_2: 0.51238
	loss_value_2: 0.05541
	loss_reward_2: 0.00658
	loss_policy_3: 0.05679
	accuracy_policy_3: 0.51199
	loss_value_3: 0.05666
	loss_reward_3: 0.00747
	loss_policy_4: 0.05651
	accuracy_policy_4: 0.51367
	loss_value_4: 0.058
	loss_reward_4: 0.00833
	loss_policy_5: 0.05706
	accuracy_policy_5: 0.50816
	loss_value_5: 0.05908
	loss_reward_5: 0.00997
	loss_policy: 0.56593
	loss_value: 0.54669
	loss_reward: 0.03602
[2025-05-08 05:20:28] nn step 4350, lr: 0.1.
	loss_policy_0: 0.27273
	accuracy_policy_0: 0.52766
	loss_value_0: 0.25972
	loss_policy_1: 0.05463
	accuracy_policy_1: 0.52324
	loss_value_1: 0.05331
	loss_reward_1: 0.00381
	loss_policy_2: 0.05498
	accuracy_policy_2: 0.51859
	loss_value_2: 0.05477
	loss_reward_2: 0.00649
	loss_policy_3: 0.05488
	accuracy_policy_3: 0.51633
	loss_value_3: 0.05592
	loss_reward_3: 0.00727
	loss_policy_4: 0.05504
	accuracy_policy_4: 0.5173
	loss_value_4: 0.05726
	loss_reward_4: 0.00816
	loss_policy_5: 0.05555
	accuracy_policy_5: 0.50863
	loss_value_5: 0.05855
	loss_reward_5: 0.0096
	loss_policy: 0.54781
	loss_value: 0.53952
	loss_reward: 0.03532
[2025-05-08 05:20:36] nn step 4400, lr: 0.1.
	loss_policy_0: 0.2707
	accuracy_policy_0: 0.51676
	loss_value_0: 0.25347
	loss_policy_1: 0.05419
	accuracy_policy_1: 0.5091
	loss_value_1: 0.05188
	loss_reward_1: 0.00353
	loss_policy_2: 0.05408
	accuracy_policy_2: 0.51383
	loss_value_2: 0.05288
	loss_reward_2: 0.0064
	loss_policy_3: 0.05425
	accuracy_policy_3: 0.50758
	loss_value_3: 0.05427
	loss_reward_3: 0.00682
	loss_policy_4: 0.05435
	accuracy_policy_4: 0.5082
	loss_value_4: 0.05558
	loss_reward_4: 0.00795
	loss_policy_5: 0.05468
	accuracy_policy_5: 0.50352
	loss_value_5: 0.05692
	loss_reward_5: 0.00944
	loss_policy: 0.54225
	loss_value: 0.525
	loss_reward: 0.03414
Optimization_Done 4400
[2025-05-08 05:21:43] [command] train weight_iter_4400.pkl 4 23
[2025-05-08 05:21:52] nn step 4450, lr: 0.1.
	loss_policy_0: 0.26437
	accuracy_policy_0: 0.55102
	loss_value_0: 0.2675
	loss_policy_1: 0.05274
	accuracy_policy_1: 0.55117
	loss_value_1: 0.05484
	loss_reward_1: 0.00368
	loss_policy_2: 0.05312
	accuracy_policy_2: 0.54621
	loss_value_2: 0.05616
	loss_reward_2: 0.00652
	loss_policy_3: 0.05305
	accuracy_policy_3: 0.54789
	loss_value_3: 0.05721
	loss_reward_3: 0.00749
	loss_policy_4: 0.05314
	accuracy_policy_4: 0.54516
	loss_value_4: 0.05859
	loss_reward_4: 0.0081
	loss_policy_5: 0.05352
	accuracy_policy_5: 0.54312
	loss_value_5: 0.05979
	loss_reward_5: 0.00972
	loss_policy: 0.52994
	loss_value: 0.55409
	loss_reward: 0.03551
[2025-05-08 05:22:00] nn step 4500, lr: 0.1.
	loss_policy_0: 0.24009
	accuracy_policy_0: 0.55527
	loss_value_0: 0.24106
	loss_policy_1: 0.04844
	accuracy_policy_1: 0.54547
	loss_value_1: 0.04941
	loss_reward_1: 0.00302
	loss_policy_2: 0.04852
	accuracy_policy_2: 0.54508
	loss_value_2: 0.0506
	loss_reward_2: 0.00567
	loss_policy_3: 0.04851
	accuracy_policy_3: 0.54668
	loss_value_3: 0.05202
	loss_reward_3: 0.00626
	loss_policy_4: 0.04862
	accuracy_policy_4: 0.54375
	loss_value_4: 0.05314
	loss_reward_4: 0.0073
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.53398
	loss_value_5: 0.05413
	loss_reward_5: 0.00858
	loss_policy: 0.48345
	loss_value: 0.50036
	loss_reward: 0.03083
[2025-05-08 05:22:08] nn step 4550, lr: 0.1.
	loss_policy_0: 0.2606
	accuracy_policy_0: 0.54984
	loss_value_0: 0.25788
	loss_policy_1: 0.05215
	accuracy_policy_1: 0.54727
	loss_value_1: 0.05305
	loss_reward_1: 0.00359
	loss_policy_2: 0.05205
	accuracy_policy_2: 0.55055
	loss_value_2: 0.05438
	loss_reward_2: 0.00599
	loss_policy_3: 0.05228
	accuracy_policy_3: 0.54605
	loss_value_3: 0.0557
	loss_reward_3: 0.00675
	loss_policy_4: 0.05259
	accuracy_policy_4: 0.54102
	loss_value_4: 0.05696
	loss_reward_4: 0.00788
	loss_policy_5: 0.05278
	accuracy_policy_5: 0.53977
	loss_value_5: 0.058
	loss_reward_5: 0.00946
	loss_policy: 0.52245
	loss_value: 0.53597
	loss_reward: 0.03366
[2025-05-08 05:22:14] nn step 4600, lr: 0.1.
	loss_policy_0: 0.26269
	accuracy_policy_0: 0.54992
	loss_value_0: 0.26344
	loss_policy_1: 0.05279
	accuracy_policy_1: 0.55039
	loss_value_1: 0.0542
	loss_reward_1: 0.0034
	loss_policy_2: 0.05301
	accuracy_policy_2: 0.54496
	loss_value_2: 0.05532
	loss_reward_2: 0.00631
	loss_policy_3: 0.05333
	accuracy_policy_3: 0.5423
	loss_value_3: 0.05648
	loss_reward_3: 0.00675
	loss_policy_4: 0.05314
	accuracy_policy_4: 0.54602
	loss_value_4: 0.0578
	loss_reward_4: 0.00778
	loss_policy_5: 0.05351
	accuracy_policy_5: 0.5402
	loss_value_5: 0.05927
	loss_reward_5: 0.0093
	loss_policy: 0.52846
	loss_value: 0.54651
	loss_reward: 0.03354
Optimization_Done 4600
[2025-05-08 05:23:22] [command] train weight_iter_4600.pkl 5 24
[2025-05-08 05:23:30] nn step 4650, lr: 0.1.
	loss_policy_0: 0.22509
	accuracy_policy_0: 0.59023
	loss_value_0: 0.24479
	loss_policy_1: 0.04513
	accuracy_policy_1: 0.58395
	loss_value_1: 0.05047
	loss_reward_1: 0.00306
	loss_policy_2: 0.04544
	accuracy_policy_2: 0.58102
	loss_value_2: 0.0516
	loss_reward_2: 0.00546
	loss_policy_3: 0.04555
	accuracy_policy_3: 0.58094
	loss_value_3: 0.05268
	loss_reward_3: 0.00647
	loss_policy_4: 0.04553
	accuracy_policy_4: 0.58125
	loss_value_4: 0.05388
	loss_reward_4: 0.0071
	loss_policy_5: 0.04606
	accuracy_policy_5: 0.57465
	loss_value_5: 0.05485
	loss_reward_5: 0.00823
	loss_policy: 0.45279
	loss_value: 0.50826
	loss_reward: 0.03032
[2025-05-08 05:23:37] nn step 4700, lr: 0.1.
	loss_policy_0: 0.22406
	accuracy_policy_0: 0.58477
	loss_value_0: 0.23819
	loss_policy_1: 0.04511
	accuracy_policy_1: 0.5741
	loss_value_1: 0.04887
	loss_reward_1: 0.00301
	loss_policy_2: 0.04526
	accuracy_policy_2: 0.57688
	loss_value_2: 0.0501
	loss_reward_2: 0.00513
	loss_policy_3: 0.04558
	accuracy_policy_3: 0.57148
	loss_value_3: 0.05126
	loss_reward_3: 0.00622
	loss_policy_4: 0.04552
	accuracy_policy_4: 0.57148
	loss_value_4: 0.052
	loss_reward_4: 0.00707
	loss_policy_5: 0.0456
	accuracy_policy_5: 0.56801
	loss_value_5: 0.05332
	loss_reward_5: 0.00796
	loss_policy: 0.45115
	loss_value: 0.49373
	loss_reward: 0.02939
[2025-05-08 05:23:45] nn step 4750, lr: 0.1.
	loss_policy_0: 0.24151
	accuracy_policy_0: 0.58312
	loss_value_0: 0.25941
	loss_policy_1: 0.04852
	accuracy_policy_1: 0.57984
	loss_value_1: 0.05326
	loss_reward_1: 0.00325
	loss_policy_2: 0.04868
	accuracy_policy_2: 0.57898
	loss_value_2: 0.05448
	loss_reward_2: 0.00575
	loss_policy_3: 0.04878
	accuracy_policy_3: 0.57617
	loss_value_3: 0.05596
	loss_reward_3: 0.00667
	loss_policy_4: 0.04898
	accuracy_policy_4: 0.57539
	loss_value_4: 0.05699
	loss_reward_4: 0.00726
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.56969
	loss_value_5: 0.05834
	loss_reward_5: 0.00908
	loss_policy: 0.48591
	loss_value: 0.53844
	loss_reward: 0.032
[2025-05-08 05:23:53] nn step 4800, lr: 0.1.
	loss_policy_0: 0.23325
	accuracy_policy_0: 0.57227
	loss_value_0: 0.24644
	loss_policy_1: 0.04681
	accuracy_policy_1: 0.5707
	loss_value_1: 0.05048
	loss_reward_1: 0.00304
	loss_policy_2: 0.04674
	accuracy_policy_2: 0.57008
	loss_value_2: 0.05176
	loss_reward_2: 0.00555
	loss_policy_3: 0.04687
	accuracy_policy_3: 0.56805
	loss_value_3: 0.05301
	loss_reward_3: 0.0063
	loss_policy_4: 0.0472
	accuracy_policy_4: 0.56441
	loss_value_4: 0.054
	loss_reward_4: 0.00723
	loss_policy_5: 0.04752
	accuracy_policy_5: 0.55883
	loss_value_5: 0.05509
	loss_reward_5: 0.00865
	loss_policy: 0.46839
	loss_value: 0.51078
	loss_reward: 0.03078
Optimization_Done 4800
[2025-05-08 05:24:59] [command] train weight_iter_4800.pkl 6 25
[2025-05-08 05:25:08] nn step 4850, lr: 0.1.
	loss_policy_0: 0.20392
	accuracy_policy_0: 0.61559
	loss_value_0: 0.23575
	loss_policy_1: 0.0411
	accuracy_policy_1: 0.61121
	loss_value_1: 0.04828
	loss_reward_1: 0.00277
	loss_policy_2: 0.04107
	accuracy_policy_2: 0.60906
	loss_value_2: 0.04942
	loss_reward_2: 0.00519
	loss_policy_3: 0.04124
	accuracy_policy_3: 0.61023
	loss_value_3: 0.05067
	loss_reward_3: 0.00569
	loss_policy_4: 0.04143
	accuracy_policy_4: 0.6098
	loss_value_4: 0.05179
	loss_reward_4: 0.0067
	loss_policy_5: 0.04155
	accuracy_policy_5: 0.60324
	loss_value_5: 0.05271
	loss_reward_5: 0.00773
	loss_policy: 0.41032
	loss_value: 0.48863
	loss_reward: 0.02809
[2025-05-08 05:25:16] nn step 4900, lr: 0.1.
	loss_policy_0: 0.21351
	accuracy_policy_0: 0.61305
	loss_value_0: 0.24563
	loss_policy_1: 0.04293
	accuracy_policy_1: 0.60574
	loss_value_1: 0.05043
	loss_reward_1: 0.00285
	loss_policy_2: 0.0432
	accuracy_policy_2: 0.60715
	loss_value_2: 0.05153
	loss_reward_2: 0.00553
	loss_policy_3: 0.04314
	accuracy_policy_3: 0.60508
	loss_value_3: 0.05268
	loss_reward_3: 0.0063
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.60492
	loss_value_4: 0.05364
	loss_reward_4: 0.0067
	loss_policy_5: 0.04356
	accuracy_policy_5: 0.59598
	loss_value_5: 0.05497
	loss_reward_5: 0.00781
	loss_policy: 0.42966
	loss_value: 0.50889
	loss_reward: 0.02918
[2025-05-08 05:25:24] nn step 4950, lr: 0.1.
	loss_policy_0: 0.22194
	accuracy_policy_0: 0.61238
	loss_value_0: 0.25438
	loss_policy_1: 0.04456
	accuracy_policy_1: 0.60363
	loss_value_1: 0.05234
	loss_reward_1: 0.00304
	loss_policy_2: 0.04501
	accuracy_policy_2: 0.60199
	loss_value_2: 0.0533
	loss_reward_2: 0.00566
	loss_policy_3: 0.04481
	accuracy_policy_3: 0.6002
	loss_value_3: 0.05442
	loss_reward_3: 0.00634
	loss_policy_4: 0.04488
	accuracy_policy_4: 0.6009
	loss_value_4: 0.05542
	loss_reward_4: 0.00745
	loss_policy_5: 0.04536
	accuracy_policy_5: 0.5941
	loss_value_5: 0.05686
	loss_reward_5: 0.00851
	loss_policy: 0.44655
	loss_value: 0.52672
	loss_reward: 0.031
[2025-05-08 05:25:30] nn step 5000, lr: 0.1.
	loss_policy_0: 0.21349
	accuracy_policy_0: 0.60695
	loss_value_0: 0.24505
	loss_policy_1: 0.04302
	accuracy_policy_1: 0.60324
	loss_value_1: 0.05048
	loss_reward_1: 0.00288
	loss_policy_2: 0.04308
	accuracy_policy_2: 0.60297
	loss_value_2: 0.05146
	loss_reward_2: 0.00542
	loss_policy_3: 0.04316
	accuracy_policy_3: 0.60113
	loss_value_3: 0.05248
	loss_reward_3: 0.00591
	loss_policy_4: 0.04335
	accuracy_policy_4: 0.60004
	loss_value_4: 0.0538
	loss_reward_4: 0.00692
	loss_policy_5: 0.04361
	accuracy_policy_5: 0.59688
	loss_value_5: 0.05497
	loss_reward_5: 0.00824
	loss_policy: 0.4297
	loss_value: 0.50825
	loss_reward: 0.02937
Optimization_Done 5000
[2025-05-08 05:26:41] [command] train weight_iter_5000.pkl 7 26
[2025-05-08 05:26:49] nn step 5050, lr: 0.1.
	loss_policy_0: 0.18376
	accuracy_policy_0: 0.65594
	loss_value_0: 0.2341
	loss_policy_1: 0.0371
	accuracy_policy_1: 0.64945
	loss_value_1: 0.04796
	loss_reward_1: 0.00261
	loss_policy_2: 0.03721
	accuracy_policy_2: 0.64969
	loss_value_2: 0.0492
	loss_reward_2: 0.00475
	loss_policy_3: 0.03724
	accuracy_policy_3: 0.64773
	loss_value_3: 0.0503
	loss_reward_3: 0.00561
	loss_policy_4: 0.03752
	accuracy_policy_4: 0.64566
	loss_value_4: 0.05135
	loss_reward_4: 0.00615
	loss_policy_5: 0.03775
	accuracy_policy_5: 0.64129
	loss_value_5: 0.0525
	loss_reward_5: 0.00759
	loss_policy: 0.37059
	loss_value: 0.48541
	loss_reward: 0.0267
[2025-05-08 05:26:56] nn step 5100, lr: 0.1.
	loss_policy_0: 0.19842
	accuracy_policy_0: 0.64711
	loss_value_0: 0.25219
	loss_policy_1: 0.04009
	accuracy_policy_1: 0.64016
	loss_value_1: 0.05151
	loss_reward_1: 0.00289
	loss_policy_2: 0.04008
	accuracy_policy_2: 0.63926
	loss_value_2: 0.05281
	loss_reward_2: 0.00534
	loss_policy_3: 0.04002
	accuracy_policy_3: 0.63996
	loss_value_3: 0.05413
	loss_reward_3: 0.00592
	loss_policy_4: 0.0405
	accuracy_policy_4: 0.63961
	loss_value_4: 0.05559
	loss_reward_4: 0.00663
	loss_policy_5: 0.0408
	accuracy_policy_5: 0.63145
	loss_value_5: 0.05669
	loss_reward_5: 0.00821
	loss_policy: 0.39991
	loss_value: 0.52293
	loss_reward: 0.02899
[2025-05-08 05:27:04] nn step 5150, lr: 0.1.
	loss_policy_0: 0.18034
	accuracy_policy_0: 0.65207
	loss_value_0: 0.23109
	loss_policy_1: 0.03652
	accuracy_policy_1: 0.64418
	loss_value_1: 0.04722
	loss_reward_1: 0.00259
	loss_policy_2: 0.0367
	accuracy_policy_2: 0.64285
	loss_value_2: 0.04812
	loss_reward_2: 0.00478
	loss_policy_3: 0.03648
	accuracy_policy_3: 0.64352
	loss_value_3: 0.04902
	loss_reward_3: 0.00548
	loss_policy_4: 0.03671
	accuracy_policy_4: 0.64148
	loss_value_4: 0.05024
	loss_reward_4: 0.00593
	loss_policy_5: 0.03702
	accuracy_policy_5: 0.63633
	loss_value_5: 0.05156
	loss_reward_5: 0.0074
	loss_policy: 0.36377
	loss_value: 0.47724
	loss_reward: 0.02618
[2025-05-08 05:27:12] nn step 5200, lr: 0.1.
	loss_policy_0: 0.19037
	accuracy_policy_0: 0.64664
	loss_value_0: 0.23738
	loss_policy_1: 0.03831
	accuracy_policy_1: 0.63988
	loss_value_1: 0.0486
	loss_reward_1: 0.0026
	loss_policy_2: 0.03854
	accuracy_policy_2: 0.63855
	loss_value_2: 0.04979
	loss_reward_2: 0.00485
	loss_policy_3: 0.03867
	accuracy_policy_3: 0.63645
	loss_value_3: 0.05077
	loss_reward_3: 0.00573
	loss_policy_4: 0.03876
	accuracy_policy_4: 0.63555
	loss_value_4: 0.05197
	loss_reward_4: 0.00631
	loss_policy_5: 0.03914
	accuracy_policy_5: 0.63133
	loss_value_5: 0.05328
	loss_reward_5: 0.00756
	loss_policy: 0.3838
	loss_value: 0.49179
	loss_reward: 0.02706
Optimization_Done 5200
[2025-05-08 05:28:18] [command] train weight_iter_5200.pkl 8 27
[2025-05-08 05:28:26] nn step 5250, lr: 0.1.
	loss_policy_0: 0.156
	accuracy_policy_0: 0.69125
	loss_value_0: 0.22799
	loss_policy_1: 0.03171
	accuracy_policy_1: 0.68688
	loss_value_1: 0.04667
	loss_reward_1: 0.00237
	loss_policy_2: 0.03181
	accuracy_policy_2: 0.68316
	loss_value_2: 0.04738
	loss_reward_2: 0.00466
	loss_policy_3: 0.03179
	accuracy_policy_3: 0.68336
	loss_value_3: 0.04858
	loss_reward_3: 0.00504
	loss_policy_4: 0.03192
	accuracy_policy_4: 0.68152
	loss_value_4: 0.0496
	loss_reward_4: 0.00537
	loss_policy_5: 0.03205
	accuracy_policy_5: 0.67703
	loss_value_5: 0.05057
	loss_reward_5: 0.00668
	loss_policy: 0.31528
	loss_value: 0.47078
	loss_reward: 0.02411
[2025-05-08 05:28:34] nn step 5300, lr: 0.1.
	loss_policy_0: 0.15631
	accuracy_policy_0: 0.68879
	loss_value_0: 0.22959
	loss_policy_1: 0.03148
	accuracy_policy_1: 0.68238
	loss_value_1: 0.04666
	loss_reward_1: 0.00245
	loss_policy_2: 0.03177
	accuracy_policy_2: 0.68453
	loss_value_2: 0.04764
	loss_reward_2: 0.00446
	loss_policy_3: 0.03166
	accuracy_policy_3: 0.6825
	loss_value_3: 0.04846
	loss_reward_3: 0.00489
	loss_policy_4: 0.0322
	accuracy_policy_4: 0.67848
	loss_value_4: 0.04942
	loss_reward_4: 0.00554
	loss_policy_5: 0.03226
	accuracy_policy_5: 0.67762
	loss_value_5: 0.05078
	loss_reward_5: 0.00685
	loss_policy: 0.31568
	loss_value: 0.47255
	loss_reward: 0.02421
[2025-05-08 05:28:42] nn step 5350, lr: 0.1.
	loss_policy_0: 0.16709
	accuracy_policy_0: 0.68547
	loss_value_0: 0.23789
	loss_policy_1: 0.03383
	accuracy_policy_1: 0.67652
	loss_value_1: 0.04854
	loss_reward_1: 0.00264
	loss_policy_2: 0.03392
	accuracy_policy_2: 0.67875
	loss_value_2: 0.04947
	loss_reward_2: 0.00476
	loss_policy_3: 0.03409
	accuracy_policy_3: 0.67223
	loss_value_3: 0.05048
	loss_reward_3: 0.00507
	loss_policy_4: 0.03419
	accuracy_policy_4: 0.67203
	loss_value_4: 0.0514
	loss_reward_4: 0.00593
	loss_policy_5: 0.03423
	accuracy_policy_5: 0.67027
	loss_value_5: 0.05267
	loss_reward_5: 0.00748
	loss_policy: 0.33736
	loss_value: 0.49045
	loss_reward: 0.02587
[2025-05-08 05:28:48] nn step 5400, lr: 0.1.
	loss_policy_0: 0.15854
	accuracy_policy_0: 0.68676
	loss_value_0: 0.22798
	loss_policy_1: 0.032
	accuracy_policy_1: 0.68684
	loss_value_1: 0.04675
	loss_reward_1: 0.00246
	loss_policy_2: 0.03201
	accuracy_policy_2: 0.6816
	loss_value_2: 0.04755
	loss_reward_2: 0.00456
	loss_policy_3: 0.03233
	accuracy_policy_3: 0.67957
	loss_value_3: 0.04831
	loss_reward_3: 0.00512
	loss_policy_4: 0.03244
	accuracy_policy_4: 0.67801
	loss_value_4: 0.04934
	loss_reward_4: 0.00583
	loss_policy_5: 0.03265
	accuracy_policy_5: 0.67504
	loss_value_5: 0.05054
	loss_reward_5: 0.00719
	loss_policy: 0.31996
	loss_value: 0.47046
	loss_reward: 0.02515
Optimization_Done 5400
[2025-05-08 05:29:57] [command] train weight_iter_5400.pkl 9 28
[2025-05-08 05:30:06] nn step 5450, lr: 0.1.
	loss_policy_0: 0.13785
	accuracy_policy_0: 0.73453
	loss_value_0: 0.23438
	loss_policy_1: 0.02784
	accuracy_policy_1: 0.73094
	loss_value_1: 0.04816
	loss_reward_1: 0.00239
	loss_policy_2: 0.02801
	accuracy_policy_2: 0.72805
	loss_value_2: 0.04904
	loss_reward_2: 0.00439
	loss_policy_3: 0.02845
	accuracy_policy_3: 0.72773
	loss_value_3: 0.0499
	loss_reward_3: 0.00506
	loss_policy_4: 0.02842
	accuracy_policy_4: 0.72152
	loss_value_4: 0.05084
	loss_reward_4: 0.00554
	loss_policy_5: 0.02851
	accuracy_policy_5: 0.7216
	loss_value_5: 0.05196
	loss_reward_5: 0.00649
	loss_policy: 0.27908
	loss_value: 0.48428
	loss_reward: 0.02387
[2025-05-08 05:30:13] nn step 5500, lr: 0.1.
	loss_policy_0: 0.14549
	accuracy_policy_0: 0.73273
	loss_value_0: 0.24248
	loss_policy_1: 0.02932
	accuracy_policy_1: 0.72539
	loss_value_1: 0.04965
	loss_reward_1: 0.00258
	loss_policy_2: 0.02942
	accuracy_policy_2: 0.72652
	loss_value_2: 0.0507
	loss_reward_2: 0.00466
	loss_policy_3: 0.02955
	accuracy_policy_3: 0.72328
	loss_value_3: 0.0518
	loss_reward_3: 0.00504
	loss_policy_4: 0.03029
	accuracy_policy_4: 0.71801
	loss_value_4: 0.05273
	loss_reward_4: 0.00556
	loss_policy_5: 0.03012
	accuracy_policy_5: 0.71684
	loss_value_5: 0.05407
	loss_reward_5: 0.00697
	loss_policy: 0.2942
	loss_value: 0.50143
	loss_reward: 0.02481
[2025-05-08 05:30:21] nn step 5550, lr: 0.1.
	loss_policy_0: 0.14889
	accuracy_policy_0: 0.73031
	loss_value_0: 0.25058
	loss_policy_1: 0.02984
	accuracy_policy_1: 0.7291
	loss_value_1: 0.05091
	loss_reward_1: 0.00249
	loss_policy_2: 0.03027
	accuracy_policy_2: 0.72434
	loss_value_2: 0.05182
	loss_reward_2: 0.00465
	loss_policy_3: 0.03049
	accuracy_policy_3: 0.7207
	loss_value_3: 0.05297
	loss_reward_3: 0.00515
	loss_policy_4: 0.03069
	accuracy_policy_4: 0.71949
	loss_value_4: 0.05394
	loss_reward_4: 0.0056
	loss_policy_5: 0.0309
	accuracy_policy_5: 0.71852
	loss_value_5: 0.05546
	loss_reward_5: 0.00704
	loss_policy: 0.30107
	loss_value: 0.51568
	loss_reward: 0.02493
[2025-05-08 05:30:28] nn step 5600, lr: 0.1.
	loss_policy_0: 0.13647
	accuracy_policy_0: 0.73566
	loss_value_0: 0.22878
	loss_policy_1: 0.02781
	accuracy_policy_1: 0.72766
	loss_value_1: 0.04686
	loss_reward_1: 0.00231
	loss_policy_2: 0.02799
	accuracy_policy_2: 0.72441
	loss_value_2: 0.04767
	loss_reward_2: 0.0043
	loss_policy_3: 0.02837
	accuracy_policy_3: 0.72152
	loss_value_3: 0.04846
	loss_reward_3: 0.00472
	loss_policy_4: 0.02848
	accuracy_policy_4: 0.71855
	loss_value_4: 0.04924
	loss_reward_4: 0.0055
	loss_policy_5: 0.02862
	accuracy_policy_5: 0.71648
	loss_value_5: 0.05038
	loss_reward_5: 0.0067
	loss_policy: 0.27775
	loss_value: 0.47139
	loss_reward: 0.02352
Optimization_Done 5600
[2025-05-08 05:31:38] [command] train weight_iter_5600.pkl 10 29
[2025-05-08 05:31:47] nn step 5650, lr: 0.1.
	loss_policy_0: 0.10962
	accuracy_policy_0: 0.78383
	loss_value_0: 0.23283
	loss_policy_1: 0.02246
	accuracy_policy_1: 0.77664
	loss_value_1: 0.04766
	loss_reward_1: 0.00213
	loss_policy_2: 0.02282
	accuracy_policy_2: 0.77465
	loss_value_2: 0.04863
	loss_reward_2: 0.00384
	loss_policy_3: 0.023
	accuracy_policy_3: 0.77254
	loss_value_3: 0.04924
	loss_reward_3: 0.004
	loss_policy_4: 0.02304
	accuracy_policy_4: 0.77078
	loss_value_4: 0.05007
	loss_reward_4: 0.00471
	loss_policy_5: 0.02297
	accuracy_policy_5: 0.7702
	loss_value_5: 0.05142
	loss_reward_5: 0.00616
	loss_policy: 0.2239
	loss_value: 0.47986
	loss_reward: 0.02084
[2025-05-08 05:31:56] nn step 5700, lr: 0.1.
	loss_policy_0: 0.10682
	accuracy_policy_0: 0.78074
	loss_value_0: 0.22288
	loss_policy_1: 0.02193
	accuracy_policy_1: 0.7757
	loss_value_1: 0.04575
	loss_reward_1: 0.002
	loss_policy_2: 0.02196
	accuracy_policy_2: 0.77102
	loss_value_2: 0.0463
	loss_reward_2: 0.00391
	loss_policy_3: 0.02215
	accuracy_policy_3: 0.77137
	loss_value_3: 0.04693
	loss_reward_3: 0.00408
	loss_policy_4: 0.02237
	accuracy_policy_4: 0.76824
	loss_value_4: 0.04779
	loss_reward_4: 0.0044
	loss_policy_5: 0.02245
	accuracy_policy_5: 0.76699
	loss_value_5: 0.04904
	loss_reward_5: 0.00599
	loss_policy: 0.21767
	loss_value: 0.45869
	loss_reward: 0.02038
[2025-05-08 05:32:02] nn step 5750, lr: 0.1.
	loss_policy_0: 0.10483
	accuracy_policy_0: 0.77594
	loss_value_0: 0.21536
	loss_policy_1: 0.02142
	accuracy_policy_1: 0.77133
	loss_value_1: 0.04406
	loss_reward_1: 0.00215
	loss_policy_2: 0.02157
	accuracy_policy_2: 0.76914
	loss_value_2: 0.04466
	loss_reward_2: 0.00362
	loss_policy_3: 0.02175
	accuracy_policy_3: 0.76484
	loss_value_3: 0.0452
	loss_reward_3: 0.00387
	loss_policy_4: 0.02194
	accuracy_policy_4: 0.76625
	loss_value_4: 0.0459
	loss_reward_4: 0.00453
	loss_policy_5: 0.02205
	accuracy_policy_5: 0.76086
	loss_value_5: 0.04734
	loss_reward_5: 0.00566
	loss_policy: 0.21355
	loss_value: 0.44252
	loss_reward: 0.01982
[2025-05-08 05:32:10] nn step 5800, lr: 0.1.
	loss_policy_0: 0.10916
	accuracy_policy_0: 0.77938
	loss_value_0: 0.22551
	loss_policy_1: 0.02248
	accuracy_policy_1: 0.77078
	loss_value_1: 0.04612
	loss_reward_1: 0.00209
	loss_policy_2: 0.02279
	accuracy_policy_2: 0.76859
	loss_value_2: 0.04696
	loss_reward_2: 0.00377
	loss_policy_3: 0.02292
	accuracy_policy_3: 0.76684
	loss_value_3: 0.04772
	loss_reward_3: 0.00408
	loss_policy_4: 0.02298
	accuracy_policy_4: 0.7632
	loss_value_4: 0.04842
	loss_reward_4: 0.00464
	loss_policy_5: 0.02301
	accuracy_policy_5: 0.76207
	loss_value_5: 0.04992
	loss_reward_5: 0.00601
	loss_policy: 0.22334
	loss_value: 0.46464
	loss_reward: 0.02059
Optimization_Done 5800
[2025-05-08 05:33:18] [command] train weight_iter_5800.pkl 11 30
[2025-05-08 05:33:27] nn step 5850, lr: 0.1.
	loss_policy_0: 0.07797
	accuracy_policy_0: 0.83324
	loss_value_0: 0.21468
	loss_policy_1: 0.01587
	accuracy_policy_1: 0.82852
	loss_value_1: 0.04349
	loss_reward_1: 0.00188
	loss_policy_2: 0.01593
	accuracy_policy_2: 0.82934
	loss_value_2: 0.04449
	loss_reward_2: 0.00315
	loss_policy_3: 0.01615
	accuracy_policy_3: 0.82422
	loss_value_3: 0.04496
	loss_reward_3: 0.00351
	loss_policy_4: 0.01637
	accuracy_policy_4: 0.82184
	loss_value_4: 0.0456
	loss_reward_4: 0.0041
	loss_policy_5: 0.01618
	accuracy_policy_5: 0.82312
	loss_value_5: 0.04687
	loss_reward_5: 0.0053
	loss_policy: 0.15848
	loss_value: 0.44009
	loss_reward: 0.01794
[2025-05-08 05:33:34] nn step 5900, lr: 0.1.
	loss_policy_0: 0.08517
	accuracy_policy_0: 0.82801
	loss_value_0: 0.22552
	loss_policy_1: 0.01749
	accuracy_policy_1: 0.82152
	loss_value_1: 0.04585
	loss_reward_1: 0.00191
	loss_policy_2: 0.01771
	accuracy_policy_2: 0.82012
	loss_value_2: 0.04664
	loss_reward_2: 0.00361
	loss_policy_3: 0.01768
	accuracy_policy_3: 0.81691
	loss_value_3: 0.0472
	loss_reward_3: 0.00376
	loss_policy_4: 0.01795
	accuracy_policy_4: 0.81473
	loss_value_4: 0.048
	loss_reward_4: 0.0041
	loss_policy_5: 0.01799
	accuracy_policy_5: 0.81363
	loss_value_5: 0.04928
	loss_reward_5: 0.00548
	loss_policy: 0.17398
	loss_value: 0.46249
	loss_reward: 0.01886
[2025-05-08 05:33:42] nn step 5950, lr: 0.1.
	loss_policy_0: 0.0801
	accuracy_policy_0: 0.8341
	loss_value_0: 0.21978
	loss_policy_1: 0.01652
	accuracy_policy_1: 0.82656
	loss_value_1: 0.04497
	loss_reward_1: 0.00187
	loss_policy_2: 0.01688
	accuracy_policy_2: 0.82562
	loss_value_2: 0.04555
	loss_reward_2: 0.00358
	loss_policy_3: 0.01669
	accuracy_policy_3: 0.82277
	loss_value_3: 0.04617
	loss_reward_3: 0.00362
	loss_policy_4: 0.01701
	accuracy_policy_4: 0.81938
	loss_value_4: 0.04702
	loss_reward_4: 0.00401
	loss_policy_5: 0.01706
	accuracy_policy_5: 0.81715
	loss_value_5: 0.04844
	loss_reward_5: 0.00519
	loss_policy: 0.16426
	loss_value: 0.45193
	loss_reward: 0.01826
[2025-05-08 05:33:50] nn step 6000, lr: 0.1.
	loss_policy_0: 0.08018
	accuracy_policy_0: 0.82996
	loss_value_0: 0.21274
	loss_policy_1: 0.01663
	accuracy_policy_1: 0.82309
	loss_value_1: 0.04323
	loss_reward_1: 0.00182
	loss_policy_2: 0.01667
	accuracy_policy_2: 0.82059
	loss_value_2: 0.04388
	loss_reward_2: 0.00343
	loss_policy_3: 0.01699
	accuracy_policy_3: 0.81789
	loss_value_3: 0.04452
	loss_reward_3: 0.00355
	loss_policy_4: 0.01713
	accuracy_policy_4: 0.81387
	loss_value_4: 0.0454
	loss_reward_4: 0.00404
	loss_policy_5: 0.01714
	accuracy_policy_5: 0.81207
	loss_value_5: 0.0467
	loss_reward_5: 0.00515
	loss_policy: 0.16475
	loss_value: 0.43648
	loss_reward: 0.01799
Optimization_Done 6000
[2025-05-08 05:34:56] [command] train weight_iter_6000.pkl 12 31
[2025-05-08 05:35:04] nn step 6050, lr: 0.1.
	loss_policy_0: 0.05069
	accuracy_policy_0: 0.88277
	loss_value_0: 0.20201
	loss_policy_1: 0.01055
	accuracy_policy_1: 0.8782
	loss_value_1: 0.04112
	loss_reward_1: 0.00148
	loss_policy_2: 0.01073
	accuracy_policy_2: 0.87691
	loss_value_2: 0.04158
	loss_reward_2: 0.00292
	loss_policy_3: 0.01095
	accuracy_policy_3: 0.87488
	loss_value_3: 0.04224
	loss_reward_3: 0.00285
	loss_policy_4: 0.01115
	accuracy_policy_4: 0.8725
	loss_value_4: 0.04284
	loss_reward_4: 0.00309
	loss_policy_5: 0.01093
	accuracy_policy_5: 0.87469
	loss_value_5: 0.04387
	loss_reward_5: 0.00444
	loss_policy: 0.105
	loss_value: 0.41366
	loss_reward: 0.01478
[2025-05-08 05:35:12] nn step 6100, lr: 0.1.
	loss_policy_0: 0.05312
	accuracy_policy_0: 0.88527
	loss_value_0: 0.21056
	loss_policy_1: 0.01106
	accuracy_policy_1: 0.87953
	loss_value_1: 0.04293
	loss_reward_1: 0.00164
	loss_policy_2: 0.01119
	accuracy_policy_2: 0.87746
	loss_value_2: 0.04372
	loss_reward_2: 0.00303
	loss_policy_3: 0.01136
	accuracy_policy_3: 0.87613
	loss_value_3: 0.0444
	loss_reward_3: 0.00308
	loss_policy_4: 0.01174
	accuracy_policy_4: 0.86941
	loss_value_4: 0.04515
	loss_reward_4: 0.00329
	loss_policy_5: 0.01158
	accuracy_policy_5: 0.87133
	loss_value_5: 0.04629
	loss_reward_5: 0.00467
	loss_policy: 0.11005
	loss_value: 0.43305
	loss_reward: 0.01571
[2025-05-08 05:35:20] nn step 6150, lr: 0.1.
	loss_policy_0: 0.05301
	accuracy_policy_0: 0.8866
	loss_value_0: 0.21397
	loss_policy_1: 0.01103
	accuracy_policy_1: 0.87824
	loss_value_1: 0.04362
	loss_reward_1: 0.00165
	loss_policy_2: 0.01126
	accuracy_policy_2: 0.87844
	loss_value_2: 0.0442
	loss_reward_2: 0.00318
	loss_policy_3: 0.01129
	accuracy_policy_3: 0.87559
	loss_value_3: 0.04488
	loss_reward_3: 0.00315
	loss_policy_4: 0.0115
	accuracy_policy_4: 0.87281
	loss_value_4: 0.04561
	loss_reward_4: 0.00345
	loss_policy_5: 0.01146
	accuracy_policy_5: 0.87418
	loss_value_5: 0.04696
	loss_reward_5: 0.00484
	loss_policy: 0.10955
	loss_value: 0.43924
	loss_reward: 0.01628
[2025-05-08 05:35:27] nn step 6200, lr: 0.1.
	loss_policy_0: 0.05407
	accuracy_policy_0: 0.88152
	loss_value_0: 0.21645
	loss_policy_1: 0.0112
	accuracy_policy_1: 0.87836
	loss_value_1: 0.04409
	loss_reward_1: 0.00163
	loss_policy_2: 0.01141
	accuracy_policy_2: 0.87805
	loss_value_2: 0.04454
	loss_reward_2: 0.00317
	loss_policy_3: 0.01145
	accuracy_policy_3: 0.87418
	loss_value_3: 0.04521
	loss_reward_3: 0.00311
	loss_policy_4: 0.01154
	accuracy_policy_4: 0.8709
	loss_value_4: 0.04576
	loss_reward_4: 0.00346
	loss_policy_5: 0.01135
	accuracy_policy_5: 0.87348
	loss_value_5: 0.04689
	loss_reward_5: 0.00476
	loss_policy: 0.11102
	loss_value: 0.44295
	loss_reward: 0.01613
Optimization_Done 6200
[2025-05-08 05:36:33] [command] train weight_iter_6200.pkl 13 32
[2025-05-08 05:36:42] nn step 6250, lr: 0.1.
	loss_policy_0: 0.03682
	accuracy_policy_0: 0.91367
	loss_value_0: 0.19293
	loss_policy_1: 0.0076
	accuracy_policy_1: 0.90949
	loss_value_1: 0.03911
	loss_reward_1: 0.0012
	loss_policy_2: 0.00786
	accuracy_policy_2: 0.9093
	loss_value_2: 0.0396
	loss_reward_2: 0.0026
	loss_policy_3: 0.0081
	accuracy_policy_3: 0.90367
	loss_value_3: 0.04032
	loss_reward_3: 0.00261
	loss_policy_4: 0.0084
	accuracy_policy_4: 0.90227
	loss_value_4: 0.04107
	loss_reward_4: 0.00258
	loss_policy_5: 0.00834
	accuracy_policy_5: 0.90422
	loss_value_5: 0.04207
	loss_reward_5: 0.00391
	loss_policy: 0.07711
	loss_value: 0.39511
	loss_reward: 0.01291
[2025-05-08 05:36:49] nn step 6300, lr: 0.1.
	loss_policy_0: 0.0414
	accuracy_policy_0: 0.90828
	loss_value_0: 0.20771
	loss_policy_1: 0.0085
	accuracy_policy_1: 0.90719
	loss_value_1: 0.04217
	loss_reward_1: 0.00136
	loss_policy_2: 0.00884
	accuracy_policy_2: 0.90379
	loss_value_2: 0.0427
	loss_reward_2: 0.00274
	loss_policy_3: 0.0089
	accuracy_policy_3: 0.90156
	loss_value_3: 0.04339
	loss_reward_3: 0.00261
	loss_policy_4: 0.00915
	accuracy_policy_4: 0.89934
	loss_value_4: 0.04407
	loss_reward_4: 0.00293
	loss_policy_5: 0.00902
	accuracy_policy_5: 0.89938
	loss_value_5: 0.04561
	loss_reward_5: 0.00403
	loss_policy: 0.0858
	loss_value: 0.42566
	loss_reward: 0.01366
[2025-05-08 05:36:57] nn step 6350, lr: 0.1.
	loss_policy_0: 0.0389
	accuracy_policy_0: 0.91164
	loss_value_0: 0.19619
	loss_policy_1: 0.00818
	accuracy_policy_1: 0.90762
	loss_value_1: 0.03997
	loss_reward_1: 0.00127
	loss_policy_2: 0.00849
	accuracy_policy_2: 0.90535
	loss_value_2: 0.04048
	loss_reward_2: 0.00261
	loss_policy_3: 0.00858
	accuracy_policy_3: 0.90168
	loss_value_3: 0.04127
	loss_reward_3: 0.00253
	loss_policy_4: 0.00892
	accuracy_policy_4: 0.89855
	loss_value_4: 0.04194
	loss_reward_4: 0.00281
	loss_policy_5: 0.00876
	accuracy_policy_5: 0.89988
	loss_value_5: 0.04302
	loss_reward_5: 0.00387
	loss_policy: 0.08183
	loss_value: 0.40287
	loss_reward: 0.01309
[2025-05-08 05:37:05] nn step 6400, lr: 0.1.
	loss_policy_0: 0.03571
	accuracy_policy_0: 0.91402
	loss_value_0: 0.18586
	loss_policy_1: 0.00763
	accuracy_policy_1: 0.90852
	loss_value_1: 0.03791
	loss_reward_1: 0.00122
	loss_policy_2: 0.00776
	accuracy_policy_2: 0.90473
	loss_value_2: 0.03863
	loss_reward_2: 0.00266
	loss_policy_3: 0.00778
	accuracy_policy_3: 0.90293
	loss_value_3: 0.03941
	loss_reward_3: 0.00269
	loss_policy_4: 0.00802
	accuracy_policy_4: 0.90152
	loss_value_4: 0.04003
	loss_reward_4: 0.00265
	loss_policy_5: 0.00816
	accuracy_policy_5: 0.90059
	loss_value_5: 0.04096
	loss_reward_5: 0.00384
	loss_policy: 0.07506
	loss_value: 0.38282
	loss_reward: 0.01306
Optimization_Done 6400
[2025-05-08 05:38:14] [command] train weight_iter_6400.pkl 14 33
[2025-05-08 05:38:24] nn step 6450, lr: 0.1.
	loss_policy_0: 0.03769
	accuracy_policy_0: 0.91453
	loss_value_0: 0.19394
	loss_policy_1: 0.00781
	accuracy_policy_1: 0.90965
	loss_value_1: 0.0394
	loss_reward_1: 0.00127
	loss_policy_2: 0.00808
	accuracy_policy_2: 0.9082
	loss_value_2: 0.04066
	loss_reward_2: 0.0029
	loss_policy_3: 0.0084
	accuracy_policy_3: 0.90352
	loss_value_3: 0.04117
	loss_reward_3: 0.00286
	loss_policy_4: 0.00855
	accuracy_policy_4: 0.90191
	loss_value_4: 0.04181
	loss_reward_4: 0.00301
	loss_policy_5: 0.0086
	accuracy_policy_5: 0.9002
	loss_value_5: 0.04315
	loss_reward_5: 0.0041
	loss_policy: 0.07914
	loss_value: 0.40012
	loss_reward: 0.01415
[2025-05-08 05:38:32] nn step 6500, lr: 0.1.
	loss_policy_0: 0.03897
	accuracy_policy_0: 0.91355
	loss_value_0: 0.19994
	loss_policy_1: 0.00826
	accuracy_policy_1: 0.90793
	loss_value_1: 0.04053
	loss_reward_1: 0.0012
	loss_policy_2: 0.00856
	accuracy_policy_2: 0.90469
	loss_value_2: 0.04136
	loss_reward_2: 0.0027
	loss_policy_3: 0.00864
	accuracy_policy_3: 0.90266
	loss_value_3: 0.04198
	loss_reward_3: 0.00256
	loss_policy_4: 0.00884
	accuracy_policy_4: 0.9002
	loss_value_4: 0.04271
	loss_reward_4: 0.00287
	loss_policy_5: 0.00877
	accuracy_policy_5: 0.89844
	loss_value_5: 0.04399
	loss_reward_5: 0.00397
	loss_policy: 0.08204
	loss_value: 0.41052
	loss_reward: 0.0133
[2025-05-08 05:38:38] nn step 6550, lr: 0.1.
	loss_policy_0: 0.04061
	accuracy_policy_0: 0.90887
	loss_value_0: 0.19718
	loss_policy_1: 0.0084
	accuracy_policy_1: 0.90598
	loss_value_1: 0.03995
	loss_reward_1: 0.00133
	loss_policy_2: 0.00859
	accuracy_policy_2: 0.90219
	loss_value_2: 0.04065
	loss_reward_2: 0.00269
	loss_policy_3: 0.00891
	accuracy_policy_3: 0.89949
	loss_value_3: 0.04125
	loss_reward_3: 0.0026
	loss_policy_4: 0.00911
	accuracy_policy_4: 0.89656
	loss_value_4: 0.04183
	loss_reward_4: 0.00279
	loss_policy_5: 0.00887
	accuracy_policy_5: 0.89762
	loss_value_5: 0.043
	loss_reward_5: 0.00388
	loss_policy: 0.08449
	loss_value: 0.40386
	loss_reward: 0.01329
[2025-05-08 05:38:46] nn step 6600, lr: 0.1.
	loss_policy_0: 0.04346
	accuracy_policy_0: 0.90711
	loss_value_0: 0.20435
	loss_policy_1: 0.009
	accuracy_policy_1: 0.90359
	loss_value_1: 0.0416
	loss_reward_1: 0.0013
	loss_policy_2: 0.00933
	accuracy_policy_2: 0.89871
	loss_value_2: 0.04229
	loss_reward_2: 0.00275
	loss_policy_3: 0.00931
	accuracy_policy_3: 0.89973
	loss_value_3: 0.04312
	loss_reward_3: 0.00275
	loss_policy_4: 0.00948
	accuracy_policy_4: 0.89609
	loss_value_4: 0.04355
	loss_reward_4: 0.00308
	loss_policy_5: 0.00937
	accuracy_policy_5: 0.89566
	loss_value_5: 0.04503
	loss_reward_5: 0.00422
	loss_policy: 0.08994
	loss_value: 0.41994
	loss_reward: 0.0141
Optimization_Done 6600
[2025-05-08 05:39:55] [command] train weight_iter_6600.pkl 15 34
[2025-05-08 05:40:03] nn step 6650, lr: 0.1.
	loss_policy_0: 0.04045
	accuracy_policy_0: 0.91074
	loss_value_0: 0.19926
	loss_policy_1: 0.00861
	accuracy_policy_1: 0.90582
	loss_value_1: 0.04035
	loss_reward_1: 0.00125
	loss_policy_2: 0.00894
	accuracy_policy_2: 0.90113
	loss_value_2: 0.04112
	loss_reward_2: 0.00271
	loss_policy_3: 0.00914
	accuracy_policy_3: 0.89863
	loss_value_3: 0.04171
	loss_reward_3: 0.00272
	loss_policy_4: 0.00942
	accuracy_policy_4: 0.89691
	loss_value_4: 0.04243
	loss_reward_4: 0.00289
	loss_policy_5: 0.00921
	accuracy_policy_5: 0.89707
	loss_value_5: 0.04378
	loss_reward_5: 0.004
	loss_policy: 0.08576
	loss_value: 0.40866
	loss_reward: 0.01356
[2025-05-08 05:40:10] nn step 6700, lr: 0.1.
	loss_policy_0: 0.04107
	accuracy_policy_0: 0.90777
	loss_value_0: 0.19485
	loss_policy_1: 0.00855
	accuracy_policy_1: 0.90406
	loss_value_1: 0.03947
	loss_reward_1: 0.00125
	loss_policy_2: 0.0087
	accuracy_policy_2: 0.90316
	loss_value_2: 0.0403
	loss_reward_2: 0.00264
	loss_policy_3: 0.00896
	accuracy_policy_3: 0.89754
	loss_value_3: 0.04094
	loss_reward_3: 0.00266
	loss_policy_4: 0.0092
	accuracy_policy_4: 0.89645
	loss_value_4: 0.0413
	loss_reward_4: 0.00293
	loss_policy_5: 0.00893
	accuracy_policy_5: 0.89559
	loss_value_5: 0.0426
	loss_reward_5: 0.00394
	loss_policy: 0.08541
	loss_value: 0.39945
	loss_reward: 0.01341
[2025-05-08 05:40:18] nn step 6750, lr: 0.1.
	loss_policy_0: 0.03799
	accuracy_policy_0: 0.90891
	loss_value_0: 0.18269
	loss_policy_1: 0.00809
	accuracy_policy_1: 0.9048
	loss_value_1: 0.03713
	loss_reward_1: 0.00119
	loss_policy_2: 0.00824
	accuracy_policy_2: 0.90219
	loss_value_2: 0.03779
	loss_reward_2: 0.00272
	loss_policy_3: 0.0085
	accuracy_policy_3: 0.89891
	loss_value_3: 0.03846
	loss_reward_3: 0.00246
	loss_policy_4: 0.00879
	accuracy_policy_4: 0.89477
	loss_value_4: 0.03894
	loss_reward_4: 0.00273
	loss_policy_5: 0.00864
	accuracy_policy_5: 0.89664
	loss_value_5: 0.04036
	loss_reward_5: 0.00381
	loss_policy: 0.08026
	loss_value: 0.37537
	loss_reward: 0.01291
[2025-05-08 05:40:26] nn step 6800, lr: 0.1.
	loss_policy_0: 0.043
	accuracy_policy_0: 0.9077
	loss_value_0: 0.20432
	loss_policy_1: 0.00901
	accuracy_policy_1: 0.90457
	loss_value_1: 0.04151
	loss_reward_1: 0.00144
	loss_policy_2: 0.00928
	accuracy_policy_2: 0.90199
	loss_value_2: 0.04213
	loss_reward_2: 0.00289
	loss_policy_3: 0.00937
	accuracy_policy_3: 0.89906
	loss_value_3: 0.04282
	loss_reward_3: 0.00284
	loss_policy_4: 0.00963
	accuracy_policy_4: 0.89492
	loss_value_4: 0.04362
	loss_reward_4: 0.00291
	loss_policy_5: 0.00932
	accuracy_policy_5: 0.89492
	loss_value_5: 0.04476
	loss_reward_5: 0.0043
	loss_policy: 0.0896
	loss_value: 0.41917
	loss_reward: 0.01437
Optimization_Done 6800
[2025-05-08 05:41:35] [command] train weight_iter_6800.pkl 16 35
[2025-05-08 05:41:44] nn step 6850, lr: 0.1.
	loss_policy_0: 0.0402
	accuracy_policy_0: 0.91293
	loss_value_0: 0.19532
	loss_policy_1: 0.0082
	accuracy_policy_1: 0.90875
	loss_value_1: 0.03951
	loss_reward_1: 0.00144
	loss_policy_2: 0.00859
	accuracy_policy_2: 0.90621
	loss_value_2: 0.04049
	loss_reward_2: 0.00309
	loss_policy_3: 0.00889
	accuracy_policy_3: 0.90039
	loss_value_3: 0.0412
	loss_reward_3: 0.00282
	loss_policy_4: 0.00908
	accuracy_policy_4: 0.89918
	loss_value_4: 0.04193
	loss_reward_4: 0.00308
	loss_policy_5: 0.0088
	accuracy_policy_5: 0.90219
	loss_value_5: 0.04337
	loss_reward_5: 0.00438
	loss_policy: 0.08376
	loss_value: 0.40183
	loss_reward: 0.01482
[2025-05-08 05:41:52] nn step 6900, lr: 0.1.
	loss_policy_0: 0.04002
	accuracy_policy_0: 0.9102
	loss_value_0: 0.18998
	loss_policy_1: 0.00836
	accuracy_policy_1: 0.90574
	loss_value_1: 0.03853
	loss_reward_1: 0.00132
	loss_policy_2: 0.00849
	accuracy_policy_2: 0.90477
	loss_value_2: 0.03934
	loss_reward_2: 0.00279
	loss_policy_3: 0.0088
	accuracy_policy_3: 0.8977
	loss_value_3: 0.03993
	loss_reward_3: 0.00275
	loss_policy_4: 0.00911
	accuracy_policy_4: 0.8941
	loss_value_4: 0.04071
	loss_reward_4: 0.00306
	loss_policy_5: 0.00897
	accuracy_policy_5: 0.895
	loss_value_5: 0.04178
	loss_reward_5: 0.00402
	loss_policy: 0.08375
	loss_value: 0.39027
	loss_reward: 0.01394
[2025-05-08 05:41:59] nn step 6950, lr: 0.1.
	loss_policy_0: 0.04142
	accuracy_policy_0: 0.90895
	loss_value_0: 0.19131
	loss_policy_1: 0.00844
	accuracy_policy_1: 0.9052
	loss_value_1: 0.03882
	loss_reward_1: 0.00127
	loss_policy_2: 0.00865
	accuracy_policy_2: 0.90242
	loss_value_2: 0.03947
	loss_reward_2: 0.00267
	loss_policy_3: 0.00874
	accuracy_policy_3: 0.89801
	loss_value_3: 0.04017
	loss_reward_3: 0.00265
	loss_policy_4: 0.00914
	accuracy_policy_4: 0.89371
	loss_value_4: 0.04092
	loss_reward_4: 0.00287
	loss_policy_5: 0.00902
	accuracy_policy_5: 0.895
	loss_value_5: 0.04225
	loss_reward_5: 0.00402
	loss_policy: 0.08541
	loss_value: 0.39294
	loss_reward: 0.01348
[2025-05-08 05:42:07] nn step 7000, lr: 0.1.
	loss_policy_0: 0.04146
	accuracy_policy_0: 0.90957
	loss_value_0: 0.19653
	loss_policy_1: 0.00868
	accuracy_policy_1: 0.90484
	loss_value_1: 0.03985
	loss_reward_1: 0.00135
	loss_policy_2: 0.00904
	accuracy_policy_2: 0.90078
	loss_value_2: 0.04044
	loss_reward_2: 0.00282
	loss_policy_3: 0.00918
	accuracy_policy_3: 0.89652
	loss_value_3: 0.04102
	loss_reward_3: 0.00277
	loss_policy_4: 0.00951
	accuracy_policy_4: 0.89266
	loss_value_4: 0.04151
	loss_reward_4: 0.00298
	loss_policy_5: 0.00927
	accuracy_policy_5: 0.89504
	loss_value_5: 0.04293
	loss_reward_5: 0.00419
	loss_policy: 0.08714
	loss_value: 0.40227
	loss_reward: 0.01412
Optimization_Done 7000
[2025-05-08 05:43:16] [command] train weight_iter_7000.pkl 17 36
[2025-05-08 05:43:23] nn step 7050, lr: 0.1.
	loss_policy_0: 0.03751
	accuracy_policy_0: 0.9077
	loss_value_0: 0.17325
	loss_policy_1: 0.0079
	accuracy_policy_1: 0.90215
	loss_value_1: 0.03507
	loss_reward_1: 0.00122
	loss_policy_2: 0.00804
	accuracy_policy_2: 0.89922
	loss_value_2: 0.03555
	loss_reward_2: 0.00243
	loss_policy_3: 0.00823
	accuracy_policy_3: 0.89602
	loss_value_3: 0.03597
	loss_reward_3: 0.00246
	loss_policy_4: 0.00851
	accuracy_policy_4: 0.89324
	loss_value_4: 0.03675
	loss_reward_4: 0.00263
	loss_policy_5: 0.00828
	accuracy_policy_5: 0.89367
	loss_value_5: 0.03774
	loss_reward_5: 0.00362
	loss_policy: 0.07846
	loss_value: 0.35432
	loss_reward: 0.01236
[2025-05-08 05:43:31] nn step 7100, lr: 0.1.
	loss_policy_0: 0.04204
	accuracy_policy_0: 0.90949
	loss_value_0: 0.19525
	loss_policy_1: 0.00874
	accuracy_policy_1: 0.90559
	loss_value_1: 0.03934
	loss_reward_1: 0.00131
	loss_policy_2: 0.00911
	accuracy_policy_2: 0.89875
	loss_value_2: 0.04008
	loss_reward_2: 0.00279
	loss_policy_3: 0.00939
	accuracy_policy_3: 0.89641
	loss_value_3: 0.04037
	loss_reward_3: 0.00274
	loss_policy_4: 0.00951
	accuracy_policy_4: 0.8916
	loss_value_4: 0.04109
	loss_reward_4: 0.00292
	loss_policy_5: 0.00919
	accuracy_policy_5: 0.89598
	loss_value_5: 0.04241
	loss_reward_5: 0.00418
	loss_policy: 0.08798
	loss_value: 0.39854
	loss_reward: 0.01394
[2025-05-08 05:43:40] nn step 7150, lr: 0.1.
	loss_policy_0: 0.04062
	accuracy_policy_0: 0.91051
	loss_value_0: 0.18905
	loss_policy_1: 0.00843
	accuracy_policy_1: 0.90477
	loss_value_1: 0.03828
	loss_reward_1: 0.0014
	loss_policy_2: 0.00869
	accuracy_policy_2: 0.90137
	loss_value_2: 0.03884
	loss_reward_2: 0.00291
	loss_policy_3: 0.00894
	accuracy_policy_3: 0.89648
	loss_value_3: 0.03947
	loss_reward_3: 0.00258
	loss_policy_4: 0.00907
	accuracy_policy_4: 0.89488
	loss_value_4: 0.03992
	loss_reward_4: 0.003
	loss_policy_5: 0.00882
	accuracy_policy_5: 0.89652
	loss_value_5: 0.04105
	loss_reward_5: 0.00413
	loss_policy: 0.08457
	loss_value: 0.38662
	loss_reward: 0.01402
[2025-05-08 05:43:48] nn step 7200, lr: 0.1.
	loss_policy_0: 0.04078
	accuracy_policy_0: 0.90941
	loss_value_0: 0.19232
	loss_policy_1: 0.00861
	accuracy_policy_1: 0.90383
	loss_value_1: 0.0388
	loss_reward_1: 0.00139
	loss_policy_2: 0.0087
	accuracy_policy_2: 0.90188
	loss_value_2: 0.03965
	loss_reward_2: 0.00282
	loss_policy_3: 0.00915
	accuracy_policy_3: 0.89617
	loss_value_3: 0.04003
	loss_reward_3: 0.00269
	loss_policy_4: 0.00958
	accuracy_policy_4: 0.89145
	loss_value_4: 0.04037
	loss_reward_4: 0.00314
	loss_policy_5: 0.00934
	accuracy_policy_5: 0.89531
	loss_value_5: 0.0419
	loss_reward_5: 0.00443
	loss_policy: 0.08616
	loss_value: 0.39307
	loss_reward: 0.01446
Optimization_Done 7200
[2025-05-08 05:44:58] [command] train weight_iter_7200.pkl 18 37
[2025-05-08 05:45:07] nn step 7250, lr: 0.1.
	loss_policy_0: 0.03903
	accuracy_policy_0: 0.91117
	loss_value_0: 0.18556
	loss_policy_1: 0.00825
	accuracy_policy_1: 0.90539
	loss_value_1: 0.03731
	loss_reward_1: 0.00132
	loss_policy_2: 0.00838
	accuracy_policy_2: 0.90312
	loss_value_2: 0.03799
	loss_reward_2: 0.00265
	loss_policy_3: 0.00848
	accuracy_policy_3: 0.89977
	loss_value_3: 0.03839
	loss_reward_3: 0.00267
	loss_policy_4: 0.00881
	accuracy_policy_4: 0.8934
	loss_value_4: 0.03898
	loss_reward_4: 0.00279
	loss_policy_5: 0.00855
	accuracy_policy_5: 0.89605
	loss_value_5: 0.04012
	loss_reward_5: 0.00398
	loss_policy: 0.08149
	loss_value: 0.37837
	loss_reward: 0.01341
[2025-05-08 05:45:13] nn step 7300, lr: 0.1.
	loss_policy_0: 0.04127
	accuracy_policy_0: 0.90898
	loss_value_0: 0.1908
	loss_policy_1: 0.00864
	accuracy_policy_1: 0.9043
	loss_value_1: 0.03837
	loss_reward_1: 0.00144
	loss_policy_2: 0.00904
	accuracy_policy_2: 0.90008
	loss_value_2: 0.03915
	loss_reward_2: 0.00293
	loss_policy_3: 0.00904
	accuracy_policy_3: 0.89594
	loss_value_3: 0.03956
	loss_reward_3: 0.00279
	loss_policy_4: 0.00924
	accuracy_policy_4: 0.89254
	loss_value_4: 0.0401
	loss_reward_4: 0.00294
	loss_policy_5: 0.00892
	accuracy_policy_5: 0.89438
	loss_value_5: 0.0414
	loss_reward_5: 0.00445
	loss_policy: 0.08616
	loss_value: 0.38939
	loss_reward: 0.01455
[2025-05-08 05:45:21] nn step 7350, lr: 0.1.
	loss_policy_0: 0.03693
	accuracy_policy_0: 0.91012
	loss_value_0: 0.17832
	loss_policy_1: 0.00773
	accuracy_policy_1: 0.90453
	loss_value_1: 0.03589
	loss_reward_1: 0.00131
	loss_policy_2: 0.00799
	accuracy_policy_2: 0.90039
	loss_value_2: 0.03664
	loss_reward_2: 0.00265
	loss_policy_3: 0.00831
	accuracy_policy_3: 0.8966
	loss_value_3: 0.037
	loss_reward_3: 0.00248
	loss_policy_4: 0.00844
	accuracy_policy_4: 0.89379
	loss_value_4: 0.03746
	loss_reward_4: 0.00272
	loss_policy_5: 0.00816
	accuracy_policy_5: 0.89676
	loss_value_5: 0.03863
	loss_reward_5: 0.00394
	loss_policy: 0.07756
	loss_value: 0.36395
	loss_reward: 0.01311
[2025-05-08 05:45:29] nn step 7400, lr: 0.1.
	loss_policy_0: 0.03893
	accuracy_policy_0: 0.91039
	loss_value_0: 0.18489
	loss_policy_1: 0.00831
	accuracy_policy_1: 0.90375
	loss_value_1: 0.03729
	loss_reward_1: 0.00138
	loss_policy_2: 0.00855
	accuracy_policy_2: 0.89957
	loss_value_2: 0.03799
	loss_reward_2: 0.00284
	loss_policy_3: 0.00873
	accuracy_policy_3: 0.89645
	loss_value_3: 0.03841
	loss_reward_3: 0.00257
	loss_policy_4: 0.00901
	accuracy_policy_4: 0.89156
	loss_value_4: 0.03882
	loss_reward_4: 0.00288
	loss_policy_5: 0.00866
	accuracy_policy_5: 0.89445
	loss_value_5: 0.03993
	loss_reward_5: 0.00419
	loss_policy: 0.08219
	loss_value: 0.37734
	loss_reward: 0.01385
Optimization_Done 7400
[2025-05-08 05:46:38] [command] train weight_iter_7400.pkl 19 38
[2025-05-08 05:46:45] nn step 7450, lr: 0.1.
	loss_policy_0: 0.0194
	accuracy_policy_0: 0.9534
	loss_value_0: 0.17385
	loss_policy_1: 0.00399
	accuracy_policy_1: 0.9523
	loss_value_1: 0.03478
	loss_reward_1: 0.00107
	loss_policy_2: 0.0042
	accuracy_policy_2: 0.94836
	loss_value_2: 0.03536
	loss_reward_2: 0.00238
	loss_policy_3: 0.00434
	accuracy_policy_3: 0.94734
	loss_value_3: 0.03587
	loss_reward_3: 0.00206
	loss_policy_4: 0.00454
	accuracy_policy_4: 0.94406
	loss_value_4: 0.03642
	loss_reward_4: 0.00227
	loss_policy_5: 0.00439
	accuracy_policy_5: 0.94613
	loss_value_5: 0.03753
	loss_reward_5: 0.00336
	loss_policy: 0.04087
	loss_value: 0.35382
	loss_reward: 0.01114
[2025-05-08 05:46:53] nn step 7500, lr: 0.1.
	loss_policy_0: 0.02019
	accuracy_policy_0: 0.95152
	loss_value_0: 0.17719
	loss_policy_1: 0.00426
	accuracy_policy_1: 0.94879
	loss_value_1: 0.03565
	loss_reward_1: 0.0011
	loss_policy_2: 0.00436
	accuracy_policy_2: 0.9475
	loss_value_2: 0.03643
	loss_reward_2: 0.00241
	loss_policy_3: 0.00454
	accuracy_policy_3: 0.9452
	loss_value_3: 0.0369
	loss_reward_3: 0.00213
	loss_policy_4: 0.00463
	accuracy_policy_4: 0.9423
	loss_value_4: 0.03731
	loss_reward_4: 0.00229
	loss_policy_5: 0.00465
	accuracy_policy_5: 0.94203
	loss_value_5: 0.03865
	loss_reward_5: 0.00356
	loss_policy: 0.04263
	loss_value: 0.36212
	loss_reward: 0.01148
[2025-05-08 05:47:01] nn step 7550, lr: 0.1.
	loss_policy_0: 0.01894
	accuracy_policy_0: 0.95359
	loss_value_0: 0.17133
	loss_policy_1: 0.00402
	accuracy_policy_1: 0.95035
	loss_value_1: 0.03462
	loss_reward_1: 0.00107
	loss_policy_2: 0.00424
	accuracy_policy_2: 0.94699
	loss_value_2: 0.03517
	loss_reward_2: 0.00244
	loss_policy_3: 0.00433
	accuracy_policy_3: 0.94477
	loss_value_3: 0.0356
	loss_reward_3: 0.00216
	loss_policy_4: 0.00448
	accuracy_policy_4: 0.94219
	loss_value_4: 0.03611
	loss_reward_4: 0.00226
	loss_policy_5: 0.00443
	accuracy_policy_5: 0.94418
	loss_value_5: 0.03722
	loss_reward_5: 0.00341
	loss_policy: 0.04045
	loss_value: 0.35006
	loss_reward: 0.01133
[2025-05-08 05:47:07] nn step 7600, lr: 0.1.
	loss_policy_0: 0.01962
	accuracy_policy_0: 0.95184
	loss_value_0: 0.17704
	loss_policy_1: 0.00414
	accuracy_policy_1: 0.94945
	loss_value_1: 0.03569
	loss_reward_1: 0.00113
	loss_policy_2: 0.00444
	accuracy_policy_2: 0.94629
	loss_value_2: 0.03635
	loss_reward_2: 0.00252
	loss_policy_3: 0.00462
	accuracy_policy_3: 0.94414
	loss_value_3: 0.03693
	loss_reward_3: 0.00225
	loss_policy_4: 0.00503
	accuracy_policy_4: 0.93949
	loss_value_4: 0.03752
	loss_reward_4: 0.00241
	loss_policy_5: 0.00459
	accuracy_policy_5: 0.94312
	loss_value_5: 0.03873
	loss_reward_5: 0.00368
	loss_policy: 0.04244
	loss_value: 0.36226
	loss_reward: 0.01199
Optimization_Done 7600
[2025-05-08 05:48:16] [command] train weight_iter_7600.pkl 20 39
[2025-05-08 05:48:25] nn step 7650, lr: 0.1.
	loss_policy_0: 0.0126
	accuracy_policy_0: 0.96719
	loss_value_0: 0.16492
	loss_policy_1: 0.00268
	accuracy_policy_1: 0.9648
	loss_value_1: 0.03326
	loss_reward_1: 0.00089
	loss_policy_2: 0.00277
	accuracy_policy_2: 0.96336
	loss_value_2: 0.03384
	loss_reward_2: 0.00208
	loss_policy_3: 0.00293
	accuracy_policy_3: 0.95996
	loss_value_3: 0.03423
	loss_reward_3: 0.0019
	loss_policy_4: 0.00306
	accuracy_policy_4: 0.95883
	loss_value_4: 0.03477
	loss_reward_4: 0.00189
	loss_policy_5: 0.00296
	accuracy_policy_5: 0.96012
	loss_value_5: 0.03596
	loss_reward_5: 0.00297
	loss_policy: 0.027
	loss_value: 0.33698
	loss_reward: 0.00973
[2025-05-08 05:48:31] nn step 7700, lr: 0.1.
	loss_policy_0: 0.0131
	accuracy_policy_0: 0.96824
	loss_value_0: 0.18151
	loss_policy_1: 0.00289
	accuracy_policy_1: 0.96512
	loss_value_1: 0.03669
	loss_reward_1: 0.00105
	loss_policy_2: 0.00291
	accuracy_policy_2: 0.9634
	loss_value_2: 0.03732
	loss_reward_2: 0.00228
	loss_policy_3: 0.00308
	accuracy_policy_3: 0.96062
	loss_value_3: 0.03789
	loss_reward_3: 0.00196
	loss_policy_4: 0.00324
	accuracy_policy_4: 0.95793
	loss_value_4: 0.03826
	loss_reward_4: 0.00219
	loss_policy_5: 0.00308
	accuracy_policy_5: 0.96047
	loss_value_5: 0.03953
	loss_reward_5: 0.00334
	loss_policy: 0.02829
	loss_value: 0.3712
	loss_reward: 0.01082
[2025-05-08 05:48:39] nn step 7750, lr: 0.1.
	loss_policy_0: 0.01199
	accuracy_policy_0: 0.96926
	loss_value_0: 0.17641
	loss_policy_1: 0.00262
	accuracy_policy_1: 0.96645
	loss_value_1: 0.03543
	loss_reward_1: 0.00099
	loss_policy_2: 0.00262
	accuracy_policy_2: 0.96465
	loss_value_2: 0.03608
	loss_reward_2: 0.00224
	loss_policy_3: 0.00283
	accuracy_policy_3: 0.96254
	loss_value_3: 0.03654
	loss_reward_3: 0.00192
	loss_policy_4: 0.00299
	accuracy_policy_4: 0.96066
	loss_value_4: 0.03683
	loss_reward_4: 0.00217
	loss_policy_5: 0.00283
	accuracy_policy_5: 0.96215
	loss_value_5: 0.03804
	loss_reward_5: 0.00351
	loss_policy: 0.02587
	loss_value: 0.35932
	loss_reward: 0.01083
[2025-05-08 05:48:48] nn step 7800, lr: 0.1.
	loss_policy_0: 0.01216
	accuracy_policy_0: 0.96805
	loss_value_0: 0.16422
	loss_policy_1: 0.00257
	accuracy_policy_1: 0.96605
	loss_value_1: 0.03297
	loss_reward_1: 0.001
	loss_policy_2: 0.00267
	accuracy_policy_2: 0.96355
	loss_value_2: 0.03357
	loss_reward_2: 0.0022
	loss_policy_3: 0.00284
	accuracy_policy_3: 0.96027
	loss_value_3: 0.03394
	loss_reward_3: 0.00189
	loss_policy_4: 0.00304
	accuracy_policy_4: 0.95742
	loss_value_4: 0.03441
	loss_reward_4: 0.00204
	loss_policy_5: 0.0028
	accuracy_policy_5: 0.96008
	loss_value_5: 0.03557
	loss_reward_5: 0.00326
	loss_policy: 0.02609
	loss_value: 0.33468
	loss_reward: 0.01039
Optimization_Done 7800
[2025-05-08 05:49:57] [command] train weight_iter_7800.pkl 21 40
[2025-05-08 05:50:05] nn step 7850, lr: 0.1.
	loss_policy_0: 0.01375
	accuracy_policy_0: 0.96727
	loss_value_0: 0.17737
	loss_policy_1: 0.00287
	accuracy_policy_1: 0.96398
	loss_value_1: 0.03549
	loss_reward_1: 0.00098
	loss_policy_2: 0.00299
	accuracy_policy_2: 0.9616
	loss_value_2: 0.03596
	loss_reward_2: 0.00225
	loss_policy_3: 0.00298
	accuracy_policy_3: 0.96082
	loss_value_3: 0.03638
	loss_reward_3: 0.00201
	loss_policy_4: 0.00321
	accuracy_policy_4: 0.95734
	loss_value_4: 0.03684
	loss_reward_4: 0.00203
	loss_policy_5: 0.00305
	accuracy_policy_5: 0.95902
	loss_value_5: 0.03792
	loss_reward_5: 0.00333
	loss_policy: 0.02886
	loss_value: 0.35996
	loss_reward: 0.0106
[2025-05-08 05:50:13] nn step 7900, lr: 0.1.
	loss_policy_0: 0.01363
	accuracy_policy_0: 0.96699
	loss_value_0: 0.17892
	loss_policy_1: 0.00291
	accuracy_policy_1: 0.96367
	loss_value_1: 0.03591
	loss_reward_1: 0.00104
	loss_policy_2: 0.00308
	accuracy_policy_2: 0.96176
	loss_value_2: 0.03657
	loss_reward_2: 0.00227
	loss_policy_3: 0.00321
	accuracy_policy_3: 0.96004
	loss_value_3: 0.03702
	loss_reward_3: 0.00209
	loss_policy_4: 0.00343
	accuracy_policy_4: 0.95633
	loss_value_4: 0.03728
	loss_reward_4: 0.00203
	loss_policy_5: 0.00324
	accuracy_policy_5: 0.95816
	loss_value_5: 0.0384
	loss_reward_5: 0.00347
	loss_policy: 0.02949
	loss_value: 0.3641
	loss_reward: 0.0109
[2025-05-08 05:50:22] nn step 7950, lr: 0.1.
	loss_policy_0: 0.01292
	accuracy_policy_0: 0.96867
	loss_value_0: 0.18458
	loss_policy_1: 0.00275
	accuracy_policy_1: 0.96617
	loss_value_1: 0.03705
	loss_reward_1: 0.00098
	loss_policy_2: 0.00286
	accuracy_policy_2: 0.96395
	loss_value_2: 0.03762
	loss_reward_2: 0.00264
	loss_policy_3: 0.003
	accuracy_policy_3: 0.96219
	loss_value_3: 0.03795
	loss_reward_3: 0.00238
	loss_policy_4: 0.00329
	accuracy_policy_4: 0.95879
	loss_value_4: 0.03828
	loss_reward_4: 0.00236
	loss_policy_5: 0.00309
	accuracy_policy_5: 0.96078
	loss_value_5: 0.03961
	loss_reward_5: 0.00391
	loss_policy: 0.02791
	loss_value: 0.37509
	loss_reward: 0.01227
[2025-05-08 05:50:28] nn step 8000, lr: 0.1.
	loss_policy_0: 0.01146
	accuracy_policy_0: 0.96984
	loss_value_0: 0.16987
	loss_policy_1: 0.00257
	accuracy_policy_1: 0.96676
	loss_value_1: 0.03405
	loss_reward_1: 0.00097
	loss_policy_2: 0.00267
	accuracy_policy_2: 0.96504
	loss_value_2: 0.03474
	loss_reward_2: 0.00218
	loss_policy_3: 0.0028
	accuracy_policy_3: 0.9634
	loss_value_3: 0.03506
	loss_reward_3: 0.00188
	loss_policy_4: 0.00305
	accuracy_policy_4: 0.95922
	loss_value_4: 0.03539
	loss_reward_4: 0.00208
	loss_policy_5: 0.00305
	accuracy_policy_5: 0.9609
	loss_value_5: 0.03644
	loss_reward_5: 0.00322
	loss_policy: 0.0256
	loss_value: 0.34555
	loss_reward: 0.01033
Optimization_Done 8000
[2025-05-08 05:51:34] [command] train weight_iter_8000.pkl 22 41
[2025-05-08 05:51:43] nn step 8050, lr: 0.1.
	loss_policy_0: 0.01105
	accuracy_policy_0: 0.97109
	loss_value_0: 0.16549
	loss_policy_1: 0.00257
	accuracy_policy_1: 0.96602
	loss_value_1: 0.03335
	loss_reward_1: 0.00086
	loss_policy_2: 0.00255
	accuracy_policy_2: 0.96598
	loss_value_2: 0.03383
	loss_reward_2: 0.00205
	loss_policy_3: 0.00256
	accuracy_policy_3: 0.96457
	loss_value_3: 0.03396
	loss_reward_3: 0.00178
	loss_policy_4: 0.00281
	accuracy_policy_4: 0.9609
	loss_value_4: 0.03433
	loss_reward_4: 0.00198
	loss_policy_5: 0.00275
	accuracy_policy_5: 0.96289
	loss_value_5: 0.03522
	loss_reward_5: 0.00303
	loss_policy: 0.02429
	loss_value: 0.33617
	loss_reward: 0.0097
[2025-05-08 05:51:51] nn step 8100, lr: 0.1.
	loss_policy_0: 0.01262
	accuracy_policy_0: 0.96992
	loss_value_0: 0.17742
	loss_policy_1: 0.0027
	accuracy_policy_1: 0.96703
	loss_value_1: 0.03583
	loss_reward_1: 0.00099
	loss_policy_2: 0.0028
	accuracy_policy_2: 0.96469
	loss_value_2: 0.03639
	loss_reward_2: 0.0024
	loss_policy_3: 0.00293
	accuracy_policy_3: 0.96344
	loss_value_3: 0.03694
	loss_reward_3: 0.00201
	loss_policy_4: 0.0031
	accuracy_policy_4: 0.96031
	loss_value_4: 0.03726
	loss_reward_4: 0.0021
	loss_policy_5: 0.00302
	accuracy_policy_5: 0.96227
	loss_value_5: 0.03831
	loss_reward_5: 0.0035
	loss_policy: 0.02717
	loss_value: 0.36214
	loss_reward: 0.01099
[2025-05-08 05:51:57] nn step 8150, lr: 0.1.
	loss_policy_0: 0.01262
	accuracy_policy_0: 0.96832
	loss_value_0: 0.18213
	loss_policy_1: 0.00267
	accuracy_policy_1: 0.96469
	loss_value_1: 0.03664
	loss_reward_1: 0.00097
	loss_policy_2: 0.00284
	accuracy_policy_2: 0.9627
	loss_value_2: 0.03734
	loss_reward_2: 0.00228
	loss_policy_3: 0.00287
	accuracy_policy_3: 0.96121
	loss_value_3: 0.03788
	loss_reward_3: 0.00191
	loss_policy_4: 0.00306
	accuracy_policy_4: 0.95836
	loss_value_4: 0.03822
	loss_reward_4: 0.00206
	loss_policy_5: 0.00305
	accuracy_policy_5: 0.95957
	loss_value_5: 0.03928
	loss_reward_5: 0.00353
	loss_policy: 0.02712
	loss_value: 0.37149
	loss_reward: 0.01075
[2025-05-08 05:52:05] nn step 8200, lr: 0.1.
	loss_policy_0: 0.01289
	accuracy_policy_0: 0.96797
	loss_value_0: 0.18085
	loss_policy_1: 0.00274
	accuracy_policy_1: 0.96512
	loss_value_1: 0.03624
	loss_reward_1: 0.00102
	loss_policy_2: 0.0028
	accuracy_policy_2: 0.96328
	loss_value_2: 0.03666
	loss_reward_2: 0.00232
	loss_policy_3: 0.00301
	accuracy_policy_3: 0.9598
	loss_value_3: 0.03709
	loss_reward_3: 0.00206
	loss_policy_4: 0.00324
	accuracy_policy_4: 0.95707
	loss_value_4: 0.03752
	loss_reward_4: 0.00212
	loss_policy_5: 0.00309
	accuracy_policy_5: 0.9593
	loss_value_5: 0.03871
	loss_reward_5: 0.0035
	loss_policy: 0.02778
	loss_value: 0.36707
	loss_reward: 0.01102
Optimization_Done 8200
[2025-05-08 05:53:14] [command] train weight_iter_8200.pkl 23 42
[2025-05-08 05:53:21] nn step 8250, lr: 0.1.
	loss_policy_0: 0.01268
	accuracy_policy_0: 0.96816
	loss_value_0: 0.1864
	loss_policy_1: 0.0028
	accuracy_policy_1: 0.965
	loss_value_1: 0.03732
	loss_reward_1: 0.00101
	loss_policy_2: 0.00291
	accuracy_policy_2: 0.96336
	loss_value_2: 0.03785
	loss_reward_2: 0.00239
	loss_policy_3: 0.00295
	accuracy_policy_3: 0.96184
	loss_value_3: 0.03823
	loss_reward_3: 0.00205
	loss_policy_4: 0.00314
	accuracy_policy_4: 0.95938
	loss_value_4: 0.03864
	loss_reward_4: 0.00216
	loss_policy_5: 0.00306
	accuracy_policy_5: 0.96027
	loss_value_5: 0.03973
	loss_reward_5: 0.00355
	loss_policy: 0.02754
	loss_value: 0.37817
	loss_reward: 0.01116
[2025-05-08 05:53:29] nn step 8300, lr: 0.1.
	loss_policy_0: 0.01329
	accuracy_policy_0: 0.96672
	loss_value_0: 0.18459
	loss_policy_1: 0.00288
	accuracy_policy_1: 0.96383
	loss_value_1: 0.03703
	loss_reward_1: 0.00096
	loss_policy_2: 0.00299
	accuracy_policy_2: 0.96113
	loss_value_2: 0.03764
	loss_reward_2: 0.00243
	loss_policy_3: 0.00317
	accuracy_policy_3: 0.95816
	loss_value_3: 0.03823
	loss_reward_3: 0.0021
	loss_policy_4: 0.00328
	accuracy_policy_4: 0.9568
	loss_value_4: 0.03869
	loss_reward_4: 0.00214
	loss_policy_5: 0.00316
	accuracy_policy_5: 0.95754
	loss_value_5: 0.0396
	loss_reward_5: 0.0035
	loss_policy: 0.02878
	loss_value: 0.37577
	loss_reward: 0.01112
[2025-05-08 05:53:37] nn step 8350, lr: 0.1.
	loss_policy_0: 0.01202
	accuracy_policy_0: 0.9677
	loss_value_0: 0.16943
	loss_policy_1: 0.00266
	accuracy_policy_1: 0.96465
	loss_value_1: 0.03412
	loss_reward_1: 0.00093
	loss_policy_2: 0.00267
	accuracy_policy_2: 0.96289
	loss_value_2: 0.03473
	loss_reward_2: 0.0022
	loss_policy_3: 0.00279
	accuracy_policy_3: 0.96031
	loss_value_3: 0.03504
	loss_reward_3: 0.00188
	loss_policy_4: 0.00297
	accuracy_policy_4: 0.95859
	loss_value_4: 0.03539
	loss_reward_4: 0.00191
	loss_policy_5: 0.00279
	accuracy_policy_5: 0.96016
	loss_value_5: 0.03635
	loss_reward_5: 0.00327
	loss_policy: 0.02591
	loss_value: 0.34504
	loss_reward: 0.01019
[2025-05-08 05:53:44] nn step 8400, lr: 0.1.
	loss_policy_0: 0.01264
	accuracy_policy_0: 0.96895
	loss_value_0: 0.18315
	loss_policy_1: 0.0027
	accuracy_policy_1: 0.96594
	loss_value_1: 0.03664
	loss_reward_1: 0.00104
	loss_policy_2: 0.00277
	accuracy_policy_2: 0.96309
	loss_value_2: 0.03721
	loss_reward_2: 0.00238
	loss_policy_3: 0.00296
	accuracy_policy_3: 0.96129
	loss_value_3: 0.03776
	loss_reward_3: 0.00204
	loss_policy_4: 0.00321
	accuracy_policy_4: 0.9584
	loss_value_4: 0.03793
	loss_reward_4: 0.0022
	loss_policy_5: 0.00303
	accuracy_policy_5: 0.95996
	loss_value_5: 0.03904
	loss_reward_5: 0.00362
	loss_policy: 0.0273
	loss_value: 0.37173
	loss_reward: 0.01128
Optimization_Done 8400
[2025-05-08 05:54:54] [command] train weight_iter_8400.pkl 24 43
[2025-05-08 05:55:03] nn step 8450, lr: 0.1.
	loss_policy_0: 0.01113
	accuracy_policy_0: 0.96707
	loss_value_0: 0.15934
	loss_policy_1: 0.00237
	accuracy_policy_1: 0.9634
	loss_value_1: 0.03202
	loss_reward_1: 0.001
	loss_policy_2: 0.0025
	accuracy_policy_2: 0.96219
	loss_value_2: 0.03258
	loss_reward_2: 0.0025
	loss_policy_3: 0.0026
	accuracy_policy_3: 0.95984
	loss_value_3: 0.03295
	loss_reward_3: 0.00196
	loss_policy_4: 0.00278
	accuracy_policy_4: 0.95762
	loss_value_4: 0.03317
	loss_reward_4: 0.00199
	loss_policy_5: 0.00268
	accuracy_policy_5: 0.95926
	loss_value_5: 0.0343
	loss_reward_5: 0.00335
	loss_policy: 0.02407
	loss_value: 0.32436
	loss_reward: 0.0108
[2025-05-08 05:55:09] nn step 8500, lr: 0.1.
	loss_policy_0: 0.01209
	accuracy_policy_0: 0.9668
	loss_value_0: 0.17268
	loss_policy_1: 0.00268
	accuracy_policy_1: 0.96289
	loss_value_1: 0.03466
	loss_reward_1: 0.00099
	loss_policy_2: 0.00271
	accuracy_policy_2: 0.96266
	loss_value_2: 0.03515
	loss_reward_2: 0.00227
	loss_policy_3: 0.00303
	accuracy_policy_3: 0.95902
	loss_value_3: 0.03551
	loss_reward_3: 0.00195
	loss_policy_4: 0.00311
	accuracy_policy_4: 0.95633
	loss_value_4: 0.03587
	loss_reward_4: 0.00204
	loss_policy_5: 0.00293
	accuracy_policy_5: 0.95805
	loss_value_5: 0.03679
	loss_reward_5: 0.00333
	loss_policy: 0.02654
	loss_value: 0.35066
	loss_reward: 0.01058
[2025-05-08 05:55:18] nn step 8550, lr: 0.1.
	loss_policy_0: 0.01393
	accuracy_policy_0: 0.96621
	loss_value_0: 0.18788
	loss_policy_1: 0.00303
	accuracy_policy_1: 0.96238
	loss_value_1: 0.03791
	loss_reward_1: 0.0011
	loss_policy_2: 0.00304
	accuracy_policy_2: 0.96062
	loss_value_2: 0.03821
	loss_reward_2: 0.00247
	loss_policy_3: 0.00325
	accuracy_policy_3: 0.95766
	loss_value_3: 0.03865
	loss_reward_3: 0.00215
	loss_policy_4: 0.00344
	accuracy_policy_4: 0.95305
	loss_value_4: 0.0392
	loss_reward_4: 0.00222
	loss_policy_5: 0.00328
	accuracy_policy_5: 0.95609
	loss_value_5: 0.04039
	loss_reward_5: 0.00359
	loss_policy: 0.02996
	loss_value: 0.38224
	loss_reward: 0.01153
[2025-05-08 05:55:26] nn step 8600, lr: 0.1.
	loss_policy_0: 0.01277
	accuracy_policy_0: 0.96656
	loss_value_0: 0.17642
	loss_policy_1: 0.00279
	accuracy_policy_1: 0.96312
	loss_value_1: 0.03547
	loss_reward_1: 0.00104
	loss_policy_2: 0.00284
	accuracy_policy_2: 0.96141
	loss_value_2: 0.03577
	loss_reward_2: 0.00238
	loss_policy_3: 0.00307
	accuracy_policy_3: 0.95746
	loss_value_3: 0.03633
	loss_reward_3: 0.00201
	loss_policy_4: 0.0033
	accuracy_policy_4: 0.95508
	loss_value_4: 0.03671
	loss_reward_4: 0.00213
	loss_policy_5: 0.00306
	accuracy_policy_5: 0.9575
	loss_value_5: 0.03766
	loss_reward_5: 0.00352
	loss_policy: 0.02783
	loss_value: 0.35835
	loss_reward: 0.01108
Optimization_Done 8600
[2025-05-08 05:56:32] [command] train weight_iter_8600.pkl 25 44
[2025-05-08 05:56:40] nn step 8650, lr: 0.1.
	loss_policy_0: 0.0112
	accuracy_policy_0: 0.97035
	loss_value_0: 0.16947
	loss_policy_1: 0.00261
	accuracy_policy_1: 0.96465
	loss_value_1: 0.03409
	loss_reward_1: 0.00093
	loss_policy_2: 0.00257
	accuracy_policy_2: 0.96363
	loss_value_2: 0.03456
	loss_reward_2: 0.00217
	loss_policy_3: 0.00271
	accuracy_policy_3: 0.96262
	loss_value_3: 0.03497
	loss_reward_3: 0.0019
	loss_policy_4: 0.00293
	accuracy_policy_4: 0.95887
	loss_value_4: 0.03528
	loss_reward_4: 0.00194
	loss_policy_5: 0.00278
	accuracy_policy_5: 0.96023
	loss_value_5: 0.0363
	loss_reward_5: 0.00328
	loss_policy: 0.02479
	loss_value: 0.34467
	loss_reward: 0.01022
[2025-05-08 05:56:49] nn step 8700, lr: 0.1.
	loss_policy_0: 0.01279
	accuracy_policy_0: 0.96738
	loss_value_0: 0.17194
	loss_policy_1: 0.0027
	accuracy_policy_1: 0.96488
	loss_value_1: 0.03465
	loss_reward_1: 0.00093
	loss_policy_2: 0.00281
	accuracy_policy_2: 0.96105
	loss_value_2: 0.03528
	loss_reward_2: 0.00229
	loss_policy_3: 0.00301
	accuracy_policy_3: 0.95949
	loss_value_3: 0.03581
	loss_reward_3: 0.00198
	loss_policy_4: 0.00323
	accuracy_policy_4: 0.95594
	loss_value_4: 0.03595
	loss_reward_4: 0.00203
	loss_policy_5: 0.00298
	accuracy_policy_5: 0.95895
	loss_value_5: 0.03678
	loss_reward_5: 0.0034
	loss_policy: 0.02752
	loss_value: 0.35041
	loss_reward: 0.01063
[2025-05-08 05:56:57] nn step 8750, lr: 0.1.
	loss_policy_0: 0.01274
	accuracy_policy_0: 0.96609
	loss_value_0: 0.17363
	loss_policy_1: 0.00269
	accuracy_policy_1: 0.96133
	loss_value_1: 0.03485
	loss_reward_1: 0.00094
	loss_policy_2: 0.00281
	accuracy_policy_2: 0.96141
	loss_value_2: 0.03536
	loss_reward_2: 0.0023
	loss_policy_3: 0.00301
	accuracy_policy_3: 0.95859
	loss_value_3: 0.03565
	loss_reward_3: 0.00192
	loss_policy_4: 0.00316
	accuracy_policy_4: 0.95594
	loss_value_4: 0.03598
	loss_reward_4: 0.00212
	loss_policy_5: 0.00296
	accuracy_policy_5: 0.95828
	loss_value_5: 0.03703
	loss_reward_5: 0.00342
	loss_policy: 0.02737
	loss_value: 0.3525
	loss_reward: 0.01069
[2025-05-08 05:57:03] nn step 8800, lr: 0.1.
	loss_policy_0: 0.01247
	accuracy_policy_0: 0.96676
	loss_value_0: 0.17184
	loss_policy_1: 0.00272
	accuracy_policy_1: 0.96355
	loss_value_1: 0.0346
	loss_reward_1: 0.00095
	loss_policy_2: 0.00274
	accuracy_policy_2: 0.96156
	loss_value_2: 0.03507
	loss_reward_2: 0.00231
	loss_policy_3: 0.00285
	accuracy_policy_3: 0.95922
	loss_value_3: 0.03538
	loss_reward_3: 0.00193
	loss_policy_4: 0.00301
	accuracy_policy_4: 0.95738
	loss_value_4: 0.03584
	loss_reward_4: 0.00201
	loss_policy_5: 0.00304
	accuracy_policy_5: 0.95844
	loss_value_5: 0.03679
	loss_reward_5: 0.00341
	loss_policy: 0.02682
	loss_value: 0.34953
	loss_reward: 0.01061
Optimization_Done 8800
[2025-05-08 05:58:14] [command] train weight_iter_8800.pkl 26 45
[2025-05-08 05:58:23] nn step 8850, lr: 0.1.
	loss_policy_0: 0.01299
	accuracy_policy_0: 0.96648
	loss_value_0: 0.17633
	loss_policy_1: 0.00283
	accuracy_policy_1: 0.96355
	loss_value_1: 0.03545
	loss_reward_1: 0.00095
	loss_policy_2: 0.00293
	accuracy_policy_2: 0.96102
	loss_value_2: 0.03582
	loss_reward_2: 0.0023
	loss_policy_3: 0.00313
	accuracy_policy_3: 0.95805
	loss_value_3: 0.03632
	loss_reward_3: 0.00197
	loss_policy_4: 0.00318
	accuracy_policy_4: 0.95492
	loss_value_4: 0.03661
	loss_reward_4: 0.00207
	loss_policy_5: 0.00307
	accuracy_policy_5: 0.95938
	loss_value_5: 0.03767
	loss_reward_5: 0.00339
	loss_policy: 0.02814
	loss_value: 0.3582
	loss_reward: 0.01068
[2025-05-08 05:58:29] nn step 8900, lr: 0.1.
	loss_policy_0: 0.0123
	accuracy_policy_0: 0.96801
	loss_value_0: 0.17632
	loss_policy_1: 0.00262
	accuracy_policy_1: 0.965
	loss_value_1: 0.03559
	loss_reward_1: 0.00104
	loss_policy_2: 0.00276
	accuracy_policy_2: 0.96328
	loss_value_2: 0.03602
	loss_reward_2: 0.00232
	loss_policy_3: 0.0029
	accuracy_policy_3: 0.96031
	loss_value_3: 0.0366
	loss_reward_3: 0.0019
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.95676
	loss_value_4: 0.03691
	loss_reward_4: 0.00211
	loss_policy_5: 0.00308
	accuracy_policy_5: 0.95875
	loss_value_5: 0.03794
	loss_reward_5: 0.00345
	loss_policy: 0.02687
	loss_value: 0.35939
	loss_reward: 0.01082
[2025-05-08 05:58:37] nn step 8950, lr: 0.1.
	loss_policy_0: 0.01244
	accuracy_policy_0: 0.96645
	loss_value_0: 0.17182
	loss_policy_1: 0.00271
	accuracy_policy_1: 0.96188
	loss_value_1: 0.0345
	loss_reward_1: 0.00098
	loss_policy_2: 0.00265
	accuracy_policy_2: 0.96242
	loss_value_2: 0.03494
	loss_reward_2: 0.00232
	loss_policy_3: 0.00283
	accuracy_policy_3: 0.95801
	loss_value_3: 0.03513
	loss_reward_3: 0.00192
	loss_policy_4: 0.00296
	accuracy_policy_4: 0.95621
	loss_value_4: 0.03544
	loss_reward_4: 0.00205
	loss_policy_5: 0.00281
	accuracy_policy_5: 0.95805
	loss_value_5: 0.03648
	loss_reward_5: 0.00342
	loss_policy: 0.02639
	loss_value: 0.34831
	loss_reward: 0.01069
[2025-05-08 05:58:45] nn step 9000, lr: 0.1.
	loss_policy_0: 0.01257
	accuracy_policy_0: 0.96742
	loss_value_0: 0.17156
	loss_policy_1: 0.00285
	accuracy_policy_1: 0.96273
	loss_value_1: 0.03465
	loss_reward_1: 0.00097
	loss_policy_2: 0.00288
	accuracy_policy_2: 0.96113
	loss_value_2: 0.03513
	loss_reward_2: 0.00223
	loss_policy_3: 0.00307
	accuracy_policy_3: 0.95801
	loss_value_3: 0.03558
	loss_reward_3: 0.00213
	loss_policy_4: 0.00315
	accuracy_policy_4: 0.95539
	loss_value_4: 0.03574
	loss_reward_4: 0.00213
	loss_policy_5: 0.00283
	accuracy_policy_5: 0.95855
	loss_value_5: 0.0367
	loss_reward_5: 0.00349
	loss_policy: 0.02735
	loss_value: 0.34936
	loss_reward: 0.01095
Optimization_Done 9000
[2025-05-08 05:59:53] [command] train weight_iter_9000.pkl 27 46
[2025-05-08 06:00:02] nn step 9050, lr: 0.1.
	loss_policy_0: 0.01267
	accuracy_policy_0: 0.9659
	loss_value_0: 0.17379
	loss_policy_1: 0.00268
	accuracy_policy_1: 0.96277
	loss_value_1: 0.03498
	loss_reward_1: 0.00104
	loss_policy_2: 0.00281
	accuracy_policy_2: 0.95957
	loss_value_2: 0.03542
	loss_reward_2: 0.00234
	loss_policy_3: 0.00295
	accuracy_policy_3: 0.95879
	loss_value_3: 0.03575
	loss_reward_3: 0.00193
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.95414
	loss_value_4: 0.03587
	loss_reward_4: 0.00221
	loss_policy_5: 0.00305
	accuracy_policy_5: 0.95582
	loss_value_5: 0.03686
	loss_reward_5: 0.00348
	loss_policy: 0.02737
	loss_value: 0.35267
	loss_reward: 0.011
[2025-05-08 06:00:10] nn step 9100, lr: 0.1.
	loss_policy_0: 0.01384
	accuracy_policy_0: 0.96543
	loss_value_0: 0.18454
	loss_policy_1: 0.00299
	accuracy_policy_1: 0.9623
	loss_value_1: 0.03717
	loss_reward_1: 0.00103
	loss_policy_2: 0.00308
	accuracy_policy_2: 0.95961
	loss_value_2: 0.03761
	loss_reward_2: 0.00252
	loss_policy_3: 0.00327
	accuracy_policy_3: 0.95711
	loss_value_3: 0.03794
	loss_reward_3: 0.00223
	loss_policy_4: 0.00335
	accuracy_policy_4: 0.95523
	loss_value_4: 0.03839
	loss_reward_4: 0.00231
	loss_policy_5: 0.00321
	accuracy_policy_5: 0.95496
	loss_value_5: 0.03949
	loss_reward_5: 0.00368
	loss_policy: 0.02974
	loss_value: 0.37515
	loss_reward: 0.01177
[2025-05-08 06:00:19] nn step 9150, lr: 0.1.
	loss_policy_0: 0.01226
	accuracy_policy_0: 0.96715
	loss_value_0: 0.17454
	loss_policy_1: 0.00265
	accuracy_policy_1: 0.96383
	loss_value_1: 0.03513
	loss_reward_1: 0.001
	loss_policy_2: 0.00288
	accuracy_policy_2: 0.96125
	loss_value_2: 0.03542
	loss_reward_2: 0.00251
	loss_policy_3: 0.00283
	accuracy_policy_3: 0.95945
	loss_value_3: 0.03592
	loss_reward_3: 0.00194
	loss_policy_4: 0.00306
	accuracy_policy_4: 0.95602
	loss_value_4: 0.03615
	loss_reward_4: 0.00205
	loss_policy_5: 0.00288
	accuracy_policy_5: 0.95996
	loss_value_5: 0.03698
	loss_reward_5: 0.00365
	loss_policy: 0.02656
	loss_value: 0.35413
	loss_reward: 0.01114
[2025-05-08 06:00:25] nn step 9200, lr: 0.1.
	loss_policy_0: 0.01324
	accuracy_policy_0: 0.96629
	loss_value_0: 0.18517
	loss_policy_1: 0.00287
	accuracy_policy_1: 0.96352
	loss_value_1: 0.03716
	loss_reward_1: 0.00108
	loss_policy_2: 0.00301
	accuracy_policy_2: 0.96043
	loss_value_2: 0.03758
	loss_reward_2: 0.00258
	loss_policy_3: 0.00304
	accuracy_policy_3: 0.95902
	loss_value_3: 0.03798
	loss_reward_3: 0.00232
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.9559
	loss_value_4: 0.03811
	loss_reward_4: 0.00228
	loss_policy_5: 0.00304
	accuracy_policy_5: 0.95672
	loss_value_5: 0.0393
	loss_reward_5: 0.00375
	loss_policy: 0.0284
	loss_value: 0.37531
	loss_reward: 0.01201
Optimization_Done 9200
[2025-05-08 06:01:35] [command] train weight_iter_9200.pkl 28 47
[2025-05-08 06:01:44] nn step 9250, lr: 0.1.
	loss_policy_0: 0.00838
	accuracy_policy_0: 0.98004
	loss_value_0: 0.17683
	loss_policy_1: 0.00186
	accuracy_policy_1: 0.97566
	loss_value_1: 0.03561
	loss_reward_1: 0.0009
	loss_policy_2: 0.00198
	accuracy_policy_2: 0.97527
	loss_value_2: 0.03597
	loss_reward_2: 0.0025
	loss_policy_3: 0.00209
	accuracy_policy_3: 0.97258
	loss_value_3: 0.03625
	loss_reward_3: 0.0019
	loss_policy_4: 0.00219
	accuracy_policy_4: 0.97117
	loss_value_4: 0.03659
	loss_reward_4: 0.00201
	loss_policy_5: 0.00209
	accuracy_policy_5: 0.97344
	loss_value_5: 0.03749
	loss_reward_5: 0.00356
	loss_policy: 0.01861
	loss_value: 0.35875
	loss_reward: 0.01085
[2025-05-08 06:01:50] nn step 9300, lr: 0.1.
	loss_policy_0: 0.00822
	accuracy_policy_0: 0.98105
	loss_value_0: 0.16933
	loss_policy_1: 0.00175
	accuracy_policy_1: 0.97703
	loss_value_1: 0.03408
	loss_reward_1: 0.00094
	loss_policy_2: 0.00192
	accuracy_policy_2: 0.97539
	loss_value_2: 0.03439
	loss_reward_2: 0.00227
	loss_policy_3: 0.00209
	accuracy_policy_3: 0.97348
	loss_value_3: 0.03475
	loss_reward_3: 0.00207
	loss_policy_4: 0.0022
	accuracy_policy_4: 0.97277
	loss_value_4: 0.03507
	loss_reward_4: 0.00194
	loss_policy_5: 0.0021
	accuracy_policy_5: 0.97312
	loss_value_5: 0.03603
	loss_reward_5: 0.00335
	loss_policy: 0.01827
	loss_value: 0.34366
	loss_reward: 0.01057
[2025-05-08 06:01:59] nn step 9350, lr: 0.1.
	loss_policy_0: 0.00786
	accuracy_policy_0: 0.98023
	loss_value_0: 0.16905
	loss_policy_1: 0.00179
	accuracy_policy_1: 0.97758
	loss_value_1: 0.03404
	loss_reward_1: 0.00088
	loss_policy_2: 0.00182
	accuracy_policy_2: 0.97699
	loss_value_2: 0.03423
	loss_reward_2: 0.00227
	loss_policy_3: 0.00197
	accuracy_policy_3: 0.97434
	loss_value_3: 0.03464
	loss_reward_3: 0.0018
	loss_policy_4: 0.00207
	accuracy_policy_4: 0.97355
	loss_value_4: 0.03491
	loss_reward_4: 0.00199
	loss_policy_5: 0.00197
	accuracy_policy_5: 0.97465
	loss_value_5: 0.0358
	loss_reward_5: 0.00345
	loss_policy: 0.01746
	loss_value: 0.34266
	loss_reward: 0.0104
[2025-05-08 06:02:07] nn step 9400, lr: 0.1.
	loss_policy_0: 0.00808
	accuracy_policy_0: 0.97957
	loss_value_0: 0.16725
	loss_policy_1: 0.00173
	accuracy_policy_1: 0.97895
	loss_value_1: 0.03368
	loss_reward_1: 0.00087
	loss_policy_2: 0.00186
	accuracy_policy_2: 0.97691
	loss_value_2: 0.03411
	loss_reward_2: 0.0023
	loss_policy_3: 0.00194
	accuracy_policy_3: 0.97582
	loss_value_3: 0.0345
	loss_reward_3: 0.00182
	loss_policy_4: 0.00199
	accuracy_policy_4: 0.97402
	loss_value_4: 0.0347
	loss_reward_4: 0.00185
	loss_policy_5: 0.00192
	accuracy_policy_5: 0.97516
	loss_value_5: 0.03571
	loss_reward_5: 0.00337
	loss_policy: 0.01751
	loss_value: 0.33995
	loss_reward: 0.0102
Optimization_Done 9400
[2025-05-08 06:03:14] [command] train weight_iter_9400.pkl 29 48
[2025-05-08 06:03:22] nn step 9450, lr: 0.1.
	loss_policy_0: 0.00672
	accuracy_policy_0: 0.98406
	loss_value_0: 0.16936
	loss_policy_1: 0.0015
	accuracy_policy_1: 0.98219
	loss_value_1: 0.03403
	loss_reward_1: 0.00083
	loss_policy_2: 0.00153
	accuracy_policy_2: 0.98109
	loss_value_2: 0.03433
	loss_reward_2: 0.00222
	loss_policy_3: 0.00172
	accuracy_policy_3: 0.97895
	loss_value_3: 0.03468
	loss_reward_3: 0.00189
	loss_policy_4: 0.00181
	accuracy_policy_4: 0.9775
	loss_value_4: 0.03479
	loss_reward_4: 0.00191
	loss_policy_5: 0.00169
	accuracy_policy_5: 0.9791
	loss_value_5: 0.03579
	loss_reward_5: 0.00335
	loss_policy: 0.01497
	loss_value: 0.34298
	loss_reward: 0.0102
[2025-05-08 06:03:31] nn step 9500, lr: 0.1.
	loss_policy_0: 0.00629
	accuracy_policy_0: 0.9841
	loss_value_0: 0.1775
	loss_policy_1: 0.00143
	accuracy_policy_1: 0.98309
	loss_value_1: 0.03581
	loss_reward_1: 0.00089
	loss_policy_2: 0.00147
	accuracy_policy_2: 0.98129
	loss_value_2: 0.036
	loss_reward_2: 0.0023
	loss_policy_3: 0.00157
	accuracy_policy_3: 0.9809
	loss_value_3: 0.03643
	loss_reward_3: 0.00201
	loss_policy_4: 0.00161
	accuracy_policy_4: 0.97961
	loss_value_4: 0.03652
	loss_reward_4: 0.00198
	loss_policy_5: 0.00154
	accuracy_policy_5: 0.97984
	loss_value_5: 0.03756
	loss_reward_5: 0.00347
	loss_policy: 0.01391
	loss_value: 0.35983
	loss_reward: 0.01064
[2025-05-08 06:03:37] nn step 9550, lr: 0.1.
	loss_policy_0: 0.00647
	accuracy_policy_0: 0.98312
	loss_value_0: 0.17312
	loss_policy_1: 0.00143
	accuracy_policy_1: 0.9816
	loss_value_1: 0.03499
	loss_reward_1: 0.00086
	loss_policy_2: 0.00151
	accuracy_policy_2: 0.98066
	loss_value_2: 0.03531
	loss_reward_2: 0.00231
	loss_policy_3: 0.00151
	accuracy_policy_3: 0.98059
	loss_value_3: 0.03559
	loss_reward_3: 0.00193
	loss_policy_4: 0.00175
	accuracy_policy_4: 0.97773
	loss_value_4: 0.03594
	loss_reward_4: 0.00198
	loss_policy_5: 0.00166
	accuracy_policy_5: 0.97859
	loss_value_5: 0.03691
	loss_reward_5: 0.00343
	loss_policy: 0.01434
	loss_value: 0.35185
	loss_reward: 0.01051
[2025-05-08 06:03:45] nn step 9600, lr: 0.1.
	loss_policy_0: 0.00704
	accuracy_policy_0: 0.98301
	loss_value_0: 0.169
	loss_policy_1: 0.00146
	accuracy_policy_1: 0.98109
	loss_value_1: 0.03415
	loss_reward_1: 0.00089
	loss_policy_2: 0.00159
	accuracy_policy_2: 0.97961
	loss_value_2: 0.03449
	loss_reward_2: 0.00236
	loss_policy_3: 0.0017
	accuracy_policy_3: 0.97812
	loss_value_3: 0.03477
	loss_reward_3: 0.00184
	loss_policy_4: 0.00172
	accuracy_policy_4: 0.97789
	loss_value_4: 0.03481
	loss_reward_4: 0.00198
	loss_policy_5: 0.00168
	accuracy_policy_5: 0.97902
	loss_value_5: 0.0359
	loss_reward_5: 0.00359
	loss_policy: 0.01518
	loss_value: 0.34312
	loss_reward: 0.01066
Optimization_Done 9600
[2025-05-08 06:04:56] [command] train weight_iter_9600.pkl 30 49
[2025-05-08 06:05:03] nn step 9650, lr: 0.1.
	loss_policy_0: 0.00653
	accuracy_policy_0: 0.98281
	loss_value_0: 0.17607
	loss_policy_1: 0.0015
	accuracy_policy_1: 0.98148
	loss_value_1: 0.03553
	loss_reward_1: 0.0009
	loss_policy_2: 0.00157
	accuracy_policy_2: 0.98043
	loss_value_2: 0.03583
	loss_reward_2: 0.00238
	loss_policy_3: 0.00165
	accuracy_policy_3: 0.97977
	loss_value_3: 0.03612
	loss_reward_3: 0.00189
	loss_policy_4: 0.00175
	accuracy_policy_4: 0.97828
	loss_value_4: 0.03621
	loss_reward_4: 0.00195
	loss_policy_5: 0.00166
	accuracy_policy_5: 0.97855
	loss_value_5: 0.03715
	loss_reward_5: 0.0036
	loss_policy: 0.01466
	loss_value: 0.35692
	loss_reward: 0.01072
[2025-05-08 06:05:10] nn step 9700, lr: 0.1.
	loss_policy_0: 0.00633
	accuracy_policy_0: 0.98223
	loss_value_0: 0.16223
	loss_policy_1: 0.00138
	accuracy_policy_1: 0.98023
	loss_value_1: 0.03268
	loss_reward_1: 0.00083
	loss_policy_2: 0.0015
	accuracy_policy_2: 0.97926
	loss_value_2: 0.0328
	loss_reward_2: 0.00233
	loss_policy_3: 0.00161
	accuracy_policy_3: 0.97816
	loss_value_3: 0.03305
	loss_reward_3: 0.00173
	loss_policy_4: 0.00166
	accuracy_policy_4: 0.97746
	loss_value_4: 0.03323
	loss_reward_4: 0.00189
	loss_policy_5: 0.00162
	accuracy_policy_5: 0.97797
	loss_value_5: 0.03416
	loss_reward_5: 0.00336
	loss_policy: 0.01409
	loss_value: 0.32816
	loss_reward: 0.01014
[2025-05-08 06:05:19] nn step 9750, lr: 0.1.
	loss_policy_0: 0.00589
	accuracy_policy_0: 0.98387
	loss_value_0: 0.16412
	loss_policy_1: 0.00132
	accuracy_policy_1: 0.98227
	loss_value_1: 0.03311
	loss_reward_1: 0.00082
	loss_policy_2: 0.00145
	accuracy_policy_2: 0.98137
	loss_value_2: 0.03348
	loss_reward_2: 0.0022
	loss_policy_3: 0.00152
	accuracy_policy_3: 0.97918
	loss_value_3: 0.03365
	loss_reward_3: 0.00182
	loss_policy_4: 0.00158
	accuracy_policy_4: 0.97984
	loss_value_4: 0.03386
	loss_reward_4: 0.00188
	loss_policy_5: 0.00177
	accuracy_policy_5: 0.97891
	loss_value_5: 0.03484
	loss_reward_5: 0.0033
	loss_policy: 0.01352
	loss_value: 0.33307
	loss_reward: 0.01003
[2025-05-08 06:05:27] nn step 9800, lr: 0.1.
	loss_policy_0: 0.00633
	accuracy_policy_0: 0.98344
	loss_value_0: 0.1706
	loss_policy_1: 0.00136
	accuracy_policy_1: 0.98234
	loss_value_1: 0.03438
	loss_reward_1: 0.00078
	loss_policy_2: 0.00139
	accuracy_policy_2: 0.98094
	loss_value_2: 0.03455
	loss_reward_2: 0.00236
	loss_policy_3: 0.00144
	accuracy_policy_3: 0.98086
	loss_value_3: 0.03477
	loss_reward_3: 0.00195
	loss_policy_4: 0.00154
	accuracy_policy_4: 0.97875
	loss_value_4: 0.03487
	loss_reward_4: 0.00187
	loss_policy_5: 0.00157
	accuracy_policy_5: 0.97891
	loss_value_5: 0.03582
	loss_reward_5: 0.00348
	loss_policy: 0.01363
	loss_value: 0.34498
	loss_reward: 0.01044
Optimization_Done 9800
[2025-05-08 06:06:35] [command] train weight_iter_9800.pkl 31 50
[2025-05-08 06:06:44] nn step 9850, lr: 0.1.
	loss_policy_0: 0.00151
	accuracy_policy_0: 0.99645
	loss_value_0: 0.16741
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.99621
	loss_value_1: 0.03377
	loss_reward_1: 0.00076
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.9959
	loss_value_2: 0.03395
	loss_reward_2: 0.00218
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99633
	loss_value_3: 0.03419
	loss_reward_3: 0.00171
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99637
	loss_value_4: 0.03445
	loss_reward_4: 0.00186
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99539
	loss_value_5: 0.03524
	loss_reward_5: 0.00323
	loss_policy: 0.00357
	loss_value: 0.33901
	loss_reward: 0.00974
[2025-05-08 06:06:52] nn step 9900, lr: 0.1.
	loss_policy_0: 0.00154
	accuracy_policy_0: 0.99578
	loss_value_0: 0.16402
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99559
	loss_value_1: 0.03316
	loss_reward_1: 0.00071
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99535
	loss_value_2: 0.03344
	loss_reward_2: 0.00225
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99523
	loss_value_3: 0.03363
	loss_reward_3: 0.00172
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99504
	loss_value_4: 0.03374
	loss_reward_4: 0.00175
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99453
	loss_value_5: 0.03458
	loss_reward_5: 0.00333
	loss_policy: 0.00362
	loss_value: 0.33257
	loss_reward: 0.00977
[2025-05-08 06:06:59] nn step 9950, lr: 0.1.
	loss_policy_0: 0.00138
	accuracy_policy_0: 0.9968
	loss_value_0: 0.16819
	loss_policy_1: 0.00038
	accuracy_policy_1: 0.99621
	loss_value_1: 0.03411
	loss_reward_1: 0.0007
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99539
	loss_value_2: 0.03423
	loss_reward_2: 0.00227
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99523
	loss_value_3: 0.0344
	loss_reward_3: 0.00184
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99535
	loss_value_4: 0.03442
	loss_reward_4: 0.00178
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99539
	loss_value_5: 0.03553
	loss_reward_5: 0.0034
	loss_policy: 0.00364
	loss_value: 0.34088
	loss_reward: 0.00999
[2025-05-08 06:07:07] nn step 10000, lr: 0.1.
	loss_policy_0: 0.00134
	accuracy_policy_0: 0.99602
	loss_value_0: 0.16034
	loss_policy_1: 0.00034
	accuracy_policy_1: 0.99566
	loss_value_1: 0.03253
	loss_reward_1: 0.00071
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99527
	loss_value_2: 0.03269
	loss_reward_2: 0.00219
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99508
	loss_value_3: 0.03298
	loss_reward_3: 0.00164
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.9952
	loss_value_4: 0.03303
	loss_reward_4: 0.00173
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.9952
	loss_value_5: 0.03406
	loss_reward_5: 0.00329
	loss_policy: 0.00371
	loss_value: 0.32563
	loss_reward: 0.00955
Optimization_Done 10000
[2025-05-08 06:08:15] [command] train weight_iter_10000.pkl 32 51
[2025-05-08 06:08:22] nn step 10050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16679
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.03377
	loss_reward_1: 0.00066
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.03403
	loss_reward_2: 0.00223
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99957
	loss_value_3: 0.03426
	loss_reward_3: 0.00167
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99938
	loss_value_4: 0.03433
	loss_reward_4: 0.00168
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99914
	loss_value_5: 0.03531
	loss_reward_5: 0.00323
	loss_policy: 0.00077
	loss_value: 0.33848
	loss_reward: 0.00948
[2025-05-08 06:08:30] nn step 10100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16092
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.03256
	loss_reward_1: 0.00066
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.03283
	loss_reward_2: 0.0021
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.03309
	loss_reward_3: 0.00165
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99977
	loss_value_4: 0.0332
	loss_reward_4: 0.00168
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99957
	loss_value_5: 0.03412
	loss_reward_5: 0.00325
	loss_policy: 0.00042
	loss_value: 0.32672
	loss_reward: 0.00933
[2025-05-08 06:08:39] nn step 10150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.17
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.03451
	loss_reward_1: 0.00062
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.03487
	loss_reward_2: 0.00228
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99965
	loss_value_3: 0.03521
	loss_reward_3: 0.00176
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99949
	loss_value_4: 0.03541
	loss_reward_4: 0.0018
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99938
	loss_value_5: 0.0364
	loss_reward_5: 0.00344
	loss_policy: 0.00056
	loss_value: 0.3464
	loss_reward: 0.0099
[2025-05-08 06:08:47] nn step 10200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16735
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.03381
	loss_reward_1: 0.00062
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.0343
	loss_reward_2: 0.00232
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.03454
	loss_reward_3: 0.0017
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99965
	loss_value_4: 0.03459
	loss_reward_4: 0.00179
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99961
	loss_value_5: 0.03565
	loss_reward_5: 0.00362
	loss_policy: 0.00052
	loss_value: 0.34024
	loss_reward: 0.01004
Optimization_Done 10200
[2025-05-08 06:09:54] [command] train weight_iter_10200.pkl 33 52
[2025-05-08 06:10:03] nn step 10250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16743
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.03372
	loss_reward_1: 0.00057
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.9998
	loss_value_2: 0.03386
	loss_reward_2: 0.00233
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99965
	loss_value_3: 0.03433
	loss_reward_3: 0.00176
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99953
	loss_value_4: 0.03436
	loss_reward_4: 0.00171
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.9993
	loss_value_5: 0.03556
	loss_reward_5: 0.00349
	loss_policy: 0.0007
	loss_value: 0.33926
	loss_reward: 0.00985
[2025-05-08 06:10:11] nn step 10300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16675
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.03381
	loss_reward_1: 0.00057
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.034
	loss_reward_2: 0.00231
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.03435
	loss_reward_3: 0.00176
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.0345
	loss_reward_4: 0.00177
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99977
	loss_value_5: 0.03561
	loss_reward_5: 0.00349
	loss_policy: 0.00031
	loss_value: 0.33901
	loss_reward: 0.00989
[2025-05-08 06:10:18] nn step 10350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16648
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.03368
	loss_reward_1: 0.00062
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.03408
	loss_reward_2: 0.00234
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.03435
	loss_reward_3: 0.00176
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99969
	loss_value_4: 0.03444
	loss_reward_4: 0.00178
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99945
	loss_value_5: 0.03563
	loss_reward_5: 0.00354
	loss_policy: 0.00043
	loss_value: 0.33867
	loss_reward: 0.01004
[2025-05-08 06:10:26] nn step 10400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16192
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0328
	loss_reward_1: 0.00053
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.03325
	loss_reward_2: 0.00239
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99965
	loss_value_3: 0.03361
	loss_reward_3: 0.00182
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99941
	loss_value_4: 0.03363
	loss_reward_4: 0.00168
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99922
	loss_value_5: 0.03479
	loss_reward_5: 0.00352
	loss_policy: 0.00065
	loss_value: 0.32999
	loss_reward: 0.00994
Optimization_Done 10400
[2025-05-08 06:11:35] [command] train weight_iter_10400.pkl 34 53
[2025-05-08 06:11:43] nn step 10450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.15499
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.03135
	loss_reward_1: 0.00057
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.9998
	loss_value_2: 0.03169
	loss_reward_2: 0.00212
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99977
	loss_value_3: 0.03214
	loss_reward_3: 0.00169
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99973
	loss_value_4: 0.03211
	loss_reward_4: 0.00165
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99965
	loss_value_5: 0.03323
	loss_reward_5: 0.00329
	loss_policy: 0.00056
	loss_value: 0.31552
	loss_reward: 0.00932
[2025-05-08 06:11:51] nn step 10500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1623
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.03298
	loss_reward_1: 0.00057
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99973
	loss_value_2: 0.03331
	loss_reward_2: 0.00235
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99965
	loss_value_3: 0.03369
	loss_reward_3: 0.00181
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99953
	loss_value_4: 0.03378
	loss_reward_4: 0.00184
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99938
	loss_value_5: 0.03501
	loss_reward_5: 0.00354
	loss_policy: 0.00077
	loss_value: 0.33108
	loss_reward: 0.01011
[2025-05-08 06:11:59] nn step 10550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14836
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.03022
	loss_reward_1: 0.00051
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.03051
	loss_reward_2: 0.00218
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99973
	loss_value_3: 0.03093
	loss_reward_3: 0.00172
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99961
	loss_value_4: 0.03092
	loss_reward_4: 0.00163
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99934
	loss_value_5: 0.03204
	loss_reward_5: 0.00336
	loss_policy: 0.00061
	loss_value: 0.30298
	loss_reward: 0.0094
[2025-05-08 06:12:06] nn step 10600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.15357
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0313
	loss_reward_1: 0.00052
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.03171
	loss_reward_2: 0.00222
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.03209
	loss_reward_3: 0.00164
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99961
	loss_value_4: 0.03216
	loss_reward_4: 0.00176
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99953
	loss_value_5: 0.03345
	loss_reward_5: 0.00344
	loss_policy: 0.00047
	loss_value: 0.31428
	loss_reward: 0.00958
Optimization_Done 10600
[2025-05-08 06:13:14] [command] train weight_iter_10600.pkl 35 54
[2025-05-08 06:13:23] nn step 10650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.15294
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.03089
	loss_reward_1: 0.00051
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99973
	loss_value_2: 0.03133
	loss_reward_2: 0.00225
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99957
	loss_value_3: 0.03166
	loss_reward_3: 0.00166
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99949
	loss_value_4: 0.03173
	loss_reward_4: 0.00172
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99938
	loss_value_5: 0.03297
	loss_reward_5: 0.0035
	loss_policy: 0.00082
	loss_value: 0.31152
	loss_reward: 0.00965
[2025-05-08 06:13:30] nn step 10700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.15724
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0319
	loss_reward_1: 0.00058
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0324
	loss_reward_2: 0.00238
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.0329
	loss_reward_3: 0.00176
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.0329
	loss_reward_4: 0.0018
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99961
	loss_value_5: 0.03404
	loss_reward_5: 0.00368
	loss_policy: 0.00026
	loss_value: 0.32138
	loss_reward: 0.0102
[2025-05-08 06:13:38] nn step 10750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.16146
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0327
	loss_reward_1: 0.00056
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99965
	loss_value_2: 0.03302
	loss_reward_2: 0.0025
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99941
	loss_value_3: 0.03335
	loss_reward_3: 0.00184
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99934
	loss_value_4: 0.03339
	loss_reward_4: 0.00182
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.9991
	loss_value_5: 0.03467
	loss_reward_5: 0.00394
	loss_policy: 0.001
	loss_value: 0.32859
	loss_reward: 0.01066
[2025-05-08 06:13:46] nn step 10800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1461
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02952
	loss_reward_1: 0.00047
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99961
	loss_value_2: 0.02988
	loss_reward_2: 0.00229
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99949
	loss_value_3: 0.0302
	loss_reward_3: 0.00171
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99934
	loss_value_4: 0.03023
	loss_reward_4: 0.00174
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.9993
	loss_value_5: 0.03146
	loss_reward_5: 0.00354
	loss_policy: 0.00064
	loss_value: 0.29739
	loss_reward: 0.00977
Optimization_Done 10800
[2025-05-08 06:14:58] [command] train weight_iter_10800.pkl 36 55
[2025-05-08 06:15:07] nn step 10850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14991
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.03043
	loss_reward_1: 0.00048
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.03058
	loss_reward_2: 0.0024
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.03099
	loss_reward_3: 0.00177
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99969
	loss_value_4: 0.03098
	loss_reward_4: 0.00175
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99953
	loss_value_5: 0.0323
	loss_reward_5: 0.00369
	loss_policy: 0.00044
	loss_value: 0.30519
	loss_reward: 0.01009
[2025-05-08 06:15:15] nn step 10900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14252
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02896
	loss_reward_1: 0.00044
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02914
	loss_reward_2: 0.00229
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02938
	loss_reward_3: 0.00177
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99957
	loss_value_4: 0.02938
	loss_reward_4: 0.00167
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.9993
	loss_value_5: 0.0305
	loss_reward_5: 0.00355
	loss_policy: 0.00068
	loss_value: 0.28988
	loss_reward: 0.00972
[2025-05-08 06:15:23] nn step 10950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14007
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02825
	loss_reward_1: 0.00051
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02848
	loss_reward_2: 0.00231
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99957
	loss_value_3: 0.02891
	loss_reward_3: 0.00159
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99938
	loss_value_4: 0.02895
	loss_reward_4: 0.00175
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.9991
	loss_value_5: 0.03007
	loss_reward_5: 0.00356
	loss_policy: 0.00076
	loss_value: 0.28473
	loss_reward: 0.00972
[2025-05-08 06:15:30] nn step 11000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14146
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02861
	loss_reward_1: 0.00049
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02887
	loss_reward_2: 0.0023
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.0292
	loss_reward_3: 0.00172
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99961
	loss_value_4: 0.02915
	loss_reward_4: 0.00175
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99953
	loss_value_5: 0.03031
	loss_reward_5: 0.00349
	loss_policy: 0.00052
	loss_value: 0.28761
	loss_reward: 0.00976
Optimization_Done 11000
[2025-05-08 06:16:41] [command] train weight_iter_11000.pkl 37 56
[2025-05-08 06:16:47] nn step 11050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14882
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.03005
	loss_reward_1: 0.00053
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.0302
	loss_reward_2: 0.00239
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.03049
	loss_reward_3: 0.00185
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99965
	loss_value_4: 0.03033
	loss_reward_4: 0.00185
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99949
	loss_value_5: 0.0316
	loss_reward_5: 0.00378
	loss_policy: 0.00042
	loss_value: 0.30149
	loss_reward: 0.0104
[2025-05-08 06:16:56] nn step 11100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13952
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02812
	loss_reward_1: 0.00046
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02838
	loss_reward_2: 0.00235
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02874
	loss_reward_3: 0.00174
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99934
	loss_value_4: 0.0286
	loss_reward_4: 0.00171
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99922
	loss_value_5: 0.0297
	loss_reward_5: 0.00361
	loss_policy: 0.00084
	loss_value: 0.28306
	loss_reward: 0.00988
[2025-05-08 06:17:04] nn step 11150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.141
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02849
	loss_reward_1: 0.00051
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02869
	loss_reward_2: 0.00243
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99965
	loss_value_3: 0.02895
	loss_reward_3: 0.00166
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99945
	loss_value_4: 0.02888
	loss_reward_4: 0.00182
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99938
	loss_value_5: 0.03003
	loss_reward_5: 0.0037
	loss_policy: 0.00071
	loss_value: 0.28604
	loss_reward: 0.01012
[2025-05-08 06:17:12] nn step 11200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1402
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02829
	loss_reward_1: 0.00048
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02849
	loss_reward_2: 0.00245
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99977
	loss_value_3: 0.02877
	loss_reward_3: 0.00179
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99969
	loss_value_4: 0.0286
	loss_reward_4: 0.00176
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02981
	loss_reward_5: 0.00377
	loss_policy: 0.00049
	loss_value: 0.28416
	loss_reward: 0.01025
Optimization_Done 11200
[2025-05-08 06:18:19] [command] train weight_iter_11200.pkl 38 57
[2025-05-08 06:18:28] nn step 11250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14037
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02822
	loss_reward_1: 0.00043
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02835
	loss_reward_2: 0.00244
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.0286
	loss_reward_3: 0.00178
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99965
	loss_value_4: 0.02846
	loss_reward_4: 0.00178
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99965
	loss_value_5: 0.0296
	loss_reward_5: 0.00377
	loss_policy: 0.00036
	loss_value: 0.28359
	loss_reward: 0.01019
[2025-05-08 06:18:36] nn step 11300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14008
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02811
	loss_reward_1: 0.00045
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02817
	loss_reward_2: 0.00248
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.02846
	loss_reward_3: 0.00178
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02824
	loss_reward_4: 0.00182
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99957
	loss_value_5: 0.0295
	loss_reward_5: 0.00382
	loss_policy: 0.00031
	loss_value: 0.28255
	loss_reward: 0.01035
[2025-05-08 06:18:42] nn step 11350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14422
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02905
	loss_reward_1: 0.0005
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02903
	loss_reward_2: 0.00257
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02921
	loss_reward_3: 0.00192
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02912
	loss_reward_4: 0.00184
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.0303
	loss_reward_5: 0.00391
	loss_policy: 0.00035
	loss_value: 0.29093
	loss_reward: 0.01074
[2025-05-08 06:18:51] nn step 11400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13766
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02755
	loss_reward_1: 0.00048
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99969
	loss_value_2: 0.02756
	loss_reward_2: 0.00248
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02768
	loss_reward_3: 0.00183
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99949
	loss_value_4: 0.02746
	loss_reward_4: 0.00186
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.9993
	loss_value_5: 0.02859
	loss_reward_5: 0.00387
	loss_policy: 0.00075
	loss_value: 0.2765
	loss_reward: 0.01052
Optimization_Done 11400
[2025-05-08 06:20:00] [command] train weight_iter_11400.pkl 39 58
[2025-05-08 06:20:08] nn step 11450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13219
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02634
	loss_reward_1: 0.00044
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02631
	loss_reward_2: 0.00236
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.02648
	loss_reward_3: 0.00169
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02622
	loss_reward_4: 0.00172
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99965
	loss_value_5: 0.02732
	loss_reward_5: 0.00373
	loss_policy: 0.0002
	loss_value: 0.26485
	loss_reward: 0.00994
[2025-05-08 06:20:16] nn step 11500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1375
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02746
	loss_reward_1: 0.00045
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02734
	loss_reward_2: 0.00251
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.02752
	loss_reward_3: 0.00179
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99973
	loss_value_4: 0.0272
	loss_reward_4: 0.00182
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99961
	loss_value_5: 0.02831
	loss_reward_5: 0.00396
	loss_policy: 0.0004
	loss_value: 0.27533
	loss_reward: 0.01053
[2025-05-08 06:20:24] nn step 11550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13914
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02779
	loss_reward_1: 0.00047
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.02759
	loss_reward_2: 0.00259
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99965
	loss_value_3: 0.02772
	loss_reward_3: 0.00189
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.9993
	loss_value_4: 0.02735
	loss_reward_4: 0.00182
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99961
	loss_value_5: 0.02856
	loss_reward_5: 0.00401
	loss_policy: 0.00039
	loss_value: 0.27815
	loss_reward: 0.0108
[2025-05-08 06:20:32] nn step 11600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13618
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02717
	loss_reward_1: 0.00041
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02705
	loss_reward_2: 0.00259
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02702
	loss_reward_3: 0.00184
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99918
	loss_value_4: 0.02662
	loss_reward_4: 0.00189
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99949
	loss_value_5: 0.0278
	loss_reward_5: 0.00407
	loss_policy: 0.00055
	loss_value: 0.27185
	loss_reward: 0.01081
Optimization_Done 11600
[2025-05-08 06:21:40] [command] train weight_iter_11600.pkl 40 59
[2025-05-08 06:21:49] nn step 11650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13817
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02745
	loss_reward_1: 0.00049
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02718
	loss_reward_2: 0.00258
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02718
	loss_reward_3: 0.00189
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02668
	loss_reward_4: 0.00182
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99965
	loss_value_5: 0.02784
	loss_reward_5: 0.00413
	loss_policy: 0.00035
	loss_value: 0.2745
	loss_reward: 0.01091
[2025-05-08 06:21:57] nn step 11700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13312
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02648
	loss_reward_1: 0.00045
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02634
	loss_reward_2: 0.0026
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02626
	loss_reward_3: 0.00187
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02577
	loss_reward_4: 0.00192
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02692
	loss_reward_5: 0.004
	loss_policy: 0.00014
	loss_value: 0.2649
	loss_reward: 0.01083
[2025-05-08 06:22:04] nn step 11750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13686
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02732
	loss_reward_1: 0.00051
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02713
	loss_reward_2: 0.0026
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99965
	loss_value_3: 0.02706
	loss_reward_3: 0.00185
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99945
	loss_value_4: 0.02657
	loss_reward_4: 0.00203
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99941
	loss_value_5: 0.02759
	loss_reward_5: 0.00414
	loss_policy: 0.00078
	loss_value: 0.27252
	loss_reward: 0.01115
[2025-05-08 06:22:12] nn step 11800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13825
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02755
	loss_reward_1: 0.00048
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02725
	loss_reward_2: 0.00273
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02713
	loss_reward_3: 0.00194
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99953
	loss_value_4: 0.0265
	loss_reward_4: 0.00206
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99934
	loss_value_5: 0.0277
	loss_reward_5: 0.00421
	loss_policy: 0.00073
	loss_value: 0.27439
	loss_reward: 0.01142
Optimization_Done 11800
[2025-05-08 06:23:22] [command] train weight_iter_11800.pkl 41 60
[2025-05-08 06:23:29] nn step 11850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12656
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02526
	loss_reward_1: 0.00047
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02493
	loss_reward_2: 0.00246
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02483
	loss_reward_3: 0.00188
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99941
	loss_value_4: 0.02433
	loss_reward_4: 0.0018
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.9993
	loss_value_5: 0.02534
	loss_reward_5: 0.00389
	loss_policy: 0.00068
	loss_value: 0.25124
	loss_reward: 0.01051
[2025-05-08 06:23:38] nn step 11900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13498
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02692
	loss_reward_1: 0.00043
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.0266
	loss_reward_2: 0.00275
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02657
	loss_reward_3: 0.00185
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.026
	loss_reward_4: 0.00181
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99973
	loss_value_5: 0.02709
	loss_reward_5: 0.00424
	loss_policy: 0.0003
	loss_value: 0.26815
	loss_reward: 0.01107
[2025-05-08 06:23:46] nn step 11950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14011
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02791
	loss_reward_1: 0.00048
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02747
	loss_reward_2: 0.00279
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02742
	loss_reward_3: 0.00202
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99945
	loss_value_4: 0.02685
	loss_reward_4: 0.00197
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99938
	loss_value_5: 0.02799
	loss_reward_5: 0.00432
	loss_policy: 0.00058
	loss_value: 0.27774
	loss_reward: 0.01158
[2025-05-08 06:23:52] nn step 12000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13398
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02676
	loss_reward_1: 0.00045
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.0264
	loss_reward_2: 0.00276
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.02622
	loss_reward_3: 0.00198
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99961
	loss_value_4: 0.02565
	loss_reward_4: 0.00198
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99949
	loss_value_5: 0.02666
	loss_reward_5: 0.00425
	loss_policy: 0.00061
	loss_value: 0.26568
	loss_reward: 0.01143
Optimization_Done 12000
[2025-05-08 06:25:01] [command] train weight_iter_12000.pkl 42 61
[2025-05-08 06:25:10] nn step 12050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12641
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02529
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02491
	loss_reward_2: 0.00256
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02476
	loss_reward_3: 0.00177
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99969
	loss_value_4: 0.02422
	loss_reward_4: 0.00184
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02513
	loss_reward_5: 0.00395
	loss_policy: 0.00045
	loss_value: 0.25071
	loss_reward: 0.01056
[2025-05-08 06:25:16] nn step 12100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13142
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02619
	loss_reward_1: 0.00044
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02583
	loss_reward_2: 0.00258
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02582
	loss_reward_3: 0.0019
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.02519
	loss_reward_4: 0.00188
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99969
	loss_value_5: 0.02615
	loss_reward_5: 0.00406
	loss_policy: 0.00026
	loss_value: 0.26059
	loss_reward: 0.01086
[2025-05-08 06:25:24] nn step 12150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13368
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02667
	loss_reward_1: 0.00043
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02627
	loss_reward_2: 0.00278
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02616
	loss_reward_3: 0.00191
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02549
	loss_reward_4: 0.00197
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99969
	loss_value_5: 0.02648
	loss_reward_5: 0.00431
	loss_policy: 0.00025
	loss_value: 0.26473
	loss_reward: 0.01142
[2025-05-08 06:25:33] nn step 12200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13465
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02691
	loss_reward_1: 0.00048
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02652
	loss_reward_2: 0.00279
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.02636
	loss_reward_3: 0.00207
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99965
	loss_value_4: 0.0257
	loss_reward_4: 0.00209
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02673
	loss_reward_5: 0.00443
	loss_policy: 0.00033
	loss_value: 0.26687
	loss_reward: 0.01187
Optimization_Done 12200
[2025-05-08 06:26:42] [command] train weight_iter_12200.pkl 43 62
[2025-05-08 06:26:49] nn step 12250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12381
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02474
	loss_reward_1: 0.00046
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02427
	loss_reward_2: 0.00258
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02416
	loss_reward_3: 0.00193
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99973
	loss_value_4: 0.02351
	loss_reward_4: 0.0019
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99953
	loss_value_5: 0.02428
	loss_reward_5: 0.00405
	loss_policy: 0.00038
	loss_value: 0.24478
	loss_reward: 0.01093
[2025-05-08 06:26:58] nn step 12300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13294
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02659
	loss_reward_1: 0.00047
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02614
	loss_reward_2: 0.00278
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02591
	loss_reward_3: 0.0021
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99969
	loss_value_4: 0.0253
	loss_reward_4: 0.00203
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99965
	loss_value_5: 0.02623
	loss_reward_5: 0.00438
	loss_policy: 0.00041
	loss_value: 0.26312
	loss_reward: 0.01176
[2025-05-08 06:27:06] nn step 12350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12984
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02599
	loss_reward_1: 0.00047
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02564
	loss_reward_2: 0.00269
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02552
	loss_reward_3: 0.00197
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99973
	loss_value_4: 0.02499
	loss_reward_4: 0.00206
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99953
	loss_value_5: 0.02587
	loss_reward_5: 0.00422
	loss_policy: 0.00043
	loss_value: 0.25784
	loss_reward: 0.01141
[2025-05-08 06:27:13] nn step 12400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12865
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02576
	loss_reward_1: 0.00045
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02543
	loss_reward_2: 0.00276
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99973
	loss_value_3: 0.02525
	loss_reward_3: 0.00204
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99965
	loss_value_4: 0.02466
	loss_reward_4: 0.00202
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02566
	loss_reward_5: 0.00436
	loss_policy: 0.00051
	loss_value: 0.25541
	loss_reward: 0.01163
Optimization_Done 12400
[2025-05-08 06:28:23] [command] train weight_iter_12400.pkl 44 63
[2025-05-08 06:28:32] nn step 12450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1304
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02602
	loss_reward_1: 0.00042
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02565
	loss_reward_2: 0.00274
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.02543
	loss_reward_3: 0.00195
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99961
	loss_value_4: 0.02479
	loss_reward_4: 0.00202
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99961
	loss_value_5: 0.02569
	loss_reward_5: 0.00434
	loss_policy: 0.00033
	loss_value: 0.25799
	loss_reward: 0.01145
[2025-05-08 06:28:38] nn step 12500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12584
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02521
	loss_reward_1: 0.0005
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02489
	loss_reward_2: 0.00268
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.02483
	loss_reward_3: 0.00193
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99973
	loss_value_4: 0.02427
	loss_reward_4: 0.00192
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99949
	loss_value_5: 0.02516
	loss_reward_5: 0.00417
	loss_policy: 0.00039
	loss_value: 0.2502
	loss_reward: 0.0112
[2025-05-08 06:28:47] nn step 12550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1316
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02625
	loss_reward_1: 0.00048
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02589
	loss_reward_2: 0.00295
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02575
	loss_reward_3: 0.00204
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99973
	loss_value_4: 0.02509
	loss_reward_4: 0.00211
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.02605
	loss_reward_5: 0.00451
	loss_policy: 0.00036
	loss_value: 0.26063
	loss_reward: 0.01208
[2025-05-08 06:28:55] nn step 12600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12886
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02576
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02542
	loss_reward_2: 0.00288
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.0254
	loss_reward_3: 0.002
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02477
	loss_reward_4: 0.00194
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02574
	loss_reward_5: 0.00445
	loss_policy: 0.00035
	loss_value: 0.25595
	loss_reward: 0.01166
Optimization_Done 12600
[2025-05-08 06:30:03] [command] train weight_iter_12600.pkl 45 64
[2025-05-08 06:30:12] nn step 12650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12954
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02586
	loss_reward_1: 0.00049
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02551
	loss_reward_2: 0.00281
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02538
	loss_reward_3: 0.00199
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02471
	loss_reward_4: 0.00205
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99969
	loss_value_5: 0.02554
	loss_reward_5: 0.00432
	loss_policy: 0.00054
	loss_value: 0.25654
	loss_reward: 0.01166
[2025-05-08 06:30:20] nn step 12700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12149
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02433
	loss_reward_1: 0.00041
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02395
	loss_reward_2: 0.00272
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02385
	loss_reward_3: 0.00184
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02328
	loss_reward_4: 0.00192
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99965
	loss_value_5: 0.02405
	loss_reward_5: 0.00425
	loss_policy: 0.00037
	loss_value: 0.24096
	loss_reward: 0.01113
[2025-05-08 06:30:27] nn step 12750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12188
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02454
	loss_reward_1: 0.00045
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02421
	loss_reward_2: 0.00264
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.9998
	loss_value_3: 0.0241
	loss_reward_3: 0.00192
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99973
	loss_value_4: 0.0235
	loss_reward_4: 0.00194
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99973
	loss_value_5: 0.02426
	loss_reward_5: 0.00421
	loss_policy: 0.0006
	loss_value: 0.24249
	loss_reward: 0.01116
[2025-05-08 06:30:35] nn step 12800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12695
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02538
	loss_reward_1: 0.00041
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02497
	loss_reward_2: 0.00286
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02484
	loss_reward_3: 0.002
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02422
	loss_reward_4: 0.00207
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99973
	loss_value_5: 0.02497
	loss_reward_5: 0.00444
	loss_policy: 0.00033
	loss_value: 0.25133
	loss_reward: 0.01177
Optimization_Done 12800
[2025-05-08 06:31:44] [command] train weight_iter_12800.pkl 46 65
[2025-05-08 06:31:53] nn step 12850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12196
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02443
	loss_reward_1: 0.00042
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02404
	loss_reward_2: 0.0027
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02391
	loss_reward_3: 0.00183
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.02322
	loss_reward_4: 0.00191
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02395
	loss_reward_5: 0.00422
	loss_policy: 0.00023
	loss_value: 0.24152
	loss_reward: 0.01108
[2025-05-08 06:32:00] nn step 12900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12911
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02597
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.0255
	loss_reward_2: 0.00284
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02529
	loss_reward_3: 0.00215
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02458
	loss_reward_4: 0.00208
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.0254
	loss_reward_5: 0.00447
	loss_policy: 0.00029
	loss_value: 0.25584
	loss_reward: 0.01198
[2025-05-08 06:32:08] nn step 12950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12124
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02435
	loss_reward_1: 0.00042
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02387
	loss_reward_2: 0.00273
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.02373
	loss_reward_3: 0.00196
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02315
	loss_reward_4: 0.00198
	loss_policy_5: 4e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02381
	loss_reward_5: 0.00427
	loss_policy: 0.00018
	loss_value: 0.24015
	loss_reward: 0.01136
[2025-05-08 06:32:16] nn step 13000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12488
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02512
	loss_reward_1: 0.00045
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02463
	loss_reward_2: 0.00291
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.0244
	loss_reward_3: 0.00207
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02371
	loss_reward_4: 0.00205
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99973
	loss_value_5: 0.02455
	loss_reward_5: 0.00447
	loss_policy: 0.00029
	loss_value: 0.24728
	loss_reward: 0.01195
Optimization_Done 13000
[2025-05-08 06:33:26] [command] train weight_iter_13000.pkl 47 66
[2025-05-08 06:33:34] nn step 13050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11852
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02365
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02315
	loss_reward_2: 0.00265
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02291
	loss_reward_3: 0.00192
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02235
	loss_reward_4: 0.00193
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02306
	loss_reward_5: 0.00411
	loss_policy: 0.00021
	loss_value: 0.23363
	loss_reward: 0.01102
[2025-05-08 06:33:43] nn step 13100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11694
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02349
	loss_reward_1: 0.00039
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02302
	loss_reward_2: 0.00265
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02288
	loss_reward_3: 0.0019
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02224
	loss_reward_4: 0.0019
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02286
	loss_reward_5: 0.00413
	loss_policy: 0.00033
	loss_value: 0.23143
	loss_reward: 0.01097
[2025-05-08 06:33:50] nn step 13150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1212
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02436
	loss_reward_1: 0.00043
	loss_policy_2: 1e-05
	accuracy_policy_2: 1.0
	loss_value_2: 0.02388
	loss_reward_2: 0.00272
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02364
	loss_reward_3: 0.002
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02294
	loss_reward_4: 0.00201
	loss_policy_5: 6e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02365
	loss_reward_5: 0.00425
	loss_policy: 0.00013
	loss_value: 0.23966
	loss_reward: 0.0114
[2025-05-08 06:33:58] nn step 13200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11434
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.023
	loss_reward_1: 0.00044
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02267
	loss_reward_2: 0.00259
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02244
	loss_reward_3: 0.00187
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02176
	loss_reward_4: 0.00194
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02244
	loss_reward_5: 0.00413
	loss_policy: 0.00034
	loss_value: 0.22665
	loss_reward: 0.01096
Optimization_Done 13200
[2025-05-08 06:35:07] [command] train weight_iter_13200.pkl 48 67
[2025-05-08 06:35:14] nn step 13250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12303
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02461
	loss_reward_1: 0.00043
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02405
	loss_reward_2: 0.00277
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02375
	loss_reward_3: 0.00202
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02319
	loss_reward_4: 0.00202
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.0238
	loss_reward_5: 0.00431
	loss_policy: 0.00023
	loss_value: 0.24242
	loss_reward: 0.01155
[2025-05-08 06:35:22] nn step 13300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12687
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0254
	loss_reward_1: 0.00041
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0249
	loss_reward_2: 0.00291
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02468
	loss_reward_3: 0.00208
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02389
	loss_reward_4: 0.00204
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02461
	loss_reward_5: 0.00454
	loss_policy: 0.0002
	loss_value: 0.25034
	loss_reward: 0.01197
[2025-05-08 06:35:30] nn step 13350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12626
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02535
	loss_reward_1: 0.00051
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02487
	loss_reward_2: 0.003
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02462
	loss_reward_3: 0.00203
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99973
	loss_value_4: 0.02387
	loss_reward_4: 0.00223
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99961
	loss_value_5: 0.02461
	loss_reward_5: 0.00469
	loss_policy: 0.00096
	loss_value: 0.24957
	loss_reward: 0.01247
[2025-05-08 06:35:39] nn step 13400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12305
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02476
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0242
	loss_reward_2: 0.00289
	loss_policy_3: 0.0
	accuracy_policy_3: 1.0
	loss_value_3: 0.02405
	loss_reward_3: 0.00201
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02332
	loss_reward_4: 0.00206
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02398
	loss_reward_5: 0.00452
	loss_policy: 0.00012
	loss_value: 0.24336
	loss_reward: 0.0119
Optimization_Done 13400
[2025-05-08 06:36:48] [command] train weight_iter_13400.pkl 49 68
[2025-05-08 06:36:57] nn step 13450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12684
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02551
	loss_reward_1: 0.00045
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02502
	loss_reward_2: 0.00291
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02464
	loss_reward_3: 0.00208
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.9998
	loss_value_4: 0.02394
	loss_reward_4: 0.00206
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99977
	loss_value_5: 0.0246
	loss_reward_5: 0.00454
	loss_policy: 0.00035
	loss_value: 0.25055
	loss_reward: 0.01204
[2025-05-08 06:37:03] nn step 13500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11765
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02368
	loss_reward_1: 0.00048
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02323
	loss_reward_2: 0.00271
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02296
	loss_reward_3: 0.00192
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.0222
	loss_reward_4: 0.00203
	loss_policy_5: 4e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02277
	loss_reward_5: 0.00421
	loss_policy: 0.00015
	loss_value: 0.23249
	loss_reward: 0.01135
[2025-05-08 06:37:12] nn step 13550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11814
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02366
	loss_reward_1: 0.00043
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02317
	loss_reward_2: 0.00284
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02285
	loss_reward_3: 0.00202
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02215
	loss_reward_4: 0.00188
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.9998
	loss_value_5: 0.02277
	loss_reward_5: 0.00446
	loss_policy: 0.00035
	loss_value: 0.23273
	loss_reward: 0.01163
[2025-05-08 06:37:20] nn step 13600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11337
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02269
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02218
	loss_reward_2: 0.00266
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02196
	loss_reward_3: 0.00182
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02132
	loss_reward_4: 0.00187
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.9998
	loss_value_5: 0.02194
	loss_reward_5: 0.00426
	loss_policy: 0.00025
	loss_value: 0.22346
	loss_reward: 0.01101
Optimization_Done 13600
[2025-05-08 06:38:28] [command] train weight_iter_13600.pkl 50 69
[2025-05-08 06:38:37] nn step 13650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12471
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02489
	loss_reward_1: 0.00046
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02431
	loss_reward_2: 0.00292
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02395
	loss_reward_3: 0.00209
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02319
	loss_reward_4: 0.00208
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02383
	loss_reward_5: 0.00462
	loss_policy: 0.00011
	loss_value: 0.24489
	loss_reward: 0.01217
[2025-05-08 06:38:46] nn step 13700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11859
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02377
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02326
	loss_reward_2: 0.00285
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.02301
	loss_reward_3: 0.00199
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.02225
	loss_reward_4: 0.00208
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02284
	loss_reward_5: 0.00452
	loss_policy: 0.00036
	loss_value: 0.23373
	loss_reward: 0.01185
[2025-05-08 06:38:54] nn step 13750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11478
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02298
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02244
	loss_reward_2: 0.00278
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02223
	loss_reward_3: 0.00191
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02139
	loss_reward_4: 0.00198
	loss_policy_5: 2e-05
	accuracy_policy_5: 0.99996
	loss_value_5: 0.022
	loss_reward_5: 0.00434
	loss_policy: 8e-05
	loss_value: 0.22583
	loss_reward: 0.01143
[2025-05-08 06:39:01] nn step 13800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12117
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02436
	loss_reward_1: 0.00046
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02375
	loss_reward_2: 0.0029
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02342
	loss_reward_3: 0.00205
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.02267
	loss_reward_4: 0.00207
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02324
	loss_reward_5: 0.00457
	loss_policy: 0.00015
	loss_value: 0.23861
	loss_reward: 0.01206
Optimization_Done 13800
[2025-05-08 06:40:11] [command] train weight_iter_13800.pkl 51 70
[2025-05-08 06:40:20] nn step 13850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11774
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02362
	loss_reward_1: 0.00044
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02313
	loss_reward_2: 0.00269
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02285
	loss_reward_3: 0.00204
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02205
	loss_reward_4: 0.00196
	loss_policy_5: 2e-05
	accuracy_policy_5: 0.99996
	loss_value_5: 0.02258
	loss_reward_5: 0.00429
	loss_policy: 8e-05
	loss_value: 0.23198
	loss_reward: 0.01142
[2025-05-08 06:40:27] nn step 13900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1197
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0239
	loss_reward_1: 0.00047
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0233
	loss_reward_2: 0.00293
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02297
	loss_reward_3: 0.00203
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02214
	loss_reward_4: 0.00209
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02277
	loss_reward_5: 0.00459
	loss_policy: 0.00024
	loss_value: 0.23478
	loss_reward: 0.01212
[2025-05-08 06:40:35] nn step 13950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11727
	loss_policy_1: 1e-05
	accuracy_policy_1: 1.0
	loss_value_1: 0.02358
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02297
	loss_reward_2: 0.00288
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02268
	loss_reward_3: 0.00205
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99926
	loss_value_4: 0.02194
	loss_reward_4: 0.00201
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99973
	loss_value_5: 0.02245
	loss_reward_5: 0.00455
	loss_policy: 0.00079
	loss_value: 0.23088
	loss_reward: 0.01191
[2025-05-08 06:40:44] nn step 14000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11414
	loss_policy_1: 1e-05
	accuracy_policy_1: 1.0
	loss_value_1: 0.02278
	loss_reward_1: 0.00039
	loss_policy_2: 1e-05
	accuracy_policy_2: 1.0
	loss_value_2: 0.02233
	loss_reward_2: 0.00279
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02195
	loss_reward_3: 0.00198
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02112
	loss_reward_4: 0.00189
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.9998
	loss_value_5: 0.02175
	loss_reward_5: 0.0043
	loss_policy: 0.00031
	loss_value: 0.22408
	loss_reward: 0.01135
Optimization_Done 14000
[2025-05-08 06:41:51] [command] train weight_iter_14000.pkl 52 71
[2025-05-08 06:42:00] nn step 14050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11296
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0226
	loss_reward_1: 0.00041
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02207
	loss_reward_2: 0.00268
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02175
	loss_reward_3: 0.00195
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02092
	loss_reward_4: 0.00205
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02145
	loss_reward_5: 0.00432
	loss_policy: 0.00018
	loss_value: 0.22175
	loss_reward: 0.01142
[2025-05-08 06:42:09] nn step 14100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11518
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02309
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02254
	loss_reward_2: 0.00287
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02222
	loss_reward_3: 0.00199
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.02135
	loss_reward_4: 0.00209
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02186
	loss_reward_5: 0.00451
	loss_policy: 0.00018
	loss_value: 0.22623
	loss_reward: 0.01187
[2025-05-08 06:42:15] nn step 14150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12583
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02517
	loss_reward_1: 0.00045
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02451
	loss_reward_2: 0.00315
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02411
	loss_reward_3: 0.00222
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02324
	loss_reward_4: 0.00224
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02386
	loss_reward_5: 0.00492
	loss_policy: 0.00021
	loss_value: 0.24673
	loss_reward: 0.01298
[2025-05-08 06:42:24] nn step 14200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11722
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02348
	loss_reward_1: 0.00043
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02284
	loss_reward_2: 0.00292
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02254
	loss_reward_3: 0.00198
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02164
	loss_reward_4: 0.00204
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02215
	loss_reward_5: 0.00457
	loss_policy: 0.00028
	loss_value: 0.22986
	loss_reward: 0.01194
Optimization_Done 14200
[2025-05-08 06:43:35] [command] train weight_iter_14200.pkl 53 72
[2025-05-08 06:43:42] nn step 14250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11183
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02237
	loss_reward_1: 0.00046
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02186
	loss_reward_2: 0.00266
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02146
	loss_reward_3: 0.00199
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.02064
	loss_reward_4: 0.00205
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02108
	loss_reward_5: 0.00419
	loss_policy: 0.00027
	loss_value: 0.21924
	loss_reward: 0.01134
[2025-05-08 06:43:51] nn step 14300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10681
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0214
	loss_reward_1: 0.00035
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02086
	loss_reward_2: 0.0026
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02055
	loss_reward_3: 0.00194
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01982
	loss_reward_4: 0.00189
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.9998
	loss_value_5: 0.02025
	loss_reward_5: 0.00422
	loss_policy: 0.0004
	loss_value: 0.2097
	loss_reward: 0.011
[2025-05-08 06:43:59] nn step 14350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11136
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02232
	loss_reward_1: 0.00039
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02174
	loss_reward_2: 0.00288
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02141
	loss_reward_3: 0.00199
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.0206
	loss_reward_4: 0.00206
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02109
	loss_reward_5: 0.00441
	loss_policy: 0.00025
	loss_value: 0.21853
	loss_reward: 0.01173
[2025-05-08 06:44:07] nn step 14400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11219
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.0225
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02197
	loss_reward_2: 0.00283
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02165
	loss_reward_3: 0.00203
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02078
	loss_reward_4: 0.00204
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02127
	loss_reward_5: 0.00451
	loss_policy: 0.00023
	loss_value: 0.22035
	loss_reward: 0.01181
Optimization_Done 14400
[2025-05-08 06:45:16] [command] train weight_iter_14400.pkl 54 73
[2025-05-08 06:45:25] nn step 14450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1075
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02157
	loss_reward_1: 0.00038
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02105
	loss_reward_2: 0.00265
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02067
	loss_reward_3: 0.00192
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01983
	loss_reward_4: 0.00196
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02029
	loss_reward_5: 0.00422
	loss_policy: 0.00015
	loss_value: 0.21093
	loss_reward: 0.01113
[2025-05-08 06:45:31] nn step 14500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10488
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02101
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02044
	loss_reward_2: 0.00266
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02014
	loss_reward_3: 0.00183
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.0193
	loss_reward_4: 0.00198
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01974
	loss_reward_5: 0.00421
	loss_policy: 0.00014
	loss_value: 0.2055
	loss_reward: 0.01107
[2025-05-08 06:45:40] nn step 14550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10632
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02125
	loss_reward_1: 0.00039
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02072
	loss_reward_2: 0.00264
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02039
	loss_reward_3: 0.00194
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01956
	loss_reward_4: 0.0019
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.02004
	loss_reward_5: 0.00419
	loss_policy: 0.00025
	loss_value: 0.20828
	loss_reward: 0.01105
[2025-05-08 06:45:48] nn step 14600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10978
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02197
	loss_reward_1: 0.00038
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02138
	loss_reward_2: 0.00286
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02104
	loss_reward_3: 0.00208
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02014
	loss_reward_4: 0.00203
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02065
	loss_reward_5: 0.00448
	loss_policy: 0.0002
	loss_value: 0.21497
	loss_reward: 0.01183
Optimization_Done 14600
[2025-05-08 06:46:56] [command] train weight_iter_14600.pkl 55 74
[2025-05-08 06:47:05] nn step 14650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11224
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02246
	loss_reward_1: 0.00041
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02192
	loss_reward_2: 0.00284
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02159
	loss_reward_3: 0.00203
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02068
	loss_reward_4: 0.00207
	loss_policy_5: 6e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02113
	loss_reward_5: 0.00457
	loss_policy: 0.00027
	loss_value: 0.22001
	loss_reward: 0.01193
[2025-05-08 06:47:14] nn step 14700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10808
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02171
	loss_reward_1: 0.0004
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02104
	loss_reward_2: 0.00287
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0207
	loss_reward_3: 0.00202
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01992
	loss_reward_4: 0.00201
	loss_policy_5: 4e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.02029
	loss_reward_5: 0.00449
	loss_policy: 0.00022
	loss_value: 0.21176
	loss_reward: 0.01179
[2025-05-08 06:47:23] nn step 14750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10249
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02064
	loss_reward_1: 0.00036
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02005
	loss_reward_2: 0.0027
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01978
	loss_reward_3: 0.0019
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01891
	loss_reward_4: 0.00191
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01942
	loss_reward_5: 0.00423
	loss_policy: 0.00012
	loss_value: 0.20129
	loss_reward: 0.01111
[2025-05-08 06:47:30] nn step 14800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11118
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02223
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02166
	loss_reward_2: 0.00299
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0213
	loss_reward_3: 0.00206
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.02043
	loss_reward_4: 0.00206
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99977
	loss_value_5: 0.02105
	loss_reward_5: 0.00454
	loss_policy: 0.00029
	loss_value: 0.21785
	loss_reward: 0.01206
Optimization_Done 14800
[2025-05-08 06:48:39] [command] train weight_iter_14800.pkl 56 75
[2025-05-08 06:48:48] nn step 14850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10858
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02174
	loss_reward_1: 0.00036
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02109
	loss_reward_2: 0.00292
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.0207
	loss_reward_3: 0.00208
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01981
	loss_reward_4: 0.00199
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.02022
	loss_reward_5: 0.00457
	loss_policy: 0.00017
	loss_value: 0.21214
	loss_reward: 0.01191
[2025-05-08 06:48:54] nn step 14900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10678
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02138
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02082
	loss_reward_2: 0.00282
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.02046
	loss_reward_3: 0.00198
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01954
	loss_reward_4: 0.00202
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01996
	loss_reward_5: 0.00446
	loss_policy: 0.00022
	loss_value: 0.20893
	loss_reward: 0.01169
[2025-05-08 06:49:03] nn step 14950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11042
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02224
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02171
	loss_reward_2: 0.00291
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.02133
	loss_reward_3: 0.00204
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.02045
	loss_reward_4: 0.00211
	loss_policy_5: 2e-05
	accuracy_policy_5: 0.99996
	loss_value_5: 0.02091
	loss_reward_5: 0.00464
	loss_policy: 0.00011
	loss_value: 0.21706
	loss_reward: 0.01209
[2025-05-08 06:49:11] nn step 15000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10393
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02088
	loss_reward_1: 0.00038
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02027
	loss_reward_2: 0.00275
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01993
	loss_reward_3: 0.00194
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.0191
	loss_reward_4: 0.00198
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01961
	loss_reward_5: 0.00437
	loss_policy: 0.00022
	loss_value: 0.20373
	loss_reward: 0.01142
Optimization_Done 15000
[2025-05-08 06:50:23] [command] train weight_iter_15000.pkl 57 76
[2025-05-08 06:50:32] nn step 15050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09786
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01964
	loss_reward_1: 0.00038
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01908
	loss_reward_2: 0.00256
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01874
	loss_reward_3: 0.00191
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01793
	loss_reward_4: 0.00185
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01833
	loss_reward_5: 0.00411
	loss_policy: 0.00011
	loss_value: 0.19159
	loss_reward: 0.01081
[2025-05-08 06:50:40] nn step 15100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10472
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02094
	loss_reward_1: 0.0004
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02044
	loss_reward_2: 0.00293
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02013
	loss_reward_3: 0.00198
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01929
	loss_reward_4: 0.00196
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01977
	loss_reward_5: 0.00456
	loss_policy: 0.00022
	loss_value: 0.20529
	loss_reward: 0.01184
[2025-05-08 06:50:47] nn step 15150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10053
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02014
	loss_reward_1: 0.00036
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01961
	loss_reward_2: 0.00271
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01931
	loss_reward_3: 0.0019
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.0185
	loss_reward_4: 0.00189
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01899
	loss_reward_5: 0.00427
	loss_policy: 0.00022
	loss_value: 0.19709
	loss_reward: 0.01112
[2025-05-08 06:50:55] nn step 15200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09795
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01965
	loss_reward_1: 0.00036
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0191
	loss_reward_2: 0.00265
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0188
	loss_reward_3: 0.00189
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01801
	loss_reward_4: 0.00191
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01849
	loss_reward_5: 0.00423
	loss_policy: 0.0002
	loss_value: 0.19198
	loss_reward: 0.01104
Optimization_Done 15200
[2025-05-08 06:52:05] [command] train weight_iter_15200.pkl 58 77
[2025-05-08 06:52:13] nn step 15250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10422
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02083
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02027
	loss_reward_2: 0.00287
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01993
	loss_reward_3: 0.00195
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01908
	loss_reward_4: 0.00202
	loss_policy_5: 4e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01952
	loss_reward_5: 0.0045
	loss_policy: 0.00019
	loss_value: 0.20384
	loss_reward: 0.01176
[2025-05-08 06:52:21] nn step 15300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10056
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02016
	loss_reward_1: 0.00039
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01966
	loss_reward_2: 0.00276
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01926
	loss_reward_3: 0.002
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01844
	loss_reward_4: 0.00199
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01894
	loss_reward_5: 0.00437
	loss_policy: 0.00013
	loss_value: 0.19702
	loss_reward: 0.0115
[2025-05-08 06:52:29] nn step 15350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10584
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02111
	loss_reward_1: 0.00039
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02053
	loss_reward_2: 0.00292
	loss_policy_3: 0.0
	accuracy_policy_3: 1.0
	loss_value_3: 0.0202
	loss_reward_3: 0.00201
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01924
	loss_reward_4: 0.00206
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01978
	loss_reward_5: 0.00463
	loss_policy: 0.00015
	loss_value: 0.2067
	loss_reward: 0.012
[2025-05-08 06:52:35] nn step 15400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09972
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0199
	loss_reward_1: 0.00035
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0194
	loss_reward_2: 0.00282
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01902
	loss_reward_3: 0.00199
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01815
	loss_reward_4: 0.00202
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01868
	loss_reward_5: 0.0044
	loss_policy: 0.00045
	loss_value: 0.19487
	loss_reward: 0.01158
Optimization_Done 15400
[2025-05-08 06:53:49] [command] train weight_iter_15400.pkl 59 78
[2025-05-08 06:53:58] nn step 15450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10372
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02075
	loss_reward_1: 0.0004
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02023
	loss_reward_2: 0.00279
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01987
	loss_reward_3: 0.00203
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01907
	loss_reward_4: 0.00205
	loss_policy_5: 2e-05
	accuracy_policy_5: 0.99996
	loss_value_5: 0.01953
	loss_reward_5: 0.00447
	loss_policy: 6e-05
	loss_value: 0.20317
	loss_reward: 0.01174
[2025-05-08 06:54:05] nn step 15500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10239
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02043
	loss_reward_1: 0.00037
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01987
	loss_reward_2: 0.00295
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0195
	loss_reward_3: 0.00209
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01865
	loss_reward_4: 0.002
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01916
	loss_reward_5: 0.00464
	loss_policy: 0.00022
	loss_value: 0.20001
	loss_reward: 0.01206
[2025-05-08 06:54:13] nn step 15550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0972
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01948
	loss_reward_1: 0.00039
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0189
	loss_reward_2: 0.00277
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01862
	loss_reward_3: 0.00195
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01785
	loss_reward_4: 0.00194
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01823
	loss_reward_5: 0.00441
	loss_policy: 0.00044
	loss_value: 0.19028
	loss_reward: 0.01146
[2025-05-08 06:54:21] nn step 15600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10157
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0202
	loss_reward_1: 0.00039
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01967
	loss_reward_2: 0.00294
	loss_policy_3: 0.0
	accuracy_policy_3: 1.0
	loss_value_3: 0.01937
	loss_reward_3: 0.00203
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01849
	loss_reward_4: 0.00214
	loss_policy_5: 2e-05
	accuracy_policy_5: 0.99996
	loss_value_5: 0.01903
	loss_reward_5: 0.00465
	loss_policy: 6e-05
	loss_value: 0.19832
	loss_reward: 0.01216
Optimization_Done 15600
[2025-05-08 06:55:29] [command] train weight_iter_15600.pkl 60 79
[2025-05-08 06:55:38] nn step 15650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09392
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01876
	loss_reward_1: 0.00035
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01819
	loss_reward_2: 0.00257
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01783
	loss_reward_3: 0.00191
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01708
	loss_reward_4: 0.00188
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01755
	loss_reward_5: 0.00413
	loss_policy: 0.00036
	loss_value: 0.18334
	loss_reward: 0.01084
[2025-05-08 06:55:47] nn step 15700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09454
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01895
	loss_reward_1: 0.00036
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01845
	loss_reward_2: 0.00273
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01815
	loss_reward_3: 0.00195
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01738
	loss_reward_4: 0.0019
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01782
	loss_reward_5: 0.00428
	loss_policy: 0.00017
	loss_value: 0.18527
	loss_reward: 0.01122
[2025-05-08 06:55:53] nn step 15750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09546
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01909
	loss_reward_1: 0.0004
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01862
	loss_reward_2: 0.00274
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01834
	loss_reward_3: 0.00191
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01753
	loss_reward_4: 0.00197
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01798
	loss_reward_5: 0.00431
	loss_policy: 0.00028
	loss_value: 0.18703
	loss_reward: 0.01132
[2025-05-08 06:56:02] nn step 15800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09071
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01822
	loss_reward_1: 0.00034
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01774
	loss_reward_2: 0.00264
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01746
	loss_reward_3: 0.00187
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01663
	loss_reward_4: 0.00186
	loss_policy_5: 6e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01703
	loss_reward_5: 0.00419
	loss_policy: 0.00017
	loss_value: 0.17779
	loss_reward: 0.0109
Optimization_Done 15800
[2025-05-08 06:57:14] [command] train weight_iter_15800.pkl 61 80
[2025-05-08 06:57:21] nn step 15850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09121
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01816
	loss_reward_1: 0.00036
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01766
	loss_reward_2: 0.00262
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01734
	loss_reward_3: 0.00181
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01652
	loss_reward_4: 0.00193
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99973
	loss_value_5: 0.01691
	loss_reward_5: 0.0042
	loss_policy: 0.00045
	loss_value: 0.1778
	loss_reward: 0.01093
[2025-05-08 06:57:29] nn step 15900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09196
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01835
	loss_reward_1: 0.00033
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01795
	loss_reward_2: 0.00272
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0176
	loss_reward_3: 0.0019
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01679
	loss_reward_4: 0.00191
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01725
	loss_reward_5: 0.00427
	loss_policy: 0.00027
	loss_value: 0.1799
	loss_reward: 0.01112
[2025-05-08 06:57:38] nn step 15950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09172
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.0183
	loss_reward_1: 0.00036
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01778
	loss_reward_2: 0.00271
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01742
	loss_reward_3: 0.00191
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01669
	loss_reward_4: 0.00193
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01719
	loss_reward_5: 0.0042
	loss_policy: 0.00035
	loss_value: 0.1791
	loss_reward: 0.01111
[2025-05-08 06:57:46] nn step 16000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09547
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01909
	loss_reward_1: 0.00039
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01853
	loss_reward_2: 0.00287
	loss_policy_3: 0.0
	accuracy_policy_3: 1.0
	loss_value_3: 0.01817
	loss_reward_3: 0.00202
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01737
	loss_reward_4: 0.00199
	loss_policy_5: 4e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01776
	loss_reward_5: 0.0045
	loss_policy: 7e-05
	loss_value: 0.18639
	loss_reward: 0.01177
Optimization_Done 16000
[2025-05-08 06:58:53] [command] train weight_iter_16000.pkl 62 81
[2025-05-08 06:59:02] nn step 16050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09574
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01908
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01853
	loss_reward_2: 0.00277
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01815
	loss_reward_3: 0.00199
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01736
	loss_reward_4: 0.00199
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.0178
	loss_reward_5: 0.00438
	loss_policy: 0.00026
	loss_value: 0.18666
	loss_reward: 0.01153
[2025-05-08 06:59:11] nn step 16100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09096
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01816
	loss_reward_1: 0.00037
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01768
	loss_reward_2: 0.00266
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01738
	loss_reward_3: 0.00183
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99977
	loss_value_4: 0.0166
	loss_reward_4: 0.00194
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99961
	loss_value_5: 0.017
	loss_reward_5: 0.00424
	loss_policy: 0.00042
	loss_value: 0.17776
	loss_reward: 0.01104
[2025-05-08 06:59:18] nn step 16150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08924
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01784
	loss_reward_1: 0.00036
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01731
	loss_reward_2: 0.00258
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01694
	loss_reward_3: 0.00191
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01616
	loss_reward_4: 0.00191
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01661
	loss_reward_5: 0.00405
	loss_policy: 0.00042
	loss_value: 0.17409
	loss_reward: 0.01081
[2025-05-08 06:59:26] nn step 16200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09121
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01818
	loss_reward_1: 0.00037
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01765
	loss_reward_2: 0.00271
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01731
	loss_reward_3: 0.00188
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01651
	loss_reward_4: 0.00194
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01691
	loss_reward_5: 0.00427
	loss_policy: 0.00019
	loss_value: 0.17778
	loss_reward: 0.01116
Optimization_Done 16200
[2025-05-08 07:00:36] [command] train weight_iter_16200.pkl 63 82
[2025-05-08 07:00:45] nn step 16250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08772
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01746
	loss_reward_1: 0.00036
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01695
	loss_reward_2: 0.00263
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01665
	loss_reward_3: 0.00175
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01593
	loss_reward_4: 0.00186
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01633
	loss_reward_5: 0.00416
	loss_policy: 0.00039
	loss_value: 0.17104
	loss_reward: 0.01076
[2025-05-08 07:00:54] nn step 16300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09258
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.0185
	loss_reward_1: 0.00035
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01787
	loss_reward_2: 0.00273
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01748
	loss_reward_3: 0.00196
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01677
	loss_reward_4: 0.0019
	loss_policy_5: 3e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.01712
	loss_reward_5: 0.00432
	loss_policy: 0.00011
	loss_value: 0.18031
	loss_reward: 0.01126
[2025-05-08 07:01:02] nn step 16350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0911
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01815
	loss_reward_1: 0.00037
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01763
	loss_reward_2: 0.00284
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01729
	loss_reward_3: 0.00192
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01651
	loss_reward_4: 0.00196
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01694
	loss_reward_5: 0.00449
	loss_policy: 0.00014
	loss_value: 0.17763
	loss_reward: 0.01158
[2025-05-08 07:01:09] nn step 16400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09027
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.0181
	loss_reward_1: 0.0004
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01757
	loss_reward_2: 0.00269
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01722
	loss_reward_3: 0.00193
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01647
	loss_reward_4: 0.00194
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01683
	loss_reward_5: 0.00425
	loss_policy: 0.00018
	loss_value: 0.17646
	loss_reward: 0.01121
Optimization_Done 16400
[2025-05-08 07:02:19] [command] train weight_iter_16400.pkl 64 83
[2025-05-08 07:02:28] nn step 16450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09034
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01802
	loss_reward_1: 0.00035
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.0175
	loss_reward_2: 0.00269
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01717
	loss_reward_3: 0.00192
	loss_policy_4: 2e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01633
	loss_reward_4: 0.00194
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99992
	loss_value_5: 0.0167
	loss_reward_5: 0.0042
	loss_policy: 0.00015
	loss_value: 0.17606
	loss_reward: 0.0111
[2025-05-08 07:02:34] nn step 16500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09654
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01928
	loss_reward_1: 0.00043
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01863
	loss_reward_2: 0.00289
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01816
	loss_reward_3: 0.00211
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.0173
	loss_reward_4: 0.00213
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01778
	loss_reward_5: 0.00462
	loss_policy: 0.00023
	loss_value: 0.18768
	loss_reward: 0.01217
[2025-05-08 07:02:42] nn step 16550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09106
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01815
	loss_reward_1: 0.00041
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01765
	loss_reward_2: 0.00277
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01729
	loss_reward_3: 0.00195
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01654
	loss_reward_4: 0.00198
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99973
	loss_value_5: 0.01695
	loss_reward_5: 0.00435
	loss_policy: 0.00046
	loss_value: 0.17764
	loss_reward: 0.01145
[2025-05-08 07:02:51] nn step 16600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09094
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01814
	loss_reward_1: 0.00037
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01762
	loss_reward_2: 0.00282
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01724
	loss_reward_3: 0.00205
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01641
	loss_reward_4: 0.00195
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01682
	loss_reward_5: 0.00452
	loss_policy: 0.00043
	loss_value: 0.17718
	loss_reward: 0.01171
Optimization_Done 16600
[2025-05-08 07:04:01] [command] train weight_iter_16600.pkl 65 84
[2025-05-08 07:04:11] nn step 16650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08848
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01772
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01722
	loss_reward_2: 0.0027
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01688
	loss_reward_3: 0.00186
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01611
	loss_reward_4: 0.00196
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01643
	loss_reward_5: 0.00426
	loss_policy: 0.00036
	loss_value: 0.17284
	loss_reward: 0.01119
[2025-05-08 07:04:19] nn step 16700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08515
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01703
	loss_reward_1: 0.00036
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01643
	loss_reward_2: 0.00261
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01605
	loss_reward_3: 0.00184
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01531
	loss_reward_4: 0.00186
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01569
	loss_reward_5: 0.00416
	loss_policy: 0.0006
	loss_value: 0.16566
	loss_reward: 0.01082
[2025-05-08 07:04:26] nn step 16750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08782
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0175
	loss_reward_1: 0.00038
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01693
	loss_reward_2: 0.00278
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01659
	loss_reward_3: 0.00193
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01583
	loss_reward_4: 0.00194
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01628
	loss_reward_5: 0.00439
	loss_policy: 0.00025
	loss_value: 0.17095
	loss_reward: 0.01141
[2025-05-08 07:04:34] nn step 16800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08585
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01707
	loss_reward_1: 0.00032
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01654
	loss_reward_2: 0.0027
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01619
	loss_reward_3: 0.00188
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01549
	loss_reward_4: 0.00181
	loss_policy_5: 6e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01592
	loss_reward_5: 0.00421
	loss_policy: 0.00025
	loss_value: 0.16705
	loss_reward: 0.01092
Optimization_Done 16800
[2025-05-08 07:05:45] [command] train weight_iter_16800.pkl 66 85
[2025-05-08 07:05:52] nn step 16850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08939
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01789
	loss_reward_1: 0.00036
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01735
	loss_reward_2: 0.00274
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01694
	loss_reward_3: 0.00194
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01614
	loss_reward_4: 0.00203
	loss_policy_5: 5e-05
	accuracy_policy_5: 0.99988
	loss_value_5: 0.01644
	loss_reward_5: 0.00432
	loss_policy: 0.00023
	loss_value: 0.17414
	loss_reward: 0.0114
[2025-05-08 07:06:00] nn step 16900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08083
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0161
	loss_reward_1: 0.00034
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0156
	loss_reward_2: 0.00245
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01527
	loss_reward_3: 0.00177
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99973
	loss_value_4: 0.0145
	loss_reward_4: 0.00181
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01493
	loss_reward_5: 0.00394
	loss_policy: 0.00056
	loss_value: 0.15724
	loss_reward: 0.01031
[2025-05-08 07:06:09] nn step 16950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08489
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01697
	loss_reward_1: 0.00038
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01642
	loss_reward_2: 0.00268
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.016
	loss_reward_3: 0.00189
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01531
	loss_reward_4: 0.00192
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01572
	loss_reward_5: 0.0042
	loss_policy: 0.00025
	loss_value: 0.16532
	loss_reward: 0.01107
[2025-05-08 07:06:16] nn step 17000, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.089
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01824
	loss_reward_1: 0.00105
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01806
	loss_reward_2: 0.00386
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99906
	loss_value_3: 0.01814
	loss_reward_3: 0.00304
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99895
	loss_value_4: 0.01766
	loss_reward_4: 0.00341
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01863
	loss_reward_5: 0.00577
	loss_policy: 0.00103
	loss_value: 0.17972
	loss_reward: 0.01713
Optimization_Done 17000
[2025-05-08 07:07:27] [command] train weight_iter_17000.pkl 67 86
[2025-05-08 07:07:36] nn step 17050, lr: 0.1.
	loss_policy_0: 0.00497
	accuracy_policy_0: 0.99051
	loss_value_0: 0.16981
	loss_policy_1: 0.00076
	accuracy_policy_1: 0.99371
	loss_value_1: 0.03372
	loss_reward_1: 0.00349
	loss_policy_2: 0.0006
	accuracy_policy_2: 0.99613
	loss_value_2: 0.03351
	loss_reward_2: 0.00613
	loss_policy_3: 0.00068
	accuracy_policy_3: 0.99719
	loss_value_3: 0.03328
	loss_reward_3: 0.00491
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99812
	loss_value_4: 0.03248
	loss_reward_4: 0.00524
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99953
	loss_value_5: 0.03221
	loss_reward_5: 0.00779
	loss_policy: 0.00817
	loss_value: 0.335
	loss_reward: 0.02755
[2025-05-08 07:07:43] nn step 17100, lr: 0.1.
	loss_policy_0: 0.0032
	accuracy_policy_0: 0.99227
	loss_value_0: 0.11579
	loss_policy_1: 0.00069
	accuracy_policy_1: 0.99434
	loss_value_1: 0.02337
	loss_reward_1: 0.00323
	loss_policy_2: 0.00054
	accuracy_policy_2: 0.99602
	loss_value_2: 0.02321
	loss_reward_2: 0.00573
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99695
	loss_value_3: 0.02345
	loss_reward_3: 0.00463
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99863
	loss_value_4: 0.02332
	loss_reward_4: 0.00504
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99961
	loss_value_5: 0.02396
	loss_reward_5: 0.00717
	loss_policy: 0.00548
	loss_value: 0.2331
	loss_reward: 0.0258
[2025-05-08 07:07:51] nn step 17150, lr: 0.1.
	loss_policy_0: 0.00159
	accuracy_policy_0: 0.99695
	loss_value_0: 0.1048
	loss_policy_1: 0.00048
	accuracy_policy_1: 0.99516
	loss_value_1: 0.02112
	loss_reward_1: 0.00259
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99656
	loss_value_2: 0.0211
	loss_reward_2: 0.00445
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99727
	loss_value_3: 0.02094
	loss_reward_3: 0.00477
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99832
	loss_value_4: 0.02057
	loss_reward_4: 0.00518
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99938
	loss_value_5: 0.02102
	loss_reward_5: 0.00592
	loss_policy: 0.00372
	loss_value: 0.20955
	loss_reward: 0.02291
[2025-05-08 07:08:00] nn step 17200, lr: 0.1.
	loss_policy_0: 0.00036
	accuracy_policy_0: 1.0
	loss_value_0: 0.10541
	loss_policy_1: 0.00054
	accuracy_policy_1: 0.99777
	loss_value_1: 0.02139
	loss_reward_1: 0.0015
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99742
	loss_value_2: 0.02154
	loss_reward_2: 0.00407
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.9968
	loss_value_3: 0.02142
	loss_reward_3: 0.00462
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.9982
	loss_value_4: 0.02099
	loss_reward_4: 0.00408
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99906
	loss_value_5: 0.02189
	loss_reward_5: 0.00628
	loss_policy: 0.00261
	loss_value: 0.21263
	loss_reward: 0.02054
Optimization_Done 17200
[2025-05-08 07:09:08] [command] train weight_iter_17200.pkl 68 87
[2025-05-08 07:09:17] nn step 17250, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 1.0
	loss_value_0: 0.11225
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99754
	loss_value_1: 0.02267
	loss_reward_1: 0.0012
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99773
	loss_value_2: 0.02287
	loss_reward_2: 0.00394
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99789
	loss_value_3: 0.02279
	loss_reward_3: 0.00475
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99812
	loss_value_4: 0.02245
	loss_reward_4: 0.00432
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99824
	loss_value_5: 0.02319
	loss_reward_5: 0.00649
	loss_policy: 0.00175
	loss_value: 0.22622
	loss_reward: 0.02071
[2025-05-08 07:09:25] nn step 17300, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10099
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99863
	loss_value_1: 0.02032
	loss_reward_1: 0.0012
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99855
	loss_value_2: 0.02049
	loss_reward_2: 0.00369
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.9975
	loss_value_3: 0.0206
	loss_reward_3: 0.00427
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99758
	loss_value_4: 0.02027
	loss_reward_4: 0.00376
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99883
	loss_value_5: 0.02122
	loss_reward_5: 0.00617
	loss_policy: 0.0015
	loss_value: 0.20389
	loss_reward: 0.01909
[2025-05-08 07:09:32] nn step 17350, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10223
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02055
	loss_reward_1: 0.00083
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02068
	loss_reward_2: 0.00357
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99848
	loss_value_3: 0.02069
	loss_reward_3: 0.00358
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.9977
	loss_value_4: 0.02035
	loss_reward_4: 0.00374
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99922
	loss_value_5: 0.02073
	loss_reward_5: 0.00582
	loss_policy: 0.00131
	loss_value: 0.20523
	loss_reward: 0.01755
[2025-05-08 07:09:41] nn step 17400, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10385
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02096
	loss_reward_1: 0.00081
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99828
	loss_value_2: 0.021
	loss_reward_2: 0.00365
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99781
	loss_value_3: 0.02091
	loss_reward_3: 0.00337
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99812
	loss_value_4: 0.02062
	loss_reward_4: 0.00352
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99906
	loss_value_5: 0.02112
	loss_reward_5: 0.0062
	loss_policy: 0.00146
	loss_value: 0.20846
	loss_reward: 0.01755
Optimization_Done 17400
[2025-05-08 07:10:52] [command] train weight_iter_17400.pkl 69 88
[2025-05-08 07:10:59] nn step 17450, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10636
	loss_policy_1: 0.00038
	accuracy_policy_1: 0.9984
	loss_value_1: 0.02121
	loss_reward_1: 0.00091
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99797
	loss_value_2: 0.02126
	loss_reward_2: 0.00346
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.9973
	loss_value_3: 0.02129
	loss_reward_3: 0.00315
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99723
	loss_value_4: 0.02116
	loss_reward_4: 0.00327
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99781
	loss_value_5: 0.02185
	loss_reward_5: 0.00521
	loss_policy: 0.00242
	loss_value: 0.21313
	loss_reward: 0.01601
[2025-05-08 07:11:08] nn step 17500, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09311
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01867
	loss_reward_1: 0.00057
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01876
	loss_reward_2: 0.00293
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99812
	loss_value_3: 0.01872
	loss_reward_3: 0.00219
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01849
	loss_reward_4: 0.00217
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01897
	loss_reward_5: 0.0044
	loss_policy: 0.00136
	loss_value: 0.18672
	loss_reward: 0.01225
[2025-05-08 07:11:16] nn step 17550, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0867
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01743
	loss_reward_1: 0.00053
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01743
	loss_reward_2: 0.00264
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99703
	loss_value_3: 0.01734
	loss_reward_3: 0.00184
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99703
	loss_value_4: 0.01713
	loss_reward_4: 0.00196
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99793
	loss_value_5: 0.0175
	loss_reward_5: 0.00408
	loss_policy: 0.00168
	loss_value: 0.17352
	loss_reward: 0.01105
[2025-05-08 07:11:25] nn step 17600, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09024
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01812
	loss_reward_1: 0.00056
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01806
	loss_reward_2: 0.00273
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99715
	loss_value_3: 0.01801
	loss_reward_3: 0.00201
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01766
	loss_reward_4: 0.00216
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01812
	loss_reward_5: 0.00444
	loss_policy: 0.00196
	loss_value: 0.18019
	loss_reward: 0.0119
Optimization_Done 17600
[2025-05-08 07:12:34] [command] train weight_iter_17600.pkl 70 89
[2025-05-08 07:12:43] nn step 17650, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10681
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99895
	loss_value_1: 0.02137
	loss_reward_1: 0.00064
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99754
	loss_value_2: 0.02148
	loss_reward_2: 0.00279
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02141
	loss_reward_3: 0.00229
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99734
	loss_value_4: 0.02089
	loss_reward_4: 0.00234
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99828
	loss_value_5: 0.02112
	loss_reward_5: 0.00431
	loss_policy: 0.00174
	loss_value: 0.21308
	loss_reward: 0.01236
[2025-05-08 07:12:51] nn step 17700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10282
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99961
	loss_value_1: 0.02053
	loss_reward_1: 0.00064
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99863
	loss_value_2: 0.0205
	loss_reward_2: 0.00297
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99684
	loss_value_3: 0.02041
	loss_reward_3: 0.00254
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99645
	loss_value_4: 0.02003
	loss_reward_4: 0.00251
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99742
	loss_value_5: 0.02035
	loss_reward_5: 0.00458
	loss_policy: 0.00208
	loss_value: 0.20464
	loss_reward: 0.01324
[2025-05-08 07:12:58] nn step 17750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09067
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01814
	loss_reward_1: 0.00052
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01808
	loss_reward_2: 0.00247
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99664
	loss_value_3: 0.01793
	loss_reward_3: 0.00175
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99691
	loss_value_4: 0.0176
	loss_reward_4: 0.0018
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01807
	loss_reward_5: 0.00381
	loss_policy: 0.00172
	loss_value: 0.18048
	loss_reward: 0.01035
[2025-05-08 07:13:06] nn step 17800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09601
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01917
	loss_reward_1: 0.00056
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01897
	loss_reward_2: 0.00274
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99738
	loss_value_3: 0.01891
	loss_reward_3: 0.0019
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99676
	loss_value_4: 0.0185
	loss_reward_4: 0.00209
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99742
	loss_value_5: 0.01881
	loss_reward_5: 0.00416
	loss_policy: 0.00175
	loss_value: 0.19037
	loss_reward: 0.01145
Optimization_Done 17800
[2025-05-08 07:14:15] [command] train weight_iter_17800.pkl 71 90
[2025-05-08 07:14:24] nn step 17850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1035
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02063
	loss_reward_1: 0.00056
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02048
	loss_reward_2: 0.00247
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.9973
	loss_value_3: 0.02042
	loss_reward_3: 0.00174
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99668
	loss_value_4: 0.02005
	loss_reward_4: 0.00191
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99762
	loss_value_5: 0.02044
	loss_reward_5: 0.00379
	loss_policy: 0.00194
	loss_value: 0.20552
	loss_reward: 0.01047
[2025-05-08 07:14:33] nn step 17900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0997
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01981
	loss_reward_1: 0.00058
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99891
	loss_value_2: 0.0196
	loss_reward_2: 0.00256
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99742
	loss_value_3: 0.01955
	loss_reward_3: 0.00183
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99598
	loss_value_4: 0.01925
	loss_reward_4: 0.0019
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99727
	loss_value_5: 0.01947
	loss_reward_5: 0.00397
	loss_policy: 0.00183
	loss_value: 0.19739
	loss_reward: 0.01084
[2025-05-08 07:14:41] nn step 17950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09821
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01947
	loss_reward_1: 0.00055
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01929
	loss_reward_2: 0.00259
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99734
	loss_value_3: 0.01917
	loss_reward_3: 0.00192
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99629
	loss_value_4: 0.01884
	loss_reward_4: 0.00187
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01918
	loss_reward_5: 0.00395
	loss_policy: 0.00219
	loss_value: 0.19416
	loss_reward: 0.01088
[2025-05-08 07:14:48] nn step 18000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09473
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01878
	loss_reward_1: 0.00052
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01849
	loss_reward_2: 0.00259
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.9977
	loss_value_3: 0.0185
	loss_reward_3: 0.00179
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99613
	loss_value_4: 0.01812
	loss_reward_4: 0.00187
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01839
	loss_reward_5: 0.00401
	loss_policy: 0.00206
	loss_value: 0.18702
	loss_reward: 0.01078
Optimization_Done 18000
[2025-05-08 07:15:59] [command] train weight_iter_18000.pkl 72 91
[2025-05-08 07:16:08] nn step 18050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1098
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99898
	loss_value_1: 0.02173
	loss_reward_1: 0.00061
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02159
	loss_reward_2: 0.00272
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99742
	loss_value_3: 0.02145
	loss_reward_3: 0.00202
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99641
	loss_value_4: 0.02111
	loss_reward_4: 0.00197
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99645
	loss_value_5: 0.02137
	loss_reward_5: 0.00402
	loss_policy: 0.0025
	loss_value: 0.21705
	loss_reward: 0.01134
[2025-05-08 07:16:15] nn step 18100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10103
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02006
	loss_reward_1: 0.00055
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01988
	loss_reward_2: 0.00262
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.9966
	loss_value_3: 0.01982
	loss_reward_3: 0.00185
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99555
	loss_value_4: 0.01952
	loss_reward_4: 0.00186
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99656
	loss_value_5: 0.01978
	loss_reward_5: 0.00398
	loss_policy: 0.00254
	loss_value: 0.20009
	loss_reward: 0.01087
[2025-05-08 07:16:23] nn step 18150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10189
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02026
	loss_reward_1: 0.00054
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99875
	loss_value_2: 0.01995
	loss_reward_2: 0.00264
	loss_policy_3: 0.00047
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01987
	loss_reward_3: 0.00193
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99621
	loss_value_4: 0.01939
	loss_reward_4: 0.00198
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99707
	loss_value_5: 0.01959
	loss_reward_5: 0.00405
	loss_policy: 0.00209
	loss_value: 0.20095
	loss_reward: 0.01114
[2025-05-08 07:16:32] nn step 18200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10392
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99906
	loss_value_1: 0.0205
	loss_reward_1: 0.00052
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99836
	loss_value_2: 0.02023
	loss_reward_2: 0.00281
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99785
	loss_value_3: 0.02
	loss_reward_3: 0.00208
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.99617
	loss_value_4: 0.0195
	loss_reward_4: 0.00205
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99711
	loss_value_5: 0.01987
	loss_reward_5: 0.00434
	loss_policy: 0.00219
	loss_value: 0.20402
	loss_reward: 0.01181
Optimization_Done 18200
[2025-05-08 07:17:43] [command] train weight_iter_18200.pkl 73 92
[2025-05-08 07:17:53] nn step 18250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09694
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01914
	loss_reward_1: 0.00048
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99824
	loss_value_2: 0.01882
	loss_reward_2: 0.00245
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.99668
	loss_value_3: 0.01874
	loss_reward_3: 0.00178
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99586
	loss_value_4: 0.01838
	loss_reward_4: 0.00177
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99656
	loss_value_5: 0.01857
	loss_reward_5: 0.0038
	loss_policy: 0.0024
	loss_value: 0.19059
	loss_reward: 0.01028
[2025-05-08 07:18:01] nn step 18300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09317
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01842
	loss_reward_1: 0.00048
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01811
	loss_reward_2: 0.00249
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99727
	loss_value_3: 0.01801
	loss_reward_3: 0.00164
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99598
	loss_value_4: 0.01755
	loss_reward_4: 0.00182
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99633
	loss_value_5: 0.01784
	loss_reward_5: 0.00376
	loss_policy: 0.00243
	loss_value: 0.18311
	loss_reward: 0.0102
[2025-05-08 07:18:08] nn step 18350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09971
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01987
	loss_reward_1: 0.00057
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01959
	loss_reward_2: 0.00279
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.9975
	loss_value_3: 0.01932
	loss_reward_3: 0.00222
	loss_policy_4: 0.00066
	accuracy_policy_4: 0.99586
	loss_value_4: 0.01884
	loss_reward_4: 0.00205
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99633
	loss_value_5: 0.01927
	loss_reward_5: 0.00419
	loss_policy: 0.00254
	loss_value: 0.19659
	loss_reward: 0.01182
[2025-05-08 07:18:16] nn step 18400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09299
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01842
	loss_reward_1: 0.00048
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01798
	loss_reward_2: 0.00256
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99762
	loss_value_3: 0.01778
	loss_reward_3: 0.00181
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.9968
	loss_value_4: 0.01736
	loss_reward_4: 0.00194
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.9966
	loss_value_5: 0.01767
	loss_reward_5: 0.00389
	loss_policy: 0.00186
	loss_value: 0.1822
	loss_reward: 0.01069
Optimization_Done 18400
[2025-05-08 07:19:26] [command] train weight_iter_18400.pkl 74 93
[2025-05-08 07:19:36] nn step 18450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09501
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99902
	loss_value_1: 0.0188
	loss_reward_1: 0.00053
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99805
	loss_value_2: 0.01847
	loss_reward_2: 0.00258
	loss_policy_3: 0.00065
	accuracy_policy_3: 0.99656
	loss_value_3: 0.01828
	loss_reward_3: 0.0018
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99461
	loss_value_4: 0.01777
	loss_reward_4: 0.00191
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.99523
	loss_value_5: 0.01812
	loss_reward_5: 0.00394
	loss_policy: 0.00297
	loss_value: 0.18646
	loss_reward: 0.01076
[2025-05-08 07:19:44] nn step 18500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10208
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02024
	loss_reward_1: 0.00057
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01993
	loss_reward_2: 0.00279
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01971
	loss_reward_3: 0.00198
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.99578
	loss_value_4: 0.01907
	loss_reward_4: 0.00212
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01938
	loss_reward_5: 0.00431
	loss_policy: 0.0024
	loss_value: 0.20041
	loss_reward: 0.01178
[2025-05-08 07:19:53] nn step 18550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09583
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01896
	loss_reward_1: 0.00052
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01864
	loss_reward_2: 0.00267
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01845
	loss_reward_3: 0.0019
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.99578
	loss_value_4: 0.01781
	loss_reward_4: 0.00197
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99617
	loss_value_5: 0.01812
	loss_reward_5: 0.00413
	loss_policy: 0.00219
	loss_value: 0.18781
	loss_reward: 0.01118
[2025-05-08 07:19:59] nn step 18600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09692
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01912
	loss_reward_1: 0.0005
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01878
	loss_reward_2: 0.0027
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99781
	loss_value_3: 0.01859
	loss_reward_3: 0.00183
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99676
	loss_value_4: 0.01792
	loss_reward_4: 0.00192
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99645
	loss_value_5: 0.01822
	loss_reward_5: 0.00419
	loss_policy: 0.00222
	loss_value: 0.18954
	loss_reward: 0.01114
Optimization_Done 18600
[2025-05-08 07:21:09] [command] train weight_iter_18600.pkl 75 94
[2025-05-08 07:21:18] nn step 18650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09838
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99883
	loss_value_1: 0.01944
	loss_reward_1: 0.00051
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.9982
	loss_value_2: 0.01908
	loss_reward_2: 0.00251
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99723
	loss_value_3: 0.01888
	loss_reward_3: 0.00182
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.9952
	loss_value_4: 0.01846
	loss_reward_4: 0.0019
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.9957
	loss_value_5: 0.01866
	loss_reward_5: 0.00385
	loss_policy: 0.003
	loss_value: 0.1929
	loss_reward: 0.0106
[2025-05-08 07:21:24] nn step 18700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10091
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99906
	loss_value_1: 0.01992
	loss_reward_1: 0.00052
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01959
	loss_reward_2: 0.00269
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01942
	loss_reward_3: 0.00193
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.9957
	loss_value_4: 0.01872
	loss_reward_4: 0.00204
	loss_policy_5: 0.0007
	accuracy_policy_5: 0.99605
	loss_value_5: 0.01894
	loss_reward_5: 0.00417
	loss_policy: 0.00251
	loss_value: 0.19751
	loss_reward: 0.01134
[2025-05-08 07:21:33] nn step 18750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09206
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01822
	loss_reward_1: 0.00046
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99914
	loss_value_2: 0.0179
	loss_reward_2: 0.00241
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99844
	loss_value_3: 0.01769
	loss_reward_3: 0.00174
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01703
	loss_reward_4: 0.00184
	loss_policy_5: 0.00069
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01722
	loss_reward_5: 0.00372
	loss_policy: 0.00198
	loss_value: 0.18012
	loss_reward: 0.01017
[2025-05-08 07:21:41] nn step 18800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09828
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01934
	loss_reward_1: 0.00048
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01897
	loss_reward_2: 0.00272
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01879
	loss_reward_3: 0.00187
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01813
	loss_reward_4: 0.00203
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01841
	loss_reward_5: 0.00423
	loss_policy: 0.00213
	loss_value: 0.19192
	loss_reward: 0.01133
Optimization_Done 18800
[2025-05-08 07:22:53] [command] train weight_iter_18800.pkl 76 95
[2025-05-08 07:23:02] nn step 18850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09592
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01896
	loss_reward_1: 0.00047
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01861
	loss_reward_2: 0.00266
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99773
	loss_value_3: 0.01852
	loss_reward_3: 0.00188
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99676
	loss_value_4: 0.01793
	loss_reward_4: 0.00203
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01815
	loss_reward_5: 0.00425
	loss_policy: 0.00225
	loss_value: 0.18809
	loss_reward: 0.01129
[2025-05-08 07:23:10] nn step 18900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09815
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99902
	loss_value_1: 0.0194
	loss_reward_1: 0.00049
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01892
	loss_reward_2: 0.00276
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99762
	loss_value_3: 0.01874
	loss_reward_3: 0.0019
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01811
	loss_reward_4: 0.00192
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99648
	loss_value_5: 0.0183
	loss_reward_5: 0.00422
	loss_policy: 0.0021
	loss_value: 0.19163
	loss_reward: 0.01129
[2025-05-08 07:23:17] nn step 18950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10277
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99922
	loss_value_1: 0.02031
	loss_reward_1: 0.00051
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.9982
	loss_value_2: 0.01982
	loss_reward_2: 0.00291
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99719
	loss_value_3: 0.01962
	loss_reward_3: 0.00202
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99637
	loss_value_4: 0.0189
	loss_reward_4: 0.00196
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.99543
	loss_value_5: 0.01928
	loss_reward_5: 0.00446
	loss_policy: 0.00287
	loss_value: 0.20071
	loss_reward: 0.01186
[2025-05-08 07:23:26] nn step 19000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09455
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01868
	loss_reward_1: 0.00047
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01813
	loss_reward_2: 0.00266
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01784
	loss_reward_3: 0.00184
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.9966
	loss_value_4: 0.0172
	loss_reward_4: 0.00191
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01747
	loss_reward_5: 0.00412
	loss_policy: 0.00234
	loss_value: 0.18389
	loss_reward: 0.011
Optimization_Done 19000
[2025-05-08 07:24:37] [command] train weight_iter_19000.pkl 77 96
[2025-05-08 07:24:44] nn step 19050, lr: 0.1.
	loss_policy_0: 0.00041
	accuracy_policy_0: 0.99965
	loss_value_0: 0.10996
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99906
	loss_value_1: 0.02137
	loss_reward_1: 0.00058
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02138
	loss_reward_2: 0.00318
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99742
	loss_value_3: 0.02142
	loss_reward_3: 0.00262
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.99637
	loss_value_4: 0.02087
	loss_reward_4: 0.00221
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99707
	loss_value_5: 0.02103
	loss_reward_5: 0.00441
	loss_policy: 0.00295
	loss_value: 0.21602
	loss_reward: 0.01299
[2025-05-08 07:24:53] nn step 19100, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.107
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99859
	loss_value_1: 0.02106
	loss_reward_1: 0.00057
	loss_policy_2: 0.00053
	accuracy_policy_2: 0.99703
	loss_value_2: 0.02083
	loss_reward_2: 0.00286
	loss_policy_3: 0.00066
	accuracy_policy_3: 0.99551
	loss_value_3: 0.02062
	loss_reward_3: 0.00205
	loss_policy_4: 0.00088
	accuracy_policy_4: 0.99535
	loss_value_4: 0.01998
	loss_reward_4: 0.00209
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99562
	loss_value_5: 0.02023
	loss_reward_5: 0.00424
	loss_policy: 0.00333
	loss_value: 0.20973
	loss_reward: 0.01181
[2025-05-08 07:25:02] nn step 19150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1011
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01995
	loss_reward_1: 0.00051
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01953
	loss_reward_2: 0.00267
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99637
	loss_value_3: 0.01936
	loss_reward_3: 0.00189
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.995
	loss_value_4: 0.01861
	loss_reward_4: 0.00201
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01897
	loss_reward_5: 0.00408
	loss_policy: 0.00278
	loss_value: 0.19752
	loss_reward: 0.01116
[2025-05-08 07:25:09] nn step 19200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09347
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01846
	loss_reward_1: 0.00043
	loss_policy_2: 0.00047
	accuracy_policy_2: 0.9982
	loss_value_2: 0.01806
	loss_reward_2: 0.00254
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.99688
	loss_value_3: 0.01783
	loss_reward_3: 0.00177
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99582
	loss_value_4: 0.01734
	loss_reward_4: 0.00184
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99574
	loss_value_5: 0.01755
	loss_reward_5: 0.00388
	loss_policy: 0.00282
	loss_value: 0.18271
	loss_reward: 0.01046
Optimization_Done 19200
[2025-05-08 07:26:21] [command] train weight_iter_19200.pkl 78 97
[2025-05-08 07:26:30] nn step 19250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09878
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01949
	loss_reward_1: 0.00049
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99875
	loss_value_2: 0.01916
	loss_reward_2: 0.00247
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99785
	loss_value_3: 0.019
	loss_reward_3: 0.00181
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99594
	loss_value_4: 0.01835
	loss_reward_4: 0.00176
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99621
	loss_value_5: 0.0186
	loss_reward_5: 0.00376
	loss_policy: 0.00258
	loss_value: 0.19339
	loss_reward: 0.01029
[2025-05-08 07:26:37] nn step 19300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10429
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99898
	loss_value_1: 0.02059
	loss_reward_1: 0.00052
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99805
	loss_value_2: 0.02012
	loss_reward_2: 0.00272
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99691
	loss_value_3: 0.01999
	loss_reward_3: 0.00191
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99578
	loss_value_4: 0.01926
	loss_reward_4: 0.00195
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99648
	loss_value_5: 0.01956
	loss_reward_5: 0.00419
	loss_policy: 0.00277
	loss_value: 0.20381
	loss_reward: 0.01128
[2025-05-08 07:26:46] nn step 19350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.101
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01991
	loss_reward_1: 0.00051
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01945
	loss_reward_2: 0.00263
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99742
	loss_value_3: 0.01929
	loss_reward_3: 0.00189
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99586
	loss_value_4: 0.01855
	loss_reward_4: 0.00197
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99625
	loss_value_5: 0.01874
	loss_reward_5: 0.00404
	loss_policy: 0.00244
	loss_value: 0.19695
	loss_reward: 0.01105
[2025-05-08 07:26:55] nn step 19400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10332
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99902
	loss_value_1: 0.02037
	loss_reward_1: 0.00047
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01971
	loss_reward_2: 0.00279
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01949
	loss_reward_3: 0.0019
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.9959
	loss_value_4: 0.01881
	loss_reward_4: 0.0019
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01907
	loss_reward_5: 0.00429
	loss_policy: 0.00258
	loss_value: 0.20077
	loss_reward: 0.01135
Optimization_Done 19400
[2025-05-08 07:28:03] [command] train weight_iter_19400.pkl 79 98
[2025-05-08 07:28:12] nn step 19450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10247
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01986
	loss_reward_1: 0.00046
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01945
	loss_reward_2: 0.00262
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01951
	loss_reward_3: 0.00181
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99617
	loss_value_4: 0.01883
	loss_reward_4: 0.00188
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99645
	loss_value_5: 0.01889
	loss_reward_5: 0.004
	loss_policy: 0.00279
	loss_value: 0.19901
	loss_reward: 0.01077
[2025-05-08 07:28:21] nn step 19500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10033
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01977
	loss_reward_1: 0.0005
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01928
	loss_reward_2: 0.00257
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01921
	loss_reward_3: 0.00181
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01857
	loss_reward_4: 0.0019
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.9966
	loss_value_5: 0.01881
	loss_reward_5: 0.00401
	loss_policy: 0.00228
	loss_value: 0.19598
	loss_reward: 0.0108
[2025-05-08 07:28:27] nn step 19550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10336
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99945
	loss_value_1: 0.0204
	loss_reward_1: 0.00052
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01986
	loss_reward_2: 0.00273
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99699
	loss_value_3: 0.01972
	loss_reward_3: 0.00191
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99562
	loss_value_4: 0.01902
	loss_reward_4: 0.00194
	loss_policy_5: 0.00066
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01925
	loss_reward_5: 0.00419
	loss_policy: 0.00252
	loss_value: 0.20161
	loss_reward: 0.01128
[2025-05-08 07:28:36] nn step 19600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10116
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02003
	loss_reward_1: 0.00047
	loss_policy_2: 0.00047
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01945
	loss_reward_2: 0.00269
	loss_policy_3: 0.00063
	accuracy_policy_3: 0.99762
	loss_value_3: 0.0192
	loss_reward_3: 0.00197
	loss_policy_4: 0.00088
	accuracy_policy_4: 0.99578
	loss_value_4: 0.0185
	loss_reward_4: 0.00192
	loss_policy_5: 0.00094
	accuracy_policy_5: 0.99559
	loss_value_5: 0.01876
	loss_reward_5: 0.00413
	loss_policy: 0.0032
	loss_value: 0.1971
	loss_reward: 0.01116
Optimization_Done 19600
[2025-05-08 07:29:38] [command] train weight_iter_19600.pkl 80 99
[2025-05-08 07:29:47] nn step 19650, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.12074
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02358
	loss_reward_1: 0.00089
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99902
	loss_value_2: 0.02389
	loss_reward_2: 0.00362
	loss_policy_3: 0.00058
	accuracy_policy_3: 0.9977
	loss_value_3: 0.02401
	loss_reward_3: 0.00288
	loss_policy_4: 0.00076
	accuracy_policy_4: 0.99648
	loss_value_4: 0.02354
	loss_reward_4: 0.00296
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.99648
	loss_value_5: 0.0241
	loss_reward_5: 0.00487
	loss_policy: 0.00273
	loss_value: 0.23986
	loss_reward: 0.01523
[2025-05-08 07:29:54] nn step 19700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11072
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02187
	loss_reward_1: 0.00054
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99871
	loss_value_2: 0.02179
	loss_reward_2: 0.00277
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99809
	loss_value_3: 0.02185
	loss_reward_3: 0.00219
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99652
	loss_value_4: 0.02139
	loss_reward_4: 0.00211
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.9966
	loss_value_5: 0.02178
	loss_reward_5: 0.0044
	loss_policy: 0.0026
	loss_value: 0.2194
	loss_reward: 0.01201
[2025-05-08 07:30:03] nn step 19750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10455
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02063
	loss_reward_1: 0.00049
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.99812
	loss_value_2: 0.02035
	loss_reward_2: 0.00253
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02026
	loss_reward_3: 0.0019
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.99613
	loss_value_4: 0.01977
	loss_reward_4: 0.00186
	loss_policy_5: 0.001
	accuracy_policy_5: 0.99543
	loss_value_5: 0.02
	loss_reward_5: 0.00392
	loss_policy: 0.00318
	loss_value: 0.20556
	loss_reward: 0.0107
[2025-05-08 07:30:11] nn step 19800, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10679
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99891
	loss_value_1: 0.02144
	loss_reward_1: 0.0016
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02134
	loss_reward_2: 0.00366
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99715
	loss_value_3: 0.02131
	loss_reward_3: 0.00321
	loss_policy_4: 0.00083
	accuracy_policy_4: 0.99629
	loss_value_4: 0.02089
	loss_reward_4: 0.00345
	loss_policy_5: 0.00093
	accuracy_policy_5: 0.99605
	loss_value_5: 0.02142
	loss_reward_5: 0.00537
	loss_policy: 0.00303
	loss_value: 0.2132
	loss_reward: 0.0173
Optimization_Done 19800
[2025-05-08 07:31:15] [command] train weight_iter_19800.pkl 81 100
[2025-05-08 07:31:24] nn step 19850, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1327
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99902
	loss_value_1: 0.02604
	loss_reward_1: 0.00095
	loss_policy_2: 0.00048
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02602
	loss_reward_2: 0.00286
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02619
	loss_reward_3: 0.0027
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99609
	loss_value_4: 0.02597
	loss_reward_4: 0.00277
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99625
	loss_value_5: 0.02665
	loss_reward_5: 0.00437
	loss_policy: 0.0029
	loss_value: 0.26357
	loss_reward: 0.01365
[2025-05-08 07:31:33] nn step 19900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11798
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99898
	loss_value_1: 0.02313
	loss_reward_1: 0.00051
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99848
	loss_value_2: 0.0231
	loss_reward_2: 0.00254
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.9975
	loss_value_3: 0.02311
	loss_reward_3: 0.00198
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99621
	loss_value_4: 0.02266
	loss_reward_4: 0.00203
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.99641
	loss_value_5: 0.02297
	loss_reward_5: 0.00399
	loss_policy: 0.0029
	loss_value: 0.23295
	loss_reward: 0.01105
[2025-05-08 07:31:39] nn step 19950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11218
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99938
	loss_value_1: 0.02219
	loss_reward_1: 0.00053
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99852
	loss_value_2: 0.02201
	loss_reward_2: 0.00242
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99762
	loss_value_3: 0.02204
	loss_reward_3: 0.00186
	loss_policy_4: 0.00086
	accuracy_policy_4: 0.99574
	loss_value_4: 0.02156
	loss_reward_4: 0.00196
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.99559
	loss_value_5: 0.02175
	loss_reward_5: 0.0038
	loss_policy: 0.00315
	loss_value: 0.22173
	loss_reward: 0.01058
[2025-05-08 07:31:48] nn step 20000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10538
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02074
	loss_reward_1: 0.00048
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99879
	loss_value_2: 0.02054
	loss_reward_2: 0.00245
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99805
	loss_value_3: 0.02045
	loss_reward_3: 0.00177
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99656
	loss_value_4: 0.01991
	loss_reward_4: 0.00179
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99613
	loss_value_5: 0.02026
	loss_reward_5: 0.00379
	loss_policy: 0.00252
	loss_value: 0.20728
	loss_reward: 0.01027
Optimization_Done 20000
[2025-05-08 07:33:00] [command] train weight_iter_20000.pkl 82 101
[2025-05-08 07:33:07] nn step 20050, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12404
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.99926
	loss_value_1: 0.02457
	loss_reward_1: 0.00056
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99844
	loss_value_2: 0.02441
	loss_reward_2: 0.00264
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.9977
	loss_value_3: 0.02425
	loss_reward_3: 0.0021
	loss_policy_4: 0.00079
	accuracy_policy_4: 0.99688
	loss_value_4: 0.02385
	loss_reward_4: 0.00207
	loss_policy_5: 0.00093
	accuracy_policy_5: 0.99652
	loss_value_5: 0.02424
	loss_reward_5: 0.00401
	loss_policy: 0.00303
	loss_value: 0.24536
	loss_reward: 0.01139
[2025-05-08 07:33:16] nn step 20100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10606
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02096
	loss_reward_1: 0.00054
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99852
	loss_value_2: 0.02081
	loss_reward_2: 0.00244
	loss_policy_3: 0.00047
	accuracy_policy_3: 0.99785
	loss_value_3: 0.02076
	loss_reward_3: 0.00168
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99684
	loss_value_4: 0.02041
	loss_reward_4: 0.00177
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.99602
	loss_value_5: 0.02061
	loss_reward_5: 0.00375
	loss_policy: 0.00242
	loss_value: 0.20961
	loss_reward: 0.01019
[2025-05-08 07:33:25] nn step 20150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11052
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99941
	loss_value_1: 0.0218
	loss_reward_1: 0.00048
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02156
	loss_reward_2: 0.00262
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.99777
	loss_value_3: 0.02151
	loss_reward_3: 0.00184
	loss_policy_4: 0.00078
	accuracy_policy_4: 0.99672
	loss_value_4: 0.02091
	loss_reward_4: 0.00187
	loss_policy_5: 0.001
	accuracy_policy_5: 0.99562
	loss_value_5: 0.02139
	loss_reward_5: 0.004
	loss_policy: 0.003
	loss_value: 0.21768
	loss_reward: 0.01082
[2025-05-08 07:33:34] nn step 20200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10649
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02112
	loss_reward_1: 0.00054
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02079
	loss_reward_2: 0.00253
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99809
	loss_value_3: 0.02078
	loss_reward_3: 0.0018
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99727
	loss_value_4: 0.02029
	loss_reward_4: 0.00193
	loss_policy_5: 0.00083
	accuracy_policy_5: 0.99645
	loss_value_5: 0.0206
	loss_reward_5: 0.00395
	loss_policy: 0.00269
	loss_value: 0.21007
	loss_reward: 0.01076
Optimization_Done 20200
[2025-05-08 07:34:47] [command] train weight_iter_20200.pkl 83 102
[2025-05-08 07:34:57] nn step 20250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11459
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02266
	loss_reward_1: 0.00047
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99891
	loss_value_2: 0.02248
	loss_reward_2: 0.00245
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99781
	loss_value_3: 0.02252
	loss_reward_3: 0.00182
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99707
	loss_value_4: 0.02206
	loss_reward_4: 0.00176
	loss_policy_5: 0.00095
	accuracy_policy_5: 0.99598
	loss_value_5: 0.02247
	loss_reward_5: 0.00387
	loss_policy: 0.00273
	loss_value: 0.22679
	loss_reward: 0.01037
[2025-05-08 07:35:03] nn step 20300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10476
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02078
	loss_reward_1: 0.00043
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99879
	loss_value_2: 0.02065
	loss_reward_2: 0.00248
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99801
	loss_value_3: 0.02068
	loss_reward_3: 0.00178
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99668
	loss_value_4: 0.02025
	loss_reward_4: 0.00181
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.99547
	loss_value_5: 0.02057
	loss_reward_5: 0.00373
	loss_policy: 0.00267
	loss_value: 0.20768
	loss_reward: 0.01023
[2025-05-08 07:35:12] nn step 20350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10858
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02146
	loss_reward_1: 0.00049
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99855
	loss_value_2: 0.02118
	loss_reward_2: 0.00267
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02115
	loss_reward_3: 0.00176
	loss_policy_4: 0.00078
	accuracy_policy_4: 0.99625
	loss_value_4: 0.0206
	loss_reward_4: 0.00196
	loss_policy_5: 0.00097
	accuracy_policy_5: 0.99512
	loss_value_5: 0.02104
	loss_reward_5: 0.00405
	loss_policy: 0.00277
	loss_value: 0.21401
	loss_reward: 0.01093
[2025-05-08 07:35:21] nn step 20400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10748
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02131
	loss_reward_1: 0.0005
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.9982
	loss_value_2: 0.02096
	loss_reward_2: 0.00264
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99711
	loss_value_3: 0.02101
	loss_reward_3: 0.0019
	loss_policy_4: 0.0008
	accuracy_policy_4: 0.99605
	loss_value_4: 0.02041
	loss_reward_4: 0.00194
	loss_policy_5: 0.00104
	accuracy_policy_5: 0.99457
	loss_value_5: 0.02081
	loss_reward_5: 0.00408
	loss_policy: 0.00311
	loss_value: 0.21198
	loss_reward: 0.01105
Optimization_Done 20400
[2025-05-08 07:36:40] [command] train weight_iter_20400.pkl 84 103
[2025-05-08 07:36:49] nn step 20450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12506
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02469
	loss_reward_1: 0.00058
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02468
	loss_reward_2: 0.00283
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99789
	loss_value_3: 0.02476
	loss_reward_3: 0.00209
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99668
	loss_value_4: 0.02432
	loss_reward_4: 0.00203
	loss_policy_5: 0.001
	accuracy_policy_5: 0.99574
	loss_value_5: 0.02463
	loss_reward_5: 0.00421
	loss_policy: 0.00301
	loss_value: 0.24814
	loss_reward: 0.01173
[2025-05-08 07:36:56] nn step 20500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10869
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99922
	loss_value_1: 0.02172
	loss_reward_1: 0.00045
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02159
	loss_reward_2: 0.00249
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02168
	loss_reward_3: 0.00177
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99609
	loss_value_4: 0.02115
	loss_reward_4: 0.00177
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99566
	loss_value_5: 0.02153
	loss_reward_5: 0.00381
	loss_policy: 0.00274
	loss_value: 0.21636
	loss_reward: 0.01029
[2025-05-08 07:37:05] nn step 20550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11404
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99949
	loss_value_1: 0.0227
	loss_reward_1: 0.00049
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99844
	loss_value_2: 0.02242
	loss_reward_2: 0.00271
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.9977
	loss_value_3: 0.02248
	loss_reward_3: 0.00195
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.99703
	loss_value_4: 0.02199
	loss_reward_4: 0.00202
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.99578
	loss_value_5: 0.02242
	loss_reward_5: 0.00412
	loss_policy: 0.00261
	loss_value: 0.22605
	loss_reward: 0.0113
[2025-05-08 07:37:13] nn step 20600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11251
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02237
	loss_reward_1: 0.00046
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99836
	loss_value_2: 0.02205
	loss_reward_2: 0.00278
	loss_policy_3: 0.00047
	accuracy_policy_3: 0.99801
	loss_value_3: 0.02205
	loss_reward_3: 0.00194
	loss_policy_4: 0.00087
	accuracy_policy_4: 0.99645
	loss_value_4: 0.02161
	loss_reward_4: 0.00193
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.99539
	loss_value_5: 0.02201
	loss_reward_5: 0.00415
	loss_policy: 0.00298
	loss_value: 0.2226
	loss_reward: 0.01126
Optimization_Done 20600
[2025-05-08 07:38:29] [command] train weight_iter_20600.pkl 85 104
[2025-05-08 07:38:38] nn step 20650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12638
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99938
	loss_value_1: 0.02511
	loss_reward_1: 0.00049
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02506
	loss_reward_2: 0.00256
	loss_policy_3: 0.00074
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02525
	loss_reward_3: 0.00178
	loss_policy_4: 0.0008
	accuracy_policy_4: 0.99637
	loss_value_4: 0.02501
	loss_reward_4: 0.00182
	loss_policy_5: 0.00118
	accuracy_policy_5: 0.99504
	loss_value_5: 0.02537
	loss_reward_5: 0.00383
	loss_policy: 0.00336
	loss_value: 0.25218
	loss_reward: 0.01048
[2025-05-08 07:38:45] nn step 20700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11494
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02278
	loss_reward_1: 0.00048
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99867
	loss_value_2: 0.0227
	loss_reward_2: 0.00253
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02282
	loss_reward_3: 0.00183
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99672
	loss_value_4: 0.02227
	loss_reward_4: 0.00186
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.99531
	loss_value_5: 0.02265
	loss_reward_5: 0.00387
	loss_policy: 0.00279
	loss_value: 0.22815
	loss_reward: 0.01057
[2025-05-08 07:38:53] nn step 20750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10903
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99906
	loss_value_1: 0.02162
	loss_reward_1: 0.00048
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99809
	loss_value_2: 0.02139
	loss_reward_2: 0.00244
	loss_policy_3: 0.00066
	accuracy_policy_3: 0.99695
	loss_value_3: 0.02139
	loss_reward_3: 0.00179
	loss_policy_4: 0.00086
	accuracy_policy_4: 0.99566
	loss_value_4: 0.0209
	loss_reward_4: 0.0018
	loss_policy_5: 0.00108
	accuracy_policy_5: 0.99449
	loss_value_5: 0.02137
	loss_reward_5: 0.00375
	loss_policy: 0.00326
	loss_value: 0.2157
	loss_reward: 0.01026
[2025-05-08 07:39:02] nn step 20800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11695
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02328
	loss_reward_1: 0.0005
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99848
	loss_value_2: 0.023
	loss_reward_2: 0.00274
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99762
	loss_value_3: 0.02305
	loss_reward_3: 0.00189
	loss_policy_4: 0.00087
	accuracy_policy_4: 0.99609
	loss_value_4: 0.02259
	loss_reward_4: 0.00197
	loss_policy_5: 0.00109
	accuracy_policy_5: 0.99469
	loss_value_5: 0.0231
	loss_reward_5: 0.00415
	loss_policy: 0.00312
	loss_value: 0.23197
	loss_reward: 0.01126
Optimization_Done 20800
[2025-05-08 07:40:20] [command] train weight_iter_20800.pkl 86 105
[2025-05-08 07:40:30] nn step 20850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12149
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02389
	loss_reward_1: 0.00045
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02375
	loss_reward_2: 0.00248
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99816
	loss_value_3: 0.024
	loss_reward_3: 0.00168
	loss_policy_4: 0.00068
	accuracy_policy_4: 0.99707
	loss_value_4: 0.02355
	loss_reward_4: 0.0018
	loss_policy_5: 0.00097
	accuracy_policy_5: 0.99582
	loss_value_5: 0.02383
	loss_reward_5: 0.00375
	loss_policy: 0.0025
	loss_value: 0.24051
	loss_reward: 0.01015
[2025-05-08 07:40:36] nn step 20900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11419
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02257
	loss_reward_1: 0.00044
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99852
	loss_value_2: 0.02233
	loss_reward_2: 0.00254
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99758
	loss_value_3: 0.02233
	loss_reward_3: 0.00181
	loss_policy_4: 0.00079
	accuracy_policy_4: 0.99609
	loss_value_4: 0.02186
	loss_reward_4: 0.00183
	loss_policy_5: 0.00097
	accuracy_policy_5: 0.99492
	loss_value_5: 0.02225
	loss_reward_5: 0.00388
	loss_policy: 0.00282
	loss_value: 0.22553
	loss_reward: 0.0105
[2025-05-08 07:40:45] nn step 20950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11464
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02282
	loss_reward_1: 0.00041
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.9982
	loss_value_2: 0.02256
	loss_reward_2: 0.00257
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99723
	loss_value_3: 0.02281
	loss_reward_3: 0.00184
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99637
	loss_value_4: 0.02233
	loss_reward_4: 0.00182
	loss_policy_5: 0.00093
	accuracy_policy_5: 0.9952
	loss_value_5: 0.02265
	loss_reward_5: 0.00398
	loss_policy: 0.00285
	loss_value: 0.22781
	loss_reward: 0.01062
[2025-05-08 07:40:54] nn step 21000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11751
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02341
	loss_reward_1: 0.00049
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02321
	loss_reward_2: 0.00267
	loss_policy_3: 0.00066
	accuracy_policy_3: 0.99719
	loss_value_3: 0.0234
	loss_reward_3: 0.00189
	loss_policy_4: 0.0009
	accuracy_policy_4: 0.99613
	loss_value_4: 0.02283
	loss_reward_4: 0.00197
	loss_policy_5: 0.00115
	accuracy_policy_5: 0.99484
	loss_value_5: 0.02325
	loss_reward_5: 0.00414
	loss_policy: 0.0034
	loss_value: 0.23362
	loss_reward: 0.01115
Optimization_Done 21000
[2025-05-08 07:42:13] [command] train weight_iter_21000.pkl 87 106
[2025-05-08 07:42:22] nn step 21050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11885
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02362
	loss_reward_1: 0.00045
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02353
	loss_reward_2: 0.00237
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99754
	loss_value_3: 0.02364
	loss_reward_3: 0.00171
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99652
	loss_value_4: 0.02315
	loss_reward_4: 0.00177
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99512
	loss_value_5: 0.02343
	loss_reward_5: 0.00362
	loss_policy: 0.00261
	loss_value: 0.23622
	loss_reward: 0.00992
[2025-05-08 07:42:31] nn step 21100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10919
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99938
	loss_value_1: 0.02182
	loss_reward_1: 0.00044
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99844
	loss_value_2: 0.02169
	loss_reward_2: 0.00232
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02176
	loss_reward_3: 0.00163
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99656
	loss_value_4: 0.02135
	loss_reward_4: 0.00175
	loss_policy_5: 0.00095
	accuracy_policy_5: 0.99531
	loss_value_5: 0.02182
	loss_reward_5: 0.00364
	loss_policy: 0.00281
	loss_value: 0.21763
	loss_reward: 0.00977
[2025-05-08 07:42:38] nn step 21150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11857
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02371
	loss_reward_1: 0.00052
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99879
	loss_value_2: 0.02339
	loss_reward_2: 0.00261
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99805
	loss_value_3: 0.02355
	loss_reward_3: 0.00192
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99707
	loss_value_4: 0.02298
	loss_reward_4: 0.00202
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.99609
	loss_value_5: 0.02334
	loss_reward_5: 0.00406
	loss_policy: 0.00251
	loss_value: 0.23553
	loss_reward: 0.01112
[2025-05-08 07:42:46] nn step 21200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11419
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.9993
	loss_value_1: 0.02275
	loss_reward_1: 0.00045
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99879
	loss_value_2: 0.02252
	loss_reward_2: 0.00271
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99809
	loss_value_3: 0.02247
	loss_reward_3: 0.00192
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99652
	loss_value_4: 0.02196
	loss_reward_4: 0.00188
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99555
	loss_value_5: 0.02241
	loss_reward_5: 0.00409
	loss_policy: 0.0024
	loss_value: 0.2263
	loss_reward: 0.01106
Optimization_Done 21200
[2025-05-08 07:44:05] [command] train weight_iter_21200.pkl 88 107
[2025-05-08 07:44:14] nn step 21250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12371
	loss_policy_1: 0.0003
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02459
	loss_reward_1: 0.00048
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02427
	loss_reward_2: 0.00257
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.9973
	loss_value_3: 0.0246
	loss_reward_3: 0.00186
	loss_policy_4: 0.00086
	accuracy_policy_4: 0.99598
	loss_value_4: 0.02428
	loss_reward_4: 0.0018
	loss_policy_5: 0.00115
	accuracy_policy_5: 0.9943
	loss_value_5: 0.02457
	loss_reward_5: 0.00396
	loss_policy: 0.0034
	loss_value: 0.24602
	loss_reward: 0.01066
[2025-05-08 07:44:22] nn step 21300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11831
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02353
	loss_reward_1: 0.00048
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99859
	loss_value_2: 0.02318
	loss_reward_2: 0.00254
	loss_policy_3: 0.00063
	accuracy_policy_3: 0.99727
	loss_value_3: 0.02331
	loss_reward_3: 0.00189
	loss_policy_4: 0.00077
	accuracy_policy_4: 0.99617
	loss_value_4: 0.02283
	loss_reward_4: 0.0018
	loss_policy_5: 0.00099
	accuracy_policy_5: 0.9948
	loss_value_5: 0.02319
	loss_reward_5: 0.00398
	loss_policy: 0.00289
	loss_value: 0.23436
	loss_reward: 0.01069
[2025-05-08 07:44:29] nn step 21350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11609
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99922
	loss_value_1: 0.02315
	loss_reward_1: 0.00046
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99844
	loss_value_2: 0.02286
	loss_reward_2: 0.00257
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99777
	loss_value_3: 0.02287
	loss_reward_3: 0.00184
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.99664
	loss_value_4: 0.02249
	loss_reward_4: 0.00192
	loss_policy_5: 0.00103
	accuracy_policy_5: 0.99512
	loss_value_5: 0.02286
	loss_reward_5: 0.004
	loss_policy: 0.00294
	loss_value: 0.23032
	loss_reward: 0.01079
[2025-05-08 07:44:38] nn step 21400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11319
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02263
	loss_reward_1: 0.00044
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99852
	loss_value_2: 0.02235
	loss_reward_2: 0.0026
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99758
	loss_value_3: 0.02236
	loss_reward_3: 0.00182
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99656
	loss_value_4: 0.02181
	loss_reward_4: 0.00187
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99527
	loss_value_5: 0.0221
	loss_reward_5: 0.00403
	loss_policy: 0.00266
	loss_value: 0.22444
	loss_reward: 0.01075
Optimization_Done 21400
[2025-05-08 07:45:56] [command] train weight_iter_21400.pkl 89 108
[2025-05-08 07:46:05] nn step 21450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1254
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02526
	loss_reward_1: 0.00082
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02557
	loss_reward_2: 0.00333
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99754
	loss_value_3: 0.02561
	loss_reward_3: 0.00279
	loss_policy_4: 0.00089
	accuracy_policy_4: 0.99566
	loss_value_4: 0.02542
	loss_reward_4: 0.0027
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.9948
	loss_value_5: 0.02631
	loss_reward_5: 0.00505
	loss_policy: 0.00292
	loss_value: 0.25356
	loss_reward: 0.01468
[2025-05-08 07:46:14] nn step 21500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11984
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99887
	loss_value_1: 0.02401
	loss_reward_1: 0.00056
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99816
	loss_value_2: 0.02392
	loss_reward_2: 0.00286
	loss_policy_3: 0.00066
	accuracy_policy_3: 0.99691
	loss_value_3: 0.02402
	loss_reward_3: 0.00217
	loss_policy_4: 0.00091
	accuracy_policy_4: 0.99559
	loss_value_4: 0.02339
	loss_reward_4: 0.00206
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.99477
	loss_value_5: 0.02404
	loss_reward_5: 0.00424
	loss_policy: 0.00314
	loss_value: 0.23922
	loss_reward: 0.01189
[2025-05-08 07:46:20] nn step 21550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11473
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99949
	loss_value_1: 0.023
	loss_reward_1: 0.00046
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99844
	loss_value_2: 0.02295
	loss_reward_2: 0.00262
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02301
	loss_reward_3: 0.0018
	loss_policy_4: 0.00071
	accuracy_policy_4: 0.99598
	loss_value_4: 0.02256
	loss_reward_4: 0.00185
	loss_policy_5: 0.00091
	accuracy_policy_5: 0.99527
	loss_value_5: 0.02313
	loss_reward_5: 0.00403
	loss_policy: 0.00261
	loss_value: 0.22937
	loss_reward: 0.01076
[2025-05-08 07:46:29] nn step 21600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11814
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02363
	loss_reward_1: 0.00049
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99855
	loss_value_2: 0.0234
	loss_reward_2: 0.00267
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02342
	loss_reward_3: 0.00191
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99605
	loss_value_4: 0.02295
	loss_reward_4: 0.00193
	loss_policy_5: 0.00095
	accuracy_policy_5: 0.99535
	loss_value_5: 0.0232
	loss_reward_5: 0.00416
	loss_policy: 0.00265
	loss_value: 0.23473
	loss_reward: 0.01116
Optimization_Done 21600
[2025-05-08 07:47:47] [command] train weight_iter_21600.pkl 90 109
[2025-05-08 07:47:56] nn step 21650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13368
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99945
	loss_value_1: 0.0265
	loss_reward_1: 0.00054
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02604
	loss_reward_2: 0.00285
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99727
	loss_value_3: 0.02611
	loss_reward_3: 0.00202
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.9959
	loss_value_4: 0.02563
	loss_reward_4: 0.00198
	loss_policy_5: 0.00112
	accuracy_policy_5: 0.9943
	loss_value_5: 0.02615
	loss_reward_5: 0.00439
	loss_policy: 0.00309
	loss_value: 0.26412
	loss_reward: 0.01177
[2025-05-08 07:48:05] nn step 21700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1258
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02504
	loss_reward_1: 0.00054
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99836
	loss_value_2: 0.02461
	loss_reward_2: 0.00276
	loss_policy_3: 0.00065
	accuracy_policy_3: 0.99727
	loss_value_3: 0.02473
	loss_reward_3: 0.00199
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.99562
	loss_value_4: 0.02423
	loss_reward_4: 0.002
	loss_policy_5: 0.00108
	accuracy_policy_5: 0.99473
	loss_value_5: 0.02467
	loss_reward_5: 0.00419
	loss_policy: 0.00318
	loss_value: 0.24908
	loss_reward: 0.01149
[2025-05-08 07:48:12] nn step 21750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11691
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02343
	loss_reward_1: 0.00048
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99836
	loss_value_2: 0.02312
	loss_reward_2: 0.00266
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99715
	loss_value_3: 0.02326
	loss_reward_3: 0.00187
	loss_policy_4: 0.00081
	accuracy_policy_4: 0.99602
	loss_value_4: 0.02276
	loss_reward_4: 0.00189
	loss_policy_5: 0.00094
	accuracy_policy_5: 0.99496
	loss_value_5: 0.02321
	loss_reward_5: 0.00407
	loss_policy: 0.00295
	loss_value: 0.23269
	loss_reward: 0.01097
[2025-05-08 07:48:21] nn step 21800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12075
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.0241
	loss_reward_1: 0.00053
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99891
	loss_value_2: 0.02377
	loss_reward_2: 0.00287
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99816
	loss_value_3: 0.02383
	loss_reward_3: 0.00192
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99711
	loss_value_4: 0.02315
	loss_reward_4: 0.002
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.9959
	loss_value_5: 0.02369
	loss_reward_5: 0.0043
	loss_policy: 0.00239
	loss_value: 0.23928
	loss_reward: 0.01162
Optimization_Done 21800
[2025-05-08 07:49:37] [command] train weight_iter_21800.pkl 91 110
[2025-05-08 07:49:46] nn step 21850, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13664
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.9991
	loss_value_1: 0.02722
	loss_reward_1: 0.00077
	loss_policy_2: 0.00049
	accuracy_policy_2: 0.99793
	loss_value_2: 0.02734
	loss_reward_2: 0.00273
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.9975
	loss_value_3: 0.02751
	loss_reward_3: 0.00234
	loss_policy_4: 0.00089
	accuracy_policy_4: 0.99578
	loss_value_4: 0.02732
	loss_reward_4: 0.00225
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99531
	loss_value_5: 0.02822
	loss_reward_5: 0.00413
	loss_policy: 0.00301
	loss_value: 0.27425
	loss_reward: 0.01222
[2025-05-08 07:49:54] nn step 21900, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13276
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9991
	loss_value_1: 0.0265
	loss_reward_1: 0.00049
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99824
	loss_value_2: 0.02655
	loss_reward_2: 0.0027
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99699
	loss_value_3: 0.02679
	loss_reward_3: 0.00199
	loss_policy_4: 0.0009
	accuracy_policy_4: 0.99543
	loss_value_4: 0.02631
	loss_reward_4: 0.00193
	loss_policy_5: 0.00099
	accuracy_policy_5: 0.99469
	loss_value_5: 0.02702
	loss_reward_5: 0.00401
	loss_policy: 0.00315
	loss_value: 0.26593
	loss_reward: 0.01112
[2025-05-08 07:50:01] nn step 21950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12398
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99922
	loss_value_1: 0.02488
	loss_reward_1: 0.00048
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99801
	loss_value_2: 0.02479
	loss_reward_2: 0.00253
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.99684
	loss_value_3: 0.02504
	loss_reward_3: 0.0017
	loss_policy_4: 0.00081
	accuracy_policy_4: 0.99566
	loss_value_4: 0.02446
	loss_reward_4: 0.00184
	loss_policy_5: 0.00095
	accuracy_policy_5: 0.99457
	loss_value_5: 0.02482
	loss_reward_5: 0.00389
	loss_policy: 0.00302
	loss_value: 0.24797
	loss_reward: 0.01044
[2025-05-08 07:50:10] nn step 22000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12948
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99898
	loss_value_1: 0.02584
	loss_reward_1: 0.00051
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99816
	loss_value_2: 0.02567
	loss_reward_2: 0.00267
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99715
	loss_value_3: 0.02598
	loss_reward_3: 0.00195
	loss_policy_4: 0.00078
	accuracy_policy_4: 0.9959
	loss_value_4: 0.02521
	loss_reward_4: 0.00199
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.9948
	loss_value_5: 0.02571
	loss_reward_5: 0.00416
	loss_policy: 0.00288
	loss_value: 0.2579
	loss_reward: 0.01128
Optimization_Done 22000
[2025-05-08 07:51:29] [command] train weight_iter_22000.pkl 92 111
[2025-05-08 07:51:38] nn step 22050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13172
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.9991
	loss_value_1: 0.02622
	loss_reward_1: 0.00058
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99789
	loss_value_2: 0.02606
	loss_reward_2: 0.00253
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99652
	loss_value_3: 0.02636
	loss_reward_3: 0.00176
	loss_policy_4: 0.00083
	accuracy_policy_4: 0.995
	loss_value_4: 0.02605
	loss_reward_4: 0.00182
	loss_policy_5: 0.00099
	accuracy_policy_5: 0.99406
	loss_value_5: 0.02657
	loss_reward_5: 0.0038
	loss_policy: 0.00312
	loss_value: 0.26298
	loss_reward: 0.01049
[2025-05-08 07:51:47] nn step 22100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13262
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9991
	loss_value_1: 0.02637
	loss_reward_1: 0.00052
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.99781
	loss_value_2: 0.02623
	loss_reward_2: 0.00267
	loss_policy_3: 0.00076
	accuracy_policy_3: 0.99625
	loss_value_3: 0.02659
	loss_reward_3: 0.0019
	loss_policy_4: 0.00098
	accuracy_policy_4: 0.99477
	loss_value_4: 0.02601
	loss_reward_4: 0.00186
	loss_policy_5: 0.00109
	accuracy_policy_5: 0.9934
	loss_value_5: 0.02658
	loss_reward_5: 0.0041
	loss_policy: 0.00357
	loss_value: 0.26442
	loss_reward: 0.01104
[2025-05-08 07:51:56] nn step 22150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12472
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99918
	loss_value_1: 0.0251
	loss_reward_1: 0.0005
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99793
	loss_value_2: 0.02499
	loss_reward_2: 0.00259
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99715
	loss_value_3: 0.02525
	loss_reward_3: 0.0018
	loss_policy_4: 0.00066
	accuracy_policy_4: 0.99598
	loss_value_4: 0.02467
	loss_reward_4: 0.00179
	loss_policy_5: 0.00095
	accuracy_policy_5: 0.99445
	loss_value_5: 0.0252
	loss_reward_5: 0.00392
	loss_policy: 0.00273
	loss_value: 0.24992
	loss_reward: 0.0106
[2025-05-08 07:52:02] nn step 22200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12509
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99914
	loss_value_1: 0.02491
	loss_reward_1: 0.00045
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99781
	loss_value_2: 0.02461
	loss_reward_2: 0.00263
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.9966
	loss_value_3: 0.02485
	loss_reward_3: 0.00185
	loss_policy_4: 0.00078
	accuracy_policy_4: 0.99547
	loss_value_4: 0.02442
	loss_reward_4: 0.00182
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.99418
	loss_value_5: 0.02515
	loss_reward_5: 0.00405
	loss_policy: 0.00286
	loss_value: 0.24905
	loss_reward: 0.01081
Optimization_Done 22200
[2025-05-08 07:53:22] [command] train weight_iter_22200.pkl 93 112
[2025-05-08 07:53:31] nn step 22250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13224
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02636
	loss_reward_1: 0.00057
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99797
	loss_value_2: 0.0262
	loss_reward_2: 0.00274
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99699
	loss_value_3: 0.02634
	loss_reward_3: 0.00191
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.9959
	loss_value_4: 0.02582
	loss_reward_4: 0.00188
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.99469
	loss_value_5: 0.02625
	loss_reward_5: 0.00416
	loss_policy: 0.00275
	loss_value: 0.26321
	loss_reward: 0.01125
[2025-05-08 07:53:40] nn step 22300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12934
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02582
	loss_reward_1: 0.00052
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02556
	loss_reward_2: 0.00261
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.9973
	loss_value_3: 0.02578
	loss_reward_3: 0.00198
	loss_policy_4: 0.00071
	accuracy_policy_4: 0.99586
	loss_value_4: 0.02537
	loss_reward_4: 0.00191
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99496
	loss_value_5: 0.02589
	loss_reward_5: 0.00399
	loss_policy: 0.00261
	loss_value: 0.25776
	loss_reward: 0.01101
[2025-05-08 07:53:48] nn step 22350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12408
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02476
	loss_reward_1: 0.00051
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99809
	loss_value_2: 0.02456
	loss_reward_2: 0.00257
	loss_policy_3: 0.00063
	accuracy_policy_3: 0.99656
	loss_value_3: 0.02455
	loss_reward_3: 0.0019
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99539
	loss_value_4: 0.02408
	loss_reward_4: 0.00181
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.995
	loss_value_5: 0.02465
	loss_reward_5: 0.00395
	loss_policy: 0.00267
	loss_value: 0.24668
	loss_reward: 0.01073
[2025-05-08 07:53:55] nn step 22400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11764
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02351
	loss_reward_1: 0.00048
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99824
	loss_value_2: 0.02319
	loss_reward_2: 0.0025
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99695
	loss_value_3: 0.02333
	loss_reward_3: 0.0017
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99594
	loss_value_4: 0.02286
	loss_reward_4: 0.00177
	loss_policy_5: 0.0007
	accuracy_policy_5: 0.99488
	loss_value_5: 0.02323
	loss_reward_5: 0.00394
	loss_policy: 0.00216
	loss_value: 0.23376
	loss_reward: 0.01039
Optimization_Done 22400
[2025-05-08 07:55:14] [command] train weight_iter_22400.pkl 94 113
[2025-05-08 07:55:24] nn step 22450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12585
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02515
	loss_reward_1: 0.00053
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99871
	loss_value_2: 0.02495
	loss_reward_2: 0.00259
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02484
	loss_reward_3: 0.00181
	loss_policy_4: 0.00071
	accuracy_policy_4: 0.9966
	loss_value_4: 0.0244
	loss_reward_4: 0.00192
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99578
	loss_value_5: 0.02482
	loss_reward_5: 0.00396
	loss_policy: 0.00229
	loss_value: 0.25001
	loss_reward: 0.01081
[2025-05-08 07:55:32] nn step 22500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12706
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02543
	loss_reward_1: 0.0005
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02508
	loss_reward_2: 0.00267
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99773
	loss_value_3: 0.02511
	loss_reward_3: 0.00195
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.9968
	loss_value_4: 0.0246
	loss_reward_4: 0.00185
	loss_policy_5: 0.00079
	accuracy_policy_5: 0.9957
	loss_value_5: 0.02502
	loss_reward_5: 0.00415
	loss_policy: 0.00231
	loss_value: 0.25229
	loss_reward: 0.01112
[2025-05-08 07:55:41] nn step 22550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12551
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02506
	loss_reward_1: 0.00049
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99871
	loss_value_2: 0.02488
	loss_reward_2: 0.00261
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99797
	loss_value_3: 0.02494
	loss_reward_3: 0.00187
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99699
	loss_value_4: 0.02448
	loss_reward_4: 0.00185
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.99625
	loss_value_5: 0.02499
	loss_reward_5: 0.00396
	loss_policy: 0.00226
	loss_value: 0.24985
	loss_reward: 0.01078
[2025-05-08 07:55:48] nn step 22600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1236
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99957
	loss_value_1: 0.0247
	loss_reward_1: 0.00049
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99898
	loss_value_2: 0.02441
	loss_reward_2: 0.00268
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99867
	loss_value_3: 0.02439
	loss_reward_3: 0.00194
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99797
	loss_value_4: 0.02381
	loss_reward_4: 0.00193
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99691
	loss_value_5: 0.02436
	loss_reward_5: 0.00417
	loss_policy: 0.00168
	loss_value: 0.24527
	loss_reward: 0.01122
Optimization_Done 22600
[2025-05-08 07:57:06] [command] train weight_iter_22600.pkl 95 114
[2025-05-08 07:57:13] nn step 22650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13097
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99938
	loss_value_1: 0.02608
	loss_reward_1: 0.00049
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99871
	loss_value_2: 0.02583
	loss_reward_2: 0.00256
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99789
	loss_value_3: 0.02599
	loss_reward_3: 0.00194
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99676
	loss_value_4: 0.02551
	loss_reward_4: 0.00186
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99586
	loss_value_5: 0.02593
	loss_reward_5: 0.00399
	loss_policy: 0.00232
	loss_value: 0.26031
	loss_reward: 0.01084
[2025-05-08 07:57:22] nn step 22700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13085
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.9991
	loss_value_1: 0.02625
	loss_reward_1: 0.00051
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.9982
	loss_value_2: 0.02599
	loss_reward_2: 0.00268
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99758
	loss_value_3: 0.02609
	loss_reward_3: 0.00202
	loss_policy_4: 0.00057
	accuracy_policy_4: 0.99672
	loss_value_4: 0.02547
	loss_reward_4: 0.00196
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99578
	loss_value_5: 0.02594
	loss_reward_5: 0.00418
	loss_policy: 0.00213
	loss_value: 0.26059
	loss_reward: 0.01135
[2025-05-08 07:57:31] nn step 22750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12708
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02526
	loss_reward_1: 0.00051
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99832
	loss_value_2: 0.02509
	loss_reward_2: 0.00268
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99734
	loss_value_3: 0.02513
	loss_reward_3: 0.00188
	loss_policy_4: 0.00057
	accuracy_policy_4: 0.99645
	loss_value_4: 0.0246
	loss_reward_4: 0.00191
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.99543
	loss_value_5: 0.02526
	loss_reward_5: 0.00418
	loss_policy: 0.00232
	loss_value: 0.25242
	loss_reward: 0.01115
[2025-05-08 07:57:37] nn step 22800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12449
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02494
	loss_reward_1: 0.00048
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99859
	loss_value_2: 0.0247
	loss_reward_2: 0.00265
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99785
	loss_value_3: 0.02479
	loss_reward_3: 0.00192
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99672
	loss_value_4: 0.02421
	loss_reward_4: 0.00191
	loss_policy_5: 0.00062
	accuracy_policy_5: 0.99625
	loss_value_5: 0.02473
	loss_reward_5: 0.00421
	loss_policy: 0.00202
	loss_value: 0.24786
	loss_reward: 0.01119
Optimization_Done 22800
[2025-05-08 07:58:59] [command] train weight_iter_22800.pkl 96 115
[2025-05-08 07:59:06] nn step 22850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11974
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02395
	loss_reward_1: 0.00047
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.9984
	loss_value_2: 0.02372
	loss_reward_2: 0.00253
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02375
	loss_reward_3: 0.00181
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99688
	loss_value_4: 0.02327
	loss_reward_4: 0.00182
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99625
	loss_value_5: 0.0238
	loss_reward_5: 0.00388
	loss_policy: 0.00211
	loss_value: 0.23823
	loss_reward: 0.01051
[2025-05-08 07:59:15] nn step 22900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12326
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02464
	loss_reward_1: 0.00047
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99863
	loss_value_2: 0.02454
	loss_reward_2: 0.00262
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99773
	loss_value_3: 0.02469
	loss_reward_3: 0.00193
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99691
	loss_value_4: 0.0241
	loss_reward_4: 0.00187
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99637
	loss_value_5: 0.02459
	loss_reward_5: 0.0041
	loss_policy: 0.00199
	loss_value: 0.24583
	loss_reward: 0.01098
[2025-05-08 07:59:23] nn step 22950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12054
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02414
	loss_reward_1: 0.00051
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02396
	loss_reward_2: 0.0026
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99824
	loss_value_3: 0.02405
	loss_reward_3: 0.00193
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99734
	loss_value_4: 0.02345
	loss_reward_4: 0.00191
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.9966
	loss_value_5: 0.02393
	loss_reward_5: 0.00397
	loss_policy: 0.00186
	loss_value: 0.24008
	loss_reward: 0.01092
[2025-05-08 07:59:30] nn step 23000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12303
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02466
	loss_reward_1: 0.00048
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99859
	loss_value_2: 0.02441
	loss_reward_2: 0.00265
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99812
	loss_value_3: 0.0244
	loss_reward_3: 0.00198
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99734
	loss_value_4: 0.02383
	loss_reward_4: 0.0019
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99672
	loss_value_5: 0.02431
	loss_reward_5: 0.0041
	loss_policy: 0.002
	loss_value: 0.24464
	loss_reward: 0.0111
Optimization_Done 23000
[2025-05-08 08:00:51] [command] train weight_iter_23000.pkl 97 116
[2025-05-08 08:01:00] nn step 23050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12759
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02549
	loss_reward_1: 0.00049
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02511
	loss_reward_2: 0.00274
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99844
	loss_value_3: 0.025
	loss_reward_3: 0.00192
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99777
	loss_value_4: 0.0246
	loss_reward_4: 0.00192
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99723
	loss_value_5: 0.0251
	loss_reward_5: 0.00414
	loss_policy: 0.00181
	loss_value: 0.25289
	loss_reward: 0.0112
[2025-05-08 08:01:08] nn step 23100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11597
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02312
	loss_reward_1: 0.00044
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99914
	loss_value_2: 0.02292
	loss_reward_2: 0.00249
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99871
	loss_value_3: 0.02294
	loss_reward_3: 0.00177
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99801
	loss_value_4: 0.02246
	loss_reward_4: 0.00183
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99719
	loss_value_5: 0.02294
	loss_reward_5: 0.00386
	loss_policy: 0.00168
	loss_value: 0.23034
	loss_reward: 0.01039
[2025-05-08 08:01:17] nn step 23150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11569
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02319
	loss_reward_1: 0.00046
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02296
	loss_reward_2: 0.00249
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99816
	loss_value_3: 0.02303
	loss_reward_3: 0.00178
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99754
	loss_value_4: 0.02235
	loss_reward_4: 0.00181
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99684
	loss_value_5: 0.02304
	loss_reward_5: 0.00391
	loss_policy: 0.00218
	loss_value: 0.23026
	loss_reward: 0.01046
[2025-05-08 08:01:24] nn step 23200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12356
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02475
	loss_reward_1: 0.00053
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02447
	loss_reward_2: 0.00273
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99836
	loss_value_3: 0.02439
	loss_reward_3: 0.00197
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99797
	loss_value_4: 0.0238
	loss_reward_4: 0.00195
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.9975
	loss_value_5: 0.0243
	loss_reward_5: 0.00431
	loss_policy: 0.00147
	loss_value: 0.24527
	loss_reward: 0.01148
Optimization_Done 23200
[2025-05-08 08:02:45] [command] train weight_iter_23200.pkl 98 117
[2025-05-08 08:02:54] nn step 23250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12517
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02489
	loss_reward_1: 0.00045
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02464
	loss_reward_2: 0.00256
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99867
	loss_value_3: 0.02473
	loss_reward_3: 0.00183
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99793
	loss_value_4: 0.02424
	loss_reward_4: 0.00187
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99695
	loss_value_5: 0.02463
	loss_reward_5: 0.00405
	loss_policy: 0.00156
	loss_value: 0.24829
	loss_reward: 0.01076
[2025-05-08 08:03:03] nn step 23300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12064
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02412
	loss_reward_1: 0.00045
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99906
	loss_value_2: 0.02384
	loss_reward_2: 0.00259
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99848
	loss_value_3: 0.02392
	loss_reward_3: 0.00185
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99773
	loss_value_4: 0.02348
	loss_reward_4: 0.00182
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99707
	loss_value_5: 0.02396
	loss_reward_5: 0.00399
	loss_policy: 0.00167
	loss_value: 0.23996
	loss_reward: 0.0107
[2025-05-08 08:03:12] nn step 23350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11589
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02321
	loss_reward_1: 0.00045
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02304
	loss_reward_2: 0.00244
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99848
	loss_value_3: 0.02315
	loss_reward_3: 0.00183
	loss_policy_4: 0.00051
	accuracy_policy_4: 0.99781
	loss_value_4: 0.02246
	loss_reward_4: 0.00185
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99707
	loss_value_5: 0.02302
	loss_reward_5: 0.00382
	loss_policy: 0.00186
	loss_value: 0.23077
	loss_reward: 0.01039
[2025-05-08 08:03:19] nn step 23400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11769
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02358
	loss_reward_1: 0.00044
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02337
	loss_reward_2: 0.00257
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99859
	loss_value_3: 0.02335
	loss_reward_3: 0.00182
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99793
	loss_value_4: 0.0226
	loss_reward_4: 0.0019
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99727
	loss_value_5: 0.02308
	loss_reward_5: 0.00406
	loss_policy: 0.00139
	loss_value: 0.23369
	loss_reward: 0.0108
Optimization_Done 23400
[2025-05-08 08:04:38] [command] train weight_iter_23400.pkl 99 118
[2025-05-08 08:04:47] nn step 23450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11828
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02352
	loss_reward_1: 0.00045
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02334
	loss_reward_2: 0.00251
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99883
	loss_value_3: 0.02329
	loss_reward_3: 0.00182
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99816
	loss_value_4: 0.02276
	loss_reward_4: 0.00181
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99746
	loss_value_5: 0.02331
	loss_reward_5: 0.00382
	loss_policy: 0.00125
	loss_value: 0.23451
	loss_reward: 0.01041
[2025-05-08 08:04:56] nn step 23500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1254
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02521
	loss_reward_1: 0.00055
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99926
	loss_value_2: 0.02517
	loss_reward_2: 0.00322
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99863
	loss_value_3: 0.02543
	loss_reward_3: 0.00278
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99797
	loss_value_4: 0.02476
	loss_reward_4: 0.00254
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.9973
	loss_value_5: 0.02569
	loss_reward_5: 0.00503
	loss_policy: 0.00186
	loss_value: 0.25165
	loss_reward: 0.01411
[2025-05-08 08:05:05] nn step 23550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11818
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99957
	loss_value_1: 0.0238
	loss_reward_1: 0.00051
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02358
	loss_reward_2: 0.00256
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99859
	loss_value_3: 0.02358
	loss_reward_3: 0.00193
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99801
	loss_value_4: 0.02281
	loss_reward_4: 0.0019
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.9975
	loss_value_5: 0.02357
	loss_reward_5: 0.0041
	loss_policy: 0.0016
	loss_value: 0.23552
	loss_reward: 0.011
[2025-05-08 08:05:11] nn step 23600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11375
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.0228
	loss_reward_1: 0.00047
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99926
	loss_value_2: 0.02264
	loss_reward_2: 0.00251
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99875
	loss_value_3: 0.02266
	loss_reward_3: 0.00178
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99836
	loss_value_4: 0.02208
	loss_reward_4: 0.0018
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99797
	loss_value_5: 0.02266
	loss_reward_5: 0.00389
	loss_policy: 0.00143
	loss_value: 0.22659
	loss_reward: 0.01046
Optimization_Done 23600
[2025-05-08 08:06:31] [command] train weight_iter_23600.pkl 100 119
[2025-05-08 08:06:41] nn step 23650, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13176
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99922
	loss_value_1: 0.0261
	loss_reward_1: 0.00056
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99824
	loss_value_2: 0.02613
	loss_reward_2: 0.00236
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99789
	loss_value_3: 0.02638
	loss_reward_3: 0.00186
	loss_policy_4: 0.00066
	accuracy_policy_4: 0.99676
	loss_value_4: 0.02611
	loss_reward_4: 0.00181
	loss_policy_5: 0.00069
	accuracy_policy_5: 0.99668
	loss_value_5: 0.02683
	loss_reward_5: 0.00377
	loss_policy: 0.00227
	loss_value: 0.26331
	loss_reward: 0.01036
[2025-05-08 08:06:49] nn step 23700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12618
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02533
	loss_reward_1: 0.00055
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99914
	loss_value_2: 0.02516
	loss_reward_2: 0.00256
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99848
	loss_value_3: 0.02533
	loss_reward_3: 0.00185
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.9977
	loss_value_4: 0.02497
	loss_reward_4: 0.00187
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.9968
	loss_value_5: 0.02546
	loss_reward_5: 0.00389
	loss_policy: 0.00136
	loss_value: 0.25245
	loss_reward: 0.01072
[2025-05-08 08:06:58] nn step 23750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1182
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02364
	loss_reward_1: 0.00044
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99879
	loss_value_2: 0.0234
	loss_reward_2: 0.00248
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99793
	loss_value_3: 0.02343
	loss_reward_3: 0.0018
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99695
	loss_value_4: 0.02295
	loss_reward_4: 0.00188
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99633
	loss_value_5: 0.02362
	loss_reward_5: 0.0038
	loss_policy: 0.00161
	loss_value: 0.23523
	loss_reward: 0.01041
[2025-05-08 08:07:05] nn step 23800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11936
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.0239
	loss_reward_1: 0.00051
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.9991
	loss_value_2: 0.02366
	loss_reward_2: 0.00251
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.9984
	loss_value_3: 0.02361
	loss_reward_3: 0.00181
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99801
	loss_value_4: 0.02311
	loss_reward_4: 0.00185
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99711
	loss_value_5: 0.02366
	loss_reward_5: 0.00388
	loss_policy: 0.00129
	loss_value: 0.23729
	loss_reward: 0.01055
Optimization_Done 23800
[2025-05-08 08:08:24] [command] train weight_iter_23800.pkl 101 120
[2025-05-08 08:08:33] nn step 23850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12484
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02482
	loss_reward_1: 0.00047
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99906
	loss_value_2: 0.02452
	loss_reward_2: 0.00248
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99832
	loss_value_3: 0.02465
	loss_reward_3: 0.00181
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.9977
	loss_value_4: 0.02411
	loss_reward_4: 0.00179
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99695
	loss_value_5: 0.02465
	loss_reward_5: 0.00376
	loss_policy: 0.00173
	loss_value: 0.24759
	loss_reward: 0.01032
[2025-05-08 08:08:42] nn step 23900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12557
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02498
	loss_reward_1: 0.00049
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99926
	loss_value_2: 0.0247
	loss_reward_2: 0.00254
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99879
	loss_value_3: 0.02461
	loss_reward_3: 0.00176
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99848
	loss_value_4: 0.02418
	loss_reward_4: 0.00176
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99797
	loss_value_5: 0.02467
	loss_reward_5: 0.00395
	loss_policy: 0.00106
	loss_value: 0.2487
	loss_reward: 0.0105
[2025-05-08 08:08:50] nn step 23950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12203
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02441
	loss_reward_1: 0.0004
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02401
	loss_reward_2: 0.00257
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.9982
	loss_value_3: 0.02383
	loss_reward_3: 0.00184
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99781
	loss_value_4: 0.02324
	loss_reward_4: 0.00177
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99723
	loss_value_5: 0.02384
	loss_reward_5: 0.00395
	loss_policy: 0.00157
	loss_value: 0.24137
	loss_reward: 0.01053
[2025-05-08 08:08:57] nn step 24000, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.12596
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99961
	loss_value_1: 0.0252
	loss_reward_1: 0.00044
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02485
	loss_reward_2: 0.00264
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99844
	loss_value_3: 0.02483
	loss_reward_3: 0.00193
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99719
	loss_value_4: 0.02432
	loss_reward_4: 0.00186
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99781
	loss_value_5: 0.02479
	loss_reward_5: 0.00413
	loss_policy: 0.00164
	loss_value: 0.24995
	loss_reward: 0.011
Optimization_Done 24000
[2025-05-08 08:10:17] [command] train weight_iter_24000.pkl 102 121
[2025-05-08 08:10:27] nn step 24050, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12055
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99895
	loss_value_1: 0.02413
	loss_reward_1: 0.00049
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99879
	loss_value_2: 0.02383
	loss_reward_2: 0.00246
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99855
	loss_value_3: 0.02374
	loss_reward_3: 0.00171
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.9973
	loss_value_4: 0.02326
	loss_reward_4: 0.00181
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.99793
	loss_value_5: 0.02374
	loss_reward_5: 0.00381
	loss_policy: 0.0017
	loss_value: 0.23925
	loss_reward: 0.01027
[2025-05-08 08:10:35] nn step 24100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12147
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02423
	loss_reward_1: 0.00049
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99914
	loss_value_2: 0.02403
	loss_reward_2: 0.00252
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99887
	loss_value_3: 0.024
	loss_reward_3: 0.0017
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.99793
	loss_value_4: 0.02336
	loss_reward_4: 0.00182
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99789
	loss_value_5: 0.02387
	loss_reward_5: 0.00391
	loss_policy: 0.00119
	loss_value: 0.24097
	loss_reward: 0.01044
[2025-05-08 08:10:44] nn step 24150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11472
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02295
	loss_reward_1: 0.00046
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99926
	loss_value_2: 0.02268
	loss_reward_2: 0.00242
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99875
	loss_value_3: 0.02265
	loss_reward_3: 0.00165
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99832
	loss_value_4: 0.02214
	loss_reward_4: 0.00174
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99805
	loss_value_5: 0.02271
	loss_reward_5: 0.00368
	loss_policy: 0.00105
	loss_value: 0.22786
	loss_reward: 0.00996
[2025-05-08 08:10:51] nn step 24200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11832
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02368
	loss_reward_1: 0.00043
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02348
	loss_reward_2: 0.00259
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.9993
	loss_value_3: 0.02352
	loss_reward_3: 0.00173
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99898
	loss_value_4: 0.02275
	loss_reward_4: 0.00192
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99875
	loss_value_5: 0.02335
	loss_reward_5: 0.004
	loss_policy: 0.00065
	loss_value: 0.2351
	loss_reward: 0.01068
Optimization_Done 24200
[2025-05-08 08:12:10] [command] train weight_iter_24200.pkl 103 122
[2025-05-08 08:12:19] nn step 24250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11582
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02318
	loss_reward_1: 0.00047
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99938
	loss_value_2: 0.02285
	loss_reward_2: 0.00236
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99918
	loss_value_3: 0.02273
	loss_reward_3: 0.00171
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99871
	loss_value_4: 0.02229
	loss_reward_4: 0.00179
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99848
	loss_value_5: 0.02262
	loss_reward_5: 0.00369
	loss_policy: 0.00074
	loss_value: 0.22949
	loss_reward: 0.01001
[2025-05-08 08:12:28] nn step 24300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12174
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02434
	loss_reward_1: 0.00045
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02398
	loss_reward_2: 0.0026
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99906
	loss_value_3: 0.02395
	loss_reward_3: 0.00177
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99887
	loss_value_4: 0.02335
	loss_reward_4: 0.00179
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99871
	loss_value_5: 0.02382
	loss_reward_5: 0.00397
	loss_policy: 0.00076
	loss_value: 0.24117
	loss_reward: 0.01057
[2025-05-08 08:12:36] nn step 24350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11695
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02343
	loss_reward_1: 0.00043
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02313
	loss_reward_2: 0.00253
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99898
	loss_value_3: 0.0231
	loss_reward_3: 0.00177
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99859
	loss_value_4: 0.02249
	loss_reward_4: 0.00176
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99867
	loss_value_5: 0.02297
	loss_reward_5: 0.00397
	loss_policy: 0.00086
	loss_value: 0.23206
	loss_reward: 0.01047
[2025-05-08 08:12:43] nn step 24400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12087
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02422
	loss_reward_1: 0.00042
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02397
	loss_reward_2: 0.00264
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99898
	loss_value_3: 0.02379
	loss_reward_3: 0.00183
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.9984
	loss_value_4: 0.02302
	loss_reward_4: 0.00188
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99805
	loss_value_5: 0.02355
	loss_reward_5: 0.00408
	loss_policy: 0.00112
	loss_value: 0.23943
	loss_reward: 0.01084
Optimization_Done 24400
[2025-05-08 08:14:02] [command] train weight_iter_24400.pkl 104 123
[2025-05-08 08:14:11] nn step 24450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12561
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02518
	loss_reward_1: 0.00051
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02495
	loss_reward_2: 0.00292
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99918
	loss_value_3: 0.02476
	loss_reward_3: 0.00239
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99871
	loss_value_4: 0.02439
	loss_reward_4: 0.002
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99812
	loss_value_5: 0.02498
	loss_reward_5: 0.0042
	loss_policy: 0.00092
	loss_value: 0.24987
	loss_reward: 0.01202
[2025-05-08 08:14:19] nn step 24500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12208
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02462
	loss_reward_1: 0.00074
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.9993
	loss_value_2: 0.0244
	loss_reward_2: 0.00297
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99863
	loss_value_3: 0.02428
	loss_reward_3: 0.00226
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.9973
	loss_value_4: 0.02388
	loss_reward_4: 0.00209
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.9977
	loss_value_5: 0.02455
	loss_reward_5: 0.00431
	loss_policy: 0.00154
	loss_value: 0.24381
	loss_reward: 0.01238
[2025-05-08 08:14:28] nn step 24550, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12518
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02518
	loss_reward_1: 0.00065
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.9993
	loss_value_2: 0.02508
	loss_reward_2: 0.00309
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99898
	loss_value_3: 0.02494
	loss_reward_3: 0.00266
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99816
	loss_value_4: 0.02448
	loss_reward_4: 0.00238
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99789
	loss_value_5: 0.0252
	loss_reward_5: 0.0046
	loss_policy: 0.00128
	loss_value: 0.25006
	loss_reward: 0.01337
[2025-05-08 08:14:35] nn step 24600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12669
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99906
	loss_value_1: 0.02528
	loss_reward_1: 0.00052
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99805
	loss_value_2: 0.02512
	loss_reward_2: 0.00279
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99762
	loss_value_3: 0.02496
	loss_reward_3: 0.0021
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99691
	loss_value_4: 0.02445
	loss_reward_4: 0.00215
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.9977
	loss_value_5: 0.02509
	loss_reward_5: 0.00424
	loss_policy: 0.00201
	loss_value: 0.25159
	loss_reward: 0.0118
Optimization_Done 24600
[2025-05-08 08:15:55] [command] train weight_iter_24600.pkl 105 124
[2025-05-08 08:16:02] nn step 24650, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.14068
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99938
	loss_value_1: 0.02809
	loss_reward_1: 0.00098
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99848
	loss_value_2: 0.0279
	loss_reward_2: 0.00271
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02811
	loss_reward_3: 0.00225
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99707
	loss_value_4: 0.02802
	loss_reward_4: 0.00225
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.9968
	loss_value_5: 0.0288
	loss_reward_5: 0.00387
	loss_policy: 0.0019
	loss_value: 0.28159
	loss_reward: 0.01205
[2025-05-08 08:16:11] nn step 24700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12702
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99922
	loss_value_1: 0.02533
	loss_reward_1: 0.0005
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99867
	loss_value_2: 0.02543
	loss_reward_2: 0.00245
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02566
	loss_reward_3: 0.00177
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.9968
	loss_value_4: 0.02539
	loss_reward_4: 0.00181
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99664
	loss_value_5: 0.02593
	loss_reward_5: 0.0036
	loss_policy: 0.00217
	loss_value: 0.25477
	loss_reward: 0.01013
[2025-05-08 08:16:20] nn step 24750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11839
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02364
	loss_reward_1: 0.00042
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.9991
	loss_value_2: 0.02363
	loss_reward_2: 0.00226
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.9982
	loss_value_3: 0.02364
	loss_reward_3: 0.00166
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99703
	loss_value_4: 0.02331
	loss_reward_4: 0.00165
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99695
	loss_value_5: 0.02376
	loss_reward_5: 0.00356
	loss_policy: 0.00151
	loss_value: 0.23636
	loss_reward: 0.00955
[2025-05-08 08:16:27] nn step 24800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12356
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02454
	loss_reward_1: 0.00042
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02428
	loss_reward_2: 0.00251
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99832
	loss_value_3: 0.02432
	loss_reward_3: 0.00185
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99812
	loss_value_4: 0.0238
	loss_reward_4: 0.00179
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99781
	loss_value_5: 0.02434
	loss_reward_5: 0.00387
	loss_policy: 0.00114
	loss_value: 0.24484
	loss_reward: 0.01045
Optimization_Done 24800
[2025-05-08 08:17:48] [command] train weight_iter_24800.pkl 106 125
[2025-05-08 08:17:56] nn step 24850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1267
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02528
	loss_reward_1: 0.00051
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99855
	loss_value_2: 0.02497
	loss_reward_2: 0.0025
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99777
	loss_value_3: 0.02498
	loss_reward_3: 0.00177
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99738
	loss_value_4: 0.0245
	loss_reward_4: 0.00182
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99695
	loss_value_5: 0.025
	loss_reward_5: 0.00382
	loss_policy: 0.00151
	loss_value: 0.25143
	loss_reward: 0.01042
[2025-05-08 08:18:04] nn step 24900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.13329
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.02674
	loss_reward_1: 0.00047
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99941
	loss_value_2: 0.02643
	loss_reward_2: 0.00276
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.9991
	loss_value_3: 0.0264
	loss_reward_3: 0.00196
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99848
	loss_value_4: 0.02572
	loss_reward_4: 0.00203
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99801
	loss_value_5: 0.0264
	loss_reward_5: 0.00425
	loss_policy: 0.00116
	loss_value: 0.26499
	loss_reward: 0.01147
[2025-05-08 08:18:13] nn step 24950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12325
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02467
	loss_reward_1: 0.00041
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99902
	loss_value_2: 0.02427
	loss_reward_2: 0.0026
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99844
	loss_value_3: 0.02427
	loss_reward_3: 0.00187
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99797
	loss_value_4: 0.02379
	loss_reward_4: 0.00181
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.9975
	loss_value_5: 0.02423
	loss_reward_5: 0.00399
	loss_policy: 0.00104
	loss_value: 0.24447
	loss_reward: 0.01069
[2025-05-08 08:18:20] nn step 25000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12291
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02451
	loss_reward_1: 0.0005
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99922
	loss_value_2: 0.02414
	loss_reward_2: 0.00267
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99883
	loss_value_3: 0.02397
	loss_reward_3: 0.00193
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.9984
	loss_value_4: 0.02341
	loss_reward_4: 0.00195
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99766
	loss_value_5: 0.02394
	loss_reward_5: 0.00415
	loss_policy: 0.00115
	loss_value: 0.24289
	loss_reward: 0.0112
Optimization_Done 25000
[2025-05-08 08:19:39] [command] train weight_iter_25000.pkl 107 126
[2025-05-08 08:19:46] nn step 25050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12537
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02504
	loss_reward_1: 0.00047
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02463
	loss_reward_2: 0.00268
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99898
	loss_value_3: 0.02453
	loss_reward_3: 0.00198
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99875
	loss_value_4: 0.02385
	loss_reward_4: 0.00192
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99828
	loss_value_5: 0.02434
	loss_reward_5: 0.00413
	loss_policy: 0.0008
	loss_value: 0.24775
	loss_reward: 0.01119
[2025-05-08 08:19:55] nn step 25100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12133
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99945
	loss_value_1: 0.02427
	loss_reward_1: 0.00048
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.9991
	loss_value_2: 0.02402
	loss_reward_2: 0.00262
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99867
	loss_value_3: 0.02401
	loss_reward_3: 0.0019
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99805
	loss_value_4: 0.02336
	loss_reward_4: 0.0019
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99777
	loss_value_5: 0.02392
	loss_reward_5: 0.004
	loss_policy: 0.00136
	loss_value: 0.24091
	loss_reward: 0.01091
[2025-05-08 08:20:04] nn step 25150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12162
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02445
	loss_reward_1: 0.00043
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.9993
	loss_value_2: 0.024
	loss_reward_2: 0.00267
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99891
	loss_value_3: 0.02389
	loss_reward_3: 0.00199
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99852
	loss_value_4: 0.02321
	loss_reward_4: 0.00185
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99809
	loss_value_5: 0.02368
	loss_reward_5: 0.00415
	loss_policy: 0.00075
	loss_value: 0.24084
	loss_reward: 0.01108
[2025-05-08 08:20:10] nn step 25200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11808
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02371
	loss_reward_1: 0.00046
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02338
	loss_reward_2: 0.00261
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99914
	loss_value_3: 0.02318
	loss_reward_3: 0.00186
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99867
	loss_value_4: 0.02258
	loss_reward_4: 0.00187
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99812
	loss_value_5: 0.02302
	loss_reward_5: 0.00413
	loss_policy: 0.00097
	loss_value: 0.23396
	loss_reward: 0.01093
Optimization_Done 25200
[2025-05-08 08:21:29] [command] train weight_iter_25200.pkl 108 127
[2025-05-08 08:21:37] nn step 25250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11698
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02342
	loss_reward_1: 0.00045
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99953
	loss_value_2: 0.02305
	loss_reward_2: 0.00259
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.9993
	loss_value_3: 0.02287
	loss_reward_3: 0.00183
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99883
	loss_value_4: 0.02239
	loss_reward_4: 0.00186
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99832
	loss_value_5: 0.02283
	loss_reward_5: 0.00399
	loss_policy: 0.00077
	loss_value: 0.23154
	loss_reward: 0.01072
[2025-05-08 08:21:45] nn step 25300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12208
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02437
	loss_reward_1: 0.00043
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02404
	loss_reward_2: 0.00269
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.9991
	loss_value_3: 0.02387
	loss_reward_3: 0.00191
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99871
	loss_value_4: 0.02315
	loss_reward_4: 0.002
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.9984
	loss_value_5: 0.02354
	loss_reward_5: 0.00415
	loss_policy: 0.00076
	loss_value: 0.24105
	loss_reward: 0.01117
[2025-05-08 08:21:54] nn step 25350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11672
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02345
	loss_reward_1: 0.00043
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99938
	loss_value_2: 0.02303
	loss_reward_2: 0.00266
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99883
	loss_value_3: 0.02273
	loss_reward_3: 0.00184
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99863
	loss_value_4: 0.02208
	loss_reward_4: 0.00189
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99832
	loss_value_5: 0.02247
	loss_reward_5: 0.00419
	loss_policy: 0.00103
	loss_value: 0.23049
	loss_reward: 0.01101
[2025-05-08 08:22:02] nn step 25400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11044
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02207
	loss_reward_1: 0.00039
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02177
	loss_reward_2: 0.00246
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99961
	loss_value_3: 0.0216
	loss_reward_3: 0.0018
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99918
	loss_value_4: 0.02096
	loss_reward_4: 0.00177
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99891
	loss_value_5: 0.0213
	loss_reward_5: 0.00384
	loss_policy: 0.0006
	loss_value: 0.21813
	loss_reward: 0.01025
Optimization_Done 25400
[2025-05-08 08:23:21] [command] train weight_iter_25400.pkl 109 128
[2025-05-08 08:23:28] nn step 25450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12419
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.0248
	loss_reward_1: 0.00048
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99953
	loss_value_2: 0.0244
	loss_reward_2: 0.00271
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.9993
	loss_value_3: 0.02419
	loss_reward_3: 0.002
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99883
	loss_value_4: 0.02352
	loss_reward_4: 0.00205
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.9984
	loss_value_5: 0.02406
	loss_reward_5: 0.00414
	loss_policy: 0.00071
	loss_value: 0.24515
	loss_reward: 0.01139
[2025-05-08 08:23:37] nn step 25500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11492
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02303
	loss_reward_1: 0.00045
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99953
	loss_value_2: 0.02247
	loss_reward_2: 0.00261
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.9993
	loss_value_3: 0.02229
	loss_reward_3: 0.00184
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99895
	loss_value_4: 0.02167
	loss_reward_4: 0.0018
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99863
	loss_value_5: 0.02205
	loss_reward_5: 0.00413
	loss_policy: 0.0007
	loss_value: 0.22643
	loss_reward: 0.01083
[2025-05-08 08:23:46] nn step 25550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12154
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02441
	loss_reward_1: 0.00043
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99941
	loss_value_2: 0.02397
	loss_reward_2: 0.00277
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99898
	loss_value_3: 0.02384
	loss_reward_3: 0.00197
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99867
	loss_value_4: 0.02319
	loss_reward_4: 0.00207
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.9982
	loss_value_5: 0.02375
	loss_reward_5: 0.00427
	loss_policy: 0.0011
	loss_value: 0.24069
	loss_reward: 0.01151
[2025-05-08 08:23:54] nn step 25600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1156
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02304
	loss_reward_1: 0.00044
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02256
	loss_reward_2: 0.00263
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99918
	loss_value_3: 0.02235
	loss_reward_3: 0.00185
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99867
	loss_value_4: 0.02162
	loss_reward_4: 0.00189
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99805
	loss_value_5: 0.02222
	loss_reward_5: 0.00409
	loss_policy: 0.00105
	loss_value: 0.22739
	loss_reward: 0.01091
Optimization_Done 25600
[2025-05-08 08:25:17] [command] train weight_iter_25600.pkl 110 129
[2025-05-08 08:25:24] nn step 25650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11857
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02378
	loss_reward_1: 0.00042
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.0233
	loss_reward_2: 0.00272
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99949
	loss_value_3: 0.0231
	loss_reward_3: 0.00199
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99926
	loss_value_4: 0.02248
	loss_reward_4: 0.00188
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99859
	loss_value_5: 0.02291
	loss_reward_5: 0.00426
	loss_policy: 0.00063
	loss_value: 0.23414
	loss_reward: 0.01128
[2025-05-08 08:25:33] nn step 25700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12461
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02476
	loss_reward_1: 0.00043
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02426
	loss_reward_2: 0.00291
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99922
	loss_value_3: 0.02408
	loss_reward_3: 0.00205
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99875
	loss_value_4: 0.02335
	loss_reward_4: 0.00204
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99836
	loss_value_5: 0.02382
	loss_reward_5: 0.00451
	loss_policy: 0.00114
	loss_value: 0.24487
	loss_reward: 0.01193
[2025-05-08 08:25:42] nn step 25750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11857
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02364
	loss_reward_1: 0.00039
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0232
	loss_reward_2: 0.00276
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02294
	loss_reward_3: 0.0019
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99898
	loss_value_4: 0.02225
	loss_reward_4: 0.00191
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99859
	loss_value_5: 0.02274
	loss_reward_5: 0.00423
	loss_policy: 0.00077
	loss_value: 0.23334
	loss_reward: 0.01119
[2025-05-08 08:25:49] nn step 25800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11661
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02325
	loss_reward_1: 0.00045
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99965
	loss_value_2: 0.02276
	loss_reward_2: 0.0028
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02255
	loss_reward_3: 0.00197
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99902
	loss_value_4: 0.02189
	loss_reward_4: 0.00201
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99867
	loss_value_5: 0.02246
	loss_reward_5: 0.00432
	loss_policy: 0.00072
	loss_value: 0.22951
	loss_reward: 0.01154
Optimization_Done 25800
[2025-05-08 08:27:09] [command] train weight_iter_25800.pkl 111 130
[2025-05-08 08:27:16] nn step 25850, lr: 0.1.
	loss_policy_0: 0.00027
	accuracy_policy_0: 0.99977
	loss_value_0: 0.12161
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02428
	loss_reward_1: 0.00044
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99871
	loss_value_2: 0.02363
	loss_reward_2: 0.00285
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99871
	loss_value_3: 0.02335
	loss_reward_3: 0.00209
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99871
	loss_value_4: 0.02252
	loss_reward_4: 0.00205
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.99789
	loss_value_5: 0.02295
	loss_reward_5: 0.00443
	loss_policy: 0.00166
	loss_value: 0.23835
	loss_reward: 0.01185
[2025-05-08 08:27:25] nn step 25900, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99977
	loss_value_0: 0.1151
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99938
	loss_value_1: 0.0231
	loss_reward_1: 0.00047
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99871
	loss_value_2: 0.0226
	loss_reward_2: 0.0029
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99793
	loss_value_3: 0.02233
	loss_reward_3: 0.00217
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99746
	loss_value_4: 0.02172
	loss_reward_4: 0.00199
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99875
	loss_value_5: 0.02235
	loss_reward_5: 0.0045
	loss_policy: 0.00154
	loss_value: 0.2272
	loss_reward: 0.01203
[2025-05-08 08:27:34] nn step 25950, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11776
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02348
	loss_reward_1: 0.00044
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99938
	loss_value_2: 0.02281
	loss_reward_2: 0.00286
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99859
	loss_value_3: 0.02248
	loss_reward_3: 0.002
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99848
	loss_value_4: 0.02179
	loss_reward_4: 0.00198
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99867
	loss_value_5: 0.02234
	loss_reward_5: 0.00442
	loss_policy: 0.00109
	loss_value: 0.23065
	loss_reward: 0.0117
[2025-05-08 08:27:41] nn step 26000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10573
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02111
	loss_reward_1: 0.0004
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02067
	loss_reward_2: 0.00253
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99938
	loss_value_3: 0.02037
	loss_reward_3: 0.00181
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01966
	loss_reward_4: 0.00179
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99879
	loss_value_5: 0.02011
	loss_reward_5: 0.00396
	loss_policy: 0.00087
	loss_value: 0.20765
	loss_reward: 0.01049
Optimization_Done 26000
[2025-05-08 08:29:03] [command] train weight_iter_26000.pkl 112 131
[2025-05-08 08:29:10] nn step 26050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11629
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0232
	loss_reward_1: 0.00044
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.02263
	loss_reward_2: 0.00278
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99949
	loss_value_3: 0.02228
	loss_reward_3: 0.002
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99914
	loss_value_4: 0.02144
	loss_reward_4: 0.00196
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99891
	loss_value_5: 0.02192
	loss_reward_5: 0.00431
	loss_policy: 0.00063
	loss_value: 0.22776
	loss_reward: 0.01149
[2025-05-08 08:29:19] nn step 26100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11396
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02271
	loss_reward_1: 0.00042
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02206
	loss_reward_2: 0.00276
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99953
	loss_value_3: 0.0218
	loss_reward_3: 0.002
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99926
	loss_value_4: 0.02099
	loss_reward_4: 0.00195
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99895
	loss_value_5: 0.02149
	loss_reward_5: 0.00433
	loss_policy: 0.00055
	loss_value: 0.22301
	loss_reward: 0.01147
[2025-05-08 08:29:28] nn step 26150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11036
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02199
	loss_reward_1: 0.0004
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99875
	loss_value_2: 0.02141
	loss_reward_2: 0.00275
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99852
	loss_value_3: 0.02119
	loss_reward_3: 0.00185
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99836
	loss_value_4: 0.02037
	loss_reward_4: 0.00191
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99867
	loss_value_5: 0.02089
	loss_reward_5: 0.0043
	loss_policy: 0.00098
	loss_value: 0.21621
	loss_reward: 0.01122
[2025-05-08 08:29:35] nn step 26200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10959
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02186
	loss_reward_1: 0.00041
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99902
	loss_value_2: 0.02121
	loss_reward_2: 0.00269
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99828
	loss_value_3: 0.02086
	loss_reward_3: 0.00196
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99793
	loss_value_4: 0.02012
	loss_reward_4: 0.00192
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99867
	loss_value_5: 0.02058
	loss_reward_5: 0.00418
	loss_policy: 0.00122
	loss_value: 0.21422
	loss_reward: 0.01115
Optimization_Done 26200
[2025-05-08 08:30:52] [command] train weight_iter_26200.pkl 113 132
[2025-05-08 08:30:59] nn step 26250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1168
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02324
	loss_reward_1: 0.00047
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02265
	loss_reward_2: 0.00291
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99914
	loss_value_3: 0.02225
	loss_reward_3: 0.002
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99848
	loss_value_4: 0.02142
	loss_reward_4: 0.0021
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99855
	loss_value_5: 0.02192
	loss_reward_5: 0.00446
	loss_policy: 0.00103
	loss_value: 0.22829
	loss_reward: 0.01194
[2025-05-08 08:31:08] nn step 26300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11287
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02249
	loss_reward_1: 0.00046
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02189
	loss_reward_2: 0.00278
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02161
	loss_reward_3: 0.00195
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99953
	loss_value_4: 0.02086
	loss_reward_4: 0.00198
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99934
	loss_value_5: 0.02127
	loss_reward_5: 0.00436
	loss_policy: 0.00037
	loss_value: 0.221
	loss_reward: 0.01152
[2025-05-08 08:31:17] nn step 26350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10986
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02194
	loss_reward_1: 0.00042
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.0213
	loss_reward_2: 0.00269
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02108
	loss_reward_3: 0.00197
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99934
	loss_value_4: 0.02024
	loss_reward_4: 0.00187
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99906
	loss_value_5: 0.02072
	loss_reward_5: 0.00421
	loss_policy: 0.00065
	loss_value: 0.21513
	loss_reward: 0.01115
[2025-05-08 08:31:25] nn step 26400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10974
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02193
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02135
	loss_reward_2: 0.0027
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02102
	loss_reward_3: 0.00203
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99914
	loss_value_4: 0.02017
	loss_reward_4: 0.00207
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99891
	loss_value_5: 0.02053
	loss_reward_5: 0.00424
	loss_policy: 0.00057
	loss_value: 0.21474
	loss_reward: 0.01149
Optimization_Done 26400
[2025-05-08 08:32:48] [command] train weight_iter_26400.pkl 114 133
[2025-05-08 08:32:56] nn step 26450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10635
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02129
	loss_reward_1: 0.00039
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02073
	loss_reward_2: 0.00254
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99949
	loss_value_3: 0.02039
	loss_reward_3: 0.00183
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01963
	loss_reward_4: 0.00185
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.9991
	loss_value_5: 0.02021
	loss_reward_5: 0.00399
	loss_policy: 0.00056
	loss_value: 0.2086
	loss_reward: 0.0106
[2025-05-08 08:33:05] nn step 26500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11529
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02303
	loss_reward_1: 0.00043
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02238
	loss_reward_2: 0.0029
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.02202
	loss_reward_3: 0.00204
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99941
	loss_value_4: 0.0212
	loss_reward_4: 0.00205
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99934
	loss_value_5: 0.02166
	loss_reward_5: 0.00449
	loss_policy: 0.00041
	loss_value: 0.22559
	loss_reward: 0.01192
[2025-05-08 08:33:13] nn step 26550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10867
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0217
	loss_reward_1: 0.0004
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0211
	loss_reward_2: 0.00273
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99949
	loss_value_3: 0.02081
	loss_reward_3: 0.00189
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99938
	loss_value_4: 0.02009
	loss_reward_4: 0.00196
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99902
	loss_value_5: 0.0206
	loss_reward_5: 0.0043
	loss_policy: 0.00063
	loss_value: 0.21296
	loss_reward: 0.01128
[2025-05-08 08:33:20] nn step 26600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10795
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0215
	loss_reward_1: 0.0004
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02093
	loss_reward_2: 0.00274
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02063
	loss_reward_3: 0.00191
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01975
	loss_reward_4: 0.00196
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99898
	loss_value_5: 0.02024
	loss_reward_5: 0.00428
	loss_policy: 0.0004
	loss_value: 0.21099
	loss_reward: 0.0113
Optimization_Done 26600
[2025-05-08 08:34:41] [command] train weight_iter_26600.pkl 115 134
[2025-05-08 08:34:49] nn step 26650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10911
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02183
	loss_reward_1: 0.00046
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02125
	loss_reward_2: 0.00266
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02086
	loss_reward_3: 0.00197
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99953
	loss_value_4: 0.02013
	loss_reward_4: 0.00195
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99918
	loss_value_5: 0.02055
	loss_reward_5: 0.00422
	loss_policy: 0.00048
	loss_value: 0.21373
	loss_reward: 0.01126
[2025-05-08 08:34:57] nn step 26700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10986
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02197
	loss_reward_1: 0.00041
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.02128
	loss_reward_2: 0.00277
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99938
	loss_value_3: 0.02095
	loss_reward_3: 0.00189
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99926
	loss_value_4: 0.02024
	loss_reward_4: 0.00197
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99895
	loss_value_5: 0.02061
	loss_reward_5: 0.00432
	loss_policy: 0.00077
	loss_value: 0.21492
	loss_reward: 0.01136
[2025-05-08 08:35:05] nn step 26750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10512
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02102
	loss_reward_1: 0.00043
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02036
	loss_reward_2: 0.00272
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99949
	loss_value_3: 0.02008
	loss_reward_3: 0.0019
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01944
	loss_reward_4: 0.00184
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01982
	loss_reward_5: 0.00432
	loss_policy: 0.00065
	loss_value: 0.20583
	loss_reward: 0.01122
[2025-05-08 08:35:12] nn step 26800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11005
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02198
	loss_reward_1: 0.00041
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0213
	loss_reward_2: 0.0028
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02097
	loss_reward_3: 0.00203
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99918
	loss_value_4: 0.02007
	loss_reward_4: 0.00198
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99883
	loss_value_5: 0.02045
	loss_reward_5: 0.00442
	loss_policy: 0.00069
	loss_value: 0.21483
	loss_reward: 0.01165
Optimization_Done 26800
[2025-05-08 08:36:34] [command] train weight_iter_26800.pkl 116 135
[2025-05-08 08:36:42] nn step 26850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11191
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02233
	loss_reward_1: 0.00047
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02162
	loss_reward_2: 0.0029
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99957
	loss_value_3: 0.02132
	loss_reward_3: 0.00205
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99934
	loss_value_4: 0.02046
	loss_reward_4: 0.00201
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99906
	loss_value_5: 0.02102
	loss_reward_5: 0.00449
	loss_policy: 0.00062
	loss_value: 0.21866
	loss_reward: 0.01193
[2025-05-08 08:36:51] nn step 26900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10893
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02171
	loss_reward_1: 0.00043
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02103
	loss_reward_2: 0.00286
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99898
	loss_value_3: 0.02074
	loss_reward_3: 0.00199
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01993
	loss_reward_4: 0.00202
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99926
	loss_value_5: 0.02042
	loss_reward_5: 0.00438
	loss_policy: 0.00074
	loss_value: 0.21276
	loss_reward: 0.01168
[2025-05-08 08:36:59] nn step 26950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10819
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02156
	loss_reward_1: 0.00041
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02088
	loss_reward_2: 0.00282
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99938
	loss_value_3: 0.02057
	loss_reward_3: 0.00198
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01978
	loss_reward_4: 0.00192
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99926
	loss_value_5: 0.02012
	loss_reward_5: 0.00444
	loss_policy: 0.00077
	loss_value: 0.21109
	loss_reward: 0.01158
[2025-05-08 08:37:06] nn step 27000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10604
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02119
	loss_reward_1: 0.00041
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02056
	loss_reward_2: 0.00276
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99914
	loss_value_3: 0.02013
	loss_reward_3: 0.00196
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99906
	loss_value_4: 0.0193
	loss_reward_4: 0.00206
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.9993
	loss_value_5: 0.0198
	loss_reward_5: 0.00433
	loss_policy: 0.00068
	loss_value: 0.20702
	loss_reward: 0.01152
Optimization_Done 27000
[2025-05-08 08:38:29] [command] train weight_iter_27000.pkl 117 136
[2025-05-08 08:38:37] nn step 27050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10169
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02029
	loss_reward_1: 0.00038
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01969
	loss_reward_2: 0.00258
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01933
	loss_reward_3: 0.00192
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01851
	loss_reward_4: 0.00186
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01896
	loss_reward_5: 0.00398
	loss_policy: 0.00056
	loss_value: 0.19847
	loss_reward: 0.01072
[2025-05-08 08:38:45] nn step 27100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10499
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02102
	loss_reward_1: 0.00045
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02035
	loss_reward_2: 0.00277
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02004
	loss_reward_3: 0.00196
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01923
	loss_reward_4: 0.00201
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01967
	loss_reward_5: 0.00438
	loss_policy: 0.00073
	loss_value: 0.2053
	loss_reward: 0.01157
[2025-05-08 08:38:54] nn step 27150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10085
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02011
	loss_reward_1: 0.00039
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01945
	loss_reward_2: 0.0026
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01914
	loss_reward_3: 0.00186
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01833
	loss_reward_4: 0.00181
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01883
	loss_reward_5: 0.00409
	loss_policy: 0.00042
	loss_value: 0.19672
	loss_reward: 0.01074
[2025-05-08 08:39:01] nn step 27200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10744
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02144
	loss_reward_1: 0.00042
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02078
	loss_reward_2: 0.00284
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99949
	loss_value_3: 0.02038
	loss_reward_3: 0.002
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01956
	loss_reward_4: 0.00203
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.9991
	loss_value_5: 0.02007
	loss_reward_5: 0.00439
	loss_policy: 0.00062
	loss_value: 0.20967
	loss_reward: 0.01167
Optimization_Done 27200
[2025-05-08 08:40:19] [command] train weight_iter_27200.pkl 118 137
[2025-05-08 08:40:26] nn step 27250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10856
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02157
	loss_reward_1: 0.00041
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.02093
	loss_reward_2: 0.00278
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99953
	loss_value_3: 0.02067
	loss_reward_3: 0.00194
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01979
	loss_reward_4: 0.00204
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99914
	loss_value_5: 0.02018
	loss_reward_5: 0.0044
	loss_policy: 0.00079
	loss_value: 0.21171
	loss_reward: 0.01156
[2025-05-08 08:40:35] nn step 27300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10404
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02075
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02008
	loss_reward_2: 0.00273
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01968
	loss_reward_3: 0.00197
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01899
	loss_reward_4: 0.00189
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01938
	loss_reward_5: 0.00421
	loss_policy: 0.00054
	loss_value: 0.2029
	loss_reward: 0.0112
[2025-05-08 08:40:43] nn step 27350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10625
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02112
	loss_reward_1: 0.00044
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02051
	loss_reward_2: 0.00279
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.02015
	loss_reward_3: 0.00194
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01931
	loss_reward_4: 0.00203
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01968
	loss_reward_5: 0.00442
	loss_policy: 0.00066
	loss_value: 0.20702
	loss_reward: 0.01161
[2025-05-08 08:40:52] nn step 27400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11171
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02225
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02143
	loss_reward_2: 0.003
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.02112
	loss_reward_3: 0.00205
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99961
	loss_value_4: 0.02018
	loss_reward_4: 0.00201
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99945
	loss_value_5: 0.02068
	loss_reward_5: 0.00467
	loss_policy: 0.00042
	loss_value: 0.21737
	loss_reward: 0.01213
Optimization_Done 27400
[2025-05-08 08:42:14] [command] train weight_iter_27400.pkl 119 138
[2025-05-08 08:42:22] nn step 27450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10525
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02093
	loss_reward_1: 0.00042
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02027
	loss_reward_2: 0.00271
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01985
	loss_reward_3: 0.00195
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01901
	loss_reward_4: 0.00188
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01941
	loss_reward_5: 0.00426
	loss_policy: 0.00059
	loss_value: 0.20473
	loss_reward: 0.01122
[2025-05-08 08:42:30] nn step 27500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10417
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02077
	loss_reward_1: 0.00037
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02005
	loss_reward_2: 0.00276
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01972
	loss_reward_3: 0.00195
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99973
	loss_value_4: 0.01886
	loss_reward_4: 0.00205
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01926
	loss_reward_5: 0.00431
	loss_policy: 0.0003
	loss_value: 0.20283
	loss_reward: 0.01143
[2025-05-08 08:42:39] nn step 27550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10595
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02116
	loss_reward_1: 0.00042
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02048
	loss_reward_2: 0.00278
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.02006
	loss_reward_3: 0.00195
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.0191
	loss_reward_4: 0.00206
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01953
	loss_reward_5: 0.00442
	loss_policy: 0.00035
	loss_value: 0.20627
	loss_reward: 0.01164
[2025-05-08 08:42:46] nn step 27600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1041
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02078
	loss_reward_1: 0.00041
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02015
	loss_reward_2: 0.0028
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01975
	loss_reward_3: 0.00195
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99957
	loss_value_4: 0.01893
	loss_reward_4: 0.00201
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99949
	loss_value_5: 0.0195
	loss_reward_5: 0.0044
	loss_policy: 0.0004
	loss_value: 0.20321
	loss_reward: 0.01156
Optimization_Done 27600
[2025-05-08 08:44:02] [command] train weight_iter_27600.pkl 120 139
[2025-05-08 08:44:10] nn step 27650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10461
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02085
	loss_reward_1: 0.00043
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02017
	loss_reward_2: 0.00278
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.0197
	loss_reward_3: 0.00196
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01887
	loss_reward_4: 0.002
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01926
	loss_reward_5: 0.00435
	loss_policy: 0.0004
	loss_value: 0.20345
	loss_reward: 0.01152
[2025-05-08 08:44:18] nn step 27700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10852
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02151
	loss_reward_1: 0.00043
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02081
	loss_reward_2: 0.00291
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99953
	loss_value_3: 0.02036
	loss_reward_3: 0.00193
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99914
	loss_value_4: 0.0195
	loss_reward_4: 0.00213
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01997
	loss_reward_5: 0.00454
	loss_policy: 0.00065
	loss_value: 0.21068
	loss_reward: 0.01194
[2025-05-08 08:44:27] nn step 27750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10543
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02095
	loss_reward_1: 0.00039
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02027
	loss_reward_2: 0.00294
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01982
	loss_reward_3: 0.00201
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99961
	loss_value_4: 0.0189
	loss_reward_4: 0.00202
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01934
	loss_reward_5: 0.00451
	loss_policy: 0.00039
	loss_value: 0.20472
	loss_reward: 0.01186
[2025-05-08 08:44:35] nn step 27800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10658
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0212
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.0205
	loss_reward_2: 0.00294
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.02002
	loss_reward_3: 0.00207
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01904
	loss_reward_4: 0.00203
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01958
	loss_reward_5: 0.00466
	loss_policy: 0.00061
	loss_value: 0.20691
	loss_reward: 0.01214
Optimization_Done 27800
[2025-05-08 08:45:55] [command] train weight_iter_27800.pkl 121 140
[2025-05-08 08:46:02] nn step 27850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10709
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02128
	loss_reward_1: 0.00043
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02058
	loss_reward_2: 0.00281
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99961
	loss_value_3: 0.02017
	loss_reward_3: 0.002
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01929
	loss_reward_4: 0.00209
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01968
	loss_reward_5: 0.00447
	loss_policy: 0.00073
	loss_value: 0.20809
	loss_reward: 0.0118
[2025-05-08 08:46:11] nn step 27900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1047
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02079
	loss_reward_1: 0.00042
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02009
	loss_reward_2: 0.0028
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01964
	loss_reward_3: 0.00199
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01874
	loss_reward_4: 0.00207
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01917
	loss_reward_5: 0.00434
	loss_policy: 0.00061
	loss_value: 0.20313
	loss_reward: 0.01163
[2025-05-08 08:46:20] nn step 27950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10204
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02028
	loss_reward_1: 0.00043
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01964
	loss_reward_2: 0.0028
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01914
	loss_reward_3: 0.002
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01822
	loss_reward_4: 0.00209
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01861
	loss_reward_5: 0.00434
	loss_policy: 0.00068
	loss_value: 0.19793
	loss_reward: 0.01167
[2025-05-08 08:46:28] nn step 28000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11172
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0222
	loss_reward_1: 0.00041
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02141
	loss_reward_2: 0.00302
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99965
	loss_value_3: 0.02098
	loss_reward_3: 0.00219
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99938
	loss_value_4: 0.02007
	loss_reward_4: 0.00214
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.9991
	loss_value_5: 0.02054
	loss_reward_5: 0.00475
	loss_policy: 0.0007
	loss_value: 0.21692
	loss_reward: 0.0125
Optimization_Done 28000
[2025-05-08 08:47:49] [command] train weight_iter_28000.pkl 122 141
[2025-05-08 08:47:56] nn step 28050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10548
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02101
	loss_reward_1: 0.00048
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02025
	loss_reward_2: 0.00294
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01971
	loss_reward_3: 0.00202
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01881
	loss_reward_4: 0.00215
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01926
	loss_reward_5: 0.00452
	loss_policy: 0.00031
	loss_value: 0.20453
	loss_reward: 0.01211
[2025-05-08 08:48:05] nn step 28100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10054
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01999
	loss_reward_1: 0.00044
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01928
	loss_reward_2: 0.00277
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01887
	loss_reward_3: 0.00191
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01793
	loss_reward_4: 0.00199
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01835
	loss_reward_5: 0.00437
	loss_policy: 0.00057
	loss_value: 0.19496
	loss_reward: 0.01149
[2025-05-08 08:48:13] nn step 28150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10508
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02088
	loss_reward_1: 0.00043
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.0201
	loss_reward_2: 0.00298
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01964
	loss_reward_3: 0.00199
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01873
	loss_reward_4: 0.00203
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99934
	loss_value_5: 0.01917
	loss_reward_5: 0.00458
	loss_policy: 0.00053
	loss_value: 0.20359
	loss_reward: 0.012
[2025-05-08 08:48:22] nn step 28200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10673
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02121
	loss_reward_1: 0.00038
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02042
	loss_reward_2: 0.00295
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01989
	loss_reward_3: 0.00218
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99973
	loss_value_4: 0.01889
	loss_reward_4: 0.00211
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01939
	loss_reward_5: 0.00468
	loss_policy: 0.00031
	loss_value: 0.20654
	loss_reward: 0.0123
Optimization_Done 28200
[2025-05-08 08:49:39] [command] train weight_iter_28200.pkl 123 142
[2025-05-08 08:49:48] nn step 28250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10198
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02032
	loss_reward_1: 0.00041
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01963
	loss_reward_2: 0.00291
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99965
	loss_value_3: 0.0192
	loss_reward_3: 0.00202
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01823
	loss_reward_4: 0.00209
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01871
	loss_reward_5: 0.00445
	loss_policy: 0.00082
	loss_value: 0.19807
	loss_reward: 0.01188
[2025-05-08 08:49:55] nn step 28300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09963
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0198
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01903
	loss_reward_2: 0.00283
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01852
	loss_reward_3: 0.00198
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01762
	loss_reward_4: 0.00195
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99945
	loss_value_5: 0.01802
	loss_reward_5: 0.00442
	loss_policy: 0.00028
	loss_value: 0.19262
	loss_reward: 0.01158
[2025-05-08 08:50:03] nn step 28350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10254
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0204
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01963
	loss_reward_2: 0.00283
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01913
	loss_reward_3: 0.00209
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01811
	loss_reward_4: 0.00201
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01851
	loss_reward_5: 0.0045
	loss_policy: 0.0005
	loss_value: 0.19831
	loss_reward: 0.01183
[2025-05-08 08:50:12] nn step 28400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10268
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02042
	loss_reward_1: 0.00041
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01965
	loss_reward_2: 0.00287
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01913
	loss_reward_3: 0.00199
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01819
	loss_reward_4: 0.00208
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01866
	loss_reward_5: 0.00453
	loss_policy: 0.00052
	loss_value: 0.19873
	loss_reward: 0.01189
Optimization_Done 28400
[2025-05-08 08:51:31] [command] train weight_iter_28400.pkl 124 143
[2025-05-08 08:51:40] nn step 28450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10509
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02091
	loss_reward_1: 0.00043
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02007
	loss_reward_2: 0.00295
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01959
	loss_reward_3: 0.00212
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01866
	loss_reward_4: 0.00206
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01902
	loss_reward_5: 0.00455
	loss_policy: 0.00054
	loss_value: 0.20335
	loss_reward: 0.01211
[2025-05-08 08:51:47] nn step 28500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10452
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02075
	loss_reward_1: 0.00043
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01998
	loss_reward_2: 0.003
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01944
	loss_reward_3: 0.00201
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01847
	loss_reward_4: 0.00209
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99953
	loss_value_5: 0.0189
	loss_reward_5: 0.00463
	loss_policy: 0.00043
	loss_value: 0.20205
	loss_reward: 0.01216
[2025-05-08 08:51:56] nn step 28550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10105
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02013
	loss_reward_1: 0.00043
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.0194
	loss_reward_2: 0.00284
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01888
	loss_reward_3: 0.00201
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01795
	loss_reward_4: 0.00202
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01837
	loss_reward_5: 0.00442
	loss_policy: 0.00056
	loss_value: 0.19578
	loss_reward: 0.01171
[2025-05-08 08:52:04] nn step 28600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1089
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02173
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02103
	loss_reward_2: 0.00304
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.0205
	loss_reward_3: 0.00211
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01953
	loss_reward_4: 0.00212
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01994
	loss_reward_5: 0.00483
	loss_policy: 0.00034
	loss_value: 0.21162
	loss_reward: 0.01251
Optimization_Done 28600
[2025-05-08 08:53:25] [command] train weight_iter_28600.pkl 125 144
[2025-05-08 08:53:34] nn step 28650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10181
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02021
	loss_reward_1: 0.00039
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01946
	loss_reward_2: 0.00283
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01894
	loss_reward_3: 0.002
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01791
	loss_reward_4: 0.00204
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01823
	loss_reward_5: 0.00451
	loss_policy: 0.00024
	loss_value: 0.19656
	loss_reward: 0.01177
[2025-05-08 08:53:41] nn step 28700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.104
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02066
	loss_reward_1: 0.00038
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0198
	loss_reward_2: 0.00299
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01934
	loss_reward_3: 0.00204
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99969
	loss_value_4: 0.0183
	loss_reward_4: 0.00202
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01876
	loss_reward_5: 0.00461
	loss_policy: 0.00048
	loss_value: 0.20085
	loss_reward: 0.01204
[2025-05-08 08:53:50] nn step 28750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09784
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01943
	loss_reward_1: 0.00038
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01862
	loss_reward_2: 0.0028
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01811
	loss_reward_3: 0.00194
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01713
	loss_reward_4: 0.00192
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01747
	loss_reward_5: 0.00436
	loss_policy: 0.00069
	loss_value: 0.18862
	loss_reward: 0.01141
[2025-05-08 08:53:58] nn step 28800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09353
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01862
	loss_reward_1: 0.0004
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01785
	loss_reward_2: 0.00255
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01731
	loss_reward_3: 0.00186
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99965
	loss_value_4: 0.0164
	loss_reward_4: 0.0019
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01677
	loss_reward_5: 0.00403
	loss_policy: 0.00063
	loss_value: 0.18048
	loss_reward: 0.01075
Optimization_Done 28800
[2025-05-08 08:55:19] [command] train weight_iter_28800.pkl 126 145
[2025-05-08 08:55:27] nn step 28850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10256
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02049
	loss_reward_1: 0.00039
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01977
	loss_reward_2: 0.0029
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0192
	loss_reward_3: 0.00205
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01816
	loss_reward_4: 0.0021
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01851
	loss_reward_5: 0.00447
	loss_policy: 0.00025
	loss_value: 0.19869
	loss_reward: 0.01191
[2025-05-08 08:55:35] nn step 28900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10126
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02016
	loss_reward_1: 0.00043
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01931
	loss_reward_2: 0.00281
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01873
	loss_reward_3: 0.00197
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01776
	loss_reward_4: 0.00211
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01807
	loss_reward_5: 0.00446
	loss_policy: 0.00073
	loss_value: 0.19529
	loss_reward: 0.01178
[2025-05-08 08:55:44] nn step 28950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0986
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.0195
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01865
	loss_reward_2: 0.00285
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01808
	loss_reward_3: 0.00196
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.0171
	loss_reward_4: 0.00207
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01746
	loss_reward_5: 0.00435
	loss_policy: 0.0004
	loss_value: 0.18939
	loss_reward: 0.01163
[2025-05-08 08:55:53] nn step 29000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10182
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02019
	loss_reward_1: 0.00039
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01931
	loss_reward_2: 0.00296
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01874
	loss_reward_3: 0.00204
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99957
	loss_value_4: 0.01771
	loss_reward_4: 0.00199
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99934
	loss_value_5: 0.01803
	loss_reward_5: 0.00462
	loss_policy: 0.00041
	loss_value: 0.1958
	loss_reward: 0.01199
Optimization_Done 29000
[2025-05-08 08:57:14] [command] train weight_iter_29000.pkl 127 146
[2025-05-08 08:57:22] nn step 29050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10175
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02021
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99977
	loss_value_2: 0.0194
	loss_reward_2: 0.00288
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01891
	loss_reward_3: 0.00203
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01791
	loss_reward_4: 0.00208
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01822
	loss_reward_5: 0.0045
	loss_policy: 0.00094
	loss_value: 0.1964
	loss_reward: 0.01192
[2025-05-08 08:57:30] nn step 29100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10261
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02033
	loss_reward_1: 0.00047
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01947
	loss_reward_2: 0.00294
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.0189
	loss_reward_3: 0.00207
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01784
	loss_reward_4: 0.00211
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01811
	loss_reward_5: 0.0046
	loss_policy: 0.0003
	loss_value: 0.19727
	loss_reward: 0.01218
[2025-05-08 08:57:39] nn step 29150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10297
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02039
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0195
	loss_reward_2: 0.00297
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01887
	loss_reward_3: 0.00208
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01779
	loss_reward_4: 0.00217
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01813
	loss_reward_5: 0.00474
	loss_policy: 0.00018
	loss_value: 0.19765
	loss_reward: 0.01237
[2025-05-08 08:57:47] nn step 29200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09852
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01955
	loss_reward_1: 0.0004
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01871
	loss_reward_2: 0.00282
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01818
	loss_reward_3: 0.00203
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01713
	loss_reward_4: 0.00204
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99945
	loss_value_5: 0.01741
	loss_reward_5: 0.00445
	loss_policy: 0.00033
	loss_value: 0.1895
	loss_reward: 0.01173
Optimization_Done 29200
[2025-05-08 08:59:05] [command] train weight_iter_29200.pkl 128 147
[2025-05-08 08:59:13] nn step 29250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09953
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01975
	loss_reward_1: 0.00044
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.019
	loss_reward_2: 0.00279
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01846
	loss_reward_3: 0.00202
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01749
	loss_reward_4: 0.00207
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01775
	loss_reward_5: 0.00436
	loss_policy: 0.0002
	loss_value: 0.192
	loss_reward: 0.01167
[2025-05-08 08:59:21] nn step 29300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1057
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02094
	loss_reward_1: 0.00044
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02006
	loss_reward_2: 0.00306
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01939
	loss_reward_3: 0.00216
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99957
	loss_value_4: 0.01831
	loss_reward_4: 0.00226
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99957
	loss_value_5: 0.0187
	loss_reward_5: 0.00478
	loss_policy: 0.00047
	loss_value: 0.2031
	loss_reward: 0.0127
[2025-05-08 08:59:30] nn step 29350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09521
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01891
	loss_reward_1: 0.00034
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01813
	loss_reward_2: 0.00274
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01755
	loss_reward_3: 0.00203
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01651
	loss_reward_4: 0.00184
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01684
	loss_reward_5: 0.0043
	loss_policy: 0.00034
	loss_value: 0.18315
	loss_reward: 0.01125
[2025-05-08 08:59:38] nn step 29400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09507
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01892
	loss_reward_1: 0.00037
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01815
	loss_reward_2: 0.00276
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01755
	loss_reward_3: 0.00193
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01655
	loss_reward_4: 0.00197
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99949
	loss_value_5: 0.0168
	loss_reward_5: 0.00442
	loss_policy: 0.00051
	loss_value: 0.18304
	loss_reward: 0.01145
Optimization_Done 29400
[2025-05-08 09:01:02] [command] train weight_iter_29400.pkl 129 148
[2025-05-08 09:01:09] nn step 29450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09811
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01952
	loss_reward_1: 0.00038
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01879
	loss_reward_2: 0.00272
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01814
	loss_reward_3: 0.00206
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01707
	loss_reward_4: 0.00208
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01736
	loss_reward_5: 0.00426
	loss_policy: 0.0002
	loss_value: 0.189
	loss_reward: 0.01149
[2025-05-08 09:01:18] nn step 29500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10502
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02086
	loss_reward_1: 0.00041
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01996
	loss_reward_2: 0.00297
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01929
	loss_reward_3: 0.00216
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01825
	loss_reward_4: 0.00213
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01855
	loss_reward_5: 0.00457
	loss_policy: 0.00048
	loss_value: 0.20194
	loss_reward: 0.01224
[2025-05-08 09:01:26] nn step 29550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10638
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02112
	loss_reward_1: 0.00049
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.02016
	loss_reward_2: 0.00313
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01949
	loss_reward_3: 0.00207
	loss_policy_4: 1e-05
	accuracy_policy_4: 0.99996
	loss_value_4: 0.01843
	loss_reward_4: 0.00221
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.9998
	loss_value_5: 0.0188
	loss_reward_5: 0.00485
	loss_policy: 0.00015
	loss_value: 0.20438
	loss_reward: 0.01275
[2025-05-08 09:01:33] nn step 29600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.098
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01947
	loss_reward_1: 0.00039
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.0186
	loss_reward_2: 0.00281
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01802
	loss_reward_3: 0.00192
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99969
	loss_value_4: 0.017
	loss_reward_4: 0.00199
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99934
	loss_value_5: 0.01735
	loss_reward_5: 0.00442
	loss_policy: 0.00045
	loss_value: 0.18844
	loss_reward: 0.01154
Optimization_Done 29600
[2025-05-08 09:02:55] [command] train weight_iter_29600.pkl 130 149
[2025-05-08 09:03:03] nn step 29650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09767
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01935
	loss_reward_1: 0.00042
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01852
	loss_reward_2: 0.00277
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.0179
	loss_reward_3: 0.00196
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01689
	loss_reward_4: 0.00201
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01714
	loss_reward_5: 0.00437
	loss_policy: 0.00027
	loss_value: 0.18746
	loss_reward: 0.01153
[2025-05-08 09:03:11] nn step 29700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09096
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01811
	loss_reward_1: 0.00036
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01736
	loss_reward_2: 0.00263
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01677
	loss_reward_3: 0.00188
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99957
	loss_value_4: 0.01579
	loss_reward_4: 0.0019
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01612
	loss_reward_5: 0.00411
	loss_policy: 0.00067
	loss_value: 0.17512
	loss_reward: 0.01089
[2025-05-08 09:03:20] nn step 29750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09743
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01931
	loss_reward_1: 0.00033
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01847
	loss_reward_2: 0.00287
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01785
	loss_reward_3: 0.00196
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01676
	loss_reward_4: 0.00201
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01711
	loss_reward_5: 0.00447
	loss_policy: 0.00021
	loss_value: 0.18692
	loss_reward: 0.01164
[2025-05-08 09:03:27] nn step 29800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1012
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01999
	loss_reward_1: 0.00042
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01908
	loss_reward_2: 0.00302
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01843
	loss_reward_3: 0.00204
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01739
	loss_reward_4: 0.00202
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01772
	loss_reward_5: 0.00472
	loss_policy: 0.00047
	loss_value: 0.1938
	loss_reward: 0.01222
Optimization_Done 29800
[2025-05-08 09:04:48] [command] train weight_iter_29800.pkl 131 150
[2025-05-08 09:04:56] nn step 29850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10596
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02093
	loss_reward_1: 0.00042
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.0201
	loss_reward_2: 0.00302
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01948
	loss_reward_3: 0.00206
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01835
	loss_reward_4: 0.00217
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01866
	loss_reward_5: 0.00475
	loss_policy: 0.00028
	loss_value: 0.20347
	loss_reward: 0.01242
[2025-05-08 09:05:04] nn step 29900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09377
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01853
	loss_reward_1: 0.00037
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01778
	loss_reward_2: 0.00271
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.01723
	loss_reward_3: 0.00187
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01622
	loss_reward_4: 0.00195
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99953
	loss_value_5: 0.0165
	loss_reward_5: 0.00428
	loss_policy: 0.00049
	loss_value: 0.18004
	loss_reward: 0.01119
[2025-05-08 09:05:13] nn step 29950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09504
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01882
	loss_reward_1: 0.00039
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.018
	loss_reward_2: 0.00283
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01738
	loss_reward_3: 0.00193
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01633
	loss_reward_4: 0.00196
	loss_policy_5: 3e-05
	accuracy_policy_5: 0.99984
	loss_value_5: 0.01662
	loss_reward_5: 0.00441
	loss_policy: 0.00011
	loss_value: 0.18219
	loss_reward: 0.01153
[2025-05-08 09:05:19] nn step 30000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09881
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01955
	loss_reward_1: 0.00039
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01868
	loss_reward_2: 0.00286
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01806
	loss_reward_3: 0.0021
	loss_policy_4: 3e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.017
	loss_reward_4: 0.00207
	loss_policy_5: 9e-05
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01724
	loss_reward_5: 0.00454
	loss_policy: 0.00017
	loss_value: 0.18934
	loss_reward: 0.01195
Optimization_Done 30000
[2025-05-08 09:06:43] [command] train weight_iter_30000.pkl 132 151
[2025-05-08 09:06:53] nn step 30050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09831
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01946
	loss_reward_1: 0.00038
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01865
	loss_reward_2: 0.00283
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01803
	loss_reward_3: 0.00203
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.0169
	loss_reward_4: 0.0021
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01719
	loss_reward_5: 0.00445
	loss_policy: 0.00031
	loss_value: 0.18854
	loss_reward: 0.01179
[2025-05-08 09:07:01] nn step 30100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10272
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02037
	loss_reward_1: 0.00038
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.0195
	loss_reward_2: 0.003
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01881
	loss_reward_3: 0.00213
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01773
	loss_reward_4: 0.00212
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01805
	loss_reward_5: 0.00473
	loss_policy: 0.00043
	loss_value: 0.19718
	loss_reward: 0.01236
[2025-05-08 09:07:10] nn step 30150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09964
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01972
	loss_reward_1: 0.00034
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01887
	loss_reward_2: 0.0029
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01823
	loss_reward_3: 0.00209
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01711
	loss_reward_4: 0.00198
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01745
	loss_reward_5: 0.00448
	loss_policy: 0.00027
	loss_value: 0.19103
	loss_reward: 0.0118
[2025-05-08 09:07:16] nn step 30200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10054
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01995
	loss_reward_1: 0.00039
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01906
	loss_reward_2: 0.00296
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01839
	loss_reward_3: 0.00209
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99973
	loss_value_4: 0.0173
	loss_reward_4: 0.00214
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01759
	loss_reward_5: 0.00468
	loss_policy: 0.00048
	loss_value: 0.19283
	loss_reward: 0.01226
Optimization_Done 30200
[2025-05-08 09:08:41] [command] train weight_iter_30200.pkl 133 152
[2025-05-08 09:08:50] nn step 30250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09985
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01975
	loss_reward_1: 0.0004
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01898
	loss_reward_2: 0.0029
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01837
	loss_reward_3: 0.002
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01738
	loss_reward_4: 0.00194
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01769
	loss_reward_5: 0.00447
	loss_policy: 0.00045
	loss_value: 0.19201
	loss_reward: 0.01172
[2025-05-08 09:08:59] nn step 30300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.11158
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0221
	loss_reward_1: 0.00044
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.02112
	loss_reward_2: 0.00319
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.02042
	loss_reward_3: 0.00221
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01924
	loss_reward_4: 0.00232
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01955
	loss_reward_5: 0.00505
	loss_policy: 0.0003
	loss_value: 0.214
	loss_reward: 0.01321
[2025-05-08 09:09:06] nn step 30350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09982
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01979
	loss_reward_1: 0.00039
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01897
	loss_reward_2: 0.00297
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01833
	loss_reward_3: 0.00205
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01721
	loss_reward_4: 0.0021
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01758
	loss_reward_5: 0.00469
	loss_policy: 0.00076
	loss_value: 0.19169
	loss_reward: 0.0122
[2025-05-08 09:09:15] nn step 30400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0985
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01947
	loss_reward_1: 0.0004
	loss_policy_2: 1e-05
	accuracy_policy_2: 1.0
	loss_value_2: 0.01869
	loss_reward_2: 0.00291
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01804
	loss_reward_3: 0.00203
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01699
	loss_reward_4: 0.00202
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01729
	loss_reward_5: 0.00456
	loss_policy: 0.00019
	loss_value: 0.18898
	loss_reward: 0.01192
Optimization_Done 30400
[2025-05-08 09:10:34] [command] train weight_iter_30400.pkl 134 153
[2025-05-08 09:10:44] nn step 30450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10099
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02002
	loss_reward_1: 0.00038
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01921
	loss_reward_2: 0.00289
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01853
	loss_reward_3: 0.00214
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.0174
	loss_reward_4: 0.00205
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01773
	loss_reward_5: 0.00446
	loss_policy: 0.00023
	loss_value: 0.19389
	loss_reward: 0.01193
[2025-05-08 09:10:52] nn step 30500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09624
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01906
	loss_reward_1: 0.00039
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01828
	loss_reward_2: 0.00279
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01766
	loss_reward_3: 0.00198
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01659
	loss_reward_4: 0.00201
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01684
	loss_reward_5: 0.00439
	loss_policy: 0.00038
	loss_value: 0.18467
	loss_reward: 0.01155
[2025-05-08 09:10:59] nn step 30550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10074
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01999
	loss_reward_1: 0.00043
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01913
	loss_reward_2: 0.00294
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01842
	loss_reward_3: 0.00206
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99973
	loss_value_4: 0.01734
	loss_reward_4: 0.00208
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01766
	loss_reward_5: 0.00462
	loss_policy: 0.00037
	loss_value: 0.19328
	loss_reward: 0.01213
[2025-05-08 09:11:08] nn step 30600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10206
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02016
	loss_reward_1: 0.0004
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01933
	loss_reward_2: 0.00299
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01867
	loss_reward_3: 0.00206
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99965
	loss_value_4: 0.01761
	loss_reward_4: 0.00209
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99945
	loss_value_5: 0.0179
	loss_reward_5: 0.00468
	loss_policy: 0.00057
	loss_value: 0.19574
	loss_reward: 0.01221
Optimization_Done 30600
[2025-05-08 09:12:28] [command] train weight_iter_30600.pkl 135 154
[2025-05-08 09:12:37] nn step 30650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10227
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02025
	loss_reward_1: 0.00043
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01938
	loss_reward_2: 0.00294
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01869
	loss_reward_3: 0.00208
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99973
	loss_value_4: 0.01763
	loss_reward_4: 0.00219
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01792
	loss_reward_5: 0.00458
	loss_policy: 0.00036
	loss_value: 0.19614
	loss_reward: 0.01222
[2025-05-08 09:12:46] nn step 30700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1055
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02089
	loss_reward_1: 0.00043
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01998
	loss_reward_2: 0.0032
	loss_policy_3: 0.0
	accuracy_policy_3: 1.0
	loss_value_3: 0.01935
	loss_reward_3: 0.00212
	loss_policy_4: 5e-05
	accuracy_policy_4: 0.99992
	loss_value_4: 0.01814
	loss_reward_4: 0.00217
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01841
	loss_reward_5: 0.00505
	loss_policy: 0.00018
	loss_value: 0.20226
	loss_reward: 0.01297
[2025-05-08 09:12:52] nn step 30750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10113
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01995
	loss_reward_1: 0.00041
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01916
	loss_reward_2: 0.00299
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01852
	loss_reward_3: 0.00204
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99984
	loss_value_4: 0.01741
	loss_reward_4: 0.00212
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01778
	loss_reward_5: 0.00461
	loss_policy: 0.00026
	loss_value: 0.19396
	loss_reward: 0.01217
[2025-05-08 09:13:01] nn step 30800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10428
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02056
	loss_reward_1: 0.00042
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01966
	loss_reward_2: 0.00311
	loss_policy_3: 4e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01903
	loss_reward_3: 0.00214
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01792
	loss_reward_4: 0.00215
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01822
	loss_reward_5: 0.00488
	loss_policy: 0.0002
	loss_value: 0.19967
	loss_reward: 0.0127
Optimization_Done 30800
[2025-05-08 09:14:24] [command] train weight_iter_30800.pkl 136 155
[2025-05-08 09:14:33] nn step 30850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1044
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02065
	loss_reward_1: 0.00044
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01978
	loss_reward_2: 0.00301
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.0191
	loss_reward_3: 0.00212
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01791
	loss_reward_4: 0.00225
	loss_policy_5: 0.00013
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01823
	loss_reward_5: 0.00476
	loss_policy: 0.00025
	loss_value: 0.20007
	loss_reward: 0.01258
[2025-05-08 09:14:42] nn step 30900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10143
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02008
	loss_reward_1: 0.00038
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01926
	loss_reward_2: 0.00304
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01854
	loss_reward_3: 0.00216
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01744
	loss_reward_4: 0.00214
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99973
	loss_value_5: 0.01777
	loss_reward_5: 0.0047
	loss_policy: 0.00021
	loss_value: 0.19452
	loss_reward: 0.01242
[2025-05-08 09:14:48] nn step 30950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10371
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02048
	loss_reward_1: 0.0004
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.01963
	loss_reward_2: 0.003
	loss_policy_3: 1e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.0189
	loss_reward_3: 0.00214
	loss_policy_4: 4e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.0177
	loss_reward_4: 0.00221
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.9998
	loss_value_5: 0.018
	loss_reward_5: 0.00477
	loss_policy: 0.00014
	loss_value: 0.19841
	loss_reward: 0.01251
[2025-05-08 09:14:57] nn step 31000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09528
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01881
	loss_reward_1: 0.00035
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.018
	loss_reward_2: 0.00283
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01735
	loss_reward_3: 0.00195
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01629
	loss_reward_4: 0.00194
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01662
	loss_reward_5: 0.00444
	loss_policy: 0.00031
	loss_value: 0.18236
	loss_reward: 0.0115
Optimization_Done 31000
[2025-05-08 09:16:17] [command] train weight_iter_31000.pkl 137 156
[2025-05-08 09:16:26] nn step 31050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1054
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02087
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01999
	loss_reward_2: 0.00309
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01927
	loss_reward_3: 0.00231
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.9998
	loss_value_4: 0.0181
	loss_reward_4: 0.0022
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99973
	loss_value_5: 0.01836
	loss_reward_5: 0.00487
	loss_policy: 0.00038
	loss_value: 0.20199
	loss_reward: 0.01291
[2025-05-08 09:16:35] nn step 31100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09922
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.0196
	loss_reward_1: 0.00039
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01879
	loss_reward_2: 0.00297
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01813
	loss_reward_3: 0.00207
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01703
	loss_reward_4: 0.00211
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99973
	loss_value_5: 0.01735
	loss_reward_5: 0.00454
	loss_policy: 0.00024
	loss_value: 0.19011
	loss_reward: 0.01209
[2025-05-08 09:16:42] nn step 31150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10385
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02052
	loss_reward_1: 0.0004
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.0197
	loss_reward_2: 0.00306
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99984
	loss_value_3: 0.019
	loss_reward_3: 0.00211
	loss_policy_4: 0.0001
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01778
	loss_reward_4: 0.00221
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01811
	loss_reward_5: 0.00467
	loss_policy: 0.00033
	loss_value: 0.19895
	loss_reward: 0.01245
[2025-05-08 09:16:50] nn step 31200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10689
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02111
	loss_reward_1: 0.00043
	loss_policy_2: 0.0
	accuracy_policy_2: 1.0
	loss_value_2: 0.02026
	loss_reward_2: 0.00317
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01949
	loss_reward_3: 0.00219
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01826
	loss_reward_4: 0.0022
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99957
	loss_value_5: 0.0186
	loss_reward_5: 0.00494
	loss_policy: 0.00037
	loss_value: 0.20461
	loss_reward: 0.01294
Optimization_Done 31200
[2025-05-08 09:18:09] [command] train weight_iter_31200.pkl 138 157
[2025-05-08 09:18:19] nn step 31250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10195
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02014
	loss_reward_1: 0.00042
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01935
	loss_reward_2: 0.00304
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01867
	loss_reward_3: 0.00208
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01751
	loss_reward_4: 0.00222
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99973
	loss_value_5: 0.0179
	loss_reward_5: 0.00465
	loss_policy: 0.00023
	loss_value: 0.19552
	loss_reward: 0.01241
[2025-05-08 09:18:27] nn step 31300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10307
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02037
	loss_reward_1: 0.00041
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01953
	loss_reward_2: 0.00315
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01882
	loss_reward_3: 0.00217
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01771
	loss_reward_4: 0.0021
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99945
	loss_value_5: 0.01799
	loss_reward_5: 0.00483
	loss_policy: 0.0006
	loss_value: 0.1975
	loss_reward: 0.01266
[2025-05-08 09:18:34] nn step 31350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10324
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02032
	loss_reward_1: 0.00038
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01945
	loss_reward_2: 0.00306
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01879
	loss_reward_3: 0.0022
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01755
	loss_reward_4: 0.00215
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99969
	loss_value_5: 0.01785
	loss_reward_5: 0.00484
	loss_policy: 0.00029
	loss_value: 0.1972
	loss_reward: 0.01263
[2025-05-08 09:18:42] nn step 31400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10016
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01983
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01897
	loss_reward_2: 0.00295
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01831
	loss_reward_3: 0.00211
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99977
	loss_value_4: 0.01722
	loss_reward_4: 0.00207
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01748
	loss_reward_5: 0.00461
	loss_policy: 0.00035
	loss_value: 0.19198
	loss_reward: 0.01215
Optimization_Done 31400
[2025-05-08 09:20:04] [command] train weight_iter_31400.pkl 139 158
[2025-05-08 09:20:13] nn step 31450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09757
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01929
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01854
	loss_reward_2: 0.00288
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.0179
	loss_reward_3: 0.00205
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01685
	loss_reward_4: 0.00213
	loss_policy_5: 7e-05
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01713
	loss_reward_5: 0.00452
	loss_policy: 0.00024
	loss_value: 0.18728
	loss_reward: 0.01197
[2025-05-08 09:20:22] nn step 31500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1034
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.02044
	loss_reward_1: 0.0004
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.0196
	loss_reward_2: 0.00306
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99988
	loss_value_3: 0.01893
	loss_reward_3: 0.00216
	loss_policy_4: 8e-05
	accuracy_policy_4: 0.9998
	loss_value_4: 0.01781
	loss_reward_4: 0.00219
	loss_policy_5: 8e-05
	accuracy_policy_5: 0.9998
	loss_value_5: 0.01808
	loss_reward_5: 0.00479
	loss_policy: 0.00027
	loss_value: 0.19825
	loss_reward: 0.0126
[2025-05-08 09:20:29] nn step 31550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09567
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01887
	loss_reward_1: 0.00039
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01814
	loss_reward_2: 0.00282
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01748
	loss_reward_3: 0.00196
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01641
	loss_reward_4: 0.00208
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01671
	loss_reward_5: 0.00439
	loss_policy: 0.00054
	loss_value: 0.18329
	loss_reward: 0.01165
[2025-05-08 09:20:37] nn step 31600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09827
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01935
	loss_reward_1: 0.00037
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01861
	loss_reward_2: 0.00298
	loss_policy_3: 3e-05
	accuracy_policy_3: 0.99996
	loss_value_3: 0.01797
	loss_reward_3: 0.00198
	loss_policy_4: 0.00011
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01677
	loss_reward_4: 0.00206
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01708
	loss_reward_5: 0.00466
	loss_policy: 0.00039
	loss_value: 0.18805
	loss_reward: 0.01204
Optimization_Done 31600
[2025-05-08 09:21:58] [command] train weight_iter_31600.pkl 140 159
[2025-05-08 09:22:07] nn step 31650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10019
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01982
	loss_reward_1: 0.0004
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01906
	loss_reward_2: 0.00299
	loss_policy_3: 2e-05
	accuracy_policy_3: 0.99992
	loss_value_3: 0.01835
	loss_reward_3: 0.00216
	loss_policy_4: 6e-05
	accuracy_policy_4: 0.99988
	loss_value_4: 0.01724
	loss_reward_4: 0.00218
	loss_policy_5: 0.0001
	accuracy_policy_5: 0.99977
	loss_value_5: 0.01752
	loss_reward_5: 0.00465
	loss_policy: 0.0002
	loss_value: 0.19217
	loss_reward: 0.0124
[2025-05-08 09:22:15] nn step 31700, lr: 0.1.
	loss_policy_0: 0.00051
	accuracy_policy_0: 0.99934
	loss_value_0: 0.10688
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99902
	loss_value_1: 0.02128
	loss_reward_1: 0.00147
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99898
	loss_value_2: 0.02113
	loss_reward_2: 0.00431
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99895
	loss_value_3: 0.02102
	loss_reward_3: 0.00366
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99883
	loss_value_4: 0.02058
	loss_reward_4: 0.00433
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02155
	loss_reward_5: 0.00672
	loss_policy: 0.00182
	loss_value: 0.21243
	loss_reward: 0.02049
[2025-05-08 09:22:22] nn step 31750, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.9998
	loss_value_0: 0.10912
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99934
	loss_value_1: 0.0219
	loss_reward_1: 0.00092
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99922
	loss_value_2: 0.02164
	loss_reward_2: 0.00391
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99906
	loss_value_3: 0.021
	loss_reward_3: 0.00334
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99895
	loss_value_4: 0.02011
	loss_reward_4: 0.00358
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99949
	loss_value_5: 0.02088
	loss_reward_5: 0.00618
	loss_policy: 0.00112
	loss_value: 0.21465
	loss_reward: 0.01792
[2025-05-08 09:22:31] nn step 31800, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10003
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01987
	loss_reward_1: 0.00047
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99871
	loss_value_2: 0.0194
	loss_reward_2: 0.00288
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.9984
	loss_value_3: 0.01888
	loss_reward_3: 0.00233
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01811
	loss_reward_4: 0.00262
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01848
	loss_reward_5: 0.00508
	loss_policy: 0.00115
	loss_value: 0.19476
	loss_reward: 0.01337
Optimization_Done 31800
[2025-05-08 09:23:50] [command] train weight_iter_31800.pkl 141 160
[2025-05-08 09:24:00] nn step 31850, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10428
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02066
	loss_reward_1: 0.00049
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99918
	loss_value_2: 0.0202
	loss_reward_2: 0.00296
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01959
	loss_reward_3: 0.0021
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99715
	loss_value_4: 0.01882
	loss_reward_4: 0.00218
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01919
	loss_reward_5: 0.00463
	loss_policy: 0.00118
	loss_value: 0.20274
	loss_reward: 0.01237
[2025-05-08 09:24:08] nn step 31900, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09959
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01972
	loss_reward_1: 0.00061
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99914
	loss_value_2: 0.01928
	loss_reward_2: 0.0033
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99824
	loss_value_3: 0.01885
	loss_reward_3: 0.00279
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99844
	loss_value_4: 0.01795
	loss_reward_4: 0.00293
	loss_policy_5: 0.00011
	accuracy_policy_5: 0.99965
	loss_value_5: 0.01833
	loss_reward_5: 0.00524
	loss_policy: 0.00065
	loss_value: 0.19371
	loss_reward: 0.01487
[2025-05-08 09:24:15] nn step 31950, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10786
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02141
	loss_reward_1: 0.00078
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99938
	loss_value_2: 0.02099
	loss_reward_2: 0.00325
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99848
	loss_value_3: 0.02055
	loss_reward_3: 0.00286
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99828
	loss_value_4: 0.01965
	loss_reward_4: 0.00287
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99938
	loss_value_5: 0.02004
	loss_reward_5: 0.00546
	loss_policy: 0.00098
	loss_value: 0.2105
	loss_reward: 0.01523
[2025-05-08 09:24:24] nn step 32000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10713
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02121
	loss_reward_1: 0.0005
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.02053
	loss_reward_2: 0.00312
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01996
	loss_reward_3: 0.00232
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01891
	loss_reward_4: 0.00223
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99961
	loss_value_5: 0.01932
	loss_reward_5: 0.00486
	loss_policy: 0.00084
	loss_value: 0.20705
	loss_reward: 0.01303
Optimization_Done 32000
[2025-05-08 09:25:45] [command] train weight_iter_32000.pkl 142 161
[2025-05-08 09:25:54] nn step 32050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10213
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02014
	loss_reward_1: 0.00047
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01957
	loss_reward_2: 0.00291
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01907
	loss_reward_3: 0.00202
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01815
	loss_reward_4: 0.00206
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01847
	loss_reward_5: 0.00447
	loss_policy: 0.001
	loss_value: 0.19753
	loss_reward: 0.01192
[2025-05-08 09:26:01] nn step 32100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09922
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01959
	loss_reward_1: 0.00045
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01898
	loss_reward_2: 0.00282
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99859
	loss_value_3: 0.01852
	loss_reward_3: 0.002
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.99785
	loss_value_4: 0.01759
	loss_reward_4: 0.00204
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01793
	loss_reward_5: 0.00437
	loss_policy: 0.0011
	loss_value: 0.19182
	loss_reward: 0.01168
[2025-05-08 09:26:10] nn step 32150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10531
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02081
	loss_reward_1: 0.00045
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.02018
	loss_reward_2: 0.003
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01957
	loss_reward_3: 0.00216
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99863
	loss_value_4: 0.01846
	loss_reward_4: 0.00226
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01878
	loss_reward_5: 0.00466
	loss_policy: 0.00082
	loss_value: 0.20311
	loss_reward: 0.01252
[2025-05-08 09:26:18] nn step 32200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10574
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02086
	loss_reward_1: 0.00049
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02015
	loss_reward_2: 0.0031
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01954
	loss_reward_3: 0.00215
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.9993
	loss_value_4: 0.01854
	loss_reward_4: 0.00223
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99945
	loss_value_5: 0.01886
	loss_reward_5: 0.00481
	loss_policy: 0.00064
	loss_value: 0.20369
	loss_reward: 0.01278
Optimization_Done 32200
[2025-05-08 09:27:39] [command] train weight_iter_32200.pkl 143 162
[2025-05-08 09:27:48] nn step 32250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10513
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02077
	loss_reward_1: 0.00053
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.9993
	loss_value_2: 0.02029
	loss_reward_2: 0.00285
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01982
	loss_reward_3: 0.00216
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01885
	loss_reward_4: 0.00226
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01916
	loss_reward_5: 0.00463
	loss_policy: 0.00125
	loss_value: 0.20402
	loss_reward: 0.01243
[2025-05-08 09:27:55] nn step 32300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10811
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.0214
	loss_reward_1: 0.00051
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02074
	loss_reward_2: 0.00299
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99887
	loss_value_3: 0.02021
	loss_reward_3: 0.00214
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99789
	loss_value_4: 0.01925
	loss_reward_4: 0.00219
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01954
	loss_reward_5: 0.00464
	loss_policy: 0.00135
	loss_value: 0.20925
	loss_reward: 0.01248
[2025-05-08 09:28:04] nn step 32350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10006
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01971
	loss_reward_1: 0.00047
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01907
	loss_reward_2: 0.00282
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01856
	loss_reward_3: 0.00203
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01756
	loss_reward_4: 0.00203
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01796
	loss_reward_5: 0.00436
	loss_policy: 0.00171
	loss_value: 0.19293
	loss_reward: 0.01172
[2025-05-08 09:28:12] nn step 32400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10652
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02106
	loss_reward_1: 0.00048
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99938
	loss_value_2: 0.0203
	loss_reward_2: 0.00301
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99875
	loss_value_3: 0.01974
	loss_reward_3: 0.00221
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99777
	loss_value_4: 0.01873
	loss_reward_4: 0.00216
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01902
	loss_reward_5: 0.00464
	loss_policy: 0.00114
	loss_value: 0.20538
	loss_reward: 0.0125
Optimization_Done 32400
[2025-05-08 09:29:32] [command] train weight_iter_32400.pkl 144 163
[2025-05-08 09:29:41] nn step 32450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10316
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02045
	loss_reward_1: 0.00042
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01989
	loss_reward_2: 0.00285
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99945
	loss_value_3: 0.0194
	loss_reward_3: 0.0021
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99871
	loss_value_4: 0.01832
	loss_reward_4: 0.0021
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99898
	loss_value_5: 0.0186
	loss_reward_5: 0.00449
	loss_policy: 0.00082
	loss_value: 0.19981
	loss_reward: 0.01196
[2025-05-08 09:29:48] nn step 32500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10606
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02102
	loss_reward_1: 0.00044
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.02035
	loss_reward_2: 0.00296
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01985
	loss_reward_3: 0.00213
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99859
	loss_value_4: 0.01876
	loss_reward_4: 0.00215
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99844
	loss_value_5: 0.01898
	loss_reward_5: 0.00464
	loss_policy: 0.00087
	loss_value: 0.20503
	loss_reward: 0.01232
[2025-05-08 09:29:56] nn step 32550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10434
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02063
	loss_reward_1: 0.00043
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01989
	loss_reward_2: 0.00297
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01935
	loss_reward_3: 0.00214
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01835
	loss_reward_4: 0.00211
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01865
	loss_reward_5: 0.00461
	loss_policy: 0.00072
	loss_value: 0.20121
	loss_reward: 0.01226
[2025-05-08 09:30:05] nn step 32600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10467
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02064
	loss_reward_1: 0.00045
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01992
	loss_reward_2: 0.00307
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01945
	loss_reward_3: 0.00207
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99789
	loss_value_4: 0.01836
	loss_reward_4: 0.00211
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99922
	loss_value_5: 0.0187
	loss_reward_5: 0.00475
	loss_policy: 0.00084
	loss_value: 0.20174
	loss_reward: 0.01245
Optimization_Done 32600
[2025-05-08 09:31:29] [command] train weight_iter_32600.pkl 145 164
[2025-05-08 09:31:38] nn step 32650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10562
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02087
	loss_reward_1: 0.00046
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02026
	loss_reward_2: 0.00293
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99844
	loss_value_3: 0.01971
	loss_reward_3: 0.00212
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01866
	loss_reward_4: 0.00207
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01894
	loss_reward_5: 0.00463
	loss_policy: 0.00127
	loss_value: 0.20407
	loss_reward: 0.01221
[2025-05-08 09:31:45] nn step 32700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10595
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02094
	loss_reward_1: 0.00046
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.0202
	loss_reward_2: 0.00305
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01963
	loss_reward_3: 0.00214
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01858
	loss_reward_4: 0.00217
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99926
	loss_value_5: 0.0189
	loss_reward_5: 0.00476
	loss_policy: 0.0006
	loss_value: 0.2042
	loss_reward: 0.01259
[2025-05-08 09:31:54] nn step 32750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10803
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.0214
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99965
	loss_value_2: 0.02064
	loss_reward_2: 0.00307
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99945
	loss_value_3: 0.02007
	loss_reward_3: 0.00213
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01899
	loss_reward_4: 0.00215
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01923
	loss_reward_5: 0.00476
	loss_policy: 0.00089
	loss_value: 0.20837
	loss_reward: 0.01253
[2025-05-08 09:32:02] nn step 32800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09449
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01865
	loss_reward_1: 0.0004
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01799
	loss_reward_2: 0.00278
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01751
	loss_reward_3: 0.00192
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.9993
	loss_value_4: 0.01659
	loss_reward_4: 0.00185
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01689
	loss_reward_5: 0.00426
	loss_policy: 0.00052
	loss_value: 0.18212
	loss_reward: 0.01119
Optimization_Done 32800
[2025-05-08 09:33:22] [command] train weight_iter_32800.pkl 146 165
[2025-05-08 09:33:32] nn step 32850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09887
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01955
	loss_reward_1: 0.00038
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01891
	loss_reward_2: 0.00277
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01843
	loss_reward_3: 0.00197
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01744
	loss_reward_4: 0.002
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01771
	loss_reward_5: 0.00435
	loss_policy: 0.0008
	loss_value: 0.19091
	loss_reward: 0.01147
[2025-05-08 09:33:39] nn step 32900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10757
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02127
	loss_reward_1: 0.00048
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.0206
	loss_reward_2: 0.00299
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99914
	loss_value_3: 0.02002
	loss_reward_3: 0.00214
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01896
	loss_reward_4: 0.00225
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01925
	loss_reward_5: 0.00469
	loss_policy: 0.00098
	loss_value: 0.20767
	loss_reward: 0.01254
[2025-05-08 09:33:47] nn step 32950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10097
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01993
	loss_reward_1: 0.00038
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01922
	loss_reward_2: 0.00287
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01869
	loss_reward_3: 0.00202
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99863
	loss_value_4: 0.01765
	loss_reward_4: 0.00208
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99844
	loss_value_5: 0.01797
	loss_reward_5: 0.00444
	loss_policy: 0.00114
	loss_value: 0.19444
	loss_reward: 0.0118
[2025-05-08 09:33:56] nn step 33000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10377
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02053
	loss_reward_1: 0.00039
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01979
	loss_reward_2: 0.003
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01926
	loss_reward_3: 0.00204
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01821
	loss_reward_4: 0.00213
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01849
	loss_reward_5: 0.00471
	loss_policy: 0.00082
	loss_value: 0.20006
	loss_reward: 0.01226
Optimization_Done 33000
[2025-05-08 09:35:19] [command] train weight_iter_33000.pkl 147 166
[2025-05-08 09:35:27] nn step 33050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10116
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01998
	loss_reward_1: 0.0004
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01925
	loss_reward_2: 0.00283
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01879
	loss_reward_3: 0.00205
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.9993
	loss_value_4: 0.01779
	loss_reward_4: 0.00198
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01803
	loss_reward_5: 0.00447
	loss_policy: 0.0006
	loss_value: 0.19501
	loss_reward: 0.01174
[2025-05-08 09:35:35] nn step 33100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09871
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01948
	loss_reward_1: 0.00042
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01878
	loss_reward_2: 0.00283
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01829
	loss_reward_3: 0.00196
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01731
	loss_reward_4: 0.00202
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99852
	loss_value_5: 0.0176
	loss_reward_5: 0.00442
	loss_policy: 0.00108
	loss_value: 0.19018
	loss_reward: 0.01165
[2025-05-08 09:35:44] nn step 33150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10841
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02143
	loss_reward_1: 0.00042
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.02065
	loss_reward_2: 0.0031
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.9993
	loss_value_3: 0.02011
	loss_reward_3: 0.00224
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01903
	loss_reward_4: 0.00212
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01935
	loss_reward_5: 0.00486
	loss_policy: 0.00089
	loss_value: 0.20898
	loss_reward: 0.01275
[2025-05-08 09:35:52] nn step 33200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1019
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02008
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01939
	loss_reward_2: 0.0029
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01881
	loss_reward_3: 0.00203
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01773
	loss_reward_4: 0.00211
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01803
	loss_reward_5: 0.00458
	loss_policy: 0.00087
	loss_value: 0.19594
	loss_reward: 0.01198
Optimization_Done 33200
[2025-05-08 09:37:03] [command] train weight_iter_33200.pkl 148 167
[2025-05-08 09:37:12] nn step 33250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1017
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02005
	loss_reward_1: 0.00036
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01937
	loss_reward_2: 0.003
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01881
	loss_reward_3: 0.00206
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01781
	loss_reward_4: 0.00209
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01818
	loss_reward_5: 0.00462
	loss_policy: 0.00082
	loss_value: 0.19593
	loss_reward: 0.01213
[2025-05-08 09:37:18] nn step 33300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10766
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02121
	loss_reward_1: 0.00044
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99945
	loss_value_2: 0.02053
	loss_reward_2: 0.00309
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99859
	loss_value_3: 0.01986
	loss_reward_3: 0.00222
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01873
	loss_reward_4: 0.00219
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01912
	loss_reward_5: 0.00484
	loss_policy: 0.00112
	loss_value: 0.2071
	loss_reward: 0.01278
[2025-05-08 09:37:26] nn step 33350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0969
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01912
	loss_reward_1: 0.00034
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01848
	loss_reward_2: 0.00281
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01795
	loss_reward_3: 0.00193
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01702
	loss_reward_4: 0.00203
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99898
	loss_value_5: 0.0173
	loss_reward_5: 0.00428
	loss_policy: 0.00063
	loss_value: 0.18678
	loss_reward: 0.0114
[2025-05-08 09:37:34] nn step 33400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10582
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02084
	loss_reward_1: 0.00041
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02007
	loss_reward_2: 0.00317
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01944
	loss_reward_3: 0.00213
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01836
	loss_reward_4: 0.00217
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01874
	loss_reward_5: 0.00485
	loss_policy: 0.0009
	loss_value: 0.20328
	loss_reward: 0.01273
Optimization_Done 33400
[2025-05-08 09:38:51] [command] train weight_iter_33400.pkl 149 168
[2025-05-08 09:39:00] nn step 33450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10022
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01974
	loss_reward_1: 0.00041
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01909
	loss_reward_2: 0.00285
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01854
	loss_reward_3: 0.00202
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01746
	loss_reward_4: 0.00209
	loss_policy_5: 0.00015
	accuracy_policy_5: 0.9993
	loss_value_5: 0.0178
	loss_reward_5: 0.00451
	loss_policy: 0.00044
	loss_value: 0.19285
	loss_reward: 0.01188
[2025-05-08 09:39:08] nn step 33500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10519
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02076
	loss_reward_1: 0.00043
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99965
	loss_value_2: 0.02015
	loss_reward_2: 0.00303
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01952
	loss_reward_3: 0.00219
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01843
	loss_reward_4: 0.00216
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01884
	loss_reward_5: 0.00482
	loss_policy: 0.00099
	loss_value: 0.20289
	loss_reward: 0.01263
[2025-05-08 09:39:14] nn step 33550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10318
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02034
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01968
	loss_reward_2: 0.00303
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01902
	loss_reward_3: 0.00215
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01796
	loss_reward_4: 0.0021
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01835
	loss_reward_5: 0.00485
	loss_policy: 0.00088
	loss_value: 0.19852
	loss_reward: 0.01255
[2025-05-08 09:39:22] nn step 33600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09659
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01896
	loss_reward_1: 0.00038
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01839
	loss_reward_2: 0.00284
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01784
	loss_reward_3: 0.00196
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.9991
	loss_value_4: 0.0168
	loss_reward_4: 0.002
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01715
	loss_reward_5: 0.00445
	loss_policy: 0.00074
	loss_value: 0.18572
	loss_reward: 0.01163
Optimization_Done 33600
[2025-05-08 09:40:38] [command] train weight_iter_33600.pkl 150 169
[2025-05-08 09:40:47] nn step 33650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10409
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02057
	loss_reward_1: 0.0004
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01995
	loss_reward_2: 0.00296
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99945
	loss_value_3: 0.0193
	loss_reward_3: 0.00214
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01826
	loss_reward_4: 0.0022
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01856
	loss_reward_5: 0.00464
	loss_policy: 0.00086
	loss_value: 0.20073
	loss_reward: 0.01233
[2025-05-08 09:40:55] nn step 33700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09903
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01948
	loss_reward_1: 0.00042
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01887
	loss_reward_2: 0.00296
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01827
	loss_reward_3: 0.00196
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01723
	loss_reward_4: 0.0021
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01753
	loss_reward_5: 0.00463
	loss_policy: 0.0008
	loss_value: 0.1904
	loss_reward: 0.01207
[2025-05-08 09:41:03] nn step 33750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.1033
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02034
	loss_reward_1: 0.00041
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01962
	loss_reward_2: 0.00308
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01898
	loss_reward_3: 0.00217
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99902
	loss_value_4: 0.0179
	loss_reward_4: 0.00212
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.9982
	loss_value_5: 0.01824
	loss_reward_5: 0.00478
	loss_policy: 0.00106
	loss_value: 0.19838
	loss_reward: 0.01256
[2025-05-08 09:41:09] nn step 33800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10187
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02004
	loss_reward_1: 0.00042
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01939
	loss_reward_2: 0.00299
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01875
	loss_reward_3: 0.00206
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01767
	loss_reward_4: 0.00214
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01802
	loss_reward_5: 0.00471
	loss_policy: 0.00062
	loss_value: 0.19575
	loss_reward: 0.01232
Optimization_Done 33800
[2025-05-08 09:42:25] [command] train weight_iter_33800.pkl 151 170
[2025-05-08 09:42:32] nn step 33850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10418
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.02054
	loss_reward_1: 0.00036
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01984
	loss_reward_2: 0.00313
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01928
	loss_reward_3: 0.00213
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01814
	loss_reward_4: 0.00215
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01849
	loss_reward_5: 0.00489
	loss_policy: 0.00083
	loss_value: 0.20047
	loss_reward: 0.01265
[2025-05-08 09:42:40] nn step 33900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10672
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02102
	loss_reward_1: 0.00046
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02032
	loss_reward_2: 0.00312
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01969
	loss_reward_3: 0.00221
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01855
	loss_reward_4: 0.00231
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01894
	loss_reward_5: 0.00493
	loss_policy: 0.00086
	loss_value: 0.20523
	loss_reward: 0.01302
[2025-05-08 09:42:48] nn step 33950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10404
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02051
	loss_reward_1: 0.00041
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.0198
	loss_reward_2: 0.00304
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01921
	loss_reward_3: 0.0021
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01809
	loss_reward_4: 0.00221
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01844
	loss_reward_5: 0.00469
	loss_policy: 0.00064
	loss_value: 0.2001
	loss_reward: 0.01245
[2025-05-08 09:42:56] nn step 34000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0983
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01934
	loss_reward_1: 0.00037
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01873
	loss_reward_2: 0.00284
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01816
	loss_reward_3: 0.00203
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99902
	loss_value_4: 0.0171
	loss_reward_4: 0.00204
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99852
	loss_value_5: 0.01738
	loss_reward_5: 0.00453
	loss_policy: 0.00086
	loss_value: 0.18901
	loss_reward: 0.01181
Optimization_Done 34000
[2025-05-08 09:44:10] [command] train weight_iter_34000.pkl 152 171
[2025-05-08 09:44:19] nn step 34050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10472
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02065
	loss_reward_1: 0.0004
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01997
	loss_reward_2: 0.00301
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.9991
	loss_value_3: 0.01931
	loss_reward_3: 0.0022
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01826
	loss_reward_4: 0.00217
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99852
	loss_value_5: 0.01861
	loss_reward_5: 0.00476
	loss_policy: 0.00116
	loss_value: 0.20152
	loss_reward: 0.01256
[2025-05-08 09:44:25] nn step 34100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10021
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01979
	loss_reward_1: 0.0004
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01911
	loss_reward_2: 0.00292
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01858
	loss_reward_3: 0.00208
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99941
	loss_value_4: 0.0176
	loss_reward_4: 0.00205
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01786
	loss_reward_5: 0.00462
	loss_policy: 0.00092
	loss_value: 0.19314
	loss_reward: 0.01205
[2025-05-08 09:44:33] nn step 34150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10326
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02034
	loss_reward_1: 0.00037
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0196
	loss_reward_2: 0.00304
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01911
	loss_reward_3: 0.00213
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01801
	loss_reward_4: 0.00213
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01829
	loss_reward_5: 0.00478
	loss_policy: 0.00091
	loss_value: 0.19862
	loss_reward: 0.01243
[2025-05-08 09:44:41] nn step 34200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10164
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01997
	loss_reward_1: 0.0004
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01925
	loss_reward_2: 0.00297
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01867
	loss_reward_3: 0.00212
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99875
	loss_value_4: 0.01764
	loss_reward_4: 0.0021
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01797
	loss_reward_5: 0.00468
	loss_policy: 0.00104
	loss_value: 0.19514
	loss_reward: 0.01226
Optimization_Done 34200
[2025-05-08 09:45:57] [command] train weight_iter_34200.pkl 153 172
[2025-05-08 09:46:06] nn step 34250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10027
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01976
	loss_reward_1: 0.00036
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.0191
	loss_reward_2: 0.0028
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.0185
	loss_reward_3: 0.00204
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01752
	loss_reward_4: 0.00198
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01784
	loss_reward_5: 0.0044
	loss_policy: 0.0006
	loss_value: 0.19299
	loss_reward: 0.01158
[2025-05-08 09:46:14] nn step 34300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09807
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0193
	loss_reward_1: 0.00037
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01866
	loss_reward_2: 0.00291
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01806
	loss_reward_3: 0.00206
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99875
	loss_value_4: 0.01705
	loss_reward_4: 0.00202
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01738
	loss_reward_5: 0.00457
	loss_policy: 0.00123
	loss_value: 0.18853
	loss_reward: 0.01193
[2025-05-08 09:46:20] nn step 34350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10807
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02123
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.02054
	loss_reward_2: 0.00314
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01986
	loss_reward_3: 0.00221
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01872
	loss_reward_4: 0.00233
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99871
	loss_value_5: 0.019
	loss_reward_5: 0.00493
	loss_policy: 0.00081
	loss_value: 0.20744
	loss_reward: 0.01302
[2025-05-08 09:46:28] nn step 34400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10175
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02006
	loss_reward_1: 0.00037
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01938
	loss_reward_2: 0.00302
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01883
	loss_reward_3: 0.00201
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.9993
	loss_value_4: 0.01777
	loss_reward_4: 0.00214
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99867
	loss_value_5: 0.0181
	loss_reward_5: 0.00468
	loss_policy: 0.00063
	loss_value: 0.19589
	loss_reward: 0.01223
Optimization_Done 34400
[2025-05-08 09:47:40] [command] train weight_iter_34400.pkl 154 173
[2025-05-08 09:47:49] nn step 34450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10597
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0211
	loss_reward_1: 0.00067
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99949
	loss_value_2: 0.02047
	loss_reward_2: 0.00345
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99922
	loss_value_3: 0.02
	loss_reward_3: 0.00258
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01918
	loss_reward_4: 0.00275
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01961
	loss_reward_5: 0.00521
	loss_policy: 0.00112
	loss_value: 0.20633
	loss_reward: 0.01467
[2025-05-08 09:47:57] nn step 34500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10273
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02035
	loss_reward_1: 0.00042
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01988
	loss_reward_2: 0.00319
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.9991
	loss_value_3: 0.01932
	loss_reward_3: 0.00236
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99867
	loss_value_4: 0.0184
	loss_reward_4: 0.00247
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01888
	loss_reward_5: 0.0051
	loss_policy: 0.00115
	loss_value: 0.19955
	loss_reward: 0.01355
[2025-05-08 09:48:05] nn step 34550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10421
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0206
	loss_reward_1: 0.00039
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99961
	loss_value_2: 0.02001
	loss_reward_2: 0.00309
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01947
	loss_reward_3: 0.00213
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01849
	loss_reward_4: 0.00211
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01885
	loss_reward_5: 0.00483
	loss_policy: 0.00107
	loss_value: 0.20162
	loss_reward: 0.01254
[2025-05-08 09:48:11] nn step 34600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10165
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02001
	loss_reward_1: 0.00043
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01936
	loss_reward_2: 0.00301
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01873
	loss_reward_3: 0.00213
	loss_policy_4: 0.00043
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01775
	loss_reward_4: 0.0022
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99855
	loss_value_5: 0.01809
	loss_reward_5: 0.00462
	loss_policy: 0.00133
	loss_value: 0.19559
	loss_reward: 0.01238
Optimization_Done 34600
[2025-05-08 09:49:28] [command] train weight_iter_34600.pkl 155 174
[2025-05-08 09:49:35] nn step 34650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10318
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02031
	loss_reward_1: 0.00043
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01975
	loss_reward_2: 0.00292
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99949
	loss_value_3: 0.0192
	loss_reward_3: 0.00207
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99922
	loss_value_4: 0.0182
	loss_reward_4: 0.00214
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01852
	loss_reward_5: 0.00463
	loss_policy: 0.0009
	loss_value: 0.19917
	loss_reward: 0.0122
[2025-05-08 09:49:43] nn step 34700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09701
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01915
	loss_reward_1: 0.00038
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.0185
	loss_reward_2: 0.00284
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01795
	loss_reward_3: 0.00197
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99914
	loss_value_4: 0.017
	loss_reward_4: 0.002
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01731
	loss_reward_5: 0.00441
	loss_policy: 0.00076
	loss_value: 0.18691
	loss_reward: 0.01159
[2025-05-08 09:49:51] nn step 34750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09821
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01932
	loss_reward_1: 0.00038
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01866
	loss_reward_2: 0.00289
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01814
	loss_reward_3: 0.00203
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01719
	loss_reward_4: 0.00202
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01742
	loss_reward_5: 0.00456
	loss_policy: 0.00072
	loss_value: 0.18894
	loss_reward: 0.01187
[2025-05-08 09:49:59] nn step 34800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10729
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02117
	loss_reward_1: 0.00042
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.02046
	loss_reward_2: 0.0032
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.0198
	loss_reward_3: 0.00219
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99934
	loss_value_4: 0.01873
	loss_reward_4: 0.00231
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01896
	loss_reward_5: 0.00503
	loss_policy: 0.00066
	loss_value: 0.20641
	loss_reward: 0.01315
Optimization_Done 34800
[2025-05-08 09:51:11] [command] train weight_iter_34800.pkl 156 175
[2025-05-08 09:51:20] nn step 34850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10274
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02021
	loss_reward_1: 0.0004
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01962
	loss_reward_2: 0.00297
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01902
	loss_reward_3: 0.00219
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01794
	loss_reward_4: 0.00216
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99855
	loss_value_5: 0.01824
	loss_reward_5: 0.00471
	loss_policy: 0.00087
	loss_value: 0.19776
	loss_reward: 0.01243
[2025-05-08 09:51:27] nn step 34900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09436
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01866
	loss_reward_1: 0.00038
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01801
	loss_reward_2: 0.00281
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01744
	loss_reward_3: 0.00199
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99922
	loss_value_4: 0.0165
	loss_reward_4: 0.00199
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01681
	loss_reward_5: 0.00438
	loss_policy: 0.00072
	loss_value: 0.18179
	loss_reward: 0.01155
[2025-05-08 09:51:35] nn step 34950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10269
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02025
	loss_reward_1: 0.00048
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01961
	loss_reward_2: 0.00298
	loss_policy_3: 6e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01901
	loss_reward_3: 0.00217
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99941
	loss_value_4: 0.018
	loss_reward_4: 0.00219
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01834
	loss_reward_5: 0.00464
	loss_policy: 0.00053
	loss_value: 0.1979
	loss_reward: 0.01245
[2025-05-08 09:51:43] nn step 35000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10109
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01987
	loss_reward_1: 0.00037
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.0192
	loss_reward_2: 0.003
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01862
	loss_reward_3: 0.00213
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01759
	loss_reward_4: 0.00209
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01794
	loss_reward_5: 0.00469
	loss_policy: 0.00067
	loss_value: 0.19431
	loss_reward: 0.01228
Optimization_Done 35000
[2025-05-08 09:52:57] [command] train weight_iter_35000.pkl 157 176
[2025-05-08 09:53:05] nn step 35050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10613
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02088
	loss_reward_1: 0.0004
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99953
	loss_value_2: 0.02025
	loss_reward_2: 0.00304
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01971
	loss_reward_3: 0.00218
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01861
	loss_reward_4: 0.00227
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01894
	loss_reward_5: 0.00481
	loss_policy: 0.00117
	loss_value: 0.20453
	loss_reward: 0.0127
[2025-05-08 09:53:13] nn step 35100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10401
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02055
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01991
	loss_reward_2: 0.00305
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01942
	loss_reward_3: 0.00217
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01836
	loss_reward_4: 0.00219
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01869
	loss_reward_5: 0.00483
	loss_policy: 0.00101
	loss_value: 0.20094
	loss_reward: 0.01267
[2025-05-08 09:53:20] nn step 35150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10605
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99961
	loss_value_1: 0.02095
	loss_reward_1: 0.00044
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99941
	loss_value_2: 0.02029
	loss_reward_2: 0.00309
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.9991
	loss_value_3: 0.0197
	loss_reward_3: 0.00218
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01865
	loss_reward_4: 0.00221
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01899
	loss_reward_5: 0.00486
	loss_policy: 0.0015
	loss_value: 0.20464
	loss_reward: 0.01278
[2025-05-08 09:53:28] nn step 35200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10274
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02027
	loss_reward_1: 0.0004
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01957
	loss_reward_2: 0.00311
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99879
	loss_value_3: 0.019
	loss_reward_3: 0.00211
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99871
	loss_value_4: 0.01798
	loss_reward_4: 0.00211
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99855
	loss_value_5: 0.01826
	loss_reward_5: 0.00488
	loss_policy: 0.00121
	loss_value: 0.19782
	loss_reward: 0.0126
Optimization_Done 35200
[2025-05-08 09:54:41] [command] train weight_iter_35200.pkl 158 177
[2025-05-08 09:54:48] nn step 35250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09995
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01967
	loss_reward_1: 0.0004
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99945
	loss_value_2: 0.0191
	loss_reward_2: 0.00297
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01855
	loss_reward_3: 0.00209
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99812
	loss_value_4: 0.0175
	loss_reward_4: 0.00212
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01785
	loss_reward_5: 0.00472
	loss_policy: 0.00122
	loss_value: 0.19262
	loss_reward: 0.0123
[2025-05-08 09:54:57] nn step 35300, lr: 0.1.
	loss_policy_0: 0.00038
	accuracy_policy_0: 0.99973
	loss_value_0: 0.10375
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99879
	loss_value_1: 0.02047
	loss_reward_1: 0.0004
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01981
	loss_reward_2: 0.00313
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01924
	loss_reward_3: 0.0022
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99805
	loss_value_4: 0.01821
	loss_reward_4: 0.00211
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01853
	loss_reward_5: 0.00494
	loss_policy: 0.00203
	loss_value: 0.2
	loss_reward: 0.01278
[2025-05-08 09:55:05] nn step 35350, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10364
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99941
	loss_value_1: 0.02044
	loss_reward_1: 0.00044
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01983
	loss_reward_2: 0.00313
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01929
	loss_reward_3: 0.00211
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.9977
	loss_value_4: 0.01828
	loss_reward_4: 0.00219
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99852
	loss_value_5: 0.0186
	loss_reward_5: 0.00491
	loss_policy: 0.00176
	loss_value: 0.20008
	loss_reward: 0.01277
[2025-05-08 09:55:11] nn step 35400, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10137
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01999
	loss_reward_1: 0.00041
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.0193
	loss_reward_2: 0.00306
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01871
	loss_reward_3: 0.00215
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01766
	loss_reward_4: 0.00221
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99836
	loss_value_5: 0.01802
	loss_reward_5: 0.00474
	loss_policy: 0.00129
	loss_value: 0.19505
	loss_reward: 0.01256
Optimization_Done 35400
[2025-05-08 09:56:31] [command] train weight_iter_35400.pkl 159 178
[2025-05-08 09:56:38] nn step 35450, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10573
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02092
	loss_reward_1: 0.00036
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99918
	loss_value_2: 0.02027
	loss_reward_2: 0.0031
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01969
	loss_reward_3: 0.0022
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01863
	loss_reward_4: 0.00217
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01891
	loss_reward_5: 0.00487
	loss_policy: 0.00103
	loss_value: 0.20414
	loss_reward: 0.0127
[2025-05-08 09:56:46] nn step 35500, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09893
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01955
	loss_reward_1: 0.00038
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99957
	loss_value_2: 0.0189
	loss_reward_2: 0.00293
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99906
	loss_value_3: 0.01835
	loss_reward_3: 0.00219
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01734
	loss_reward_4: 0.00213
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01764
	loss_reward_5: 0.00469
	loss_policy: 0.00136
	loss_value: 0.19071
	loss_reward: 0.01231
[2025-05-08 09:56:55] nn step 35550, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10357
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02043
	loss_reward_1: 0.00041
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01973
	loss_reward_2: 0.00304
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01914
	loss_reward_3: 0.00217
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01804
	loss_reward_4: 0.00216
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01833
	loss_reward_5: 0.00479
	loss_policy: 0.0011
	loss_value: 0.19925
	loss_reward: 0.01257
[2025-05-08 09:57:03] nn step 35600, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10152
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02005
	loss_reward_1: 0.00038
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01941
	loss_reward_2: 0.00302
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.9991
	loss_value_3: 0.0188
	loss_reward_3: 0.00216
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01777
	loss_reward_4: 0.0022
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01808
	loss_reward_5: 0.00472
	loss_policy: 0.00111
	loss_value: 0.19563
	loss_reward: 0.01248
Optimization_Done 35600
[2025-05-08 09:58:17] [command] train weight_iter_35600.pkl 160 179
[2025-05-08 09:58:26] nn step 35650, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09415
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01862
	loss_reward_1: 0.00035
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01797
	loss_reward_2: 0.00281
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99871
	loss_value_3: 0.01744
	loss_reward_3: 0.00208
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01644
	loss_reward_4: 0.00196
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01673
	loss_reward_5: 0.0044
	loss_policy: 0.00133
	loss_value: 0.18135
	loss_reward: 0.01161
[2025-05-08 09:58:32] nn step 35700, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09682
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01908
	loss_reward_1: 0.00039
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99922
	loss_value_2: 0.0184
	loss_reward_2: 0.00293
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01786
	loss_reward_3: 0.00213
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99789
	loss_value_4: 0.01686
	loss_reward_4: 0.00197
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01711
	loss_reward_5: 0.00463
	loss_policy: 0.00127
	loss_value: 0.18614
	loss_reward: 0.01205
[2025-05-08 09:58:40] nn step 35750, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09589
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01887
	loss_reward_1: 0.00035
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01822
	loss_reward_2: 0.00293
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01768
	loss_reward_3: 0.00205
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01671
	loss_reward_4: 0.00205
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01701
	loss_reward_5: 0.00453
	loss_policy: 0.00077
	loss_value: 0.18437
	loss_reward: 0.01191
[2025-05-08 09:58:48] nn step 35800, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10338
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.02032
	loss_reward_1: 0.0004
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01963
	loss_reward_2: 0.00312
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01894
	loss_reward_3: 0.00221
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99863
	loss_value_4: 0.01781
	loss_reward_4: 0.00216
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01816
	loss_reward_5: 0.00486
	loss_policy: 0.00107
	loss_value: 0.19825
	loss_reward: 0.01275
Optimization_Done 35800
[2025-05-08 10:00:03] [command] train weight_iter_35800.pkl 161 180
[2025-05-08 10:00:12] nn step 35850, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10101
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01992
	loss_reward_1: 0.00039
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01925
	loss_reward_2: 0.00303
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01866
	loss_reward_3: 0.00221
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01759
	loss_reward_4: 0.00218
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01788
	loss_reward_5: 0.0048
	loss_policy: 0.00078
	loss_value: 0.19431
	loss_reward: 0.01261
[2025-05-08 10:00:20] nn step 35900, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09819
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01936
	loss_reward_1: 0.00036
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01873
	loss_reward_2: 0.00293
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01815
	loss_reward_3: 0.00209
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.99879
	loss_value_4: 0.0171
	loss_reward_4: 0.00211
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01741
	loss_reward_5: 0.00462
	loss_policy: 0.00131
	loss_value: 0.18893
	loss_reward: 0.0121
[2025-05-08 10:00:26] nn step 35950, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99984
	loss_value_0: 0.10263
	loss_policy_1: 1e-05
	accuracy_policy_1: 1.0
	loss_value_1: 0.02025
	loss_reward_1: 0.00037
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01961
	loss_reward_2: 0.00309
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99863
	loss_value_3: 0.019
	loss_reward_3: 0.00217
	loss_policy_4: 0.00043
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01796
	loss_reward_4: 0.00219
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01833
	loss_reward_5: 0.00482
	loss_policy: 0.00153
	loss_value: 0.19779
	loss_reward: 0.01264
[2025-05-08 10:00:34] nn step 36000, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10209
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0201
	loss_reward_1: 0.00041
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01946
	loss_reward_2: 0.00301
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01892
	loss_reward_3: 0.00211
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01786
	loss_reward_4: 0.00218
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01811
	loss_reward_5: 0.00483
	loss_policy: 0.00098
	loss_value: 0.19654
	loss_reward: 0.01254
Optimization_Done 36000
[2025-05-08 10:01:51] [command] train weight_iter_36000.pkl 162 181
[2025-05-08 10:01:59] nn step 36050, lr: 0.1.
	loss_policy_0: 0.00012
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10286
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02031
	loss_reward_1: 0.0004
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01963
	loss_reward_2: 0.00308
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01904
	loss_reward_3: 0.00218
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01794
	loss_reward_4: 0.00222
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01823
	loss_reward_5: 0.00486
	loss_policy: 0.00075
	loss_value: 0.19801
	loss_reward: 0.01274
[2025-05-08 10:02:07] nn step 36100, lr: 0.1.
	loss_policy_0: 0.00012
	accuracy_policy_0: 0.99988
	loss_value_0: 0.1014
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02001
	loss_reward_1: 0.00036
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01937
	loss_reward_2: 0.00309
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01872
	loss_reward_3: 0.00215
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01772
	loss_reward_4: 0.00209
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01812
	loss_reward_5: 0.00482
	loss_policy: 0.00112
	loss_value: 0.19533
	loss_reward: 0.0125
[2025-05-08 10:02:14] nn step 36150, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.9998
	loss_value_0: 0.10189
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02006
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01939
	loss_reward_2: 0.00308
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01879
	loss_reward_3: 0.00211
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01772
	loss_reward_4: 0.00207
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99895
	loss_value_5: 0.018
	loss_reward_5: 0.00481
	loss_policy: 0.001
	loss_value: 0.19585
	loss_reward: 0.01242
[2025-05-08 10:02:22] nn step 36200, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10227
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99973
	loss_value_1: 0.02015
	loss_reward_1: 0.00043
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01947
	loss_reward_2: 0.00307
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01887
	loss_reward_3: 0.0022
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01778
	loss_reward_4: 0.00217
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01814
	loss_reward_5: 0.00484
	loss_policy: 0.00105
	loss_value: 0.19668
	loss_reward: 0.01271
Optimization_Done 36200
[2025-05-08 10:03:38] [command] train weight_iter_36200.pkl 163 182
[2025-05-08 10:03:47] nn step 36250, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09672
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0191
	loss_reward_1: 0.00038
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01855
	loss_reward_2: 0.00292
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01799
	loss_reward_3: 0.00209
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01704
	loss_reward_4: 0.00206
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99938
	loss_value_5: 0.0174
	loss_reward_5: 0.00462
	loss_policy: 0.00091
	loss_value: 0.18681
	loss_reward: 0.01208
[2025-05-08 10:03:55] nn step 36300, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10758
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02123
	loss_reward_1: 0.00041
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99965
	loss_value_2: 0.02053
	loss_reward_2: 0.00334
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01987
	loss_reward_3: 0.00227
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01878
	loss_reward_4: 0.0023
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01911
	loss_reward_5: 0.00514
	loss_policy: 0.00096
	loss_value: 0.2071
	loss_reward: 0.01345
[2025-05-08 10:04:03] nn step 36350, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.9998
	loss_value_0: 0.10157
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02003
	loss_reward_1: 0.00036
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01937
	loss_reward_2: 0.00313
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01879
	loss_reward_3: 0.00215
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01769
	loss_reward_4: 0.00217
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01798
	loss_reward_5: 0.00495
	loss_policy: 0.00091
	loss_value: 0.19543
	loss_reward: 0.01276
[2025-05-08 10:04:09] nn step 36400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10605
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02089
	loss_reward_1: 0.00043
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.02024
	loss_reward_2: 0.00323
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01958
	loss_reward_3: 0.00224
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99918
	loss_value_4: 0.0184
	loss_reward_4: 0.00233
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01871
	loss_reward_5: 0.00507
	loss_policy: 0.00075
	loss_value: 0.20386
	loss_reward: 0.01329
Optimization_Done 36400
[2025-05-08 10:05:27] [command] train weight_iter_36400.pkl 164 183
[2025-05-08 10:05:34] nn step 36450, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.10595
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02089
	loss_reward_1: 0.00044
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.02024
	loss_reward_2: 0.00313
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01955
	loss_reward_3: 0.00233
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99859
	loss_value_4: 0.01845
	loss_reward_4: 0.00235
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01872
	loss_reward_5: 0.00497
	loss_policy: 0.00082
	loss_value: 0.2038
	loss_reward: 0.01322
[2025-05-08 10:05:42] nn step 36500, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10149
	loss_policy_1: 1e-05
	accuracy_policy_1: 1.0
	loss_value_1: 0.02
	loss_reward_1: 0.0004
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01931
	loss_reward_2: 0.00312
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01871
	loss_reward_3: 0.00214
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01764
	loss_reward_4: 0.0022
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01791
	loss_reward_5: 0.00488
	loss_policy: 0.00064
	loss_value: 0.19506
	loss_reward: 0.01274
[2025-05-08 10:05:50] nn step 36550, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09731
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01926
	loss_reward_1: 0.00038
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01856
	loss_reward_2: 0.00295
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01793
	loss_reward_3: 0.00213
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01692
	loss_reward_4: 0.0021
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01719
	loss_reward_5: 0.00464
	loss_policy: 0.0008
	loss_value: 0.18717
	loss_reward: 0.0122
[2025-05-08 10:05:58] nn step 36600, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09644
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.019
	loss_reward_1: 0.00038
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01837
	loss_reward_2: 0.00296
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01773
	loss_reward_3: 0.00211
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01667
	loss_reward_4: 0.00211
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01693
	loss_reward_5: 0.00469
	loss_policy: 0.00103
	loss_value: 0.18515
	loss_reward: 0.01226
Optimization_Done 36600
[2025-05-08 10:07:16] [command] train weight_iter_36600.pkl 165 184
[2025-05-08 10:07:24] nn step 36650, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10377
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.02052
	loss_reward_1: 0.00043
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01986
	loss_reward_2: 0.00298
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01922
	loss_reward_3: 0.00222
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99844
	loss_value_4: 0.01821
	loss_reward_4: 0.00222
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01844
	loss_reward_5: 0.0048
	loss_policy: 0.0014
	loss_value: 0.20001
	loss_reward: 0.01266
[2025-05-08 10:07:31] nn step 36700, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10718
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.02118
	loss_reward_1: 0.00043
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99977
	loss_value_2: 0.02043
	loss_reward_2: 0.00325
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99957
	loss_value_3: 0.0198
	loss_reward_3: 0.00225
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01865
	loss_reward_4: 0.00224
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01894
	loss_reward_5: 0.00516
	loss_policy: 0.00103
	loss_value: 0.20619
	loss_reward: 0.01333
[2025-05-08 10:07:39] nn step 36750, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.0993
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01958
	loss_reward_1: 0.00037
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01889
	loss_reward_2: 0.00302
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01835
	loss_reward_3: 0.00213
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99891
	loss_value_4: 0.01725
	loss_reward_4: 0.00211
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99922
	loss_value_5: 0.01751
	loss_reward_5: 0.00477
	loss_policy: 0.00102
	loss_value: 0.19088
	loss_reward: 0.01239
[2025-05-08 10:07:47] nn step 36800, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.10381
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02046
	loss_reward_1: 0.00035
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01982
	loss_reward_2: 0.0032
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01919
	loss_reward_3: 0.00213
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01806
	loss_reward_4: 0.00219
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01834
	loss_reward_5: 0.005
	loss_policy: 0.00088
	loss_value: 0.19967
	loss_reward: 0.01287
Optimization_Done 36800
[2025-05-08 10:09:04] [command] train weight_iter_36800.pkl 166 185
[2025-05-08 10:09:13] nn step 36850, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09833
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01943
	loss_reward_1: 0.00041
	loss_policy_2: 1e-05
	accuracy_policy_2: 0.99996
	loss_value_2: 0.01876
	loss_reward_2: 0.00304
	loss_policy_3: 5e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01817
	loss_reward_3: 0.00213
	loss_policy_4: 0.00012
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01717
	loss_reward_4: 0.00215
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99945
	loss_value_5: 0.0174
	loss_reward_5: 0.0048
	loss_policy: 0.00039
	loss_value: 0.18926
	loss_reward: 0.01253
[2025-05-08 10:09:19] nn step 36900, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09905
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0195
	loss_reward_1: 0.00036
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01881
	loss_reward_2: 0.00307
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01824
	loss_reward_3: 0.00207
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01721
	loss_reward_4: 0.00215
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01747
	loss_reward_5: 0.00476
	loss_policy: 0.00073
	loss_value: 0.19027
	loss_reward: 0.01239
[2025-05-08 10:09:27] nn step 36950, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09872
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01948
	loss_reward_1: 0.0004
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01876
	loss_reward_2: 0.00306
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01819
	loss_reward_3: 0.00214
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99949
	loss_value_4: 0.01717
	loss_reward_4: 0.0022
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99953
	loss_value_5: 0.01744
	loss_reward_5: 0.00478
	loss_policy: 0.00051
	loss_value: 0.18976
	loss_reward: 0.01257
[2025-05-08 10:09:35] nn step 37000, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09958
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01964
	loss_reward_1: 0.00042
	loss_policy_2: 3e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01896
	loss_reward_2: 0.00305
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.9998
	loss_value_3: 0.01835
	loss_reward_3: 0.00209
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01733
	loss_reward_4: 0.00207
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.9993
	loss_value_5: 0.0176
	loss_reward_5: 0.00478
	loss_policy: 0.0006
	loss_value: 0.19146
	loss_reward: 0.01241
Optimization_Done 37000
[2025-05-08 10:10:55] [command] train weight_iter_37000.pkl 167 186
[2025-05-08 10:11:04] nn step 37050, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09242
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01822
	loss_reward_1: 0.00036
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01756
	loss_reward_2: 0.00282
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01702
	loss_reward_3: 0.00196
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01604
	loss_reward_4: 0.00198
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01625
	loss_reward_5: 0.00451
	loss_policy: 0.00105
	loss_value: 0.17751
	loss_reward: 0.01163
[2025-05-08 10:11:10] nn step 37100, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09559
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01886
	loss_reward_1: 0.00033
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01823
	loss_reward_2: 0.0029
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01767
	loss_reward_3: 0.00199
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01662
	loss_reward_4: 0.00205
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01687
	loss_reward_5: 0.00465
	loss_policy: 0.0008
	loss_value: 0.18385
	loss_reward: 0.01192
[2025-05-08 10:11:18] nn step 37150, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09704
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01913
	loss_reward_1: 0.00036
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01849
	loss_reward_2: 0.00295
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01787
	loss_reward_3: 0.00212
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01682
	loss_reward_4: 0.00212
	loss_policy_5: 0.00017
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01711
	loss_reward_5: 0.00467
	loss_policy: 0.00055
	loss_value: 0.18645
	loss_reward: 0.01222
[2025-05-08 10:11:26] nn step 37200, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99977
	loss_value_0: 0.09862
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01949
	loss_reward_1: 0.00054
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01904
	loss_reward_2: 0.00359
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01831
	loss_reward_3: 0.00275
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01728
	loss_reward_4: 0.00254
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01788
	loss_reward_5: 0.00542
	loss_policy: 0.00127
	loss_value: 0.19062
	loss_reward: 0.01485
Optimization_Done 37200
[2025-05-08 10:12:42] [command] train weight_iter_37200.pkl 168 187
[2025-05-08 10:12:51] nn step 37250, lr: 0.1.
	loss_policy_0: 0.00083
	accuracy_policy_0: 0.99934
	loss_value_0: 0.12537
	loss_policy_1: 0.00038
	accuracy_policy_1: 0.99656
	loss_value_1: 0.02524
	loss_reward_1: 0.00222
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99625
	loss_value_2: 0.02534
	loss_reward_2: 0.00459
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.9975
	loss_value_3: 0.02526
	loss_reward_3: 0.00528
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99836
	loss_value_4: 0.02468
	loss_reward_4: 0.00552
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99883
	loss_value_5: 0.02481
	loss_reward_5: 0.00703
	loss_policy: 0.00267
	loss_value: 0.25071
	loss_reward: 0.02465
[2025-05-08 10:12:59] nn step 37300, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99992
	loss_value_0: 0.11714
	loss_policy_1: 0.00034
	accuracy_policy_1: 0.99801
	loss_value_1: 0.02383
	loss_reward_1: 0.00145
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99773
	loss_value_2: 0.02345
	loss_reward_2: 0.00454
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.9982
	loss_value_3: 0.02304
	loss_reward_3: 0.00484
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99762
	loss_value_4: 0.02226
	loss_reward_4: 0.00499
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99879
	loss_value_5: 0.0226
	loss_reward_5: 0.00702
	loss_policy: 0.00187
	loss_value: 0.23232
	loss_reward: 0.02284
[2025-05-08 10:13:05] nn step 37350, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10785
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.9977
	loss_value_1: 0.02172
	loss_reward_1: 0.00092
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99781
	loss_value_2: 0.02168
	loss_reward_2: 0.00406
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99785
	loss_value_3: 0.02115
	loss_reward_3: 0.00395
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99738
	loss_value_4: 0.02064
	loss_reward_4: 0.00396
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99918
	loss_value_5: 0.02079
	loss_reward_5: 0.0064
	loss_policy: 0.00145
	loss_value: 0.21384
	loss_reward: 0.01929
[2025-05-08 10:13:13] nn step 37400, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99992
	loss_value_0: 0.12551
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99887
	loss_value_1: 0.02485
	loss_reward_1: 0.00261
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.9977
	loss_value_2: 0.02478
	loss_reward_2: 0.00596
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99766
	loss_value_3: 0.02436
	loss_reward_3: 0.00595
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99793
	loss_value_4: 0.02342
	loss_reward_4: 0.00633
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99957
	loss_value_5: 0.02391
	loss_reward_5: 0.0083
	loss_policy: 0.0018
	loss_value: 0.24683
	loss_reward: 0.02915
Optimization_Done 37400
[2025-05-08 10:14:31] [command] train weight_iter_37400.pkl 169 188
[2025-05-08 10:14:40] nn step 37450, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99977
	loss_value_0: 0.11096
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99379
	loss_value_1: 0.02236
	loss_reward_1: 0.00186
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99438
	loss_value_2: 0.02263
	loss_reward_2: 0.00438
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.9968
	loss_value_3: 0.02229
	loss_reward_3: 0.00452
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99762
	loss_value_4: 0.02147
	loss_reward_4: 0.00517
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99883
	loss_value_5: 0.02191
	loss_reward_5: 0.00687
	loss_policy: 0.00179
	loss_value: 0.22163
	loss_reward: 0.02281
[2025-05-08 10:14:48] nn step 37500, lr: 0.1.
	loss_policy_0: 0.00108
	accuracy_policy_0: 0.99844
	loss_value_0: 0.10197
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.9973
	loss_value_1: 0.02048
	loss_reward_1: 0.00069
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99715
	loss_value_2: 0.02059
	loss_reward_2: 0.00335
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99707
	loss_value_3: 0.02034
	loss_reward_3: 0.00366
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99781
	loss_value_4: 0.01989
	loss_reward_4: 0.00411
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99926
	loss_value_5: 0.01983
	loss_reward_5: 0.00589
	loss_policy: 0.00275
	loss_value: 0.20311
	loss_reward: 0.0177
[2025-05-08 10:14:56] nn step 37550, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.9998
	loss_value_0: 0.1079
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99918
	loss_value_1: 0.02183
	loss_reward_1: 0.00058
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99777
	loss_value_2: 0.02167
	loss_reward_2: 0.00322
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99711
	loss_value_3: 0.02139
	loss_reward_3: 0.00306
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.99781
	loss_value_4: 0.02077
	loss_reward_4: 0.00397
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99926
	loss_value_5: 0.0207
	loss_reward_5: 0.00601
	loss_policy: 0.00149
	loss_value: 0.21426
	loss_reward: 0.01684
[2025-05-08 10:15:02] nn step 37600, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.9998
	loss_value_0: 0.10293
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99902
	loss_value_1: 0.02084
	loss_reward_1: 0.00052
	loss_policy_2: 0.00049
	accuracy_policy_2: 0.99703
	loss_value_2: 0.02065
	loss_reward_2: 0.00307
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99645
	loss_value_3: 0.02039
	loss_reward_3: 0.00259
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01973
	loss_reward_4: 0.00302
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01969
	loss_reward_5: 0.00523
	loss_policy: 0.00218
	loss_value: 0.20422
	loss_reward: 0.01442
Optimization_Done 37600
[2025-05-08 10:16:20] [command] train weight_iter_37600.pkl 170 189
[2025-05-08 10:16:28] nn step 37650, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10408
	loss_policy_1: 0.0003
	accuracy_policy_1: 0.99871
	loss_value_1: 0.02098
	loss_reward_1: 0.00053
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99688
	loss_value_2: 0.02069
	loss_reward_2: 0.00296
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99574
	loss_value_3: 0.02037
	loss_reward_3: 0.00228
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99652
	loss_value_4: 0.01994
	loss_reward_4: 0.00262
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99859
	loss_value_5: 0.02
	loss_reward_5: 0.00481
	loss_policy: 0.00218
	loss_value: 0.20606
	loss_reward: 0.0132
[2025-05-08 10:16:36] nn step 37700, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.9998
	loss_value_0: 0.10706
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99965
	loss_value_1: 0.02164
	loss_reward_1: 0.00051
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99816
	loss_value_2: 0.02135
	loss_reward_2: 0.00303
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99625
	loss_value_3: 0.0211
	loss_reward_3: 0.0024
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99637
	loss_value_4: 0.02064
	loss_reward_4: 0.00263
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99898
	loss_value_5: 0.0206
	loss_reward_5: 0.00511
	loss_policy: 0.00175
	loss_value: 0.2124
	loss_reward: 0.01368
[2025-05-08 10:16:44] nn step 37750, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99969
	loss_value_0: 0.10138
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02055
	loss_reward_1: 0.00058
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99898
	loss_value_2: 0.02035
	loss_reward_2: 0.00318
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99684
	loss_value_3: 0.01998
	loss_reward_3: 0.00235
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01951
	loss_reward_4: 0.0023
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99891
	loss_value_5: 0.0195
	loss_reward_5: 0.00453
	loss_policy: 0.0022
	loss_value: 0.20127
	loss_reward: 0.01295
[2025-05-08 10:16:51] nn step 37800, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99984
	loss_value_0: 0.10242
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0207
	loss_reward_1: 0.00054
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99875
	loss_value_2: 0.02036
	loss_reward_2: 0.00277
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99688
	loss_value_3: 0.02012
	loss_reward_3: 0.00209
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99676
	loss_value_4: 0.01958
	loss_reward_4: 0.00225
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01962
	loss_reward_5: 0.00438
	loss_policy: 0.00209
	loss_value: 0.2028
	loss_reward: 0.01203
Optimization_Done 37800
[2025-05-08 10:18:03] [command] train weight_iter_37800.pkl 171 190
[2025-05-08 10:18:12] nn step 37850, lr: 0.1.
	loss_policy_0: 0.00024
	accuracy_policy_0: 0.99961
	loss_value_0: 0.10007
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99961
	loss_value_1: 0.02032
	loss_reward_1: 0.00047
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01997
	loss_reward_2: 0.00273
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99785
	loss_value_3: 0.01972
	loss_reward_3: 0.00202
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01922
	loss_reward_4: 0.00203
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01921
	loss_reward_5: 0.00417
	loss_policy: 0.00183
	loss_value: 0.19851
	loss_reward: 0.01142
[2025-05-08 10:18:19] nn step 37900, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.9998
	loss_value_0: 0.1014
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.02044
	loss_reward_1: 0.00053
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02003
	loss_reward_2: 0.00285
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99773
	loss_value_3: 0.01973
	loss_reward_3: 0.00202
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99691
	loss_value_4: 0.0192
	loss_reward_4: 0.00203
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01906
	loss_reward_5: 0.00436
	loss_policy: 0.00153
	loss_value: 0.19985
	loss_reward: 0.01179
[2025-05-08 10:18:27] nn step 37950, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10233
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02058
	loss_reward_1: 0.00049
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.9991
	loss_value_2: 0.02027
	loss_reward_2: 0.00278
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01999
	loss_reward_3: 0.00211
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01947
	loss_reward_4: 0.00207
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01935
	loss_reward_5: 0.00439
	loss_policy: 0.00212
	loss_value: 0.20198
	loss_reward: 0.01183
[2025-05-08 10:18:35] nn step 38000, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99977
	loss_value_0: 0.10123
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99957
	loss_value_1: 0.02034
	loss_reward_1: 0.00054
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01994
	loss_reward_2: 0.00278
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99871
	loss_value_3: 0.01963
	loss_reward_3: 0.00208
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01908
	loss_reward_4: 0.00207
	loss_policy_5: 0.00053
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01891
	loss_reward_5: 0.00441
	loss_policy: 0.00184
	loss_value: 0.19913
	loss_reward: 0.01188
Optimization_Done 38000
[2025-05-08 10:19:51] [command] train weight_iter_38000.pkl 172 191
[2025-05-08 10:20:00] nn step 38050, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10843
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.9998
	loss_value_1: 0.02133
	loss_reward_1: 0.00056
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99852
	loss_value_2: 0.02071
	loss_reward_2: 0.0032
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99727
	loss_value_3: 0.0205
	loss_reward_3: 0.00239
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99691
	loss_value_4: 0.02015
	loss_reward_4: 0.00241
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99824
	loss_value_5: 0.01981
	loss_reward_5: 0.00485
	loss_policy: 0.00234
	loss_value: 0.21093
	loss_reward: 0.01341
[2025-05-08 10:20:06] nn step 38100, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10386
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.02094
	loss_reward_1: 0.00053
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99926
	loss_value_2: 0.02036
	loss_reward_2: 0.00286
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99812
	loss_value_3: 0.02022
	loss_reward_3: 0.00206
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01966
	loss_reward_4: 0.00201
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01943
	loss_reward_5: 0.00448
	loss_policy: 0.00143
	loss_value: 0.20447
	loss_reward: 0.01195
[2025-05-08 10:20:15] nn step 38150, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09593
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01924
	loss_reward_1: 0.00051
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.9991
	loss_value_2: 0.01876
	loss_reward_2: 0.00265
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01848
	loss_reward_3: 0.00197
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01808
	loss_reward_4: 0.00189
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01797
	loss_reward_5: 0.00408
	loss_policy: 0.00195
	loss_value: 0.18845
	loss_reward: 0.01108
[2025-05-08 10:20:23] nn step 38200, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09556
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01916
	loss_reward_1: 0.00045
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01868
	loss_reward_2: 0.00267
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01843
	loss_reward_3: 0.00193
	loss_policy_4: 0.00045
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01795
	loss_reward_4: 0.00189
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.9982
	loss_value_5: 0.01779
	loss_reward_5: 0.0041
	loss_policy: 0.00165
	loss_value: 0.18756
	loss_reward: 0.01106
Optimization_Done 38200
[2025-05-08 10:21:40] [command] train weight_iter_38200.pkl 173 192
[2025-05-08 10:21:49] nn step 38250, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.10333
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99949
	loss_value_1: 0.02073
	loss_reward_1: 0.00049
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99887
	loss_value_2: 0.02015
	loss_reward_2: 0.00274
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.9984
	loss_value_3: 0.01971
	loss_reward_3: 0.00206
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01913
	loss_reward_4: 0.00197
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99766
	loss_value_5: 0.01898
	loss_reward_5: 0.00425
	loss_policy: 0.00225
	loss_value: 0.20204
	loss_reward: 0.01151
[2025-05-08 10:21:57] nn step 38300, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99977
	loss_value_0: 0.10006
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02015
	loss_reward_1: 0.00051
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01955
	loss_reward_2: 0.00279
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01921
	loss_reward_3: 0.002
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01871
	loss_reward_4: 0.00196
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.9977
	loss_value_5: 0.01859
	loss_reward_5: 0.00428
	loss_policy: 0.00194
	loss_value: 0.19627
	loss_reward: 0.01154
[2025-05-08 10:22:03] nn step 38350, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09703
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01949
	loss_reward_1: 0.00046
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01904
	loss_reward_2: 0.00266
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01869
	loss_reward_3: 0.002
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99734
	loss_value_4: 0.01813
	loss_reward_4: 0.00196
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.9975
	loss_value_5: 0.01793
	loss_reward_5: 0.00417
	loss_policy: 0.00189
	loss_value: 0.19031
	loss_reward: 0.01125
[2025-05-08 10:22:12] nn step 38400, lr: 0.1.
	loss_policy_0: 0.00021
	accuracy_policy_0: 0.99977
	loss_value_0: 0.09923
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01989
	loss_reward_1: 0.00043
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01939
	loss_reward_2: 0.00281
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01899
	loss_reward_3: 0.002
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99723
	loss_value_4: 0.0184
	loss_reward_4: 0.00202
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99805
	loss_value_5: 0.01825
	loss_reward_5: 0.00429
	loss_policy: 0.00141
	loss_value: 0.19415
	loss_reward: 0.01154
Optimization_Done 38400
[2025-05-08 10:23:26] [command] train weight_iter_38400.pkl 174 193
[2025-05-08 10:23:35] nn step 38450, lr: 0.1.
	loss_policy_0: 0.00025
	accuracy_policy_0: 0.99969
	loss_value_0: 0.09908
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01995
	loss_reward_1: 0.00047
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99914
	loss_value_2: 0.0194
	loss_reward_2: 0.00273
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01905
	loss_reward_3: 0.00201
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99703
	loss_value_4: 0.01841
	loss_reward_4: 0.00194
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01828
	loss_reward_5: 0.00423
	loss_policy: 0.00192
	loss_value: 0.19416
	loss_reward: 0.01138
[2025-05-08 10:23:42] nn step 38500, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09731
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99945
	loss_value_1: 0.0196
	loss_reward_1: 0.0005
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99914
	loss_value_2: 0.0191
	loss_reward_2: 0.00277
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99855
	loss_value_3: 0.0188
	loss_reward_3: 0.00203
	loss_policy_4: 0.00048
	accuracy_policy_4: 0.99781
	loss_value_4: 0.01814
	loss_reward_4: 0.00196
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99766
	loss_value_5: 0.01792
	loss_reward_5: 0.00435
	loss_policy: 0.00182
	loss_value: 0.19087
	loss_reward: 0.01162
[2025-05-08 10:23:50] nn step 38550, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99992
	loss_value_0: 0.10482
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99953
	loss_value_1: 0.02093
	loss_reward_1: 0.00043
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99855
	loss_value_2: 0.02035
	loss_reward_2: 0.00293
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99754
	loss_value_3: 0.01995
	loss_reward_3: 0.0021
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01926
	loss_reward_4: 0.00213
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99742
	loss_value_5: 0.01921
	loss_reward_5: 0.00454
	loss_policy: 0.00208
	loss_value: 0.20452
	loss_reward: 0.01213
[2025-05-08 10:23:57] nn step 38600, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99977
	loss_value_0: 0.10099
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99949
	loss_value_1: 0.0203
	loss_reward_1: 0.00044
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01977
	loss_reward_2: 0.00289
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01946
	loss_reward_3: 0.00206
	loss_policy_4: 0.00045
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01887
	loss_reward_4: 0.00208
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01877
	loss_reward_5: 0.00453
	loss_policy: 0.00196
	loss_value: 0.19815
	loss_reward: 0.012
Optimization_Done 38600
[2025-05-08 10:25:17] [command] train weight_iter_38600.pkl 175 194
[2025-05-08 10:25:24] nn step 38650, lr: 0.1.
	loss_policy_0: 0.00012
	accuracy_policy_0: 0.9998
	loss_value_0: 0.09422
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01892
	loss_reward_1: 0.00044
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01844
	loss_reward_2: 0.00272
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.9991
	loss_value_3: 0.0181
	loss_reward_3: 0.00196
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01748
	loss_reward_4: 0.00197
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01737
	loss_reward_5: 0.00418
	loss_policy: 0.0011
	loss_value: 0.18453
	loss_reward: 0.01127
[2025-05-08 10:25:32] nn step 38700, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09458
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01913
	loss_reward_1: 0.00042
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01848
	loss_reward_2: 0.00272
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01814
	loss_reward_3: 0.00198
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01747
	loss_reward_4: 0.00195
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99762
	loss_value_5: 0.01735
	loss_reward_5: 0.00418
	loss_policy: 0.0017
	loss_value: 0.18515
	loss_reward: 0.01125
[2025-05-08 10:25:41] nn step 38750, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09693
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99949
	loss_value_1: 0.0195
	loss_reward_1: 0.00038
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.9991
	loss_value_2: 0.01896
	loss_reward_2: 0.00283
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01865
	loss_reward_3: 0.00199
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99797
	loss_value_4: 0.01807
	loss_reward_4: 0.00189
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99824
	loss_value_5: 0.01786
	loss_reward_5: 0.00436
	loss_policy: 0.00165
	loss_value: 0.18997
	loss_reward: 0.01146
[2025-05-08 10:25:47] nn step 38800, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99988
	loss_value_0: 0.10445
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99934
	loss_value_1: 0.02105
	loss_reward_1: 0.00047
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99895
	loss_value_2: 0.02043
	loss_reward_2: 0.00299
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99875
	loss_value_3: 0.02002
	loss_reward_3: 0.00216
	loss_policy_4: 0.00048
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01925
	loss_reward_4: 0.00215
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99773
	loss_value_5: 0.01919
	loss_reward_5: 0.00475
	loss_policy: 0.00178
	loss_value: 0.20439
	loss_reward: 0.01252
Optimization_Done 38800
[2025-05-08 10:27:07] [command] train weight_iter_38800.pkl 176 195
[2025-05-08 10:27:14] nn step 38850, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09574
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99949
	loss_value_1: 0.0193
	loss_reward_1: 0.00064
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01896
	loss_reward_2: 0.00322
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01865
	loss_reward_3: 0.00252
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01809
	loss_reward_4: 0.00247
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99766
	loss_value_5: 0.0181
	loss_reward_5: 0.00461
	loss_policy: 0.00186
	loss_value: 0.18886
	loss_reward: 0.01347
[2025-05-08 10:27:22] nn step 38900, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99988
	loss_value_0: 0.09503
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01909
	loss_reward_1: 0.00051
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01863
	loss_reward_2: 0.00285
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99797
	loss_value_3: 0.01829
	loss_reward_3: 0.00211
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99777
	loss_value_4: 0.01767
	loss_reward_4: 0.00212
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01771
	loss_reward_5: 0.00437
	loss_policy: 0.00152
	loss_value: 0.18642
	loss_reward: 0.01195
[2025-05-08 10:27:30] nn step 38950, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09319
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01872
	loss_reward_1: 0.00047
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01823
	loss_reward_2: 0.00273
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01786
	loss_reward_3: 0.002
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99711
	loss_value_4: 0.01725
	loss_reward_4: 0.002
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99766
	loss_value_5: 0.01721
	loss_reward_5: 0.00426
	loss_policy: 0.00195
	loss_value: 0.18247
	loss_reward: 0.01146
[2025-05-08 10:27:37] nn step 39000, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09491
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01914
	loss_reward_1: 0.00046
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01857
	loss_reward_2: 0.0029
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01817
	loss_reward_3: 0.00206
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99828
	loss_value_4: 0.01766
	loss_reward_4: 0.00199
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01756
	loss_reward_5: 0.00439
	loss_policy: 0.00114
	loss_value: 0.18601
	loss_reward: 0.0118
Optimization_Done 39000
[2025-05-08 10:28:55] [command] train weight_iter_39000.pkl 177 196
[2025-05-08 10:29:02] nn step 39050, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09202
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01846
	loss_reward_1: 0.00044
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01791
	loss_reward_2: 0.0026
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01759
	loss_reward_3: 0.0019
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99844
	loss_value_4: 0.01694
	loss_reward_4: 0.00192
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99801
	loss_value_5: 0.01685
	loss_reward_5: 0.00401
	loss_policy: 0.00157
	loss_value: 0.17976
	loss_reward: 0.01087
[2025-05-08 10:29:10] nn step 39100, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99965
	loss_value_0: 0.09523
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01917
	loss_reward_1: 0.00042
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01852
	loss_reward_2: 0.00282
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99836
	loss_value_3: 0.01817
	loss_reward_3: 0.00204
	loss_policy_4: 0.00045
	accuracy_policy_4: 0.99797
	loss_value_4: 0.0175
	loss_reward_4: 0.00196
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99836
	loss_value_5: 0.01739
	loss_reward_5: 0.00435
	loss_policy: 0.00175
	loss_value: 0.18598
	loss_reward: 0.01159
[2025-05-08 10:29:18] nn step 39150, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99977
	loss_value_0: 0.09248
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01856
	loss_reward_1: 0.00043
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01805
	loss_reward_2: 0.0028
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01762
	loss_reward_3: 0.00195
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99824
	loss_value_4: 0.01695
	loss_reward_4: 0.00189
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01686
	loss_reward_5: 0.00432
	loss_policy: 0.00156
	loss_value: 0.18052
	loss_reward: 0.0114
[2025-05-08 10:29:26] nn step 39200, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.9998
	loss_value_0: 0.09164
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99934
	loss_value_1: 0.01846
	loss_reward_1: 0.00043
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01787
	loss_reward_2: 0.00266
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01754
	loss_reward_3: 0.00196
	loss_policy_4: 0.00051
	accuracy_policy_4: 0.99711
	loss_value_4: 0.01685
	loss_reward_4: 0.00195
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01669
	loss_reward_5: 0.00434
	loss_policy: 0.00189
	loss_value: 0.17904
	loss_reward: 0.01133
Optimization_Done 39200
[2025-05-08 10:30:44] [command] train weight_iter_39200.pkl 178 197
[2025-05-08 10:30:52] nn step 39250, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.09258
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01872
	loss_reward_1: 0.00039
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01816
	loss_reward_2: 0.00272
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01787
	loss_reward_3: 0.00201
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01715
	loss_reward_4: 0.00197
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01706
	loss_reward_5: 0.00428
	loss_policy: 0.00092
	loss_value: 0.18154
	loss_reward: 0.01137
[2025-05-08 10:30:59] nn step 39300, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.092
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01848
	loss_reward_1: 0.00038
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01795
	loss_reward_2: 0.00275
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01756
	loss_reward_3: 0.00195
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01692
	loss_reward_4: 0.00192
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.9982
	loss_value_5: 0.0168
	loss_reward_5: 0.00421
	loss_policy: 0.00129
	loss_value: 0.17972
	loss_reward: 0.01121
[2025-05-08 10:31:07] nn step 39350, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.09229
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01852
	loss_reward_1: 0.0004
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01806
	loss_reward_2: 0.00266
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99879
	loss_value_3: 0.01772
	loss_reward_3: 0.00194
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01709
	loss_reward_4: 0.00197
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01693
	loss_reward_5: 0.00421
	loss_policy: 0.00139
	loss_value: 0.18062
	loss_reward: 0.01117
[2025-05-08 10:31:15] nn step 39400, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.08609
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01726
	loss_reward_1: 0.00036
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01681
	loss_reward_2: 0.00249
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01654
	loss_reward_3: 0.00179
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01591
	loss_reward_4: 0.00182
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01578
	loss_reward_5: 0.004
	loss_policy: 0.00093
	loss_value: 0.16839
	loss_reward: 0.01047
Optimization_Done 39400
[2025-05-08 10:32:33] [command] train weight_iter_39400.pkl 179 198
[2025-05-08 10:32:42] nn step 39450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09362
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01885
	loss_reward_1: 0.00039
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01834
	loss_reward_2: 0.00271
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01798
	loss_reward_3: 0.00204
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99891
	loss_value_4: 0.01732
	loss_reward_4: 0.00201
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01723
	loss_reward_5: 0.00435
	loss_policy: 0.00094
	loss_value: 0.18334
	loss_reward: 0.0115
[2025-05-08 10:32:48] nn step 39500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09505
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01913
	loss_reward_1: 0.00034
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01864
	loss_reward_2: 0.00276
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01827
	loss_reward_3: 0.002
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01755
	loss_reward_4: 0.00199
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01755
	loss_reward_5: 0.00436
	loss_policy: 0.00069
	loss_value: 0.18619
	loss_reward: 0.01145
[2025-05-08 10:32:56] nn step 39550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09019
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01817
	loss_reward_1: 0.00038
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01766
	loss_reward_2: 0.00269
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.9991
	loss_value_3: 0.0173
	loss_reward_3: 0.00194
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99875
	loss_value_4: 0.01658
	loss_reward_4: 0.00192
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.9984
	loss_value_5: 0.0165
	loss_reward_5: 0.00425
	loss_policy: 0.00093
	loss_value: 0.1764
	loss_reward: 0.01117
[2025-05-08 10:33:04] nn step 39600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08804
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01768
	loss_reward_1: 0.00034
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01722
	loss_reward_2: 0.00267
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01688
	loss_reward_3: 0.00191
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01616
	loss_reward_4: 0.00188
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01608
	loss_reward_5: 0.00415
	loss_policy: 0.00064
	loss_value: 0.17206
	loss_reward: 0.01095
Optimization_Done 39600
[2025-05-08 10:34:22] [command] train weight_iter_39600.pkl 180 199
[2025-05-08 10:34:31] nn step 39650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08842
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01776
	loss_reward_1: 0.00038
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01733
	loss_reward_2: 0.0026
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01691
	loss_reward_3: 0.00185
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99898
	loss_value_4: 0.01626
	loss_reward_4: 0.00192
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01617
	loss_reward_5: 0.00409
	loss_policy: 0.00078
	loss_value: 0.17285
	loss_reward: 0.01085
[2025-05-08 10:34:39] nn step 39700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08674
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01741
	loss_reward_1: 0.00038
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01694
	loss_reward_2: 0.0026
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01664
	loss_reward_3: 0.00183
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99875
	loss_value_4: 0.01596
	loss_reward_4: 0.00187
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01589
	loss_reward_5: 0.00415
	loss_policy: 0.00124
	loss_value: 0.16958
	loss_reward: 0.01082
[2025-05-08 10:34:45] nn step 39750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08858
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01773
	loss_reward_1: 0.00038
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01726
	loss_reward_2: 0.00257
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01686
	loss_reward_3: 0.00183
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01618
	loss_reward_4: 0.00189
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01617
	loss_reward_5: 0.00411
	loss_policy: 0.00137
	loss_value: 0.17279
	loss_reward: 0.01079
[2025-05-08 10:34:53] nn step 39800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0916
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01841
	loss_reward_1: 0.0004
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01782
	loss_reward_2: 0.00284
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99918
	loss_value_3: 0.0175
	loss_reward_3: 0.00195
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01682
	loss_reward_4: 0.00195
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01674
	loss_reward_5: 0.00443
	loss_policy: 0.00104
	loss_value: 0.17889
	loss_reward: 0.01157
Optimization_Done 39800
[2025-05-08 10:36:11] [command] train weight_iter_39800.pkl 181 200
[2025-05-08 10:36:20] nn step 39850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08714
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01758
	loss_reward_1: 0.00038
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99941
	loss_value_2: 0.017
	loss_reward_2: 0.0026
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01662
	loss_reward_3: 0.00194
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01594
	loss_reward_4: 0.00189
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99855
	loss_value_5: 0.01587
	loss_reward_5: 0.00413
	loss_policy: 0.00106
	loss_value: 0.17017
	loss_reward: 0.01094
[2025-05-08 10:36:28] nn step 39900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09283
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01866
	loss_reward_1: 0.00037
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01816
	loss_reward_2: 0.00282
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99957
	loss_value_3: 0.01772
	loss_reward_3: 0.00207
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99934
	loss_value_4: 0.01696
	loss_reward_4: 0.002
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99906
	loss_value_5: 0.01691
	loss_reward_5: 0.00443
	loss_policy: 0.0005
	loss_value: 0.18124
	loss_reward: 0.01169
[2025-05-08 10:36:34] nn step 39950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08559
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01724
	loss_reward_1: 0.00034
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99938
	loss_value_2: 0.0167
	loss_reward_2: 0.00265
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01629
	loss_reward_3: 0.00187
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01562
	loss_reward_4: 0.00186
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01561
	loss_reward_5: 0.00406
	loss_policy: 0.0007
	loss_value: 0.16705
	loss_reward: 0.01077
[2025-05-08 10:36:42] nn step 40000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09215
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01855
	loss_reward_1: 0.00035
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01799
	loss_reward_2: 0.00285
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01761
	loss_reward_3: 0.00203
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01692
	loss_reward_4: 0.00193
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01689
	loss_reward_5: 0.00446
	loss_policy: 0.00087
	loss_value: 0.18011
	loss_reward: 0.01162
Optimization_Done 40000
[2025-05-08 10:38:01] [command] train weight_iter_40000.pkl 182 201
[2025-05-08 10:38:10] nn step 40050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08623
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01728
	loss_reward_1: 0.00041
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01679
	loss_reward_2: 0.0026
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99941
	loss_value_3: 0.0164
	loss_reward_3: 0.00188
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01574
	loss_reward_4: 0.00187
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01574
	loss_reward_5: 0.00411
	loss_policy: 0.00068
	loss_value: 0.1682
	loss_reward: 0.01087
[2025-05-08 10:38:18] nn step 40100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08292
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01672
	loss_reward_1: 0.00037
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01634
	loss_reward_2: 0.00265
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01608
	loss_reward_3: 0.0019
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01537
	loss_reward_4: 0.00191
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01533
	loss_reward_5: 0.00413
	loss_policy: 0.00084
	loss_value: 0.16275
	loss_reward: 0.01097
[2025-05-08 10:38:26] nn step 40150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09274
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01861
	loss_reward_1: 0.00038
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01805
	loss_reward_2: 0.00289
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01772
	loss_reward_3: 0.0021
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99895
	loss_value_4: 0.01691
	loss_reward_4: 0.00205
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99871
	loss_value_5: 0.0169
	loss_reward_5: 0.00452
	loss_policy: 0.0009
	loss_value: 0.18094
	loss_reward: 0.01194
[2025-05-08 10:38:32] nn step 40200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08633
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01737
	loss_reward_1: 0.00036
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.9993
	loss_value_2: 0.0168
	loss_reward_2: 0.00273
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01647
	loss_reward_3: 0.00191
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01585
	loss_reward_4: 0.00188
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01587
	loss_reward_5: 0.00429
	loss_policy: 0.00095
	loss_value: 0.16869
	loss_reward: 0.01117
Optimization_Done 40200
[2025-05-08 10:39:51] [command] train weight_iter_40200.pkl 183 202
[2025-05-08 10:40:00] nn step 40250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08807
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01763
	loss_reward_1: 0.00043
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01719
	loss_reward_2: 0.00268
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99895
	loss_value_3: 0.01673
	loss_reward_3: 0.00197
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99824
	loss_value_4: 0.01604
	loss_reward_4: 0.00208
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99898
	loss_value_5: 0.0161
	loss_reward_5: 0.00417
	loss_policy: 0.00083
	loss_value: 0.17175
	loss_reward: 0.01134
[2025-05-08 10:40:08] nn step 40300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08887
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01781
	loss_reward_1: 0.00042
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.0173
	loss_reward_2: 0.00283
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01688
	loss_reward_3: 0.00202
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01613
	loss_reward_4: 0.00202
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01613
	loss_reward_5: 0.00439
	loss_policy: 0.00075
	loss_value: 0.17313
	loss_reward: 0.01169
[2025-05-08 10:40:17] nn step 40350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08677
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01745
	loss_reward_1: 0.00039
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01685
	loss_reward_2: 0.00276
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01647
	loss_reward_3: 0.00189
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99922
	loss_value_4: 0.0158
	loss_reward_4: 0.00192
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99906
	loss_value_5: 0.0158
	loss_reward_5: 0.00432
	loss_policy: 0.00079
	loss_value: 0.16915
	loss_reward: 0.01128
[2025-05-08 10:40:23] nn step 40400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08315
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01669
	loss_reward_1: 0.00035
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01617
	loss_reward_2: 0.00262
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99895
	loss_value_3: 0.01571
	loss_reward_3: 0.00188
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01499
	loss_reward_4: 0.00186
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99844
	loss_value_5: 0.015
	loss_reward_5: 0.00409
	loss_policy: 0.00113
	loss_value: 0.16171
	loss_reward: 0.01079
Optimization_Done 40400
[2025-05-08 10:41:43] [command] train weight_iter_40400.pkl 184 203
[2025-05-08 10:41:50] nn step 40450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08526
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01715
	loss_reward_1: 0.00041
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01669
	loss_reward_2: 0.00263
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99945
	loss_value_3: 0.0163
	loss_reward_3: 0.00194
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01562
	loss_reward_4: 0.00191
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01548
	loss_reward_5: 0.00417
	loss_policy: 0.00083
	loss_value: 0.1665
	loss_reward: 0.01106
[2025-05-08 10:41:59] nn step 40500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08424
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01689
	loss_reward_1: 0.00041
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01648
	loss_reward_2: 0.00284
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01609
	loss_reward_3: 0.00225
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01546
	loss_reward_4: 0.00199
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01548
	loss_reward_5: 0.00441
	loss_policy: 0.00114
	loss_value: 0.16464
	loss_reward: 0.0119
[2025-05-08 10:42:06] nn step 40550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08489
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01702
	loss_reward_1: 0.00033
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01657
	loss_reward_2: 0.00267
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01615
	loss_reward_3: 0.00192
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.9991
	loss_value_4: 0.0154
	loss_reward_4: 0.00185
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01543
	loss_reward_5: 0.00418
	loss_policy: 0.00094
	loss_value: 0.16546
	loss_reward: 0.01095
[2025-05-08 10:42:13] nn step 40600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08501
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01709
	loss_reward_1: 0.00036
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0167
	loss_reward_2: 0.00265
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01627
	loss_reward_3: 0.00192
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01551
	loss_reward_4: 0.00194
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01551
	loss_reward_5: 0.00415
	loss_policy: 0.00075
	loss_value: 0.16609
	loss_reward: 0.01102
Optimization_Done 40600
[2025-05-08 10:43:33] [command] train weight_iter_40600.pkl 185 204
[2025-05-08 10:43:40] nn step 40650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08253
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01658
	loss_reward_1: 0.00038
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01603
	loss_reward_2: 0.00255
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01576
	loss_reward_3: 0.0018
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01507
	loss_reward_4: 0.00191
	loss_policy_5: 0.00018
	accuracy_policy_5: 0.99906
	loss_value_5: 0.01498
	loss_reward_5: 0.00405
	loss_policy: 0.00059
	loss_value: 0.16096
	loss_reward: 0.0107
[2025-05-08 10:43:48] nn step 40700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07856
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01579
	loss_reward_1: 0.00036
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01538
	loss_reward_2: 0.00251
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01505
	loss_reward_3: 0.00179
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01448
	loss_reward_4: 0.00173
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01443
	loss_reward_5: 0.00397
	loss_policy: 0.00094
	loss_value: 0.15368
	loss_reward: 0.01036
[2025-05-08 10:43:56] nn step 40750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08367
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0168
	loss_reward_1: 0.00038
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01636
	loss_reward_2: 0.00264
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01593
	loss_reward_3: 0.00191
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99891
	loss_value_4: 0.01528
	loss_reward_4: 0.00191
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01524
	loss_reward_5: 0.00421
	loss_policy: 0.00116
	loss_value: 0.16328
	loss_reward: 0.01104
[2025-05-08 10:44:04] nn step 40800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09004
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01811
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01757
	loss_reward_2: 0.00287
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01721
	loss_reward_3: 0.00207
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01636
	loss_reward_4: 0.00203
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01634
	loss_reward_5: 0.00452
	loss_policy: 0.00083
	loss_value: 0.17563
	loss_reward: 0.01185
Optimization_Done 40800
[2025-05-08 10:45:23] [command] train weight_iter_40800.pkl 186 205
[2025-05-08 10:45:30] nn step 40850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08662
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01712
	loss_reward_1: 0.00044
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01659
	loss_reward_2: 0.00269
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01608
	loss_reward_3: 0.00197
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01546
	loss_reward_4: 0.00184
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01549
	loss_reward_5: 0.00409
	loss_policy: 0.00103
	loss_value: 0.16737
	loss_reward: 0.01104
[2025-05-08 10:45:38] nn step 40900, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08535
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01723
	loss_reward_1: 0.00057
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01735
	loss_reward_2: 0.00404
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99875
	loss_value_3: 0.01674
	loss_reward_3: 0.00311
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01616
	loss_reward_4: 0.00298
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99816
	loss_value_5: 0.0165
	loss_reward_5: 0.00548
	loss_policy: 0.00175
	loss_value: 0.16933
	loss_reward: 0.01618
[2025-05-08 10:45:46] nn step 40950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08264
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01672
	loss_reward_1: 0.00038
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01637
	loss_reward_2: 0.00267
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01599
	loss_reward_3: 0.00204
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99871
	loss_value_4: 0.01538
	loss_reward_4: 0.00207
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01553
	loss_reward_5: 0.00431
	loss_policy: 0.00144
	loss_value: 0.16264
	loss_reward: 0.01146
[2025-05-08 10:45:54] nn step 41000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08411
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.017
	loss_reward_1: 0.00044
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01647
	loss_reward_2: 0.00286
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01612
	loss_reward_3: 0.00218
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01542
	loss_reward_4: 0.00208
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01554
	loss_reward_5: 0.0044
	loss_policy: 0.00097
	loss_value: 0.16466
	loss_reward: 0.01196
Optimization_Done 41000
[2025-05-08 10:47:11] [command] train weight_iter_41000.pkl 187 206
[2025-05-08 10:47:20] nn step 41050, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08874
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99855
	loss_value_1: 0.01779
	loss_reward_1: 0.00051
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01747
	loss_reward_2: 0.00254
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.9973
	loss_value_3: 0.01718
	loss_reward_3: 0.00199
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01676
	loss_reward_4: 0.00229
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01695
	loss_reward_5: 0.00431
	loss_policy: 0.00145
	loss_value: 0.1749
	loss_reward: 0.01163
[2025-05-08 10:47:26] nn step 41100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08307
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99793
	loss_value_1: 0.01663
	loss_reward_1: 0.00043
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99688
	loss_value_2: 0.01633
	loss_reward_2: 0.00247
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99699
	loss_value_3: 0.01594
	loss_reward_3: 0.00186
	loss_policy_4: 0.00048
	accuracy_policy_4: 0.99648
	loss_value_4: 0.01533
	loss_reward_4: 0.00191
	loss_policy_5: 0.00053
	accuracy_policy_5: 0.99789
	loss_value_5: 0.0153
	loss_reward_5: 0.00396
	loss_policy: 0.00188
	loss_value: 0.16261
	loss_reward: 0.01064
[2025-05-08 10:47:34] nn step 41150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08439
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01699
	loss_reward_1: 0.00042
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99898
	loss_value_2: 0.0165
	loss_reward_2: 0.0026
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99867
	loss_value_3: 0.01625
	loss_reward_3: 0.00194
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.9977
	loss_value_4: 0.01562
	loss_reward_4: 0.00194
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01565
	loss_reward_5: 0.00412
	loss_policy: 0.00134
	loss_value: 0.1654
	loss_reward: 0.01102
[2025-05-08 10:47:42] nn step 41200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0841
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01689
	loss_reward_1: 0.00042
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01642
	loss_reward_2: 0.00294
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01615
	loss_reward_3: 0.00202
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.9975
	loss_value_4: 0.01548
	loss_reward_4: 0.00191
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01569
	loss_reward_5: 0.00447
	loss_policy: 0.00119
	loss_value: 0.16473
	loss_reward: 0.01176
Optimization_Done 41200
[2025-05-08 10:49:00] [command] train weight_iter_41200.pkl 188 207
[2025-05-08 10:49:09] nn step 41250, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09503
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99898
	loss_value_1: 0.0189
	loss_reward_1: 0.00049
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99809
	loss_value_2: 0.0187
	loss_reward_2: 0.00277
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99773
	loss_value_3: 0.01858
	loss_reward_3: 0.00216
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99781
	loss_value_4: 0.018
	loss_reward_4: 0.0022
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01802
	loss_reward_5: 0.00447
	loss_policy: 0.00155
	loss_value: 0.18723
	loss_reward: 0.01209
[2025-05-08 10:49:15] nn step 41300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08281
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0166
	loss_reward_1: 0.00043
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01617
	loss_reward_2: 0.00255
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99875
	loss_value_3: 0.01588
	loss_reward_3: 0.00179
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01538
	loss_reward_4: 0.00182
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01529
	loss_reward_5: 0.004
	loss_policy: 0.001
	loss_value: 0.16212
	loss_reward: 0.01059
[2025-05-08 10:49:23] nn step 41350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08421
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01694
	loss_reward_1: 0.00045
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01644
	loss_reward_2: 0.00271
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99906
	loss_value_3: 0.01614
	loss_reward_3: 0.00187
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99832
	loss_value_4: 0.01554
	loss_reward_4: 0.00189
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.99871
	loss_value_5: 0.0155
	loss_reward_5: 0.0042
	loss_policy: 0.00082
	loss_value: 0.16478
	loss_reward: 0.01113
[2025-05-08 10:49:31] nn step 41400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08017
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01612
	loss_reward_1: 0.00038
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01563
	loss_reward_2: 0.00259
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01534
	loss_reward_3: 0.00185
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99879
	loss_value_4: 0.01475
	loss_reward_4: 0.00179
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01476
	loss_reward_5: 0.00398
	loss_policy: 0.00109
	loss_value: 0.15676
	loss_reward: 0.0106
Optimization_Done 41400
[2025-05-08 10:50:48] [command] train weight_iter_41400.pkl 189 208
[2025-05-08 10:50:57] nn step 41450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08474
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01681
	loss_reward_1: 0.00042
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99926
	loss_value_2: 0.0164
	loss_reward_2: 0.00242
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.9991
	loss_value_3: 0.01619
	loss_reward_3: 0.0018
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01564
	loss_reward_4: 0.00183
	loss_policy_5: 0.00034
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01547
	loss_reward_5: 0.00381
	loss_policy: 0.00102
	loss_value: 0.16525
	loss_reward: 0.01028
[2025-05-08 10:51:05] nn step 41500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08664
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01741
	loss_reward_1: 0.00046
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01689
	loss_reward_2: 0.00265
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01662
	loss_reward_3: 0.00188
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01608
	loss_reward_4: 0.00189
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01597
	loss_reward_5: 0.00415
	loss_policy: 0.0008
	loss_value: 0.16961
	loss_reward: 0.01103
[2025-05-08 10:51:11] nn step 41550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08133
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.0163
	loss_reward_1: 0.00039
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99953
	loss_value_2: 0.0159
	loss_reward_2: 0.00253
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01557
	loss_reward_3: 0.0018
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01508
	loss_reward_4: 0.00178
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01489
	loss_reward_5: 0.00395
	loss_policy: 0.00097
	loss_value: 0.15906
	loss_reward: 0.01045
[2025-05-08 10:51:19] nn step 41600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08675
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01747
	loss_reward_1: 0.00041
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99898
	loss_value_2: 0.0169
	loss_reward_2: 0.00276
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99871
	loss_value_3: 0.01664
	loss_reward_3: 0.00198
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01592
	loss_reward_4: 0.00198
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99852
	loss_value_5: 0.01584
	loss_reward_5: 0.00431
	loss_policy: 0.00117
	loss_value: 0.16952
	loss_reward: 0.01144
Optimization_Done 41600
[2025-05-08 10:52:38] [command] train weight_iter_41600.pkl 190 209
[2025-05-08 10:52:47] nn step 41650, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99984
	loss_value_0: 0.09042
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01835
	loss_reward_1: 0.00196
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.9984
	loss_value_2: 0.01798
	loss_reward_2: 0.00402
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99723
	loss_value_3: 0.01783
	loss_reward_3: 0.00343
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99711
	loss_value_4: 0.01734
	loss_reward_4: 0.00362
	loss_policy_5: 0.00053
	accuracy_policy_5: 0.99793
	loss_value_5: 0.01754
	loss_reward_5: 0.00531
	loss_policy: 0.00225
	loss_value: 0.17946
	loss_reward: 0.01834
[2025-05-08 10:52:55] nn step 41700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08641
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99898
	loss_value_1: 0.01765
	loss_reward_1: 0.00116
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99805
	loss_value_2: 0.01742
	loss_reward_2: 0.00376
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99715
	loss_value_3: 0.01697
	loss_reward_3: 0.00328
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01641
	loss_reward_4: 0.00285
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.9982
	loss_value_5: 0.01686
	loss_reward_5: 0.00525
	loss_policy: 0.00203
	loss_value: 0.17172
	loss_reward: 0.01629
[2025-05-08 10:53:01] nn step 41750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07904
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01597
	loss_reward_1: 0.00054
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01575
	loss_reward_2: 0.00295
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99902
	loss_value_3: 0.0154
	loss_reward_3: 0.00248
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99832
	loss_value_4: 0.01487
	loss_reward_4: 0.00243
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99875
	loss_value_5: 0.0149
	loss_reward_5: 0.00431
	loss_policy: 0.00123
	loss_value: 0.15593
	loss_reward: 0.01271
[2025-05-08 10:53:09] nn step 41800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08418
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01707
	loss_reward_1: 0.00049
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01669
	loss_reward_2: 0.00314
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99762
	loss_value_3: 0.0163
	loss_reward_3: 0.00261
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.9973
	loss_value_4: 0.0157
	loss_reward_4: 0.00239
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99785
	loss_value_5: 0.01579
	loss_reward_5: 0.00455
	loss_policy: 0.00238
	loss_value: 0.16573
	loss_reward: 0.01318
Optimization_Done 41800
[2025-05-08 10:54:26] [command] train weight_iter_41800.pkl 191 210
[2025-05-08 10:54:35] nn step 41850, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.12033
	loss_policy_1: 0.0003
	accuracy_policy_1: 0.99746
	loss_value_1: 0.02362
	loss_reward_1: 0.00244
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99824
	loss_value_2: 0.02377
	loss_reward_2: 0.00435
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99793
	loss_value_3: 0.02397
	loss_reward_3: 0.00348
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99832
	loss_value_4: 0.02427
	loss_reward_4: 0.00373
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.99836
	loss_value_5: 0.0249
	loss_reward_5: 0.00551
	loss_policy: 0.00174
	loss_value: 0.24086
	loss_reward: 0.01951
[2025-05-08 10:54:43] nn step 41900, lr: 0.1.
	loss_policy_0: 3e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08895
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01791
	loss_reward_1: 0.00223
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.9977
	loss_value_2: 0.01794
	loss_reward_2: 0.00414
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99723
	loss_value_3: 0.01788
	loss_reward_3: 0.00382
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01744
	loss_reward_4: 0.00385
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99758
	loss_value_5: 0.01787
	loss_reward_5: 0.00575
	loss_policy: 0.00207
	loss_value: 0.17799
	loss_reward: 0.01979
[2025-05-08 10:54:52] nn step 41950, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09177
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01884
	loss_reward_1: 0.00167
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01896
	loss_reward_2: 0.00381
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99758
	loss_value_3: 0.01856
	loss_reward_3: 0.00364
	loss_policy_4: 0.00051
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01812
	loss_reward_4: 0.00409
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99777
	loss_value_5: 0.0186
	loss_reward_5: 0.00598
	loss_policy: 0.00206
	loss_value: 0.18485
	loss_reward: 0.01919
[2025-05-08 10:54:58] nn step 42000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0872
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01773
	loss_reward_1: 0.00094
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01773
	loss_reward_2: 0.00347
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01735
	loss_reward_3: 0.00301
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01703
	loss_reward_4: 0.00329
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01746
	loss_reward_5: 0.005
	loss_policy: 0.0021
	loss_value: 0.17449
	loss_reward: 0.01571
Optimization_Done 42000
[2025-05-08 10:56:19] [command] train weight_iter_42000.pkl 192 211
[2025-05-08 10:56:28] nn step 42050, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09095
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01839
	loss_reward_1: 0.0015
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01836
	loss_reward_2: 0.00364
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01803
	loss_reward_3: 0.00325
	loss_policy_4: 0.00068
	accuracy_policy_4: 0.99684
	loss_value_4: 0.01773
	loss_reward_4: 0.00379
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01808
	loss_reward_5: 0.00512
	loss_policy: 0.00225
	loss_value: 0.18155
	loss_reward: 0.01731
[2025-05-08 10:56:36] nn step 42100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08618
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01745
	loss_reward_1: 0.00076
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01735
	loss_reward_2: 0.00318
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99844
	loss_value_3: 0.01704
	loss_reward_3: 0.0027
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01683
	loss_reward_4: 0.00268
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99762
	loss_value_5: 0.01687
	loss_reward_5: 0.00447
	loss_policy: 0.00197
	loss_value: 0.17171
	loss_reward: 0.01378
[2025-05-08 10:56:44] nn step 42150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09019
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01809
	loss_reward_1: 0.00072
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01802
	loss_reward_2: 0.00312
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01766
	loss_reward_3: 0.0026
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99711
	loss_value_4: 0.01734
	loss_reward_4: 0.00234
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99766
	loss_value_5: 0.01759
	loss_reward_5: 0.00442
	loss_policy: 0.00187
	loss_value: 0.17889
	loss_reward: 0.01321
[2025-05-08 10:56:51] nn step 42200, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08959
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99879
	loss_value_1: 0.018
	loss_reward_1: 0.00059
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.9975
	loss_value_2: 0.01777
	loss_reward_2: 0.00291
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01748
	loss_reward_3: 0.00243
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99668
	loss_value_4: 0.0171
	loss_reward_4: 0.00257
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99727
	loss_value_5: 0.01724
	loss_reward_5: 0.00464
	loss_policy: 0.00262
	loss_value: 0.17717
	loss_reward: 0.01314
Optimization_Done 42200
[2025-05-08 10:58:10] [command] train weight_iter_42200.pkl 193 212
[2025-05-08 10:58:19] nn step 42250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08344
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01669
	loss_reward_1: 0.0006
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01648
	loss_reward_2: 0.00267
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99754
	loss_value_3: 0.0161
	loss_reward_3: 0.00206
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99648
	loss_value_4: 0.01573
	loss_reward_4: 0.00195
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.9966
	loss_value_5: 0.01589
	loss_reward_5: 0.00392
	loss_policy: 0.00236
	loss_value: 0.16434
	loss_reward: 0.01118
[2025-05-08 10:58:27] nn step 42300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.09027
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01797
	loss_reward_1: 0.00058
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01771
	loss_reward_2: 0.00269
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99805
	loss_value_3: 0.01735
	loss_reward_3: 0.00194
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01691
	loss_reward_4: 0.00199
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01706
	loss_reward_5: 0.00413
	loss_policy: 0.00202
	loss_value: 0.17727
	loss_reward: 0.01132
[2025-05-08 10:58:35] nn step 42350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0829
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01657
	loss_reward_1: 0.00048
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01621
	loss_reward_2: 0.00255
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01596
	loss_reward_3: 0.00185
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99684
	loss_value_4: 0.01539
	loss_reward_4: 0.00187
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99734
	loss_value_5: 0.01544
	loss_reward_5: 0.00392
	loss_policy: 0.00193
	loss_value: 0.16247
	loss_reward: 0.01066
[2025-05-08 10:58:41] nn step 42400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08555
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01703
	loss_reward_1: 0.0005
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99875
	loss_value_2: 0.01665
	loss_reward_2: 0.00265
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01631
	loss_reward_3: 0.00193
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01571
	loss_reward_4: 0.00194
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99711
	loss_value_5: 0.01579
	loss_reward_5: 0.00411
	loss_policy: 0.00221
	loss_value: 0.16705
	loss_reward: 0.01113
Optimization_Done 42400
[2025-05-08 11:00:02] [command] train weight_iter_42400.pkl 194 213
[2025-05-08 11:00:09] nn step 42450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08731
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01738
	loss_reward_1: 0.00054
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99875
	loss_value_2: 0.017
	loss_reward_2: 0.00269
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01665
	loss_reward_3: 0.00191
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01607
	loss_reward_4: 0.00196
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99695
	loss_value_5: 0.01612
	loss_reward_5: 0.00416
	loss_policy: 0.00206
	loss_value: 0.17052
	loss_reward: 0.01126
[2025-05-08 11:00:18] nn step 42500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08948
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01788
	loss_reward_1: 0.0005
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99906
	loss_value_2: 0.01738
	loss_reward_2: 0.00278
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99859
	loss_value_3: 0.01708
	loss_reward_3: 0.00201
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99781
	loss_value_4: 0.01639
	loss_reward_4: 0.00194
	loss_policy_5: 0.00063
	accuracy_policy_5: 0.99727
	loss_value_5: 0.01654
	loss_reward_5: 0.00426
	loss_policy: 0.00203
	loss_value: 0.17474
	loss_reward: 0.01149
[2025-05-08 11:00:25] nn step 42550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0837
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99934
	loss_value_1: 0.01666
	loss_reward_1: 0.00052
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01625
	loss_reward_2: 0.00262
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99844
	loss_value_3: 0.01591
	loss_reward_3: 0.00184
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01533
	loss_reward_4: 0.00193
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01535
	loss_reward_5: 0.004
	loss_policy: 0.00187
	loss_value: 0.1632
	loss_reward: 0.01091
[2025-05-08 11:00:32] nn step 42600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08256
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01643
	loss_reward_1: 0.00045
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01594
	loss_reward_2: 0.00265
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01569
	loss_reward_3: 0.00185
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01509
	loss_reward_4: 0.00181
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99707
	loss_value_5: 0.01516
	loss_reward_5: 0.0041
	loss_policy: 0.00228
	loss_value: 0.16086
	loss_reward: 0.01086
Optimization_Done 42600
[2025-05-08 11:01:54] [command] train weight_iter_42600.pkl 195 214
[2025-05-08 11:02:02] nn step 42650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08355
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01669
	loss_reward_1: 0.00053
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01634
	loss_reward_2: 0.00266
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99742
	loss_value_3: 0.01605
	loss_reward_3: 0.0018
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99582
	loss_value_4: 0.01536
	loss_reward_4: 0.00196
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.99621
	loss_value_5: 0.01545
	loss_reward_5: 0.00409
	loss_policy: 0.00254
	loss_value: 0.16343
	loss_reward: 0.01104
[2025-05-08 11:02:10] nn step 42700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08069
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01607
	loss_reward_1: 0.00044
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01565
	loss_reward_2: 0.00256
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99781
	loss_value_3: 0.01544
	loss_reward_3: 0.00181
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99711
	loss_value_4: 0.01482
	loss_reward_4: 0.00183
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.9966
	loss_value_5: 0.01488
	loss_reward_5: 0.00396
	loss_policy: 0.00208
	loss_value: 0.15756
	loss_reward: 0.01061
[2025-05-08 11:02:18] nn step 42750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08284
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01644
	loss_reward_1: 0.00047
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01604
	loss_reward_2: 0.00263
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99801
	loss_value_3: 0.0158
	loss_reward_3: 0.00179
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99727
	loss_value_4: 0.01517
	loss_reward_4: 0.00189
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01526
	loss_reward_5: 0.00408
	loss_policy: 0.00235
	loss_value: 0.16155
	loss_reward: 0.01085
[2025-05-08 11:02:24] nn step 42800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08681
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01732
	loss_reward_1: 0.00042
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01679
	loss_reward_2: 0.00276
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01653
	loss_reward_3: 0.002
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01571
	loss_reward_4: 0.00197
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.99727
	loss_value_5: 0.01579
	loss_reward_5: 0.0043
	loss_policy: 0.00168
	loss_value: 0.16896
	loss_reward: 0.01144
Optimization_Done 42800
[2025-05-08 11:03:47] [command] train weight_iter_42800.pkl 196 215
[2025-05-08 11:03:57] nn step 42850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08896
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01784
	loss_reward_1: 0.00058
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01749
	loss_reward_2: 0.00311
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01725
	loss_reward_3: 0.00243
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99715
	loss_value_4: 0.01664
	loss_reward_4: 0.00232
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99699
	loss_value_5: 0.01686
	loss_reward_5: 0.00471
	loss_policy: 0.00214
	loss_value: 0.17504
	loss_reward: 0.01315
[2025-05-08 11:04:05] nn step 42900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08502
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01694
	loss_reward_1: 0.00049
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01655
	loss_reward_2: 0.00273
	loss_policy_3: 0.00047
	accuracy_policy_3: 0.99801
	loss_value_3: 0.0163
	loss_reward_3: 0.00204
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.9975
	loss_value_4: 0.01563
	loss_reward_4: 0.00203
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99699
	loss_value_5: 0.01579
	loss_reward_5: 0.00424
	loss_policy: 0.00201
	loss_value: 0.16623
	loss_reward: 0.01153
[2025-05-08 11:04:13] nn step 42950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08436
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01688
	loss_reward_1: 0.00045
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01647
	loss_reward_2: 0.00267
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01615
	loss_reward_3: 0.00192
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01556
	loss_reward_4: 0.00191
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01563
	loss_reward_5: 0.00416
	loss_policy: 0.00252
	loss_value: 0.16505
	loss_reward: 0.01111
[2025-05-08 11:04:19] nn step 43000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08101
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99891
	loss_value_1: 0.01616
	loss_reward_1: 0.00042
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99801
	loss_value_2: 0.01565
	loss_reward_2: 0.00258
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99672
	loss_value_3: 0.01541
	loss_reward_3: 0.0019
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99637
	loss_value_4: 0.01474
	loss_reward_4: 0.00183
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99703
	loss_value_5: 0.01482
	loss_reward_5: 0.00399
	loss_policy: 0.00229
	loss_value: 0.1578
	loss_reward: 0.01073
Optimization_Done 43000
[2025-05-08 11:05:41] [command] train weight_iter_43000.pkl 197 216
[2025-05-08 11:05:48] nn step 43050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08157
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01623
	loss_reward_1: 0.00044
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01582
	loss_reward_2: 0.00257
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01553
	loss_reward_3: 0.00179
	loss_policy_4: 0.00057
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01489
	loss_reward_4: 0.00186
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01498
	loss_reward_5: 0.00396
	loss_policy: 0.00222
	loss_value: 0.15904
	loss_reward: 0.01062
[2025-05-08 11:05:57] nn step 43100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08282
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99957
	loss_value_1: 0.0165
	loss_reward_1: 0.00041
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01604
	loss_reward_2: 0.00265
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01584
	loss_reward_3: 0.00185
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99734
	loss_value_4: 0.01509
	loss_reward_4: 0.00188
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01519
	loss_reward_5: 0.00416
	loss_policy: 0.00209
	loss_value: 0.16149
	loss_reward: 0.01096
[2025-05-08 11:06:05] nn step 43150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08622
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01718
	loss_reward_1: 0.00045
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01664
	loss_reward_2: 0.00277
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99867
	loss_value_3: 0.01635
	loss_reward_3: 0.002
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01557
	loss_reward_4: 0.00209
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99723
	loss_value_5: 0.01563
	loss_reward_5: 0.00438
	loss_policy: 0.00165
	loss_value: 0.16759
	loss_reward: 0.01169
[2025-05-08 11:06:11] nn step 43200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08422
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99957
	loss_value_1: 0.0168
	loss_reward_1: 0.00042
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99875
	loss_value_2: 0.01627
	loss_reward_2: 0.00275
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01597
	loss_reward_3: 0.00191
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01525
	loss_reward_4: 0.00196
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99695
	loss_value_5: 0.01544
	loss_reward_5: 0.00422
	loss_policy: 0.00224
	loss_value: 0.16395
	loss_reward: 0.01126
Optimization_Done 43200
[2025-05-08 11:07:32] [command] train weight_iter_43200.pkl 198 217
[2025-05-08 11:07:39] nn step 43250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0859
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01716
	loss_reward_1: 0.00047
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01654
	loss_reward_2: 0.00272
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99844
	loss_value_3: 0.0163
	loss_reward_3: 0.00197
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99785
	loss_value_4: 0.0156
	loss_reward_4: 0.00202
	loss_policy_5: 0.00054
	accuracy_policy_5: 0.9973
	loss_value_5: 0.01568
	loss_reward_5: 0.00432
	loss_policy: 0.00169
	loss_value: 0.16718
	loss_reward: 0.01149
[2025-05-08 11:07:47] nn step 43300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07889
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99945
	loss_value_1: 0.0157
	loss_reward_1: 0.00042
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99914
	loss_value_2: 0.01518
	loss_reward_2: 0.00252
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01495
	loss_reward_3: 0.00179
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01428
	loss_reward_4: 0.00185
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99715
	loss_value_5: 0.01434
	loss_reward_5: 0.00391
	loss_policy: 0.00167
	loss_value: 0.15335
	loss_reward: 0.01049
[2025-05-08 11:07:55] nn step 43350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08422
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01682
	loss_reward_1: 0.00043
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01638
	loss_reward_2: 0.00274
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01605
	loss_reward_3: 0.00196
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99703
	loss_value_4: 0.01528
	loss_reward_4: 0.00207
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99645
	loss_value_5: 0.01538
	loss_reward_5: 0.00424
	loss_policy: 0.00236
	loss_value: 0.16412
	loss_reward: 0.01145
[2025-05-08 11:08:01] nn step 43400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08181
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01628
	loss_reward_1: 0.00043
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01574
	loss_reward_2: 0.00279
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01547
	loss_reward_3: 0.00189
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99762
	loss_value_4: 0.01478
	loss_reward_4: 0.00192
	loss_policy_5: 0.00055
	accuracy_policy_5: 0.99723
	loss_value_5: 0.01491
	loss_reward_5: 0.00432
	loss_policy: 0.0019
	loss_value: 0.15899
	loss_reward: 0.01135
Optimization_Done 43400
[2025-05-08 11:09:22] [command] train weight_iter_43400.pkl 199 218
[2025-05-08 11:09:29] nn step 43450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08597
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01698
	loss_reward_1: 0.00047
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01642
	loss_reward_2: 0.00279
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99801
	loss_value_3: 0.0162
	loss_reward_3: 0.00207
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01539
	loss_reward_4: 0.00205
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.9959
	loss_value_5: 0.01549
	loss_reward_5: 0.00437
	loss_policy: 0.00226
	loss_value: 0.16645
	loss_reward: 0.01174
[2025-05-08 11:09:37] nn step 43500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08063
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01612
	loss_reward_1: 0.00054
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01569
	loss_reward_2: 0.00289
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01531
	loss_reward_3: 0.00209
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01467
	loss_reward_4: 0.00205
	loss_policy_5: 0.00065
	accuracy_policy_5: 0.99676
	loss_value_5: 0.01484
	loss_reward_5: 0.00438
	loss_policy: 0.0021
	loss_value: 0.15726
	loss_reward: 0.01195
[2025-05-08 11:09:45] nn step 43550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07877
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01571
	loss_reward_1: 0.00038
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01522
	loss_reward_2: 0.00258
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99805
	loss_value_3: 0.01489
	loss_reward_3: 0.00198
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99707
	loss_value_4: 0.01414
	loss_reward_4: 0.00198
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01427
	loss_reward_5: 0.00412
	loss_policy: 0.00225
	loss_value: 0.153
	loss_reward: 0.01104
[2025-05-08 11:09:54] nn step 43600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08424
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01678
	loss_reward_1: 0.00043
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.9991
	loss_value_2: 0.01613
	loss_reward_2: 0.00286
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.9984
	loss_value_3: 0.01581
	loss_reward_3: 0.002
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01501
	loss_reward_4: 0.00202
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.9968
	loss_value_5: 0.01519
	loss_reward_5: 0.00439
	loss_policy: 0.002
	loss_value: 0.16316
	loss_reward: 0.0117
Optimization_Done 43600
[2025-05-08 11:11:15] [command] train weight_iter_43600.pkl 200 219
[2025-05-08 11:11:22] nn step 43650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08043
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01599
	loss_reward_1: 0.00047
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01541
	loss_reward_2: 0.00264
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01522
	loss_reward_3: 0.00182
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01444
	loss_reward_4: 0.00196
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99684
	loss_value_5: 0.01453
	loss_reward_5: 0.00417
	loss_policy: 0.00205
	loss_value: 0.15602
	loss_reward: 0.01106
[2025-05-08 11:11:31] nn step 43700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08022
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01597
	loss_reward_1: 0.00038
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99906
	loss_value_2: 0.0154
	loss_reward_2: 0.0027
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99809
	loss_value_3: 0.0151
	loss_reward_3: 0.00185
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99734
	loss_value_4: 0.01439
	loss_reward_4: 0.00186
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99656
	loss_value_5: 0.01454
	loss_reward_5: 0.00419
	loss_policy: 0.00197
	loss_value: 0.15562
	loss_reward: 0.01098
[2025-05-08 11:11:38] nn step 43750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0798
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01589
	loss_reward_1: 0.0004
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01532
	loss_reward_2: 0.00261
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01507
	loss_reward_3: 0.00192
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99762
	loss_value_4: 0.01432
	loss_reward_4: 0.0019
	loss_policy_5: 0.00053
	accuracy_policy_5: 0.99719
	loss_value_5: 0.01442
	loss_reward_5: 0.00415
	loss_policy: 0.00176
	loss_value: 0.15483
	loss_reward: 0.01098
[2025-05-08 11:11:46] nn step 43800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.083
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01643
	loss_reward_1: 0.00041
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01587
	loss_reward_2: 0.00276
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99844
	loss_value_3: 0.01555
	loss_reward_3: 0.00198
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01484
	loss_reward_4: 0.00195
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.99715
	loss_value_5: 0.015
	loss_reward_5: 0.00436
	loss_policy: 0.00215
	loss_value: 0.16069
	loss_reward: 0.01146
Optimization_Done 43800
[2025-05-08 11:13:04] [command] train weight_iter_43800.pkl 201 220
[2025-05-08 11:13:11] nn step 43850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07749
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01546
	loss_reward_1: 0.0004
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01488
	loss_reward_2: 0.00263
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01459
	loss_reward_3: 0.00182
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01392
	loss_reward_4: 0.00185
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99656
	loss_value_5: 0.01405
	loss_reward_5: 0.00408
	loss_policy: 0.00202
	loss_value: 0.15039
	loss_reward: 0.01078
[2025-05-08 11:13:19] nn step 43900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07165
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01419
	loss_reward_1: 0.00037
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01373
	loss_reward_2: 0.00244
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01342
	loss_reward_3: 0.00169
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01276
	loss_reward_4: 0.00176
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99676
	loss_value_5: 0.01285
	loss_reward_5: 0.00381
	loss_policy: 0.00153
	loss_value: 0.13859
	loss_reward: 0.01007
[2025-05-08 11:13:27] nn step 43950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07877
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01559
	loss_reward_1: 0.00041
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01502
	loss_reward_2: 0.00267
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99867
	loss_value_3: 0.01472
	loss_reward_3: 0.00192
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01399
	loss_reward_4: 0.00197
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99738
	loss_value_5: 0.01416
	loss_reward_5: 0.00422
	loss_policy: 0.00147
	loss_value: 0.15224
	loss_reward: 0.01119
[2025-05-08 11:13:36] nn step 44000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07465
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99957
	loss_value_1: 0.0148
	loss_reward_1: 0.00037
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01422
	loss_reward_2: 0.00258
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01392
	loss_reward_3: 0.00181
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99824
	loss_value_4: 0.01315
	loss_reward_4: 0.00182
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99734
	loss_value_5: 0.01329
	loss_reward_5: 0.004
	loss_policy: 0.00127
	loss_value: 0.14402
	loss_reward: 0.01057
Optimization_Done 44000
[2025-05-08 11:14:54] [command] train weight_iter_44000.pkl 202 221
[2025-05-08 11:15:03] nn step 44050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08142
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.0162
	loss_reward_1: 0.00043
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99922
	loss_value_2: 0.0155
	loss_reward_2: 0.00279
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99859
	loss_value_3: 0.01508
	loss_reward_3: 0.00206
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99785
	loss_value_4: 0.01428
	loss_reward_4: 0.00201
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99715
	loss_value_5: 0.01442
	loss_reward_5: 0.00427
	loss_policy: 0.00168
	loss_value: 0.1569
	loss_reward: 0.01157
[2025-05-08 11:15:10] nn step 44100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0809
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01611
	loss_reward_1: 0.00043
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.9991
	loss_value_2: 0.01547
	loss_reward_2: 0.00275
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01507
	loss_reward_3: 0.00195
	loss_policy_4: 0.00043
	accuracy_policy_4: 0.9977
	loss_value_4: 0.01434
	loss_reward_4: 0.00192
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99723
	loss_value_5: 0.01449
	loss_reward_5: 0.00434
	loss_policy: 0.0015
	loss_value: 0.15638
	loss_reward: 0.0114
[2025-05-08 11:15:19] nn step 44150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08433
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01672
	loss_reward_1: 0.00045
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01612
	loss_reward_2: 0.00286
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01563
	loss_reward_3: 0.00207
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01479
	loss_reward_4: 0.00208
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.9973
	loss_value_5: 0.01493
	loss_reward_5: 0.0045
	loss_policy: 0.00139
	loss_value: 0.16252
	loss_reward: 0.01197
[2025-05-08 11:15:27] nn step 44200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08116
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01605
	loss_reward_1: 0.00039
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01541
	loss_reward_2: 0.00278
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01509
	loss_reward_3: 0.00191
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01426
	loss_reward_4: 0.00201
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.9973
	loss_value_5: 0.01448
	loss_reward_5: 0.00438
	loss_policy: 0.00144
	loss_value: 0.15646
	loss_reward: 0.01148
Optimization_Done 44200
[2025-05-08 11:16:35] [command] train weight_iter_44200.pkl 203 222
[2025-05-08 11:16:44] nn step 44250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07806
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01541
	loss_reward_1: 0.0004
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01489
	loss_reward_2: 0.0026
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01457
	loss_reward_3: 0.00192
	loss_policy_4: 0.00051
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01382
	loss_reward_4: 0.00192
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99695
	loss_value_5: 0.01403
	loss_reward_5: 0.00402
	loss_policy: 0.00175
	loss_value: 0.15079
	loss_reward: 0.01086
[2025-05-08 11:16:53] nn step 44300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08145
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01613
	loss_reward_1: 0.00041
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01555
	loss_reward_2: 0.00276
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99844
	loss_value_3: 0.0152
	loss_reward_3: 0.00196
	loss_policy_4: 0.00051
	accuracy_policy_4: 0.99727
	loss_value_4: 0.01442
	loss_reward_4: 0.0019
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99676
	loss_value_5: 0.01456
	loss_reward_5: 0.00436
	loss_policy: 0.00172
	loss_value: 0.15732
	loss_reward: 0.0114
[2025-05-08 11:16:59] nn step 44350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07612
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01506
	loss_reward_1: 0.00036
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01445
	loss_reward_2: 0.00256
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99863
	loss_value_3: 0.0141
	loss_reward_3: 0.00183
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99809
	loss_value_4: 0.0133
	loss_reward_4: 0.00185
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99777
	loss_value_5: 0.01348
	loss_reward_5: 0.00405
	loss_policy: 0.00119
	loss_value: 0.14652
	loss_reward: 0.01065
[2025-05-08 11:17:08] nn step 44400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07755
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01529
	loss_reward_1: 0.00038
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01472
	loss_reward_2: 0.00268
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01432
	loss_reward_3: 0.00191
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.9982
	loss_value_4: 0.01354
	loss_reward_4: 0.00186
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01377
	loss_reward_5: 0.0042
	loss_policy: 0.00132
	loss_value: 0.1492
	loss_reward: 0.01103
Optimization_Done 44400
[2025-05-08 11:18:23] [command] train weight_iter_44400.pkl 204 223
[2025-05-08 11:18:31] nn step 44450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07852
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01556
	loss_reward_1: 0.00046
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01499
	loss_reward_2: 0.00261
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01468
	loss_reward_3: 0.00191
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01393
	loss_reward_4: 0.00193
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.99719
	loss_value_5: 0.01408
	loss_reward_5: 0.0041
	loss_policy: 0.00139
	loss_value: 0.15177
	loss_reward: 0.011
[2025-05-08 11:18:39] nn step 44500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07587
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01499
	loss_reward_1: 0.0004
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01446
	loss_reward_2: 0.00259
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01413
	loss_reward_3: 0.00183
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99832
	loss_value_4: 0.01331
	loss_reward_4: 0.00186
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01346
	loss_reward_5: 0.00407
	loss_policy: 0.00112
	loss_value: 0.14622
	loss_reward: 0.01075
[2025-05-08 11:18:48] nn step 44550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07896
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01569
	loss_reward_1: 0.00037
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99918
	loss_value_2: 0.0151
	loss_reward_2: 0.00271
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01477
	loss_reward_3: 0.00189
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01386
	loss_reward_4: 0.00194
	loss_policy_5: 0.00051
	accuracy_policy_5: 0.9973
	loss_value_5: 0.01398
	loss_reward_5: 0.00421
	loss_policy: 0.00131
	loss_value: 0.15235
	loss_reward: 0.01112
[2025-05-08 11:18:54] nn step 44600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07335
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01456
	loss_reward_1: 0.00035
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01408
	loss_reward_2: 0.00246
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01368
	loss_reward_3: 0.00182
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.9982
	loss_value_4: 0.01287
	loss_reward_4: 0.0018
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99773
	loss_value_5: 0.01299
	loss_reward_5: 0.00388
	loss_policy: 0.00114
	loss_value: 0.14153
	loss_reward: 0.01031
Optimization_Done 44600
[2025-05-08 11:20:05] [command] train weight_iter_44600.pkl 205 224
[2025-05-08 11:20:14] nn step 44650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0798
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01579
	loss_reward_1: 0.00042
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.9991
	loss_value_2: 0.01523
	loss_reward_2: 0.00268
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01487
	loss_reward_3: 0.0019
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99797
	loss_value_4: 0.01411
	loss_reward_4: 0.002
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.9973
	loss_value_5: 0.01421
	loss_reward_5: 0.00431
	loss_policy: 0.00151
	loss_value: 0.15401
	loss_reward: 0.01131
[2025-05-08 11:20:20] nn step 44700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0742
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.0147
	loss_reward_1: 0.00036
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.9993
	loss_value_2: 0.0141
	loss_reward_2: 0.00253
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99875
	loss_value_3: 0.01376
	loss_reward_3: 0.00187
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01301
	loss_reward_4: 0.00185
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01313
	loss_reward_5: 0.00403
	loss_policy: 0.00119
	loss_value: 0.1429
	loss_reward: 0.01063
[2025-05-08 11:20:28] nn step 44750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07896
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01558
	loss_reward_1: 0.00039
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01503
	loss_reward_2: 0.0027
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01462
	loss_reward_3: 0.00189
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01381
	loss_reward_4: 0.00195
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99805
	loss_value_5: 0.01403
	loss_reward_5: 0.0043
	loss_policy: 0.00116
	loss_value: 0.15203
	loss_reward: 0.01123
[2025-05-08 11:20:36] nn step 44800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0736
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01448
	loss_reward_1: 0.00038
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01397
	loss_reward_2: 0.0025
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99906
	loss_value_3: 0.01361
	loss_reward_3: 0.00183
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99859
	loss_value_4: 0.01282
	loss_reward_4: 0.00189
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01297
	loss_reward_5: 0.00401
	loss_policy: 0.00101
	loss_value: 0.14145
	loss_reward: 0.0106
Optimization_Done 44800
[2025-05-08 11:21:55] [command] train weight_iter_44800.pkl 206 225
[2025-05-08 11:22:04] nn step 44850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07662
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01514
	loss_reward_1: 0.0004
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01462
	loss_reward_2: 0.00257
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01421
	loss_reward_3: 0.00189
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99824
	loss_value_4: 0.01339
	loss_reward_4: 0.00196
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99754
	loss_value_5: 0.01347
	loss_reward_5: 0.00416
	loss_policy: 0.00131
	loss_value: 0.14745
	loss_reward: 0.01098
[2025-05-08 11:22:10] nn step 44900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07956
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01572
	loss_reward_1: 0.00042
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99941
	loss_value_2: 0.0151
	loss_reward_2: 0.00275
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99891
	loss_value_3: 0.0147
	loss_reward_3: 0.00189
	loss_policy_4: 0.00043
	accuracy_policy_4: 0.99812
	loss_value_4: 0.0139
	loss_reward_4: 0.00201
	loss_policy_5: 0.00056
	accuracy_policy_5: 0.99754
	loss_value_5: 0.0141
	loss_reward_5: 0.00437
	loss_policy: 0.00147
	loss_value: 0.15308
	loss_reward: 0.01145
[2025-05-08 11:22:18] nn step 44950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07512
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01487
	loss_reward_1: 0.00035
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01437
	loss_reward_2: 0.00258
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99895
	loss_value_3: 0.01394
	loss_reward_3: 0.00184
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01314
	loss_reward_4: 0.00189
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99793
	loss_value_5: 0.01326
	loss_reward_5: 0.00413
	loss_policy: 0.00127
	loss_value: 0.14469
	loss_reward: 0.0108
[2025-05-08 11:22:26] nn step 45000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07803
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0155
	loss_reward_1: 0.00035
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01487
	loss_reward_2: 0.00273
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99898
	loss_value_3: 0.0144
	loss_reward_3: 0.00195
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01356
	loss_reward_4: 0.00192
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01372
	loss_reward_5: 0.00427
	loss_policy: 0.00124
	loss_value: 0.15007
	loss_reward: 0.01122
Optimization_Done 45000
[2025-05-08 11:23:45] [command] train weight_iter_45000.pkl 207 226
[2025-05-08 11:23:54] nn step 45050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08032
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01579
	loss_reward_1: 0.00043
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01514
	loss_reward_2: 0.00276
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01472
	loss_reward_3: 0.00196
	loss_policy_4: 0.0003
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01387
	loss_reward_4: 0.00199
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99793
	loss_value_5: 0.01394
	loss_reward_5: 0.00436
	loss_policy: 0.00111
	loss_value: 0.15378
	loss_reward: 0.0115
[2025-05-08 11:24:00] nn step 45100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07234
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01433
	loss_reward_1: 0.00038
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99922
	loss_value_2: 0.01379
	loss_reward_2: 0.00254
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99895
	loss_value_3: 0.0134
	loss_reward_3: 0.00176
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99836
	loss_value_4: 0.01266
	loss_reward_4: 0.00184
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01281
	loss_reward_5: 0.00398
	loss_policy: 0.00122
	loss_value: 0.13931
	loss_reward: 0.0105
[2025-05-08 11:24:08] nn step 45150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07741
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.0153
	loss_reward_1: 0.00035
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01469
	loss_reward_2: 0.00268
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01425
	loss_reward_3: 0.00193
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99844
	loss_value_4: 0.01342
	loss_reward_4: 0.00188
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01355
	loss_reward_5: 0.00426
	loss_policy: 0.00099
	loss_value: 0.14862
	loss_reward: 0.01109
[2025-05-08 11:24:16] nn step 45200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07628
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01514
	loss_reward_1: 0.00035
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01456
	loss_reward_2: 0.00266
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01416
	loss_reward_3: 0.0019
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01327
	loss_reward_4: 0.00191
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01347
	loss_reward_5: 0.00418
	loss_policy: 0.00077
	loss_value: 0.14688
	loss_reward: 0.011
Optimization_Done 45200
[2025-05-08 11:25:39] [command] train weight_iter_45200.pkl 208 227
[2025-05-08 11:25:48] nn step 45250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07272
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01437
	loss_reward_1: 0.00036
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99926
	loss_value_2: 0.0138
	loss_reward_2: 0.00247
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99879
	loss_value_3: 0.01343
	loss_reward_3: 0.00177
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99812
	loss_value_4: 0.0127
	loss_reward_4: 0.00174
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99754
	loss_value_5: 0.01282
	loss_reward_5: 0.00395
	loss_policy: 0.0013
	loss_value: 0.13985
	loss_reward: 0.0103
[2025-05-08 11:25:54] nn step 45300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07941
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01572
	loss_reward_1: 0.00036
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01511
	loss_reward_2: 0.00279
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99906
	loss_value_3: 0.01466
	loss_reward_3: 0.002
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99844
	loss_value_4: 0.01379
	loss_reward_4: 0.00199
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99785
	loss_value_5: 0.01399
	loss_reward_5: 0.00436
	loss_policy: 0.00101
	loss_value: 0.15269
	loss_reward: 0.01149
[2025-05-08 11:26:02] nn step 45350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08055
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01595
	loss_reward_1: 0.00034
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99941
	loss_value_2: 0.0153
	loss_reward_2: 0.0028
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01487
	loss_reward_3: 0.00202
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99824
	loss_value_4: 0.01398
	loss_reward_4: 0.00201
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99773
	loss_value_5: 0.01413
	loss_reward_5: 0.0045
	loss_policy: 0.00125
	loss_value: 0.15478
	loss_reward: 0.01167
[2025-05-08 11:26:10] nn step 45400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08298
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01695
	loss_reward_1: 0.00111
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01635
	loss_reward_2: 0.00395
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01574
	loss_reward_3: 0.0028
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01504
	loss_reward_4: 0.00289
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01541
	loss_reward_5: 0.00531
	loss_policy: 0.0013
	loss_value: 0.16248
	loss_reward: 0.01606
Optimization_Done 45400
[2025-05-08 11:27:30] [command] train weight_iter_45400.pkl 209 228
[2025-05-08 11:27:39] nn step 45450, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.08549
	loss_policy_1: 0.00014
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01702
	loss_reward_1: 0.00099
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01676
	loss_reward_2: 0.00367
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01627
	loss_reward_3: 0.00292
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01573
	loss_reward_4: 0.00301
	loss_policy_5: 0.00063
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01609
	loss_reward_5: 0.00537
	loss_policy: 0.00193
	loss_value: 0.16738
	loss_reward: 0.01597
[2025-05-08 11:27:46] nn step 45500, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07798
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01554
	loss_reward_1: 0.00053
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99941
	loss_value_2: 0.0151
	loss_reward_2: 0.0031
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99879
	loss_value_3: 0.01471
	loss_reward_3: 0.00228
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99805
	loss_value_4: 0.014
	loss_reward_4: 0.00221
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99801
	loss_value_5: 0.01439
	loss_reward_5: 0.00459
	loss_policy: 0.00191
	loss_value: 0.15172
	loss_reward: 0.0127
[2025-05-08 11:27:54] nn step 45550, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07423
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01468
	loss_reward_1: 0.00038
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01429
	loss_reward_2: 0.00273
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99879
	loss_value_3: 0.01401
	loss_reward_3: 0.00203
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01328
	loss_reward_4: 0.00209
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01358
	loss_reward_5: 0.00429
	loss_policy: 0.00191
	loss_value: 0.14406
	loss_reward: 0.01151
[2025-05-08 11:28:02] nn step 45600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07982
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01576
	loss_reward_1: 0.0004
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01521
	loss_reward_2: 0.00277
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01478
	loss_reward_3: 0.00197
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99852
	loss_value_4: 0.01402
	loss_reward_4: 0.00199
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01419
	loss_reward_5: 0.00436
	loss_policy: 0.00113
	loss_value: 0.15379
	loss_reward: 0.01148
Optimization_Done 45600
[2025-05-08 11:29:24] [command] train weight_iter_45600.pkl 210 229
[2025-05-08 11:29:33] nn step 45650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07682
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01522
	loss_reward_1: 0.00043
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01472
	loss_reward_2: 0.0026
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01434
	loss_reward_3: 0.00191
	loss_policy_4: 0.0005
	accuracy_policy_4: 0.99789
	loss_value_4: 0.01353
	loss_reward_4: 0.00199
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99816
	loss_value_5: 0.01366
	loss_reward_5: 0.00417
	loss_policy: 0.00177
	loss_value: 0.14829
	loss_reward: 0.0111
[2025-05-08 11:29:39] nn step 45700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08314
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01647
	loss_reward_1: 0.00042
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99902
	loss_value_2: 0.01591
	loss_reward_2: 0.00278
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01548
	loss_reward_3: 0.00206
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01454
	loss_reward_4: 0.00205
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01474
	loss_reward_5: 0.0045
	loss_policy: 0.00174
	loss_value: 0.16028
	loss_reward: 0.0118
[2025-05-08 11:29:47] nn step 45750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07788
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01541
	loss_reward_1: 0.00037
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99918
	loss_value_2: 0.0148
	loss_reward_2: 0.0027
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01444
	loss_reward_3: 0.00193
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01368
	loss_reward_4: 0.00189
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01385
	loss_reward_5: 0.00421
	loss_policy: 0.0015
	loss_value: 0.15005
	loss_reward: 0.0111
[2025-05-08 11:29:56] nn step 45800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07986
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01584
	loss_reward_1: 0.0004
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01522
	loss_reward_2: 0.00278
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99922
	loss_value_3: 0.0148
	loss_reward_3: 0.002
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01397
	loss_reward_4: 0.002
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01416
	loss_reward_5: 0.00433
	loss_policy: 0.00096
	loss_value: 0.15386
	loss_reward: 0.01151
Optimization_Done 45800
[2025-05-08 11:31:15] [command] train weight_iter_45800.pkl 211 230
[2025-05-08 11:31:24] nn step 45850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07686
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01502
	loss_reward_1: 0.00043
	loss_policy_2: 0.00018
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01444
	loss_reward_2: 0.0026
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99891
	loss_value_3: 0.01403
	loss_reward_3: 0.00195
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99848
	loss_value_4: 0.01328
	loss_reward_4: 0.00188
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.99789
	loss_value_5: 0.01347
	loss_reward_5: 0.00407
	loss_policy: 0.00131
	loss_value: 0.14711
	loss_reward: 0.01093
[2025-05-08 11:31:30] nn step 45900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07817
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01543
	loss_reward_1: 0.00042
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01483
	loss_reward_2: 0.00265
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99867
	loss_value_3: 0.01448
	loss_reward_3: 0.00194
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01365
	loss_reward_4: 0.002
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99785
	loss_value_5: 0.01382
	loss_reward_5: 0.00426
	loss_policy: 0.00153
	loss_value: 0.15038
	loss_reward: 0.01127
[2025-05-08 11:31:39] nn step 45950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07832
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01548
	loss_reward_1: 0.0004
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01482
	loss_reward_2: 0.00275
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99895
	loss_value_3: 0.01443
	loss_reward_3: 0.00192
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01362
	loss_reward_4: 0.00196
	loss_policy_5: 0.00048
	accuracy_policy_5: 0.9977
	loss_value_5: 0.01378
	loss_reward_5: 0.0043
	loss_policy: 0.00129
	loss_value: 0.15045
	loss_reward: 0.01134
[2025-05-08 11:31:47] nn step 46000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07555
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99973
	loss_value_1: 0.01492
	loss_reward_1: 0.00034
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01424
	loss_reward_2: 0.00261
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01385
	loss_reward_3: 0.00189
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01305
	loss_reward_4: 0.00187
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01317
	loss_reward_5: 0.00411
	loss_policy: 0.00133
	loss_value: 0.14478
	loss_reward: 0.01082
Optimization_Done 46000
[2025-05-08 11:33:08] [command] train weight_iter_46000.pkl 212 231
[2025-05-08 11:33:17] nn step 46050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0759
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99965
	loss_value_1: 0.015
	loss_reward_1: 0.00038
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.9993
	loss_value_2: 0.01434
	loss_reward_2: 0.00266
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99863
	loss_value_3: 0.01395
	loss_reward_3: 0.00188
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.9982
	loss_value_4: 0.01319
	loss_reward_4: 0.00194
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99781
	loss_value_5: 0.0133
	loss_reward_5: 0.00418
	loss_policy: 0.00159
	loss_value: 0.14569
	loss_reward: 0.01105
[2025-05-08 11:33:24] nn step 46100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08076
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01599
	loss_reward_1: 0.00042
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01531
	loss_reward_2: 0.00274
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99895
	loss_value_3: 0.01492
	loss_reward_3: 0.00201
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01406
	loss_reward_4: 0.00198
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99793
	loss_value_5: 0.01419
	loss_reward_5: 0.0044
	loss_policy: 0.00132
	loss_value: 0.15523
	loss_reward: 0.01155
[2025-05-08 11:33:32] nn step 46150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.08041
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01588
	loss_reward_1: 0.00038
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01522
	loss_reward_2: 0.00281
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01478
	loss_reward_3: 0.00199
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01392
	loss_reward_4: 0.00201
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01404
	loss_reward_5: 0.00447
	loss_policy: 0.00103
	loss_value: 0.15425
	loss_reward: 0.01166
[2025-05-08 11:33:40] nn step 46200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07577
	loss_policy_1: 0.00015
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01491
	loss_reward_1: 0.00036
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99926
	loss_value_2: 0.01436
	loss_reward_2: 0.00263
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01395
	loss_reward_3: 0.00177
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99863
	loss_value_4: 0.01317
	loss_reward_4: 0.00196
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01329
	loss_reward_5: 0.00419
	loss_policy: 0.00148
	loss_value: 0.14546
	loss_reward: 0.0109
Optimization_Done 46200
[2025-05-08 11:35:01] [command] train weight_iter_46200.pkl 213 232
[2025-05-08 11:35:10] nn step 46250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07978
	loss_policy_1: 0.00011
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01583
	loss_reward_1: 0.00038
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01518
	loss_reward_2: 0.00276
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01474
	loss_reward_3: 0.00206
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99785
	loss_value_4: 0.01397
	loss_reward_4: 0.00196
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01408
	loss_reward_5: 0.00435
	loss_policy: 0.00129
	loss_value: 0.15357
	loss_reward: 0.01151
[2025-05-08 11:35:16] nn step 46300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07963
	loss_policy_1: 9e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01569
	loss_reward_1: 0.00037
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99922
	loss_value_2: 0.015
	loss_reward_2: 0.00284
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01455
	loss_reward_3: 0.00198
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01368
	loss_reward_4: 0.002
	loss_policy_5: 0.00042
	accuracy_policy_5: 0.99816
	loss_value_5: 0.01382
	loss_reward_5: 0.00447
	loss_policy: 0.00134
	loss_value: 0.15238
	loss_reward: 0.01165
[2025-05-08 11:35:24] nn step 46350, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07699
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01521
	loss_reward_1: 0.00036
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01459
	loss_reward_2: 0.00274
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99914
	loss_value_3: 0.01418
	loss_reward_3: 0.00185
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01331
	loss_reward_4: 0.00197
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99852
	loss_value_5: 0.0134
	loss_reward_5: 0.00433
	loss_policy: 0.00115
	loss_value: 0.14768
	loss_reward: 0.01125
[2025-05-08 11:35:32] nn step 46400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07478
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01479
	loss_reward_1: 0.00034
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01417
	loss_reward_2: 0.00268
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01371
	loss_reward_3: 0.00185
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99859
	loss_value_4: 0.0129
	loss_reward_4: 0.00182
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01301
	loss_reward_5: 0.0042
	loss_policy: 0.00111
	loss_value: 0.14336
	loss_reward: 0.01089
Optimization_Done 46400
[2025-05-08 11:36:51] [command] train weight_iter_46400.pkl 214 233
[2025-05-08 11:37:00] nn step 46450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07523
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01491
	loss_reward_1: 0.00033
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.0143
	loss_reward_2: 0.00258
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01391
	loss_reward_3: 0.00189
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01311
	loss_reward_4: 0.00184
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99859
	loss_value_5: 0.0132
	loss_reward_5: 0.00415
	loss_policy: 0.00082
	loss_value: 0.14467
	loss_reward: 0.01079
[2025-05-08 11:37:08] nn step 46500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07546
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01492
	loss_reward_1: 0.00038
	loss_policy_2: 0.00012
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01429
	loss_reward_2: 0.00269
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99922
	loss_value_3: 0.0139
	loss_reward_3: 0.00184
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99887
	loss_value_4: 0.0131
	loss_reward_4: 0.00187
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01316
	loss_reward_5: 0.0043
	loss_policy: 0.00098
	loss_value: 0.14481
	loss_reward: 0.01109
[2025-05-08 11:37:15] nn step 46550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07736
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0153
	loss_reward_1: 0.00035
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01463
	loss_reward_2: 0.00279
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99945
	loss_value_3: 0.0142
	loss_reward_3: 0.00193
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01335
	loss_reward_4: 0.00195
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99848
	loss_value_5: 0.01345
	loss_reward_5: 0.00445
	loss_policy: 0.00093
	loss_value: 0.14829
	loss_reward: 0.01147
[2025-05-08 11:37:23] nn step 46600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0711
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01408
	loss_reward_1: 0.00032
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01347
	loss_reward_2: 0.00259
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01307
	loss_reward_3: 0.00175
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01227
	loss_reward_4: 0.00176
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99789
	loss_value_5: 0.01236
	loss_reward_5: 0.00407
	loss_policy: 0.00118
	loss_value: 0.13635
	loss_reward: 0.01049
Optimization_Done 46600
[2025-05-08 11:38:40] [command] train weight_iter_46600.pkl 215 234
[2025-05-08 11:38:49] nn step 46650, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99996
	loss_value_0: 0.0728
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01445
	loss_reward_1: 0.0004
	loss_policy_2: 0.00017
	accuracy_policy_2: 0.99941
	loss_value_2: 0.01388
	loss_reward_2: 0.00255
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99918
	loss_value_3: 0.01343
	loss_reward_3: 0.00186
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01266
	loss_reward_4: 0.00187
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01273
	loss_reward_5: 0.00407
	loss_policy: 0.00128
	loss_value: 0.13995
	loss_reward: 0.01075
[2025-05-08 11:38:58] nn step 46700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07838
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01556
	loss_reward_1: 0.00039
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01487
	loss_reward_2: 0.00283
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.9991
	loss_value_3: 0.01442
	loss_reward_3: 0.00197
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99863
	loss_value_4: 0.01357
	loss_reward_4: 0.00199
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01365
	loss_reward_5: 0.00448
	loss_policy: 0.00088
	loss_value: 0.15045
	loss_reward: 0.01166
[2025-05-08 11:39:05] nn step 46750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07155
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01415
	loss_reward_1: 0.00032
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01355
	loss_reward_2: 0.00259
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99898
	loss_value_3: 0.01313
	loss_reward_3: 0.00185
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.99855
	loss_value_4: 0.01232
	loss_reward_4: 0.00183
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01242
	loss_reward_5: 0.00414
	loss_policy: 0.00113
	loss_value: 0.13712
	loss_reward: 0.01073
[2025-05-08 11:39:14] nn step 46800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07528
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01492
	loss_reward_1: 0.00038
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01429
	loss_reward_2: 0.00266
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99902
	loss_value_3: 0.01382
	loss_reward_3: 0.00188
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01299
	loss_reward_4: 0.00195
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01311
	loss_reward_5: 0.00418
	loss_policy: 0.00109
	loss_value: 0.14441
	loss_reward: 0.01104
Optimization_Done 46800
[2025-05-08 11:40:27] [command] train weight_iter_46800.pkl 216 235
[2025-05-08 11:40:36] nn step 46850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07428
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01474
	loss_reward_1: 0.00037
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99965
	loss_value_2: 0.0141
	loss_reward_2: 0.00269
	loss_policy_3: 0.00023
	accuracy_policy_3: 0.99922
	loss_value_3: 0.01364
	loss_reward_3: 0.00185
	loss_policy_4: 0.00026
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01282
	loss_reward_4: 0.00193
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01291
	loss_reward_5: 0.00426
	loss_policy: 0.00108
	loss_value: 0.1425
	loss_reward: 0.0111
[2025-05-08 11:40:45] nn step 46900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07286
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01446
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01386
	loss_reward_2: 0.00263
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01343
	loss_reward_3: 0.00187
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01263
	loss_reward_4: 0.00187
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01272
	loss_reward_5: 0.00419
	loss_policy: 0.00087
	loss_value: 0.13996
	loss_reward: 0.01093
[2025-05-08 11:40:54] nn step 46950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07407
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01465
	loss_reward_1: 0.00036
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01402
	loss_reward_2: 0.00269
	loss_policy_3: 0.00013
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01355
	loss_reward_3: 0.00191
	loss_policy_4: 0.00022
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01272
	loss_reward_4: 0.00189
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01279
	loss_reward_5: 0.00433
	loss_policy: 0.00072
	loss_value: 0.1418
	loss_reward: 0.01118
[2025-05-08 11:41:00] nn step 47000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07437
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01476
	loss_reward_1: 0.00033
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01416
	loss_reward_2: 0.00258
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01373
	loss_reward_3: 0.0019
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01288
	loss_reward_4: 0.00188
	loss_policy_5: 0.00022
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01294
	loss_reward_5: 0.00421
	loss_policy: 0.0006
	loss_value: 0.14283
	loss_reward: 0.0109
Optimization_Done 47000
[2025-05-08 11:42:17] [command] train weight_iter_47000.pkl 217 236
[2025-05-08 11:42:24] nn step 47050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06792
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01351
	loss_reward_1: 0.00035
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01291
	loss_reward_2: 0.00247
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.9991
	loss_value_3: 0.01249
	loss_reward_3: 0.00179
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99871
	loss_value_4: 0.01178
	loss_reward_4: 0.00179
	loss_policy_5: 0.00036
	accuracy_policy_5: 0.99828
	loss_value_5: 0.01188
	loss_reward_5: 0.00387
	loss_policy: 0.00101
	loss_value: 0.1305
	loss_reward: 0.01027
[2025-05-08 11:42:33] nn step 47100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0751
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01489
	loss_reward_1: 0.00034
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01426
	loss_reward_2: 0.00271
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01381
	loss_reward_3: 0.00187
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01295
	loss_reward_4: 0.00192
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99906
	loss_value_5: 0.01305
	loss_reward_5: 0.00431
	loss_policy: 0.00077
	loss_value: 0.14406
	loss_reward: 0.01115
[2025-05-08 11:42:42] nn step 47150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07146
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.0142
	loss_reward_1: 0.0003
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01358
	loss_reward_2: 0.00259
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99945
	loss_value_3: 0.01313
	loss_reward_3: 0.00179
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01231
	loss_reward_4: 0.00188
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01242
	loss_reward_5: 0.0041
	loss_policy: 0.00065
	loss_value: 0.13709
	loss_reward: 0.01067
[2025-05-08 11:42:48] nn step 47200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07324
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01454
	loss_reward_1: 0.00033
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01394
	loss_reward_2: 0.00264
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99945
	loss_value_3: 0.0134
	loss_reward_3: 0.00194
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01256
	loss_reward_4: 0.00197
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01263
	loss_reward_5: 0.00426
	loss_policy: 0.0006
	loss_value: 0.14031
	loss_reward: 0.01114
Optimization_Done 47200
[2025-05-08 11:44:07] [command] train weight_iter_47200.pkl 218 237
[2025-05-08 11:44:14] nn step 47250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07322
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01446
	loss_reward_1: 0.00034
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01395
	loss_reward_2: 0.00259
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01349
	loss_reward_3: 0.00195
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01266
	loss_reward_4: 0.002
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01277
	loss_reward_5: 0.00414
	loss_policy: 0.00092
	loss_value: 0.14055
	loss_reward: 0.01102
[2025-05-08 11:44:23] nn step 47300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07115
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01409
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01344
	loss_reward_2: 0.00263
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01299
	loss_reward_3: 0.00185
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99918
	loss_value_4: 0.01219
	loss_reward_4: 0.00182
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01228
	loss_reward_5: 0.00422
	loss_policy: 0.00087
	loss_value: 0.13614
	loss_reward: 0.01088
[2025-05-08 11:44:32] nn step 47350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07655
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01513
	loss_reward_1: 0.00039
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01451
	loss_reward_2: 0.00281
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01406
	loss_reward_3: 0.00199
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01323
	loss_reward_4: 0.00202
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99855
	loss_value_5: 0.01333
	loss_reward_5: 0.00444
	loss_policy: 0.00114
	loss_value: 0.14681
	loss_reward: 0.01165
[2025-05-08 11:44:38] nn step 47400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07237
	loss_policy_1: 8e-05
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01432
	loss_reward_1: 0.00034
	loss_policy_2: 0.00011
	accuracy_policy_2: 0.99957
	loss_value_2: 0.01372
	loss_reward_2: 0.00268
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01333
	loss_reward_3: 0.00183
	loss_policy_4: 0.00021
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01251
	loss_reward_4: 0.00181
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01267
	loss_reward_5: 0.00434
	loss_policy: 0.00082
	loss_value: 0.13891
	loss_reward: 0.011
Optimization_Done 47400
[2025-05-08 11:45:55] [command] train weight_iter_47400.pkl 219 238
[2025-05-08 11:46:04] nn step 47450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0771
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99996
	loss_value_1: 0.01533
	loss_reward_1: 0.00039
	loss_policy_2: 4e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01466
	loss_reward_2: 0.00284
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01418
	loss_reward_3: 0.00205
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01327
	loss_reward_4: 0.00203
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99875
	loss_value_5: 0.01336
	loss_reward_5: 0.00449
	loss_policy: 0.00063
	loss_value: 0.14788
	loss_reward: 0.01179
[2025-05-08 11:46:12] nn step 47500, lr: 0.1.
	loss_policy_0: 0.00026
	accuracy_policy_0: 0.9998
	loss_value_0: 0.0723
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01429
	loss_reward_1: 0.00033
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99918
	loss_value_2: 0.01373
	loss_reward_2: 0.00268
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99832
	loss_value_3: 0.0133
	loss_reward_3: 0.00181
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99859
	loss_value_4: 0.01247
	loss_reward_4: 0.00191
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99879
	loss_value_5: 0.0126
	loss_reward_5: 0.00422
	loss_policy: 0.00147
	loss_value: 0.13869
	loss_reward: 0.01095
[2025-05-08 11:46:21] nn step 47550, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07413
	loss_policy_1: 0.00038
	accuracy_policy_1: 0.99805
	loss_value_1: 0.01476
	loss_reward_1: 0.00034
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01434
	loss_reward_2: 0.00278
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01389
	loss_reward_3: 0.00208
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01315
	loss_reward_4: 0.00204
	loss_policy_5: 0.00047
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01336
	loss_reward_5: 0.00432
	loss_policy: 0.00214
	loss_value: 0.14364
	loss_reward: 0.01156
[2025-05-08 11:46:29] nn step 47600, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07328
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9984
	loss_value_1: 0.01458
	loss_reward_1: 0.00039
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01393
	loss_reward_2: 0.00281
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01349
	loss_reward_3: 0.00191
	loss_policy_4: 0.00042
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01267
	loss_reward_4: 0.00197
	loss_policy_5: 0.00033
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01282
	loss_reward_5: 0.00443
	loss_policy: 0.00158
	loss_value: 0.14077
	loss_reward: 0.0115
Optimization_Done 47600
[2025-05-08 11:47:46] [command] train weight_iter_47600.pkl 220 239
[2025-05-08 11:47:55] nn step 47650, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07356
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99781
	loss_value_1: 0.01463
	loss_reward_1: 0.00031
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99809
	loss_value_2: 0.01397
	loss_reward_2: 0.00268
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99703
	loss_value_3: 0.01352
	loss_reward_3: 0.00192
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99684
	loss_value_4: 0.01276
	loss_reward_4: 0.00189
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99797
	loss_value_5: 0.01282
	loss_reward_5: 0.00422
	loss_policy: 0.00247
	loss_value: 0.14125
	loss_reward: 0.01101
[2025-05-08 11:48:03] nn step 47700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06894
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99848
	loss_value_1: 0.01369
	loss_reward_1: 0.00031
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99805
	loss_value_2: 0.01308
	loss_reward_2: 0.00255
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01266
	loss_reward_3: 0.00189
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.9982
	loss_value_4: 0.01185
	loss_reward_4: 0.00181
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01192
	loss_reward_5: 0.00406
	loss_policy: 0.00137
	loss_value: 0.13215
	loss_reward: 0.01062
[2025-05-08 11:48:12] nn step 47750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99812
	loss_value_1: 0.01391
	loss_reward_1: 0.00032
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99816
	loss_value_2: 0.01333
	loss_reward_2: 0.00256
	loss_policy_3: 0.00022
	accuracy_policy_3: 0.99832
	loss_value_3: 0.0129
	loss_reward_3: 0.00185
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99773
	loss_value_4: 0.01212
	loss_reward_4: 0.00182
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01221
	loss_reward_5: 0.00404
	loss_policy: 0.00118
	loss_value: 0.13446
	loss_reward: 0.01059
[2025-05-08 11:48:21] nn step 47800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07465
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99766
	loss_value_1: 0.0148
	loss_reward_1: 0.00038
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01419
	loss_reward_2: 0.00276
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99797
	loss_value_3: 0.01368
	loss_reward_3: 0.00191
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99812
	loss_value_4: 0.01283
	loss_reward_4: 0.00199
	loss_policy_5: 0.00021
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01293
	loss_reward_5: 0.00439
	loss_policy: 0.00122
	loss_value: 0.14309
	loss_reward: 0.01143
Optimization_Done 47800
[2025-05-08 11:49:35] [command] train weight_iter_47800.pkl 221 240
[2025-05-08 11:49:46] nn step 47850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07199
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99746
	loss_value_1: 0.01429
	loss_reward_1: 0.00033
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.9982
	loss_value_2: 0.01367
	loss_reward_2: 0.00268
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01323
	loss_reward_3: 0.00187
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99805
	loss_value_4: 0.01236
	loss_reward_4: 0.00189
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99906
	loss_value_5: 0.01244
	loss_reward_5: 0.00425
	loss_policy: 0.00134
	loss_value: 0.13797
	loss_reward: 0.01101
[2025-05-08 11:49:53] nn step 47900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07174
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99766
	loss_value_1: 0.0142
	loss_reward_1: 0.00033
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01358
	loss_reward_2: 0.00262
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01312
	loss_reward_3: 0.00185
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99816
	loss_value_4: 0.01228
	loss_reward_4: 0.00192
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01234
	loss_reward_5: 0.00418
	loss_policy: 0.00116
	loss_value: 0.13727
	loss_reward: 0.0109
[2025-05-08 11:50:02] nn step 47950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07103
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.9977
	loss_value_1: 0.01406
	loss_reward_1: 0.00031
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01349
	loss_reward_2: 0.00264
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01305
	loss_reward_3: 0.00185
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99773
	loss_value_4: 0.01223
	loss_reward_4: 0.00184
	loss_policy_5: 0.0004
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01232
	loss_reward_5: 0.00417
	loss_policy: 0.0015
	loss_value: 0.13617
	loss_reward: 0.01081
[2025-05-08 11:50:10] nn step 48000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07449
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99793
	loss_value_1: 0.01475
	loss_reward_1: 0.00038
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99828
	loss_value_2: 0.01413
	loss_reward_2: 0.00275
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99754
	loss_value_3: 0.01361
	loss_reward_3: 0.00193
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99715
	loss_value_4: 0.01276
	loss_reward_4: 0.00204
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99684
	loss_value_5: 0.01286
	loss_reward_5: 0.00434
	loss_policy: 0.00198
	loss_value: 0.14261
	loss_reward: 0.01144
Optimization_Done 48000
[2025-05-08 11:51:26] [command] train weight_iter_48000.pkl 222 241
[2025-05-08 11:51:37] nn step 48050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07327
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99762
	loss_value_1: 0.01458
	loss_reward_1: 0.00036
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99781
	loss_value_2: 0.01402
	loss_reward_2: 0.00276
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99672
	loss_value_3: 0.01353
	loss_reward_3: 0.00194
	loss_policy_4: 0.00047
	accuracy_policy_4: 0.99629
	loss_value_4: 0.01272
	loss_reward_4: 0.00198
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99793
	loss_value_5: 0.01281
	loss_reward_5: 0.0044
	loss_policy: 0.00198
	loss_value: 0.14093
	loss_reward: 0.01144
[2025-05-08 11:51:43] nn step 48100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07194
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.9984
	loss_value_1: 0.01427
	loss_reward_1: 0.00033
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99832
	loss_value_2: 0.0137
	loss_reward_2: 0.0026
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01322
	loss_reward_3: 0.00191
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99684
	loss_value_4: 0.01242
	loss_reward_4: 0.00193
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01253
	loss_reward_5: 0.00419
	loss_policy: 0.00129
	loss_value: 0.13808
	loss_reward: 0.01096
[2025-05-08 11:51:52] nn step 48150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07125
	loss_policy_1: 0.00013
	accuracy_policy_1: 0.99906
	loss_value_1: 0.01413
	loss_reward_1: 0.00035
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01349
	loss_reward_2: 0.00263
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01307
	loss_reward_3: 0.00183
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01226
	loss_reward_4: 0.00184
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01236
	loss_reward_5: 0.00424
	loss_policy: 0.00134
	loss_value: 0.13656
	loss_reward: 0.0109
[2025-05-08 11:52:01] nn step 48200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06747
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99965
	loss_value_1: 0.01341
	loss_reward_1: 0.00031
	loss_policy_2: 0.00016
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01283
	loss_reward_2: 0.00249
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99766
	loss_value_3: 0.0124
	loss_reward_3: 0.00177
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99688
	loss_value_4: 0.0116
	loss_reward_4: 0.00182
	loss_policy_5: 0.00032
	accuracy_policy_5: 0.99883
	loss_value_5: 0.0117
	loss_reward_5: 0.00396
	loss_policy: 0.00116
	loss_value: 0.12941
	loss_reward: 0.01036
Optimization_Done 48200
[2025-05-08 11:53:15] [command] train weight_iter_48200.pkl 223 242
[2025-05-08 11:53:24] nn step 48250, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07114
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01412
	loss_reward_1: 0.00034
	loss_policy_2: 0.00015
	accuracy_policy_2: 0.99938
	loss_value_2: 0.01351
	loss_reward_2: 0.00269
	loss_policy_3: 0.00025
	accuracy_policy_3: 0.99812
	loss_value_3: 0.01303
	loss_reward_3: 0.00187
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01224
	loss_reward_4: 0.00192
	loss_policy_5: 0.00023
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01235
	loss_reward_5: 0.00422
	loss_policy: 0.00099
	loss_value: 0.13638
	loss_reward: 0.01103
[2025-05-08 11:53:33] nn step 48300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07211
	loss_policy_1: 1e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01435
	loss_reward_1: 0.00031
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99949
	loss_value_2: 0.01377
	loss_reward_2: 0.00271
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99883
	loss_value_3: 0.0133
	loss_reward_3: 0.00192
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.99797
	loss_value_4: 0.0125
	loss_reward_4: 0.00196
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01263
	loss_reward_5: 0.0043
	loss_policy: 0.00083
	loss_value: 0.13865
	loss_reward: 0.0112
[2025-05-08 11:53:40] nn step 48350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06927
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01373
	loss_reward_1: 0.00033
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01314
	loss_reward_2: 0.00266
	loss_policy_3: 0.00021
	accuracy_policy_3: 0.99867
	loss_value_3: 0.01272
	loss_reward_3: 0.00181
	loss_policy_4: 0.00032
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01196
	loss_reward_4: 0.00184
	loss_policy_5: 0.00045
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01203
	loss_reward_5: 0.00421
	loss_policy: 0.00111
	loss_value: 0.13285
	loss_reward: 0.01086
[2025-05-08 11:53:48] nn step 48400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07317
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.99961
	loss_value_1: 0.01448
	loss_reward_1: 0.00031
	loss_policy_2: 0.00021
	accuracy_policy_2: 0.99922
	loss_value_2: 0.0139
	loss_reward_2: 0.00276
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01337
	loss_reward_3: 0.00193
	loss_policy_4: 0.00039
	accuracy_policy_4: 0.9977
	loss_value_4: 0.0125
	loss_reward_4: 0.00194
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99863
	loss_value_5: 0.01265
	loss_reward_5: 0.00441
	loss_policy: 0.00141
	loss_value: 0.14005
	loss_reward: 0.01135
Optimization_Done 48400
[2025-05-08 11:55:04] [command] train weight_iter_48400.pkl 224 243
[2025-05-08 11:55:13] nn step 48450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07329
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01453
	loss_reward_1: 0.00036
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01389
	loss_reward_2: 0.00276
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99926
	loss_value_3: 0.01343
	loss_reward_3: 0.00186
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99883
	loss_value_4: 0.01259
	loss_reward_4: 0.00193
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01268
	loss_reward_5: 0.00435
	loss_policy: 0.00064
	loss_value: 0.14041
	loss_reward: 0.01126
[2025-05-08 11:55:22] nn step 48500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07281
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01444
	loss_reward_1: 0.00032
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01385
	loss_reward_2: 0.00277
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01337
	loss_reward_3: 0.00194
	loss_policy_4: 0.00033
	accuracy_policy_4: 0.99887
	loss_value_4: 0.01254
	loss_reward_4: 0.00194
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99895
	loss_value_5: 0.01269
	loss_reward_5: 0.00435
	loss_policy: 0.00106
	loss_value: 0.1397
	loss_reward: 0.01132
[2025-05-08 11:55:28] nn step 48550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06771
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01342
	loss_reward_1: 0.00031
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01286
	loss_reward_2: 0.00257
	loss_policy_3: 0.00016
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01243
	loss_reward_3: 0.00175
	loss_policy_4: 0.00024
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01164
	loss_reward_4: 0.00183
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01173
	loss_reward_5: 0.00408
	loss_policy: 0.00089
	loss_value: 0.12978
	loss_reward: 0.01054
[2025-05-08 11:55:37] nn step 48600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07158
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.0142
	loss_reward_1: 0.00033
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01362
	loss_reward_2: 0.00272
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99941
	loss_value_3: 0.01314
	loss_reward_3: 0.00191
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99902
	loss_value_4: 0.01226
	loss_reward_4: 0.00196
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01237
	loss_reward_5: 0.00435
	loss_policy: 0.00083
	loss_value: 0.13717
	loss_reward: 0.01126
Optimization_Done 48600
[2025-05-08 11:56:53] [command] train weight_iter_48600.pkl 225 244
[2025-05-08 11:57:03] nn step 48650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06946
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01378
	loss_reward_1: 0.00028
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01325
	loss_reward_2: 0.0026
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01276
	loss_reward_3: 0.00185
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01194
	loss_reward_4: 0.00187
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01207
	loss_reward_5: 0.00418
	loss_policy: 0.00093
	loss_value: 0.13325
	loss_reward: 0.01078
[2025-05-08 11:57:11] nn step 48700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07239
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01438
	loss_reward_1: 0.00034
	loss_policy_2: 0.00019
	accuracy_policy_2: 0.99953
	loss_value_2: 0.01378
	loss_reward_2: 0.00281
	loss_policy_3: 0.00024
	accuracy_policy_3: 0.99926
	loss_value_3: 0.0133
	loss_reward_3: 0.00196
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.9991
	loss_value_4: 0.01243
	loss_reward_4: 0.00206
	loss_policy_5: 0.00031
	accuracy_policy_5: 0.99891
	loss_value_5: 0.01258
	loss_reward_5: 0.00442
	loss_policy: 0.00114
	loss_value: 0.13885
	loss_reward: 0.01159
[2025-05-08 11:57:18] nn step 48750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07012
	loss_policy_1: 6e-05
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01388
	loss_reward_1: 0.00035
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01331
	loss_reward_2: 0.00272
	loss_policy_3: 0.00018
	accuracy_policy_3: 0.99934
	loss_value_3: 0.01283
	loss_reward_3: 0.00186
	loss_policy_4: 0.00028
	accuracy_policy_4: 0.99891
	loss_value_4: 0.01202
	loss_reward_4: 0.00193
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.99859
	loss_value_5: 0.01219
	loss_reward_5: 0.00429
	loss_policy: 0.00101
	loss_value: 0.13435
	loss_reward: 0.01115
[2025-05-08 11:57:27] nn step 48800, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06906
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01372
	loss_reward_1: 0.00029
	loss_policy_2: 0.00013
	accuracy_policy_2: 0.99965
	loss_value_2: 0.01316
	loss_reward_2: 0.00256
	loss_policy_3: 0.0002
	accuracy_policy_3: 0.99938
	loss_value_3: 0.01264
	loss_reward_3: 0.00184
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.99914
	loss_value_4: 0.01185
	loss_reward_4: 0.00186
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99887
	loss_value_5: 0.01198
	loss_reward_5: 0.00409
	loss_policy: 0.00116
	loss_value: 0.13239
	loss_reward: 0.01064
Optimization_Done 48800
[2025-05-08 11:58:43] [command] train weight_iter_48800.pkl 226 245
[2025-05-08 11:58:52] nn step 48850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06723
	loss_policy_1: 1e-05
	accuracy_policy_1: 1.0
	loss_value_1: 0.01332
	loss_reward_1: 0.00036
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99945
	loss_value_2: 0.01274
	loss_reward_2: 0.00255
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.99887
	loss_value_3: 0.01228
	loss_reward_3: 0.00181
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99848
	loss_value_4: 0.01148
	loss_reward_4: 0.00184
	loss_policy_5: 0.00024
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01157
	loss_reward_5: 0.00401
	loss_policy: 0.00078
	loss_value: 0.12863
	loss_reward: 0.01057
[2025-05-08 11:59:01] nn step 48900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06839
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01357
	loss_reward_1: 0.00034
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01301
	loss_reward_2: 0.00267
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01256
	loss_reward_3: 0.00179
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99934
	loss_value_4: 0.01178
	loss_reward_4: 0.00183
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.9993
	loss_value_5: 0.01186
	loss_reward_5: 0.00417
	loss_policy: 0.00047
	loss_value: 0.13116
	loss_reward: 0.0108
[2025-05-08 11:59:10] nn step 48950, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06992
	loss_policy_1: 7e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01386
	loss_reward_1: 0.00034
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01328
	loss_reward_2: 0.00268
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99949
	loss_value_3: 0.01281
	loss_reward_3: 0.00181
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01196
	loss_reward_4: 0.00186
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99938
	loss_value_5: 0.01207
	loss_reward_5: 0.0043
	loss_policy: 0.00073
	loss_value: 0.1339
	loss_reward: 0.01098
[2025-05-08 11:59:17] nn step 49000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06812
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01349
	loss_reward_1: 0.00031
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99969
	loss_value_2: 0.01292
	loss_reward_2: 0.00261
	loss_policy_3: 0.00019
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01245
	loss_reward_3: 0.00188
	loss_policy_4: 0.00023
	accuracy_policy_4: 0.99906
	loss_value_4: 0.01162
	loss_reward_4: 0.00186
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99871
	loss_value_5: 0.01172
	loss_reward_5: 0.00411
	loss_policy: 0.00085
	loss_value: 0.13033
	loss_reward: 0.01077
Optimization_Done 49000
[2025-05-08 12:00:33] [command] train weight_iter_49000.pkl 227 246
[2025-05-08 12:00:41] nn step 49050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07127
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01413
	loss_reward_1: 0.00031
	loss_policy_2: 2e-05
	accuracy_policy_2: 0.99992
	loss_value_2: 0.01354
	loss_reward_2: 0.0027
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01303
	loss_reward_3: 0.00198
	loss_policy_4: 0.00015
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01217
	loss_reward_4: 0.00198
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01229
	loss_reward_5: 0.00432
	loss_policy: 0.00056
	loss_value: 0.13642
	loss_reward: 0.01129
[2025-05-08 12:00:49] nn step 49100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06851
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.0136
	loss_reward_1: 0.00034
	loss_policy_2: 8e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01301
	loss_reward_2: 0.00267
	loss_policy_3: 0.00014
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01255
	loss_reward_3: 0.00188
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99922
	loss_value_4: 0.01174
	loss_reward_4: 0.00185
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01188
	loss_reward_5: 0.00425
	loss_policy: 0.00072
	loss_value: 0.13129
	loss_reward: 0.01098
[2025-05-08 12:00:58] nn step 49150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07026
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01388
	loss_reward_1: 0.0003
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01329
	loss_reward_2: 0.0027
	loss_policy_3: 0.00012
	accuracy_policy_3: 0.99961
	loss_value_3: 0.01285
	loss_reward_3: 0.00188
	loss_policy_4: 0.00017
	accuracy_policy_4: 0.99953
	loss_value_4: 0.01197
	loss_reward_4: 0.00189
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99934
	loss_value_5: 0.01211
	loss_reward_5: 0.00431
	loss_policy: 0.00057
	loss_value: 0.13437
	loss_reward: 0.01109
[2025-05-08 12:01:05] nn step 49200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06883
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01363
	loss_reward_1: 0.00032
	loss_policy_2: 0.0001
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01301
	loss_reward_2: 0.00267
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01254
	loss_reward_3: 0.00186
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01173
	loss_reward_4: 0.00187
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01182
	loss_reward_5: 0.00425
	loss_policy: 0.00073
	loss_value: 0.13156
	loss_reward: 0.01097
Optimization_Done 49200
[2025-05-08 12:02:23] [command] train weight_iter_49200.pkl 228 247
[2025-05-08 12:02:31] nn step 49250, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07036
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01397
	loss_reward_1: 0.0003
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99973
	loss_value_2: 0.01343
	loss_reward_2: 0.00268
	loss_policy_3: 0.00011
	accuracy_policy_3: 0.99957
	loss_value_3: 0.0129
	loss_reward_3: 0.00193
	loss_policy_4: 0.00014
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01207
	loss_reward_4: 0.00193
	loss_policy_5: 0.00014
	accuracy_policy_5: 0.99941
	loss_value_5: 0.01217
	loss_reward_5: 0.00429
	loss_policy: 0.00057
	loss_value: 0.1349
	loss_reward: 0.01112
[2025-05-08 12:02:39] nn step 49300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07333
	loss_policy_1: 0.0001
	accuracy_policy_1: 0.99977
	loss_value_1: 0.01456
	loss_reward_1: 0.00036
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99961
	loss_value_2: 0.01394
	loss_reward_2: 0.00277
	loss_policy_3: 0.00015
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01343
	loss_reward_3: 0.00206
	loss_policy_4: 0.00019
	accuracy_policy_4: 0.99941
	loss_value_4: 0.01254
	loss_reward_4: 0.00199
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01264
	loss_reward_5: 0.00449
	loss_policy: 0.00083
	loss_value: 0.14045
	loss_reward: 0.01167
[2025-05-08 12:02:48] nn step 49350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0711
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01409
	loss_reward_1: 0.00033
	loss_policy_2: 7e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01348
	loss_reward_2: 0.00274
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99965
	loss_value_3: 0.01294
	loss_reward_3: 0.00197
	loss_policy_4: 0.00016
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01205
	loss_reward_4: 0.00196
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01216
	loss_reward_5: 0.00436
	loss_policy: 0.00057
	loss_value: 0.13581
	loss_reward: 0.01136
[2025-05-08 12:02:55] nn step 49400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06836
	loss_policy_1: 4e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01357
	loss_reward_1: 0.0003
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.013
	loss_reward_2: 0.00264
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.0125
	loss_reward_3: 0.00187
	loss_policy_4: 7e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01169
	loss_reward_4: 0.00191
	loss_policy_5: 0.00012
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01179
	loss_reward_5: 0.00421
	loss_policy: 0.00037
	loss_value: 0.13091
	loss_reward: 0.01093
Optimization_Done 49400
[2025-05-08 12:04:14] [command] train weight_iter_49400.pkl 229 248
[2025-05-08 12:04:22] nn step 49450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06814
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01356
	loss_reward_1: 0.00032
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.013
	loss_reward_2: 0.00261
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.99953
	loss_value_3: 0.01253
	loss_reward_3: 0.0019
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99938
	loss_value_4: 0.01172
	loss_reward_4: 0.00189
	loss_policy_5: 0.0003
	accuracy_policy_5: 0.99914
	loss_value_5: 0.01182
	loss_reward_5: 0.00418
	loss_policy: 0.00078
	loss_value: 0.13078
	loss_reward: 0.01091
[2025-05-08 12:04:30] nn step 49500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0649
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01289
	loss_reward_1: 0.00029
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99984
	loss_value_2: 0.01238
	loss_reward_2: 0.00256
	loss_policy_3: 0.0001
	accuracy_policy_3: 0.99973
	loss_value_3: 0.01191
	loss_reward_3: 0.00176
	loss_policy_4: 0.00018
	accuracy_policy_4: 0.99945
	loss_value_4: 0.01115
	loss_reward_4: 0.00172
	loss_policy_5: 0.00026
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01126
	loss_reward_5: 0.00401
	loss_policy: 0.00063
	loss_value: 0.12449
	loss_reward: 0.01034
[2025-05-08 12:04:39] nn step 49550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06879
	loss_policy_1: 3e-05
	accuracy_policy_1: 0.99992
	loss_value_1: 0.01362
	loss_reward_1: 0.00029
	loss_policy_2: 9e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.01302
	loss_reward_2: 0.00276
	loss_policy_3: 9e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01256
	loss_reward_3: 0.0018
	loss_policy_4: 0.00013
	accuracy_policy_4: 0.99961
	loss_value_4: 0.01173
	loss_reward_4: 0.00185
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99949
	loss_value_5: 0.01185
	loss_reward_5: 0.00438
	loss_policy: 0.00053
	loss_value: 0.13157
	loss_reward: 0.01108
[2025-05-08 12:04:48] nn step 49600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06648
	loss_policy_1: 0.0
	accuracy_policy_1: 1.0
	loss_value_1: 0.01319
	loss_reward_1: 0.00033
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.99988
	loss_value_2: 0.01263
	loss_reward_2: 0.0026
	loss_policy_3: 7e-05
	accuracy_policy_3: 0.99977
	loss_value_3: 0.01217
	loss_reward_3: 0.00181
	loss_policy_4: 9e-05
	accuracy_policy_4: 0.99969
	loss_value_4: 0.01134
	loss_reward_4: 0.00183
	loss_policy_5: 0.00016
	accuracy_policy_5: 0.99957
	loss_value_5: 0.01144
	loss_reward_5: 0.00411
	loss_policy: 0.00037
	loss_value: 0.12725
	loss_reward: 0.01067
Optimization_Done 49600
[2025-05-08 12:06:03] [command] train weight_iter_49600.pkl 230 249
[2025-05-08 12:06:12] nn step 49650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06599
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99988
	loss_value_1: 0.01311
	loss_reward_1: 0.0003
	loss_policy_2: 5e-05
	accuracy_policy_2: 0.9998
	loss_value_2: 0.01252
	loss_reward_2: 0.00257
	loss_policy_3: 8e-05
	accuracy_policy_3: 0.99969
	loss_value_3: 0.01204
	loss_reward_3: 0.00182
	loss_policy_4: 0.0002
	accuracy_policy_4: 0.99926
	loss_value_4: 0.01123
	loss_reward_4: 0.00182
	loss_policy_5: 0.00019
	accuracy_policy_5: 0.99918
	loss_value_5: 0.01132
	loss_reward_5: 0.00412
	loss_policy: 0.00055
	loss_value: 0.12622
	loss_reward: 0.01063
[2025-05-08 12:06:19] nn step 49700, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06872
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01366
	loss_reward_1: 0.00031
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99914
	loss_value_2: 0.01308
	loss_reward_2: 0.0027
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.99797
	loss_value_3: 0.0126
	loss_reward_3: 0.00191
	loss_policy_4: 0.00036
	accuracy_policy_4: 0.9982
	loss_value_4: 0.01176
	loss_reward_4: 0.00187
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99883
	loss_value_5: 0.01189
	loss_reward_5: 0.00435
	loss_policy: 0.00125
	loss_value: 0.13172
	loss_reward: 0.01113
[2025-05-08 12:06:28] nn step 49750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06947
	loss_policy_1: 0.00012
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01374
	loss_reward_1: 0.00032
	loss_policy_2: 0.0002
	accuracy_policy_2: 0.99828
	loss_value_2: 0.01315
	loss_reward_2: 0.00272
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01263
	loss_reward_3: 0.00195
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01176
	loss_reward_4: 0.00192
	loss_policy_5: 0.00029
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01192
	loss_reward_5: 0.00435
	loss_policy: 0.00125
	loss_value: 0.13267
	loss_reward: 0.01124
[2025-05-08 12:06:36] nn step 49800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0688
	loss_policy_1: 5e-05
	accuracy_policy_1: 0.9998
	loss_value_1: 0.01365
	loss_reward_1: 0.0003
	loss_policy_2: 0.00014
	accuracy_policy_2: 0.99934
	loss_value_2: 0.01309
	loss_reward_2: 0.00266
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99859
	loss_value_3: 0.0126
	loss_reward_3: 0.00186
	loss_policy_4: 0.00034
	accuracy_policy_4: 0.9975
	loss_value_4: 0.01177
	loss_reward_4: 0.00187
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99898
	loss_value_5: 0.01188
	loss_reward_5: 0.00428
	loss_policy: 0.00108
	loss_value: 0.13178
	loss_reward: 0.01097
Optimization_Done 49800
[2025-05-08 12:07:51] [command] train weight_iter_49800.pkl 231 250
[2025-05-08 12:08:01] nn step 49850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06283
	loss_policy_1: 2e-05
	accuracy_policy_1: 0.99984
	loss_value_1: 0.01239
	loss_reward_1: 0.00034
	loss_policy_2: 6e-05
	accuracy_policy_2: 0.99977
	loss_value_2: 0.0119
	loss_reward_2: 0.00245
	loss_policy_3: 0.00017
	accuracy_policy_3: 0.9993
	loss_value_3: 0.01148
	loss_reward_3: 0.00187
	loss_policy_4: 0.00025
	accuracy_policy_4: 0.99867
	loss_value_4: 0.01069
	loss_reward_4: 0.00178
	loss_policy_5: 0.00027
	accuracy_policy_5: 0.99902
	loss_value_5: 0.01081
	loss_reward_5: 0.00389
	loss_policy: 0.00079
	loss_value: 0.12011
	loss_reward: 0.01032
[2025-05-08 12:08:08] nn step 49900, lr: 0.1.
	loss_policy_0: 0.00082
	accuracy_policy_0: 0.99867
	loss_value_0: 0.07681
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.99852
	loss_value_1: 0.0149
	loss_reward_1: 0.00131
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01453
	loss_reward_2: 0.00399
	loss_policy_3: 0.00031
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01428
	loss_reward_3: 0.00321
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99805
	loss_value_4: 0.01375
	loss_reward_4: 0.00307
	loss_policy_5: 0.00025
	accuracy_policy_5: 0.9991
	loss_value_5: 0.01438
	loss_reward_5: 0.00555
	loss_policy: 0.00223
	loss_value: 0.14867
	loss_reward: 0.01714
[2025-05-08 12:08:16] nn step 49950, lr: 0.1.
	loss_policy_0: 0.00153
	accuracy_policy_0: 0.99699
	loss_value_0: 0.08257
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99883
	loss_value_1: 0.01602
	loss_reward_1: 0.00242
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99816
	loss_value_2: 0.01567
	loss_reward_2: 0.00461
	loss_policy_3: 0.00041
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01566
	loss_reward_3: 0.00422
	loss_policy_4: 0.00027
	accuracy_policy_4: 0.9984
	loss_value_4: 0.01496
	loss_reward_4: 0.0042
	loss_policy_5: 0.00035
	accuracy_policy_5: 0.99734
	loss_value_5: 0.01563
	loss_reward_5: 0.00631
	loss_policy: 0.00297
	loss_value: 0.16052
	loss_reward: 0.02175
[2025-05-08 12:08:25] nn step 50000, lr: 0.1.
	loss_policy_0: 0.00158
	accuracy_policy_0: 0.99699
	loss_value_0: 0.08047
	loss_policy_1: 0.00037
	accuracy_policy_1: 0.99824
	loss_value_1: 0.01604
	loss_reward_1: 0.00214
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99746
	loss_value_2: 0.0159
	loss_reward_2: 0.00383
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.9973
	loss_value_3: 0.01581
	loss_reward_3: 0.0037
	loss_policy_4: 0.00029
	accuracy_policy_4: 0.99809
	loss_value_4: 0.01539
	loss_reward_4: 0.00437
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99867
	loss_value_5: 0.01599
	loss_reward_5: 0.00642
	loss_policy: 0.00337
	loss_value: 0.1596
	loss_reward: 0.02046
Optimization_Done 50000
[2025-05-08 12:09:41] [command] train weight_iter_50000.pkl 232 251
[2025-05-08 12:09:51] nn step 50050, lr: 0.1.
	loss_policy_0: 0.00054
	accuracy_policy_0: 0.99922
	loss_value_0: 0.08241
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99859
	loss_value_1: 0.01616
	loss_reward_1: 0.00237
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99789
	loss_value_2: 0.01618
	loss_reward_2: 0.00357
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.9975
	loss_value_3: 0.01625
	loss_reward_3: 0.0035
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01575
	loss_reward_4: 0.00431
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99742
	loss_value_5: 0.01598
	loss_reward_5: 0.00573
	loss_policy: 0.00233
	loss_value: 0.16273
	loss_reward: 0.01949
[2025-05-08 12:09:57] nn step 50100, lr: 0.1.
	loss_policy_0: 0.00039
	accuracy_policy_0: 0.99941
	loss_value_0: 0.07337
	loss_policy_1: 0.00042
	accuracy_policy_1: 0.99781
	loss_value_1: 0.0146
	loss_reward_1: 0.00222
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99781
	loss_value_2: 0.01458
	loss_reward_2: 0.00337
	loss_policy_3: 0.00036
	accuracy_policy_3: 0.99734
	loss_value_3: 0.01433
	loss_reward_3: 0.00291
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99703
	loss_value_4: 0.014
	loss_reward_4: 0.00405
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99781
	loss_value_5: 0.01449
	loss_reward_5: 0.00533
	loss_policy: 0.0024
	loss_value: 0.14537
	loss_reward: 0.01788
[2025-05-08 12:10:06] nn step 50150, lr: 0.1.
	loss_policy_0: 0.00033
	accuracy_policy_0: 0.99969
	loss_value_0: 0.07308
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99828
	loss_value_1: 0.01459
	loss_reward_1: 0.00188
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99746
	loss_value_2: 0.01466
	loss_reward_2: 0.00364
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99773
	loss_value_3: 0.0144
	loss_reward_3: 0.00298
	loss_policy_4: 0.00031
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01411
	loss_reward_4: 0.00397
	loss_policy_5: 0.0002
	accuracy_policy_5: 0.99922
	loss_value_5: 0.01465
	loss_reward_5: 0.0054
	loss_policy: 0.00166
	loss_value: 0.1455
	loss_reward: 0.01786
[2025-05-08 12:10:15] nn step 50200, lr: 0.1.
	loss_policy_0: 0.00038
	accuracy_policy_0: 0.99938
	loss_value_0: 0.07424
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.9982
	loss_value_1: 0.01485
	loss_reward_1: 0.00173
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.9977
	loss_value_2: 0.0149
	loss_reward_2: 0.0037
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01469
	loss_reward_3: 0.00312
	loss_policy_4: 0.00037
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01437
	loss_reward_4: 0.00389
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99879
	loss_value_5: 0.01488
	loss_reward_5: 0.00553
	loss_policy: 0.00214
	loss_value: 0.14793
	loss_reward: 0.01798
Optimization_Done 50200
[2025-05-08 12:11:34] [command] train weight_iter_50200.pkl 233 252
[2025-05-08 12:11:43] nn step 50250, lr: 0.1.
	loss_policy_0: 0.00037
	accuracy_policy_0: 0.99957
	loss_value_0: 0.07855
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99887
	loss_value_1: 0.01552
	loss_reward_1: 0.00191
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99805
	loss_value_2: 0.01549
	loss_reward_2: 0.00375
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99758
	loss_value_3: 0.01524
	loss_reward_3: 0.00316
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01485
	loss_reward_4: 0.00374
	loss_policy_5: 0.00044
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01531
	loss_reward_5: 0.00508
	loss_policy: 0.00244
	loss_value: 0.15497
	loss_reward: 0.01764
[2025-05-08 12:11:50] nn step 50300, lr: 0.1.
	loss_policy_0: 0.00023
	accuracy_policy_0: 0.99973
	loss_value_0: 0.07738
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99879
	loss_value_1: 0.01552
	loss_reward_1: 0.00165
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99793
	loss_value_2: 0.01565
	loss_reward_2: 0.00406
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99738
	loss_value_3: 0.01549
	loss_reward_3: 0.00343
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01504
	loss_reward_4: 0.00367
	loss_policy_5: 0.00037
	accuracy_policy_5: 0.99832
	loss_value_5: 0.01555
	loss_reward_5: 0.00529
	loss_policy: 0.00214
	loss_value: 0.15463
	loss_reward: 0.0181
[2025-05-08 12:11:59] nn step 50350, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.9998
	loss_value_0: 0.07186
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99875
	loss_value_1: 0.01447
	loss_reward_1: 0.0011
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01439
	loss_reward_2: 0.00353
	loss_policy_3: 0.00032
	accuracy_policy_3: 0.99797
	loss_value_3: 0.01424
	loss_reward_3: 0.00299
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01387
	loss_reward_4: 0.00296
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01406
	loss_reward_5: 0.00474
	loss_policy: 0.00204
	loss_value: 0.14288
	loss_reward: 0.01533
[2025-05-08 12:12:07] nn step 50400, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99973
	loss_value_0: 0.06734
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01359
	loss_reward_1: 0.00076
	loss_policy_2: 0.00024
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01365
	loss_reward_2: 0.00313
	loss_policy_3: 0.00028
	accuracy_policy_3: 0.99824
	loss_value_3: 0.01338
	loss_reward_3: 0.00267
	loss_policy_4: 0.0004
	accuracy_policy_4: 0.99781
	loss_value_4: 0.013
	loss_reward_4: 0.00274
	loss_policy_5: 0.00038
	accuracy_policy_5: 0.9984
	loss_value_5: 0.01326
	loss_reward_5: 0.00438
	loss_policy: 0.00166
	loss_value: 0.13423
	loss_reward: 0.01368
Optimization_Done 50400
[2025-05-08 12:13:22] [command] train weight_iter_50400.pkl 234 253
[2025-05-08 12:13:32] nn step 50450, lr: 0.1.
	loss_policy_0: 0.00023
	accuracy_policy_0: 0.99977
	loss_value_0: 0.07604
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01503
	loss_reward_1: 0.00128
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01503
	loss_reward_2: 0.00329
	loss_policy_3: 0.00033
	accuracy_policy_3: 0.99875
	loss_value_3: 0.01458
	loss_reward_3: 0.00288
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01417
	loss_reward_4: 0.00306
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99809
	loss_value_5: 0.01448
	loss_reward_5: 0.00444
	loss_policy: 0.00214
	loss_value: 0.14933
	loss_reward: 0.01495
[2025-05-08 12:13:40] nn step 50500, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.99977
	loss_value_0: 0.06859
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01375
	loss_reward_1: 0.00058
	loss_policy_2: 0.00023
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01363
	loss_reward_2: 0.00262
	loss_policy_3: 0.00026
	accuracy_policy_3: 0.9984
	loss_value_3: 0.0134
	loss_reward_3: 0.00223
	loss_policy_4: 0.00041
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01295
	loss_reward_4: 0.0023
	loss_policy_5: 0.00039
	accuracy_policy_5: 0.99805
	loss_value_5: 0.01312
	loss_reward_5: 0.00386
	loss_policy: 0.00165
	loss_value: 0.13544
	loss_reward: 0.0116
[2025-05-08 12:13:47] nn step 50550, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06776
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01361
	loss_reward_1: 0.00067
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01354
	loss_reward_2: 0.00298
	loss_policy_3: 0.0003
	accuracy_policy_3: 0.99855
	loss_value_3: 0.01336
	loss_reward_3: 0.00272
	loss_policy_4: 0.00035
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01302
	loss_reward_4: 0.00247
	loss_policy_5: 0.00028
	accuracy_policy_5: 0.99863
	loss_value_5: 0.0132
	loss_reward_5: 0.00423
	loss_policy: 0.00141
	loss_value: 0.13449
	loss_reward: 0.01308
[2025-05-08 12:13:56] nn step 50600, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.9998
	loss_value_0: 0.06804
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01356
	loss_reward_1: 0.00068
	loss_policy_2: 0.00026
	accuracy_policy_2: 0.99914
	loss_value_2: 0.01334
	loss_reward_2: 0.00287
	loss_policy_3: 0.00046
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01304
	loss_reward_3: 0.00209
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.9968
	loss_value_4: 0.0126
	loss_reward_4: 0.00204
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99789
	loss_value_5: 0.01281
	loss_reward_5: 0.00402
	loss_policy: 0.0023
	loss_value: 0.13339
	loss_reward: 0.0117
Optimization_Done 50600
[2025-05-08 12:15:11] [command] train weight_iter_50600.pkl 235 254
[2025-05-08 12:15:20] nn step 50650, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.9998
	loss_value_0: 0.07002
	loss_policy_1: 0.00037
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01386
	loss_reward_1: 0.00068
	loss_policy_2: 0.00025
	accuracy_policy_2: 0.99918
	loss_value_2: 0.0135
	loss_reward_2: 0.00263
	loss_policy_3: 0.00035
	accuracy_policy_3: 0.99848
	loss_value_3: 0.01318
	loss_reward_3: 0.00229
	loss_policy_4: 0.00045
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01277
	loss_reward_4: 0.0024
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99812
	loss_value_5: 0.01299
	loss_reward_5: 0.00391
	loss_policy: 0.00207
	loss_value: 0.13632
	loss_reward: 0.01192
[2025-05-08 12:15:29] nn step 50700, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99984
	loss_value_0: 0.06691
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01314
	loss_reward_1: 0.00053
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01284
	loss_reward_2: 0.00253
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01256
	loss_reward_3: 0.00188
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01211
	loss_reward_4: 0.00186
	loss_policy_5: 0.0005
	accuracy_policy_5: 0.99785
	loss_value_5: 0.01224
	loss_reward_5: 0.00378
	loss_policy: 0.00212
	loss_value: 0.1298
	loss_reward: 0.01059
[2025-05-08 12:15:36] nn step 50750, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99988
	loss_value_0: 0.07159
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01403
	loss_reward_1: 0.00084
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99871
	loss_value_2: 0.0137
	loss_reward_2: 0.00286
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99805
	loss_value_3: 0.01334
	loss_reward_3: 0.00218
	loss_policy_4: 0.00055
	accuracy_policy_4: 0.99676
	loss_value_4: 0.01294
	loss_reward_4: 0.00227
	loss_policy_5: 0.00041
	accuracy_policy_5: 0.99773
	loss_value_5: 0.01319
	loss_reward_5: 0.00415
	loss_policy: 0.00208
	loss_value: 0.13879
	loss_reward: 0.01229
[2025-05-08 12:15:44] nn step 50800, lr: 0.1.
	loss_policy_0: 0.0002
	accuracy_policy_0: 0.9998
	loss_value_0: 0.06736
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01318
	loss_reward_1: 0.00063
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99879
	loss_value_2: 0.0128
	loss_reward_2: 0.00258
	loss_policy_3: 0.00029
	accuracy_policy_3: 0.9984
	loss_value_3: 0.01251
	loss_reward_3: 0.00184
	loss_policy_4: 0.00038
	accuracy_policy_4: 0.99793
	loss_value_4: 0.01198
	loss_reward_4: 0.00181
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.9977
	loss_value_5: 0.01223
	loss_reward_5: 0.00373
	loss_policy: 0.00186
	loss_value: 0.13006
	loss_reward: 0.01059
Optimization_Done 50800
[2025-05-08 12:16:59] [command] train weight_iter_50800.pkl 236 255
[2025-05-08 12:17:08] nn step 50850, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.99984
	loss_value_0: 0.06607
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01286
	loss_reward_1: 0.00061
	loss_policy_2: 0.00028
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01252
	loss_reward_2: 0.00234
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01223
	loss_reward_3: 0.00178
	loss_policy_4: 0.00044
	accuracy_policy_4: 0.99648
	loss_value_4: 0.0118
	loss_reward_4: 0.00183
	loss_policy_5: 0.00043
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01206
	loss_reward_5: 0.00349
	loss_policy: 0.00196
	loss_value: 0.12753
	loss_reward: 0.01004
[2025-05-08 12:17:17] nn step 50900, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06937
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01353
	loss_reward_1: 0.00066
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01319
	loss_reward_2: 0.00259
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.9975
	loss_value_3: 0.0128
	loss_reward_3: 0.00194
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99566
	loss_value_4: 0.01236
	loss_reward_4: 0.00194
	loss_policy_5: 0.00063
	accuracy_policy_5: 0.9968
	loss_value_5: 0.01264
	loss_reward_5: 0.0039
	loss_policy: 0.00255
	loss_value: 0.13389
	loss_reward: 0.01102
[2025-05-08 12:17:26] nn step 50950, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06622
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01294
	loss_reward_1: 0.00056
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01252
	loss_reward_2: 0.00236
	loss_policy_3: 0.00034
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01222
	loss_reward_3: 0.0018
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01175
	loss_reward_4: 0.00186
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01199
	loss_reward_5: 0.00365
	loss_policy: 0.00219
	loss_value: 0.12764
	loss_reward: 0.01022
[2025-05-08 12:17:32] nn step 51000, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06803
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01325
	loss_reward_1: 0.00057
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99859
	loss_value_2: 0.0128
	loss_reward_2: 0.00246
	loss_policy_3: 0.00037
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01247
	loss_reward_3: 0.00175
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01197
	loss_reward_4: 0.00187
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01215
	loss_reward_5: 0.00384
	loss_policy: 0.00219
	loss_value: 0.13068
	loss_reward: 0.01049
Optimization_Done 51000
[2025-05-08 12:18:49] [command] train weight_iter_51000.pkl 237 256
[2025-05-08 12:18:57] nn step 51050, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06986
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01354
	loss_reward_1: 0.00062
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01308
	loss_reward_2: 0.00238
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99785
	loss_value_3: 0.0127
	loss_reward_3: 0.00175
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99707
	loss_value_4: 0.01206
	loss_reward_4: 0.00188
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99699
	loss_value_5: 0.01221
	loss_reward_5: 0.0037
	loss_policy: 0.00249
	loss_value: 0.13345
	loss_reward: 0.01034
[2025-05-08 12:19:06] nn step 51100, lr: 0.1.
	loss_policy_0: 0.0003
	accuracy_policy_0: 0.99965
	loss_value_0: 0.06735
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99934
	loss_value_1: 0.01313
	loss_reward_1: 0.00052
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01264
	loss_reward_2: 0.00246
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01235
	loss_reward_3: 0.00177
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99648
	loss_value_4: 0.01182
	loss_reward_4: 0.00175
	loss_policy_5: 0.00066
	accuracy_policy_5: 0.99688
	loss_value_5: 0.01202
	loss_reward_5: 0.0038
	loss_policy: 0.0028
	loss_value: 0.12932
	loss_reward: 0.01031
[2025-05-08 12:19:15] nn step 51150, lr: 0.1.
	loss_policy_0: 0.00028
	accuracy_policy_0: 0.99984
	loss_value_0: 0.07068
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01369
	loss_reward_1: 0.00055
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01314
	loss_reward_2: 0.00264
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01283
	loss_reward_3: 0.00185
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99727
	loss_value_4: 0.01217
	loss_reward_4: 0.00194
	loss_policy_5: 0.00057
	accuracy_policy_5: 0.99715
	loss_value_5: 0.01236
	loss_reward_5: 0.00406
	loss_policy: 0.00236
	loss_value: 0.13488
	loss_reward: 0.01105
[2025-05-08 12:19:22] nn step 51200, lr: 0.1.
	loss_policy_0: 0.00018
	accuracy_policy_0: 0.99984
	loss_value_0: 0.07116
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99898
	loss_value_1: 0.01387
	loss_reward_1: 0.00053
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01334
	loss_reward_2: 0.0026
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99781
	loss_value_3: 0.01305
	loss_reward_3: 0.00189
	loss_policy_4: 0.00061
	accuracy_policy_4: 0.99691
	loss_value_4: 0.0124
	loss_reward_4: 0.00193
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01264
	loss_reward_5: 0.00401
	loss_policy: 0.00274
	loss_value: 0.13645
	loss_reward: 0.01095
Optimization_Done 51200
[2025-05-08 12:20:38] [command] train weight_iter_51200.pkl 238 257
[2025-05-08 12:20:45] nn step 51250, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99992
	loss_value_0: 0.07197
	loss_policy_1: 0.0004
	accuracy_policy_1: 0.99863
	loss_value_1: 0.01403
	loss_reward_1: 0.00058
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99785
	loss_value_2: 0.01347
	loss_reward_2: 0.00258
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.99707
	loss_value_3: 0.01307
	loss_reward_3: 0.0018
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99609
	loss_value_4: 0.01247
	loss_reward_4: 0.002
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99605
	loss_value_5: 0.01265
	loss_reward_5: 0.00399
	loss_policy: 0.00309
	loss_value: 0.13766
	loss_reward: 0.01095
[2025-05-08 12:20:54] nn step 51300, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06802
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01329
	loss_reward_1: 0.00048
	loss_policy_2: 0.00059
	accuracy_policy_2: 0.99801
	loss_value_2: 0.0128
	loss_reward_2: 0.00244
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99684
	loss_value_3: 0.01253
	loss_reward_3: 0.00174
	loss_policy_4: 0.00078
	accuracy_policy_4: 0.99555
	loss_value_4: 0.01191
	loss_reward_4: 0.00186
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01214
	loss_reward_5: 0.00384
	loss_policy: 0.00317
	loss_value: 0.1307
	loss_reward: 0.01036
[2025-05-08 12:21:03] nn step 51350, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06669
	loss_policy_1: 0.00048
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01304
	loss_reward_1: 0.0005
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01254
	loss_reward_2: 0.00239
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01221
	loss_reward_3: 0.0018
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01165
	loss_reward_4: 0.00184
	loss_policy_5: 0.00079
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01183
	loss_reward_5: 0.00376
	loss_policy: 0.00295
	loss_value: 0.12796
	loss_reward: 0.01028
[2025-05-08 12:21:11] nn step 51400, lr: 0.1.
	loss_policy_0: 0.00021
	accuracy_policy_0: 0.99984
	loss_value_0: 0.07502
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01486
	loss_reward_1: 0.00066
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01458
	loss_reward_2: 0.00323
	loss_policy_3: 0.00047
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01427
	loss_reward_3: 0.00257
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01364
	loss_reward_4: 0.00266
	loss_policy_5: 0.00069
	accuracy_policy_5: 0.99695
	loss_value_5: 0.01425
	loss_reward_5: 0.00522
	loss_policy: 0.00259
	loss_value: 0.14663
	loss_reward: 0.01434
Optimization_Done 51400
[2025-05-08 12:22:28] [command] train weight_iter_51400.pkl 239 258
[2025-05-08 12:22:37] nn step 51450, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07862
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01498
	loss_reward_1: 0.00105
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99898
	loss_value_2: 0.01482
	loss_reward_2: 0.00277
	loss_policy_3: 0.00038
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01474
	loss_reward_3: 0.00257
	loss_policy_4: 0.00045
	accuracy_policy_4: 0.99789
	loss_value_4: 0.01429
	loss_reward_4: 0.00285
	loss_policy_5: 0.00046
	accuracy_policy_5: 0.99785
	loss_value_5: 0.01482
	loss_reward_5: 0.00374
	loss_policy: 0.00195
	loss_value: 0.15225
	loss_reward: 0.01299
[2025-05-08 12:22:44] nn step 51500, lr: 0.1.
	loss_policy_0: 0.00022
	accuracy_policy_0: 0.9998
	loss_value_0: 0.06943
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99902
	loss_value_1: 0.01369
	loss_reward_1: 0.00057
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01353
	loss_reward_2: 0.00263
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01338
	loss_reward_3: 0.00189
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99715
	loss_value_4: 0.01274
	loss_reward_4: 0.00183
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99676
	loss_value_5: 0.013
	loss_reward_5: 0.0039
	loss_policy: 0.00252
	loss_value: 0.13577
	loss_reward: 0.01081
[2025-05-08 12:22:53] nn step 51550, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.0738
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01453
	loss_reward_1: 0.00055
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01425
	loss_reward_2: 0.00276
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.99824
	loss_value_3: 0.0138
	loss_reward_3: 0.00204
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01308
	loss_reward_4: 0.00201
	loss_policy_5: 0.00061
	accuracy_policy_5: 0.99711
	loss_value_5: 0.01332
	loss_reward_5: 0.00406
	loss_policy: 0.00232
	loss_value: 0.14278
	loss_reward: 0.01144
[2025-05-08 12:23:01] nn step 51600, lr: 0.1.
	loss_policy_0: 0.00026
	accuracy_policy_0: 0.99984
	loss_value_0: 0.06899
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99898
	loss_value_1: 0.01355
	loss_reward_1: 0.00053
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01309
	loss_reward_2: 0.00256
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99754
	loss_value_3: 0.01283
	loss_reward_3: 0.00186
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99656
	loss_value_4: 0.01228
	loss_reward_4: 0.00188
	loss_policy_5: 0.00059
	accuracy_policy_5: 0.99645
	loss_value_5: 0.0125
	loss_reward_5: 0.00384
	loss_policy: 0.00265
	loss_value: 0.13325
	loss_reward: 0.01066
Optimization_Done 51600
[2025-05-08 12:24:14] [command] train weight_iter_51600.pkl 240 259
[2025-05-08 12:24:23] nn step 51650, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.07177
	loss_policy_1: 0.00039
	accuracy_policy_1: 0.99891
	loss_value_1: 0.01406
	loss_reward_1: 0.00065
	loss_policy_2: 0.00049
	accuracy_policy_2: 0.99793
	loss_value_2: 0.01363
	loss_reward_2: 0.00257
	loss_policy_3: 0.00065
	accuracy_policy_3: 0.99645
	loss_value_3: 0.01332
	loss_reward_3: 0.00202
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99586
	loss_value_4: 0.01278
	loss_reward_4: 0.00202
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99617
	loss_value_5: 0.01309
	loss_reward_5: 0.00391
	loss_policy: 0.00303
	loss_value: 0.13866
	loss_reward: 0.01116
[2025-05-08 12:24:30] nn step 51700, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07482
	loss_policy_1: 0.0003
	accuracy_policy_1: 0.99918
	loss_value_1: 0.0147
	loss_reward_1: 0.00052
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01412
	loss_reward_2: 0.00269
	loss_policy_3: 0.0004
	accuracy_policy_3: 0.9984
	loss_value_3: 0.01379
	loss_reward_3: 0.00196
	loss_policy_4: 0.00057
	accuracy_policy_4: 0.9975
	loss_value_4: 0.01314
	loss_reward_4: 0.00196
	loss_policy_5: 0.00062
	accuracy_policy_5: 0.99711
	loss_value_5: 0.01337
	loss_reward_5: 0.00408
	loss_policy: 0.0023
	loss_value: 0.14393
	loss_reward: 0.01121
[2025-05-08 12:24:39] nn step 51750, lr: 0.1.
	loss_policy_0: 0.00033
	accuracy_policy_0: 0.99977
	loss_value_0: 0.07313
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01431
	loss_reward_1: 0.00058
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01377
	loss_reward_2: 0.00271
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01342
	loss_reward_3: 0.0018
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99672
	loss_value_4: 0.0128
	loss_reward_4: 0.00195
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.99688
	loss_value_5: 0.01304
	loss_reward_5: 0.0042
	loss_policy: 0.00275
	loss_value: 0.14046
	loss_reward: 0.01125
[2025-05-08 12:24:47] nn step 51800, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99992
	loss_value_0: 0.07604
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01489
	loss_reward_1: 0.00056
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99809
	loss_value_2: 0.01429
	loss_reward_2: 0.00285
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.9973
	loss_value_3: 0.01391
	loss_reward_3: 0.00198
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.99641
	loss_value_4: 0.01319
	loss_reward_4: 0.00202
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99641
	loss_value_5: 0.01342
	loss_reward_5: 0.00442
	loss_policy: 0.0029
	loss_value: 0.14573
	loss_reward: 0.01184
Optimization_Done 51800
[2025-05-08 12:26:07] [command] train weight_iter_51800.pkl 241 260
[2025-05-08 12:26:17] nn step 51850, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06706
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01317
	loss_reward_1: 0.00049
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01266
	loss_reward_2: 0.00242
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01232
	loss_reward_3: 0.00179
	loss_policy_4: 0.00049
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01172
	loss_reward_4: 0.00183
	loss_policy_5: 0.00049
	accuracy_policy_5: 0.99707
	loss_value_5: 0.01193
	loss_reward_5: 0.00375
	loss_policy: 0.00213
	loss_value: 0.12887
	loss_reward: 0.01029
[2025-05-08 12:26:24] nn step 51900, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06905
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01355
	loss_reward_1: 0.0005
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01298
	loss_reward_2: 0.00253
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99758
	loss_value_3: 0.01265
	loss_reward_3: 0.00186
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99676
	loss_value_4: 0.01195
	loss_reward_4: 0.00186
	loss_policy_5: 0.0007
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01215
	loss_reward_5: 0.00398
	loss_policy: 0.00259
	loss_value: 0.13232
	loss_reward: 0.01073
[2025-05-08 12:26:32] nn step 51950, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07288
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01432
	loss_reward_1: 0.00056
	loss_policy_2: 0.0005
	accuracy_policy_2: 0.99812
	loss_value_2: 0.01385
	loss_reward_2: 0.00276
	loss_policy_3: 0.00065
	accuracy_policy_3: 0.99719
	loss_value_3: 0.01343
	loss_reward_3: 0.00199
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99672
	loss_value_4: 0.0128
	loss_reward_4: 0.00197
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.99613
	loss_value_5: 0.01311
	loss_reward_5: 0.00421
	loss_policy: 0.00296
	loss_value: 0.14039
	loss_reward: 0.0115
[2025-05-08 12:26:41] nn step 52000, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99992
	loss_value_0: 0.0693
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01365
	loss_reward_1: 0.00053
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01312
	loss_reward_2: 0.00261
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99766
	loss_value_3: 0.0128
	loss_reward_3: 0.00187
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.9968
	loss_value_4: 0.01218
	loss_reward_4: 0.00188
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99598
	loss_value_5: 0.01241
	loss_reward_5: 0.00399
	loss_policy: 0.00263
	loss_value: 0.13344
	loss_reward: 0.01087
Optimization_Done 52000
[2025-05-08 12:27:57] [command] train weight_iter_52000.pkl 242 261
[2025-05-08 12:28:07] nn step 52050, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99984
	loss_value_0: 0.0726
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01426
	loss_reward_1: 0.00051
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01373
	loss_reward_2: 0.0027
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01331
	loss_reward_3: 0.00195
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01269
	loss_reward_4: 0.00195
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01294
	loss_reward_5: 0.00421
	loss_policy: 0.0028
	loss_value: 0.13953
	loss_reward: 0.01133
[2025-05-08 12:28:16] nn step 52100, lr: 0.1.
	loss_policy_0: 0.00014
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06853
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99883
	loss_value_1: 0.01341
	loss_reward_1: 0.00046
	loss_policy_2: 0.00047
	accuracy_policy_2: 0.99797
	loss_value_2: 0.01285
	loss_reward_2: 0.00261
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99727
	loss_value_3: 0.01246
	loss_reward_3: 0.00182
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99648
	loss_value_4: 0.01184
	loss_reward_4: 0.00177
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.9957
	loss_value_5: 0.01204
	loss_reward_5: 0.00394
	loss_policy: 0.00311
	loss_value: 0.13112
	loss_reward: 0.01062
[2025-05-08 12:28:23] nn step 52150, lr: 0.1.
	loss_policy_0: 0.00012
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07183
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01411
	loss_reward_1: 0.0005
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01357
	loss_reward_2: 0.00267
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01316
	loss_reward_3: 0.00194
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01239
	loss_reward_4: 0.00202
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.99609
	loss_value_5: 0.01261
	loss_reward_5: 0.0042
	loss_policy: 0.00289
	loss_value: 0.13768
	loss_reward: 0.01133
[2025-05-08 12:28:31] nn step 52200, lr: 0.1.
	loss_policy_0: 0.0002
	accuracy_policy_0: 0.99973
	loss_value_0: 0.06668
	loss_policy_1: 0.00016
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01304
	loss_reward_1: 0.00044
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01254
	loss_reward_2: 0.00251
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99762
	loss_value_3: 0.01216
	loss_reward_3: 0.00182
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.9968
	loss_value_4: 0.01146
	loss_reward_4: 0.00182
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99645
	loss_value_5: 0.01167
	loss_reward_5: 0.00388
	loss_policy: 0.00251
	loss_value: 0.12755
	loss_reward: 0.01046
Optimization_Done 52200
[2025-05-08 12:29:49] [command] train weight_iter_52200.pkl 243 262
[2025-05-08 12:29:58] nn step 52250, lr: 0.1.
	loss_policy_0: 0.00022
	accuracy_policy_0: 0.99988
	loss_value_0: 0.07412
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99906
	loss_value_1: 0.01451
	loss_reward_1: 0.00054
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01401
	loss_reward_2: 0.00276
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.99754
	loss_value_3: 0.01358
	loss_reward_3: 0.00208
	loss_policy_4: 0.00071
	accuracy_policy_4: 0.99629
	loss_value_4: 0.01282
	loss_reward_4: 0.0021
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.99609
	loss_value_5: 0.01308
	loss_reward_5: 0.00419
	loss_policy: 0.00305
	loss_value: 0.14212
	loss_reward: 0.01166
[2025-05-08 12:30:07] nn step 52300, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99988
	loss_value_0: 0.0668
	loss_policy_1: 0.00034
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01312
	loss_reward_1: 0.00046
	loss_policy_2: 0.00049
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01255
	loss_reward_2: 0.00247
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99754
	loss_value_3: 0.01218
	loss_reward_3: 0.0019
	loss_policy_4: 0.00079
	accuracy_policy_4: 0.99637
	loss_value_4: 0.0115
	loss_reward_4: 0.00184
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.99574
	loss_value_5: 0.01169
	loss_reward_5: 0.00384
	loss_policy: 0.00316
	loss_value: 0.12784
	loss_reward: 0.01052
[2025-05-08 12:30:14] nn step 52350, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07256
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01425
	loss_reward_1: 0.0005
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01375
	loss_reward_2: 0.0027
	loss_policy_3: 0.00058
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01333
	loss_reward_3: 0.00195
	loss_policy_4: 0.00076
	accuracy_policy_4: 0.99652
	loss_value_4: 0.01259
	loss_reward_4: 0.002
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.99594
	loss_value_5: 0.01282
	loss_reward_5: 0.00425
	loss_policy: 0.00298
	loss_value: 0.1393
	loss_reward: 0.0114
[2025-05-08 12:30:22] nn step 52400, lr: 0.1.
	loss_policy_0: 0.00012
	accuracy_policy_0: 0.99988
	loss_value_0: 0.07161
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99902
	loss_value_1: 0.01405
	loss_reward_1: 0.00043
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99812
	loss_value_2: 0.01357
	loss_reward_2: 0.00262
	loss_policy_3: 0.00058
	accuracy_policy_3: 0.99742
	loss_value_3: 0.01313
	loss_reward_3: 0.0019
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99641
	loss_value_4: 0.0124
	loss_reward_4: 0.00194
	loss_policy_5: 0.00093
	accuracy_policy_5: 0.99566
	loss_value_5: 0.01256
	loss_reward_5: 0.00417
	loss_policy: 0.00314
	loss_value: 0.13732
	loss_reward: 0.01106
Optimization_Done 52400
[2025-05-08 12:31:39] [command] train weight_iter_52400.pkl 244 263
[2025-05-08 12:31:48] nn step 52450, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06894
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01347
	loss_reward_1: 0.00048
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01292
	loss_reward_2: 0.00257
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01259
	loss_reward_3: 0.00182
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01199
	loss_reward_4: 0.00188
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01209
	loss_reward_5: 0.00396
	loss_policy: 0.00221
	loss_value: 0.132
	loss_reward: 0.0107
[2025-05-08 12:31:57] nn step 52500, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06852
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99891
	loss_value_1: 0.01345
	loss_reward_1: 0.00052
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01294
	loss_reward_2: 0.00256
	loss_policy_3: 0.00058
	accuracy_policy_3: 0.9975
	loss_value_3: 0.01257
	loss_reward_3: 0.00182
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99695
	loss_value_4: 0.01188
	loss_reward_4: 0.0019
	loss_policy_5: 0.0007
	accuracy_policy_5: 0.99625
	loss_value_5: 0.0121
	loss_reward_5: 0.00397
	loss_policy: 0.00271
	loss_value: 0.13146
	loss_reward: 0.01077
[2025-05-08 12:32:04] nn step 52550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06843
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01348
	loss_reward_1: 0.00047
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01299
	loss_reward_2: 0.00249
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01257
	loss_reward_3: 0.00189
	loss_policy_4: 0.00057
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01187
	loss_reward_4: 0.00195
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01201
	loss_reward_5: 0.00397
	loss_policy: 0.00242
	loss_value: 0.13134
	loss_reward: 0.01077
[2025-05-08 12:32:13] nn step 52600, lr: 0.1.
	loss_policy_0: 0.00024
	accuracy_policy_0: 0.99988
	loss_value_0: 0.07553
	loss_policy_1: 0.00037
	accuracy_policy_1: 0.99895
	loss_value_1: 0.01483
	loss_reward_1: 0.00049
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.9984
	loss_value_2: 0.01428
	loss_reward_2: 0.00288
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99734
	loss_value_3: 0.01386
	loss_reward_3: 0.00196
	loss_policy_4: 0.00081
	accuracy_policy_4: 0.9959
	loss_value_4: 0.0131
	loss_reward_4: 0.0021
	loss_policy_5: 0.0007
	accuracy_policy_5: 0.9966
	loss_value_5: 0.01327
	loss_reward_5: 0.00454
	loss_policy: 0.0031
	loss_value: 0.14488
	loss_reward: 0.01196
Optimization_Done 52600
[2025-05-08 12:33:29] [command] train weight_iter_52600.pkl 245 264
[2025-05-08 12:33:38] nn step 52650, lr: 0.1.
	loss_policy_0: 0.00021
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06637
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01287
	loss_reward_1: 0.00046
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99801
	loss_value_2: 0.0124
	loss_reward_2: 0.00244
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.99711
	loss_value_3: 0.01205
	loss_reward_3: 0.00173
	loss_policy_4: 0.00066
	accuracy_policy_4: 0.99633
	loss_value_4: 0.0114
	loss_reward_4: 0.00179
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.99621
	loss_value_5: 0.01159
	loss_reward_5: 0.00385
	loss_policy: 0.00305
	loss_value: 0.12668
	loss_reward: 0.01027
[2025-05-08 12:33:47] nn step 52700, lr: 0.1.
	loss_policy_0: 0.00021
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06928
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99957
	loss_value_1: 0.01358
	loss_reward_1: 0.00048
	loss_policy_2: 0.00036
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01312
	loss_reward_2: 0.00256
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.99805
	loss_value_3: 0.01272
	loss_reward_3: 0.00186
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01201
	loss_reward_4: 0.00191
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.99645
	loss_value_5: 0.0122
	loss_reward_5: 0.00406
	loss_policy: 0.00277
	loss_value: 0.13292
	loss_reward: 0.01088
[2025-05-08 12:33:54] nn step 52750, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.07054
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99898
	loss_value_1: 0.01378
	loss_reward_1: 0.00047
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.99816
	loss_value_2: 0.01334
	loss_reward_2: 0.00274
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.99727
	loss_value_3: 0.01292
	loss_reward_3: 0.00189
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99645
	loss_value_4: 0.01222
	loss_reward_4: 0.00196
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.99586
	loss_value_5: 0.01243
	loss_reward_5: 0.00422
	loss_policy: 0.00312
	loss_value: 0.13523
	loss_reward: 0.01128
[2025-05-08 12:34:02] nn step 52800, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06788
	loss_policy_1: 0.00043
	accuracy_policy_1: 0.99887
	loss_value_1: 0.01332
	loss_reward_1: 0.00043
	loss_policy_2: 0.00046
	accuracy_policy_2: 0.99824
	loss_value_2: 0.01284
	loss_reward_2: 0.00263
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.99719
	loss_value_3: 0.01238
	loss_reward_3: 0.00183
	loss_policy_4: 0.00077
	accuracy_policy_4: 0.99625
	loss_value_4: 0.01171
	loss_reward_4: 0.00196
	loss_policy_5: 0.00091
	accuracy_policy_5: 0.99562
	loss_value_5: 0.01188
	loss_reward_5: 0.00403
	loss_policy: 0.00327
	loss_value: 0.13002
	loss_reward: 0.01087
Optimization_Done 52800
[2025-05-08 12:35:16] [command] train weight_iter_52800.pkl 246 265
[2025-05-08 12:35:24] nn step 52850, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99984
	loss_value_0: 0.07005
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99934
	loss_value_1: 0.01367
	loss_reward_1: 0.00056
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99879
	loss_value_2: 0.01329
	loss_reward_2: 0.00271
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99852
	loss_value_3: 0.01285
	loss_reward_3: 0.00205
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99777
	loss_value_4: 0.0122
	loss_reward_4: 0.00206
	loss_policy_5: 0.00083
	accuracy_policy_5: 0.99703
	loss_value_5: 0.01251
	loss_reward_5: 0.00406
	loss_policy: 0.0028
	loss_value: 0.13457
	loss_reward: 0.01144
[2025-05-08 12:35:33] nn step 52900, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06688
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01313
	loss_reward_1: 0.00045
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99828
	loss_value_2: 0.01266
	loss_reward_2: 0.00249
	loss_policy_3: 0.00042
	accuracy_policy_3: 0.99793
	loss_value_3: 0.0123
	loss_reward_3: 0.00181
	loss_policy_4: 0.00059
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01163
	loss_reward_4: 0.00186
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01182
	loss_reward_5: 0.00387
	loss_policy: 0.00257
	loss_value: 0.12842
	loss_reward: 0.01048
[2025-05-08 12:35:41] nn step 52950, lr: 0.1.
	loss_policy_0: 0.00013
	accuracy_policy_0: 0.99988
	loss_value_0: 0.0696
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99902
	loss_value_1: 0.01363
	loss_reward_1: 0.00049
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01316
	loss_reward_2: 0.00264
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01277
	loss_reward_3: 0.00185
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01205
	loss_reward_4: 0.0019
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.99602
	loss_value_5: 0.01228
	loss_reward_5: 0.00417
	loss_policy: 0.00302
	loss_value: 0.13348
	loss_reward: 0.01105
[2025-05-08 12:35:48] nn step 53000, lr: 0.1.
	loss_policy_0: 0.00027
	accuracy_policy_0: 0.9998
	loss_value_0: 0.07033
	loss_policy_1: 0.00038
	accuracy_policy_1: 0.99879
	loss_value_1: 0.01374
	loss_reward_1: 0.00045
	loss_policy_2: 0.00056
	accuracy_policy_2: 0.99742
	loss_value_2: 0.0133
	loss_reward_2: 0.00272
	loss_policy_3: 0.0008
	accuracy_policy_3: 0.99598
	loss_value_3: 0.01293
	loss_reward_3: 0.00186
	loss_policy_4: 0.00087
	accuracy_policy_4: 0.99555
	loss_value_4: 0.01217
	loss_reward_4: 0.00198
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.9959
	loss_value_5: 0.01241
	loss_reward_5: 0.00429
	loss_policy: 0.00378
	loss_value: 0.13488
	loss_reward: 0.0113
Optimization_Done 53000
[2025-05-08 12:37:06] [command] train weight_iter_53000.pkl 247 266
[2025-05-08 12:37:13] nn step 53050, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06499
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01261
	loss_reward_1: 0.00045
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01217
	loss_reward_2: 0.00236
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01176
	loss_reward_3: 0.00167
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99648
	loss_value_4: 0.01114
	loss_reward_4: 0.0018
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01133
	loss_reward_5: 0.00373
	loss_policy: 0.00298
	loss_value: 0.124
	loss_reward: 0.01001
[2025-05-08 12:37:22] nn step 53100, lr: 0.1.
	loss_policy_0: 0.00018
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06858
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01343
	loss_reward_1: 0.00051
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01295
	loss_reward_2: 0.00267
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99758
	loss_value_3: 0.01256
	loss_reward_3: 0.00184
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.99652
	loss_value_4: 0.01188
	loss_reward_4: 0.00194
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01211
	loss_reward_5: 0.00407
	loss_policy: 0.00296
	loss_value: 0.13152
	loss_reward: 0.01103
[2025-05-08 12:37:31] nn step 53150, lr: 0.1.
	loss_policy_0: 4e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06821
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99902
	loss_value_1: 0.01338
	loss_reward_1: 0.0005
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99832
	loss_value_2: 0.01302
	loss_reward_2: 0.00257
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01264
	loss_reward_3: 0.00183
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99656
	loss_value_4: 0.01197
	loss_reward_4: 0.002
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.99602
	loss_value_5: 0.01216
	loss_reward_5: 0.00405
	loss_policy: 0.00304
	loss_value: 0.13138
	loss_reward: 0.01095
[2025-05-08 12:37:38] nn step 53200, lr: 0.1.
	loss_policy_0: 0.00015
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06615
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.99906
	loss_value_1: 0.01298
	loss_reward_1: 0.00041
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.9982
	loss_value_2: 0.0126
	loss_reward_2: 0.0026
	loss_policy_3: 0.00066
	accuracy_policy_3: 0.99723
	loss_value_3: 0.01222
	loss_reward_3: 0.00175
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01154
	loss_reward_4: 0.0019
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.99602
	loss_value_5: 0.01173
	loss_reward_5: 0.0041
	loss_policy: 0.00318
	loss_value: 0.12722
	loss_reward: 0.01075
Optimization_Done 53200
[2025-05-08 12:38:54] [command] train weight_iter_53200.pkl 248 267
[2025-05-08 12:39:03] nn step 53250, lr: 0.1.
	loss_policy_0: 0.00017
	accuracy_policy_0: 0.99984
	loss_value_0: 0.06613
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01285
	loss_reward_1: 0.00053
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01243
	loss_reward_2: 0.00236
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01205
	loss_reward_3: 0.00186
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.9977
	loss_value_4: 0.01147
	loss_reward_4: 0.00187
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.99691
	loss_value_5: 0.01155
	loss_reward_5: 0.00362
	loss_policy: 0.00287
	loss_value: 0.12647
	loss_reward: 0.01025
[2025-05-08 12:39:10] nn step 53300, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06176
	loss_policy_1: 0.00028
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01209
	loss_reward_1: 0.00042
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01172
	loss_reward_2: 0.00241
	loss_policy_3: 0.00039
	accuracy_policy_3: 0.9982
	loss_value_3: 0.01142
	loss_reward_3: 0.00168
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.9975
	loss_value_4: 0.01081
	loss_reward_4: 0.00176
	loss_policy_5: 0.0006
	accuracy_policy_5: 0.99719
	loss_value_5: 0.01097
	loss_reward_5: 0.00373
	loss_policy: 0.00223
	loss_value: 0.11877
	loss_reward: 0.01
[2025-05-08 12:39:18] nn step 53350, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99992
	loss_value_0: 0.07027
	loss_policy_1: 0.00034
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01378
	loss_reward_1: 0.00043
	loss_policy_2: 0.0004
	accuracy_policy_2: 0.99852
	loss_value_2: 0.01335
	loss_reward_2: 0.00271
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99785
	loss_value_3: 0.01293
	loss_reward_3: 0.00184
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01226
	loss_reward_4: 0.00202
	loss_policy_5: 0.00091
	accuracy_policy_5: 0.99629
	loss_value_5: 0.01249
	loss_reward_5: 0.0042
	loss_policy: 0.00306
	loss_value: 0.13508
	loss_reward: 0.01121
[2025-05-08 12:39:27] nn step 53400, lr: 0.1.
	loss_policy_0: 0.0001
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06695
	loss_policy_1: 0.0004
	accuracy_policy_1: 0.99902
	loss_value_1: 0.01309
	loss_reward_1: 0.00043
	loss_policy_2: 0.00045
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01264
	loss_reward_2: 0.00266
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99785
	loss_value_3: 0.01226
	loss_reward_3: 0.00176
	loss_policy_4: 0.00079
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01151
	loss_reward_4: 0.0019
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.99617
	loss_value_5: 0.0117
	loss_reward_5: 0.00412
	loss_policy: 0.00312
	loss_value: 0.12816
	loss_reward: 0.01086
Optimization_Done 53400
[2025-05-08 12:40:44] [command] train weight_iter_53400.pkl 249 268
[2025-05-08 12:40:53] nn step 53450, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06686
	loss_policy_1: 0.00036
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01304
	loss_reward_1: 0.00045
	loss_policy_2: 0.00034
	accuracy_policy_2: 0.99859
	loss_value_2: 0.01268
	loss_reward_2: 0.00251
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01236
	loss_reward_3: 0.00178
	loss_policy_4: 0.00073
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01166
	loss_reward_4: 0.00191
	loss_policy_5: 0.00094
	accuracy_policy_5: 0.99598
	loss_value_5: 0.01179
	loss_reward_5: 0.00398
	loss_policy: 0.00296
	loss_value: 0.12839
	loss_reward: 0.01062
[2025-05-08 12:41:00] nn step 53500, lr: 0.1.
	loss_policy_0: 6e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06635
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01301
	loss_reward_1: 0.00046
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01267
	loss_reward_2: 0.00249
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99777
	loss_value_3: 0.0123
	loss_reward_3: 0.0017
	loss_policy_4: 0.0007
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01156
	loss_reward_4: 0.00194
	loss_policy_5: 0.00069
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01175
	loss_reward_5: 0.00403
	loss_policy: 0.00267
	loss_value: 0.12763
	loss_reward: 0.01062
[2025-05-08 12:41:09] nn step 53550, lr: 0.1.
	loss_policy_0: 0.00022
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06492
	loss_policy_1: 0.00034
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01272
	loss_reward_1: 0.00042
	loss_policy_2: 0.00044
	accuracy_policy_2: 0.9984
	loss_value_2: 0.01235
	loss_reward_2: 0.00252
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01199
	loss_reward_3: 0.00172
	loss_policy_4: 0.00087
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01129
	loss_reward_4: 0.00189
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.99574
	loss_value_5: 0.01147
	loss_reward_5: 0.00396
	loss_policy: 0.00349
	loss_value: 0.12474
	loss_reward: 0.0105
[2025-05-08 12:41:17] nn step 53600, lr: 0.1.
	loss_policy_0: 0.00019
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06711
	loss_policy_1: 0.00029
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01315
	loss_reward_1: 0.00042
	loss_policy_2: 0.00053
	accuracy_policy_2: 0.99805
	loss_value_2: 0.01283
	loss_reward_2: 0.00257
	loss_policy_3: 0.00064
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01243
	loss_reward_3: 0.00175
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01172
	loss_reward_4: 0.00197
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99625
	loss_value_5: 0.01194
	loss_reward_5: 0.00415
	loss_policy: 0.00329
	loss_value: 0.12918
	loss_reward: 0.01086
Optimization_Done 53600
[2025-05-08 12:42:31] [command] train weight_iter_53600.pkl 250 269
[2025-05-08 12:42:40] nn step 53650, lr: 0.1.
	loss_policy_0: 0.00016
	accuracy_policy_0: 0.99988
	loss_value_0: 0.06379
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01244
	loss_reward_1: 0.00043
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01205
	loss_reward_2: 0.00238
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01175
	loss_reward_3: 0.00164
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01108
	loss_reward_4: 0.00184
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.99668
	loss_value_5: 0.01118
	loss_reward_5: 0.00377
	loss_policy: 0.00266
	loss_value: 0.12229
	loss_reward: 0.01006
[2025-05-08 12:42:47] nn step 53700, lr: 0.1.
	loss_policy_0: 0.00023
	accuracy_policy_0: 0.99977
	loss_value_0: 0.06668
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01304
	loss_reward_1: 0.00043
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01267
	loss_reward_2: 0.00261
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99781
	loss_value_3: 0.01224
	loss_reward_3: 0.00179
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01152
	loss_reward_4: 0.00199
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.99672
	loss_value_5: 0.01174
	loss_reward_5: 0.00405
	loss_policy: 0.00259
	loss_value: 0.12789
	loss_reward: 0.01087
[2025-05-08 12:42:56] nn step 53750, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06243
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01221
	loss_reward_1: 0.00039
	loss_policy_2: 0.00037
	accuracy_policy_2: 0.9984
	loss_value_2: 0.0119
	loss_reward_2: 0.00243
	loss_policy_3: 0.00059
	accuracy_policy_3: 0.99754
	loss_value_3: 0.0115
	loss_reward_3: 0.00163
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99652
	loss_value_4: 0.01086
	loss_reward_4: 0.00185
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.99605
	loss_value_5: 0.01106
	loss_reward_5: 0.00379
	loss_policy: 0.00285
	loss_value: 0.11995
	loss_reward: 0.01009
[2025-05-08 12:43:04] nn step 53800, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06314
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01238
	loss_reward_1: 0.00042
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01204
	loss_reward_2: 0.00245
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01171
	loss_reward_3: 0.00166
	loss_policy_4: 0.00068
	accuracy_policy_4: 0.99684
	loss_value_4: 0.01103
	loss_reward_4: 0.00189
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.99621
	loss_value_5: 0.0112
	loss_reward_5: 0.00389
	loss_policy: 0.00266
	loss_value: 0.1215
	loss_reward: 0.01032
Optimization_Done 53800
[2025-05-08 12:44:21] [command] train weight_iter_53800.pkl 251 270
[2025-05-08 12:44:31] nn step 53850, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99992
	loss_value_0: 0.06314
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01238
	loss_reward_1: 0.0004
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01207
	loss_reward_2: 0.00246
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99801
	loss_value_3: 0.0117
	loss_reward_3: 0.0017
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01105
	loss_reward_4: 0.00186
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.99676
	loss_value_5: 0.01125
	loss_reward_5: 0.00388
	loss_policy: 0.00257
	loss_value: 0.1216
	loss_reward: 0.0103
[2025-05-08 12:44:40] nn step 53900, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06341
	loss_policy_1: 0.00026
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01238
	loss_reward_1: 0.0004
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01207
	loss_reward_2: 0.00247
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99793
	loss_value_3: 0.01172
	loss_reward_3: 0.00165
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01103
	loss_reward_4: 0.00196
	loss_policy_5: 0.00058
	accuracy_policy_5: 0.99711
	loss_value_5: 0.01124
	loss_reward_5: 0.00393
	loss_policy: 0.00233
	loss_value: 0.12184
	loss_reward: 0.01041
[2025-05-08 12:44:46] nn step 53950, lr: 0.1.
	loss_policy_0: 0.00011
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06656
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99938
	loss_value_1: 0.0131
	loss_reward_1: 0.00042
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99883
	loss_value_2: 0.01281
	loss_reward_2: 0.00264
	loss_policy_3: 0.00045
	accuracy_policy_3: 0.99824
	loss_value_3: 0.01246
	loss_reward_3: 0.00195
	loss_policy_4: 0.00064
	accuracy_policy_4: 0.99734
	loss_value_4: 0.01175
	loss_reward_4: 0.00198
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01203
	loss_reward_5: 0.00417
	loss_policy: 0.00266
	loss_value: 0.12871
	loss_reward: 0.01117
[2025-05-08 12:44:55] nn step 54000, lr: 0.1.
	loss_policy_0: 5e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06565
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01283
	loss_reward_1: 0.00041
	loss_policy_2: 0.00048
	accuracy_policy_2: 0.9984
	loss_value_2: 0.01251
	loss_reward_2: 0.00265
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.9975
	loss_value_3: 0.01214
	loss_reward_3: 0.00176
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99664
	loss_value_4: 0.01141
	loss_reward_4: 0.00199
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01161
	loss_reward_5: 0.00411
	loss_policy: 0.00286
	loss_value: 0.12614
	loss_reward: 0.01091
Optimization_Done 54000
[2025-05-08 12:46:09] [command] train weight_iter_54000.pkl 252 271
[2025-05-08 12:46:19] nn step 54050, lr: 0.1.
	loss_policy_0: 9e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.0619
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99969
	loss_value_1: 0.01213
	loss_reward_1: 0.00043
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01179
	loss_reward_2: 0.0024
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01144
	loss_reward_3: 0.00169
	loss_policy_4: 0.00054
	accuracy_policy_4: 0.99766
	loss_value_4: 0.01079
	loss_reward_4: 0.00188
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99691
	loss_value_5: 0.01099
	loss_reward_5: 0.00382
	loss_policy: 0.00236
	loss_value: 0.11903
	loss_reward: 0.01022
[2025-05-08 12:46:28] nn step 54100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06256
	loss_policy_1: 0.00033
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01227
	loss_reward_1: 0.00035
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.9984
	loss_value_2: 0.01194
	loss_reward_2: 0.00248
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99773
	loss_value_3: 0.01154
	loss_reward_3: 0.00176
	loss_policy_4: 0.00066
	accuracy_policy_4: 0.99699
	loss_value_4: 0.01086
	loss_reward_4: 0.00192
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.99652
	loss_value_5: 0.01108
	loss_reward_5: 0.00387
	loss_policy: 0.0027
	loss_value: 0.12026
	loss_reward: 0.01038
[2025-05-08 12:46:34] nn step 54150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06195
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01211
	loss_reward_1: 0.00039
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99875
	loss_value_2: 0.01179
	loss_reward_2: 0.0025
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01144
	loss_reward_3: 0.0016
	loss_policy_4: 0.00053
	accuracy_policy_4: 0.99734
	loss_value_4: 0.01079
	loss_reward_4: 0.00187
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99672
	loss_value_5: 0.01104
	loss_reward_5: 0.0039
	loss_policy: 0.00228
	loss_value: 0.11913
	loss_reward: 0.01026
[2025-05-08 12:46:43] nn step 54200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06316
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01237
	loss_reward_1: 0.00044
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99828
	loss_value_2: 0.01209
	loss_reward_2: 0.00246
	loss_policy_3: 0.00067
	accuracy_policy_3: 0.99738
	loss_value_3: 0.01167
	loss_reward_3: 0.00169
	loss_policy_4: 0.00075
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01098
	loss_reward_4: 0.00197
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01119
	loss_reward_5: 0.00392
	loss_policy: 0.00297
	loss_value: 0.12147
	loss_reward: 0.01047
Optimization_Done 54200
[2025-05-08 12:47:59] [command] train weight_iter_54200.pkl 253 272
[2025-05-08 12:48:09] nn step 54250, lr: 0.1.
	loss_policy_0: 2e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.07378
	loss_policy_1: 0.0004
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01438
	loss_reward_1: 0.00106
	loss_policy_2: 0.00053
	accuracy_policy_2: 0.99836
	loss_value_2: 0.01423
	loss_reward_2: 0.00282
	loss_policy_3: 0.00062
	accuracy_policy_3: 0.99742
	loss_value_3: 0.014
	loss_reward_3: 0.00247
	loss_policy_4: 0.00082
	accuracy_policy_4: 0.99613
	loss_value_4: 0.01333
	loss_reward_4: 0.00291
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01377
	loss_reward_5: 0.00432
	loss_policy: 0.00331
	loss_value: 0.14349
	loss_reward: 0.01358
[2025-05-08 12:48:17] nn step 54300, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06179
	loss_policy_1: 0.0003
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01208
	loss_reward_1: 0.00045
	loss_policy_2: 0.00035
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01187
	loss_reward_2: 0.00249
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99828
	loss_value_3: 0.01156
	loss_reward_3: 0.00168
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99738
	loss_value_4: 0.01102
	loss_reward_4: 0.00192
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.99645
	loss_value_5: 0.01129
	loss_reward_5: 0.00368
	loss_policy: 0.00274
	loss_value: 0.11961
	loss_reward: 0.01022
[2025-05-08 12:48:26] nn step 54350, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.0639
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99941
	loss_value_1: 0.0125
	loss_reward_1: 0.00044
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99891
	loss_value_2: 0.01223
	loss_reward_2: 0.00247
	loss_policy_3: 0.00044
	accuracy_policy_3: 0.99832
	loss_value_3: 0.01192
	loss_reward_3: 0.00167
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.99758
	loss_value_4: 0.01131
	loss_reward_4: 0.00189
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99691
	loss_value_5: 0.0115
	loss_reward_5: 0.00388
	loss_policy: 0.00239
	loss_value: 0.12334
	loss_reward: 0.01035
[2025-05-08 12:48:33] nn step 54400, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06255
	loss_policy_1: 0.00027
	accuracy_policy_1: 0.99934
	loss_value_1: 0.01217
	loss_reward_1: 0.00041
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99863
	loss_value_2: 0.01182
	loss_reward_2: 0.00252
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01147
	loss_reward_3: 0.00161
	loss_policy_4: 0.00063
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01079
	loss_reward_4: 0.00182
	loss_policy_5: 0.00066
	accuracy_policy_5: 0.99699
	loss_value_5: 0.01105
	loss_reward_5: 0.00389
	loss_policy: 0.00244
	loss_value: 0.11985
	loss_reward: 0.01025
Optimization_Done 54400
[2025-05-08 12:49:46] [command] train weight_iter_54400.pkl 254 273
[2025-05-08 12:49:54] nn step 54450, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06726
	loss_policy_1: 0.00035
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01298
	loss_reward_1: 0.00045
	loss_policy_2: 0.0005
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01261
	loss_reward_2: 0.00253
	loss_policy_3: 0.00069
	accuracy_policy_3: 0.99762
	loss_value_3: 0.01223
	loss_reward_3: 0.00166
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.99672
	loss_value_4: 0.01162
	loss_reward_4: 0.00193
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.99586
	loss_value_5: 0.01175
	loss_reward_5: 0.0038
	loss_policy: 0.0034
	loss_value: 0.12845
	loss_reward: 0.01038
[2025-05-08 12:50:03] nn step 54500, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06176
	loss_policy_1: 0.00019
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01197
	loss_reward_1: 0.00044
	loss_policy_2: 0.0003
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01164
	loss_reward_2: 0.00248
	loss_policy_3: 0.00055
	accuracy_policy_3: 0.99797
	loss_value_3: 0.01135
	loss_reward_3: 0.00162
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.9973
	loss_value_4: 0.01069
	loss_reward_4: 0.00185
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.9968
	loss_value_5: 0.01091
	loss_reward_5: 0.00382
	loss_policy: 0.00234
	loss_value: 0.11832
	loss_reward: 0.01021
[2025-05-08 12:50:11] nn step 54550, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06216
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99922
	loss_value_1: 0.01216
	loss_reward_1: 0.00043
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99855
	loss_value_2: 0.0118
	loss_reward_2: 0.00248
	loss_policy_3: 0.00058
	accuracy_policy_3: 0.99766
	loss_value_3: 0.01143
	loss_reward_3: 0.00163
	loss_policy_4: 0.00072
	accuracy_policy_4: 0.99688
	loss_value_4: 0.01077
	loss_reward_4: 0.00189
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99625
	loss_value_5: 0.01101
	loss_reward_5: 0.00388
	loss_policy: 0.00282
	loss_value: 0.11934
	loss_reward: 0.01031
[2025-05-08 12:50:18] nn step 54600, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06217
	loss_policy_1: 0.00023
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01211
	loss_reward_1: 0.00039
	loss_policy_2: 0.00031
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01181
	loss_reward_2: 0.00248
	loss_policy_3: 0.00052
	accuracy_policy_3: 0.99801
	loss_value_3: 0.01144
	loss_reward_3: 0.00163
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99742
	loss_value_4: 0.01074
	loss_reward_4: 0.00194
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.99668
	loss_value_5: 0.01095
	loss_reward_5: 0.00384
	loss_policy: 0.0024
	loss_value: 0.11921
	loss_reward: 0.01029
Optimization_Done 54600
[2025-05-08 12:51:35] [command] train weight_iter_54600.pkl 255 274
[2025-05-08 12:51:43] nn step 54650, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06373
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.9991
	loss_value_1: 0.01241
	loss_reward_1: 0.00044
	loss_policy_2: 0.00043
	accuracy_policy_2: 0.99828
	loss_value_2: 0.01197
	loss_reward_2: 0.00248
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.9975
	loss_value_3: 0.01165
	loss_reward_3: 0.00168
	loss_policy_4: 0.0008
	accuracy_policy_4: 0.99645
	loss_value_4: 0.01103
	loss_reward_4: 0.00192
	loss_policy_5: 0.00105
	accuracy_policy_5: 0.99539
	loss_value_5: 0.0112
	loss_reward_5: 0.00387
	loss_policy: 0.0032
	loss_value: 0.122
	loss_reward: 0.01039
[2025-05-08 12:51:51] nn step 54700, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06073
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99945
	loss_value_1: 0.01179
	loss_reward_1: 0.00041
	loss_policy_2: 0.00027
	accuracy_policy_2: 0.99895
	loss_value_2: 0.01142
	loss_reward_2: 0.00242
	loss_policy_3: 0.00048
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01105
	loss_reward_3: 0.00156
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99723
	loss_value_4: 0.01041
	loss_reward_4: 0.00182
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99648
	loss_value_5: 0.01061
	loss_reward_5: 0.0038
	loss_policy: 0.00231
	loss_value: 0.11601
	loss_reward: 0.01001
[2025-05-08 12:52:00] nn step 54750, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06404
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99918
	loss_value_1: 0.01245
	loss_reward_1: 0.0004
	loss_policy_2: 0.00039
	accuracy_policy_2: 0.99844
	loss_value_2: 0.01211
	loss_reward_2: 0.00263
	loss_policy_3: 0.00051
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01179
	loss_reward_3: 0.00166
	loss_policy_4: 0.00056
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01109
	loss_reward_4: 0.00197
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.99676
	loss_value_5: 0.01131
	loss_reward_5: 0.00409
	loss_policy: 0.00254
	loss_value: 0.12278
	loss_reward: 0.01075
[2025-05-08 12:52:09] nn step 54800, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06035
	loss_policy_1: 0.00031
	accuracy_policy_1: 0.99914
	loss_value_1: 0.01176
	loss_reward_1: 0.00038
	loss_policy_2: 0.00032
	accuracy_policy_2: 0.99855
	loss_value_2: 0.01147
	loss_reward_2: 0.00246
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01108
	loss_reward_3: 0.00158
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99727
	loss_value_4: 0.01044
	loss_reward_4: 0.00183
	loss_policy_5: 0.00069
	accuracy_policy_5: 0.9968
	loss_value_5: 0.01069
	loss_reward_5: 0.00379
	loss_policy: 0.00244
	loss_value: 0.11579
	loss_reward: 0.01005
Optimization_Done 54800
[2025-05-08 12:53:24] [command] train weight_iter_54800.pkl 256 275
[2025-05-08 12:53:33] nn step 54850, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06352
	loss_policy_1: 0.00017
	accuracy_policy_1: 0.99953
	loss_value_1: 0.01237
	loss_reward_1: 0.00041
	loss_policy_2: 0.00042
	accuracy_policy_2: 0.99852
	loss_value_2: 0.012
	loss_reward_2: 0.00261
	loss_policy_3: 0.00057
	accuracy_policy_3: 0.9977
	loss_value_3: 0.01168
	loss_reward_3: 0.00172
	loss_policy_4: 0.00062
	accuracy_policy_4: 0.99715
	loss_value_4: 0.01103
	loss_reward_4: 0.00195
	loss_policy_5: 0.00071
	accuracy_policy_5: 0.99668
	loss_value_5: 0.01124
	loss_reward_5: 0.00398
	loss_policy: 0.00251
	loss_value: 0.12184
	loss_reward: 0.01067
[2025-05-08 12:53:40] nn step 54900, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06045
	loss_policy_1: 0.00024
	accuracy_policy_1: 0.99926
	loss_value_1: 0.01176
	loss_reward_1: 0.00039
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99867
	loss_value_2: 0.01143
	loss_reward_2: 0.00245
	loss_policy_3: 0.00053
	accuracy_policy_3: 0.99789
	loss_value_3: 0.01108
	loss_reward_3: 0.00162
	loss_policy_4: 0.00065
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01042
	loss_reward_4: 0.00188
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.99637
	loss_value_5: 0.01065
	loss_reward_5: 0.00386
	loss_policy: 0.00249
	loss_value: 0.11578
	loss_reward: 0.01019
[2025-05-08 12:53:49] nn step 54950, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06359
	loss_policy_1: 0.0002
	accuracy_policy_1: 0.99953
	loss_value_1: 0.0124
	loss_reward_1: 0.00038
	loss_policy_2: 0.00041
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01199
	loss_reward_2: 0.00265
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99777
	loss_value_3: 0.01159
	loss_reward_3: 0.00171
	loss_policy_4: 0.00069
	accuracy_policy_4: 0.99707
	loss_value_4: 0.01095
	loss_reward_4: 0.00195
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.99641
	loss_value_5: 0.01117
	loss_reward_5: 0.00404
	loss_policy: 0.00284
	loss_value: 0.12168
	loss_reward: 0.01074
[2025-05-08 12:53:57] nn step 55000, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06069
	loss_policy_1: 0.00025
	accuracy_policy_1: 0.99941
	loss_value_1: 0.01182
	loss_reward_1: 0.00036
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99887
	loss_value_2: 0.01146
	loss_reward_2: 0.00253
	loss_policy_3: 0.00054
	accuracy_policy_3: 0.99809
	loss_value_3: 0.01108
	loss_reward_3: 0.00164
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99754
	loss_value_4: 0.01045
	loss_reward_4: 0.00184
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.9968
	loss_value_5: 0.01068
	loss_reward_5: 0.00394
	loss_policy: 0.00241
	loss_value: 0.11618
	loss_reward: 0.01031
Optimization_Done 55000
[2025-05-08 12:55:13] [command] train weight_iter_55000.pkl 257 276
[2025-05-08 12:55:22] nn step 55050, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.05908
	loss_policy_1: 0.00022
	accuracy_policy_1: 0.99938
	loss_value_1: 0.01151
	loss_reward_1: 0.0004
	loss_policy_2: 0.00029
	accuracy_policy_2: 0.99871
	loss_value_2: 0.01113
	loss_reward_2: 0.00251
	loss_policy_3: 0.00049
	accuracy_policy_3: 0.99793
	loss_value_3: 0.0108
	loss_reward_3: 0.00156
	loss_policy_4: 0.00067
	accuracy_policy_4: 0.99691
	loss_value_4: 0.01017
	loss_reward_4: 0.00187
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99617
	loss_value_5: 0.01037
	loss_reward_5: 0.00388
	loss_policy: 0.00244
	loss_value: 0.11306
	loss_reward: 0.01022
[2025-05-08 12:55:29] nn step 55100, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06095
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99949
	loss_value_1: 0.01193
	loss_reward_1: 0.00042
	loss_policy_2: 0.00033
	accuracy_policy_2: 0.99871
	loss_value_2: 0.0116
	loss_reward_2: 0.00252
	loss_policy_3: 0.00043
	accuracy_policy_3: 0.99816
	loss_value_3: 0.01125
	loss_reward_3: 0.00169
	loss_policy_4: 0.00052
	accuracy_policy_4: 0.99746
	loss_value_4: 0.01065
	loss_reward_4: 0.00193
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.99648
	loss_value_5: 0.01088
	loss_reward_5: 0.00389
	loss_policy: 0.00224
	loss_value: 0.11726
	loss_reward: 0.01045
[2025-05-08 12:55:38] nn step 55150, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06189
	loss_policy_1: 0.00021
	accuracy_policy_1: 0.9993
	loss_value_1: 0.01203
	loss_reward_1: 0.00039
	loss_policy_2: 0.00038
	accuracy_policy_2: 0.99848
	loss_value_2: 0.01173
	loss_reward_2: 0.00262
	loss_policy_3: 0.00056
	accuracy_policy_3: 0.99773
	loss_value_3: 0.01132
	loss_reward_3: 0.0017
	loss_policy_4: 0.0006
	accuracy_policy_4: 0.99719
	loss_value_4: 0.01068
	loss_reward_4: 0.00197
	loss_policy_5: 0.00067
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01096
	loss_reward_5: 0.00399
	loss_policy: 0.00243
	loss_value: 0.11861
	loss_reward: 0.01065
[2025-05-08 12:55:47] nn step 55200, lr: 0.1.
	loss_policy_0: 1e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.05715
	loss_policy_1: 0.00018
	accuracy_policy_1: 0.99953
	loss_value_1: 0.0111
	loss_reward_1: 0.00036
	loss_policy_2: 0.00022
	accuracy_policy_2: 0.99914
	loss_value_2: 0.01081
	loss_reward_2: 0.00241
	loss_policy_3: 0.00027
	accuracy_policy_3: 0.99883
	loss_value_3: 0.01046
	loss_reward_3: 0.00155
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99797
	loss_value_4: 0.00989
	loss_reward_4: 0.00173
	loss_policy_5: 0.00052
	accuracy_policy_5: 0.99746
	loss_value_5: 0.01009
	loss_reward_5: 0.00373
	loss_policy: 0.00166
	loss_value: 0.1095
	loss_reward: 0.00979
Optimization_Done 55200
[2025-05-08 12:57:02] [command] train weight_iter_55200.pkl 258 277
[2025-05-08 12:57:12] nn step 55250, lr: 0.1.
	loss_policy_0: 0.00146
	accuracy_policy_0: 0.99816
	loss_value_0: 0.09294
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99859
	loss_value_1: 0.0181
	loss_reward_1: 0.00166
	loss_policy_2: 0.00049
	accuracy_policy_2: 0.99789
	loss_value_2: 0.0185
	loss_reward_2: 0.00369
	loss_policy_3: 0.0005
	accuracy_policy_3: 0.9973
	loss_value_3: 0.01848
	loss_reward_3: 0.00334
	loss_policy_4: 0.00046
	accuracy_policy_4: 0.99801
	loss_value_4: 0.01824
	loss_reward_4: 0.00321
	loss_policy_5: 0.00068
	accuracy_policy_5: 0.99719
	loss_value_5: 0.01891
	loss_reward_5: 0.00466
	loss_policy: 0.0039
	loss_value: 0.18518
	loss_reward: 0.01656
[2025-05-08 12:57:20] nn step 55300, lr: 0.1.
	loss_policy_0: 0.00027
	accuracy_policy_0: 1.0
	loss_value_0: 0.06844
	loss_policy_1: 0.00032
	accuracy_policy_1: 0.99766
	loss_value_1: 0.01318
	loss_reward_1: 0.00119
	loss_policy_2: 0.00047
	accuracy_policy_2: 0.99742
	loss_value_2: 0.01324
	loss_reward_2: 0.00339
	loss_policy_3: 0.0006
	accuracy_policy_3: 0.99609
	loss_value_3: 0.01298
	loss_reward_3: 0.00331
	loss_policy_4: 0.00058
	accuracy_policy_4: 0.99652
	loss_value_4: 0.01261
	loss_reward_4: 0.00302
	loss_policy_5: 0.00066
	accuracy_policy_5: 0.99742
	loss_value_5: 0.01323
	loss_reward_5: 0.00472
	loss_policy: 0.00291
	loss_value: 0.13368
	loss_reward: 0.01564
[2025-05-08 12:57:27] nn step 55350, lr: 0.1.
	loss_policy_0: 7e-05
	accuracy_policy_0: 1.0
	loss_value_0: 0.06548
	loss_policy_1: 0.00051
	accuracy_policy_1: 0.99711
	loss_value_1: 0.01283
	loss_reward_1: 0.00084
	loss_policy_2: 0.00064
	accuracy_policy_2: 0.99684
	loss_value_2: 0.01279
	loss_reward_2: 0.00314
	loss_policy_3: 0.00079
	accuracy_policy_3: 0.99578
	loss_value_3: 0.01248
	loss_reward_3: 0.00272
	loss_policy_4: 0.00074
	accuracy_policy_4: 0.99605
	loss_value_4: 0.01211
	loss_reward_4: 0.00252
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.99664
	loss_value_5: 0.01228
	loss_reward_5: 0.00425
	loss_policy: 0.00366
	loss_value: 0.12797
	loss_reward: 0.01347
[2025-05-08 12:57:36] nn step 55400, lr: 0.1.
	loss_policy_0: 8e-05
	accuracy_policy_0: 0.99996
	loss_value_0: 0.06432
	loss_policy_1: 0.00049
	accuracy_policy_1: 0.99828
	loss_value_1: 0.01275
	loss_reward_1: 0.00068
	loss_policy_2: 0.00052
	accuracy_policy_2: 0.99746
	loss_value_2: 0.01256
	loss_reward_2: 0.00306
	loss_policy_3: 0.00061
	accuracy_policy_3: 0.99746
	loss_value_3: 0.01222
	loss_reward_3: 0.00236
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.99609
	loss_value_4: 0.01169
	loss_reward_4: 0.00238
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.99648
	loss_value_5: 0.0121
	loss_reward_5: 0.0042
	loss_policy: 0.00332
	loss_value: 0.12564
	loss_reward: 0.01267
Optimization_Done 55400
[2025-05-08 12:58:54] [command] train weight_iter_55400.pkl 259 278
[2025-05-08 12:59:04] nn step 55450, lr: 0.1.
	loss_policy_0: 0.00484
	accuracy_policy_0: 0.97395
	loss_value_0: 0.07503
	loss_policy_1: 0.00113
	accuracy_policy_1: 0.97363
	loss_value_1: 0.0146
	loss_reward_1: 0.0029
	loss_policy_2: 0.00144
	accuracy_policy_2: 0.97426
	loss_value_2: 0.01453
	loss_reward_2: 0.00425
	loss_policy_3: 0.00132
	accuracy_policy_3: 0.9748
	loss_value_3: 0.01449
	loss_reward_3: 0.00365
	loss_policy_4: 0.00144
	accuracy_policy_4: 0.97441
	loss_value_4: 0.01443
	loss_reward_4: 0.00412
	loss_policy_5: 0.00159
	accuracy_policy_5: 0.97488
	loss_value_5: 0.01498
	loss_reward_5: 0.00549
	loss_policy: 0.01176
	loss_value: 0.14806
	loss_reward: 0.02041
[2025-05-08 12:59:13] nn step 55500, lr: 0.1.
	loss_policy_0: 0.0022
	accuracy_policy_0: 0.98668
	loss_value_0: 0.07377
	loss_policy_1: 0.00103
	accuracy_policy_1: 0.98238
	loss_value_1: 0.01461
	loss_reward_1: 0.00212
	loss_policy_2: 0.00112
	accuracy_policy_2: 0.98523
	loss_value_2: 0.01455
	loss_reward_2: 0.00399
	loss_policy_3: 0.00113
	accuracy_policy_3: 0.98285
	loss_value_3: 0.01419
	loss_reward_3: 0.00328
	loss_policy_4: 0.0012
	accuracy_policy_4: 0.9823
	loss_value_4: 0.01381
	loss_reward_4: 0.00362
	loss_policy_5: 0.0014
	accuracy_policy_5: 0.97902
	loss_value_5: 0.01433
	loss_reward_5: 0.00543
	loss_policy: 0.00809
	loss_value: 0.14526
	loss_reward: 0.01845
[2025-05-08 12:59:20] nn step 55550, lr: 0.1.
	loss_policy_0: 0.0017
	accuracy_policy_0: 0.9898
	loss_value_0: 0.06903
	loss_policy_1: 0.00075
	accuracy_policy_1: 0.98637
	loss_value_1: 0.01375
	loss_reward_1: 0.00191
	loss_policy_2: 0.00095
	accuracy_policy_2: 0.98719
	loss_value_2: 0.01362
	loss_reward_2: 0.00355
	loss_policy_3: 0.00098
	accuracy_policy_3: 0.98582
	loss_value_3: 0.01333
	loss_reward_3: 0.00288
	loss_policy_4: 0.00104
	accuracy_policy_4: 0.98645
	loss_value_4: 0.01302
	loss_reward_4: 0.00309
	loss_policy_5: 0.00135
	accuracy_policy_5: 0.98379
	loss_value_5: 0.01342
	loss_reward_5: 0.00484
	loss_policy: 0.00677
	loss_value: 0.13617
	loss_reward: 0.01627
[2025-05-08 12:59:28] nn step 55600, lr: 0.1.
	loss_policy_0: 0.00166
	accuracy_policy_0: 0.99055
	loss_value_0: 0.06755
	loss_policy_1: 0.0008
	accuracy_policy_1: 0.9825
	loss_value_1: 0.01336
	loss_reward_1: 0.00187
	loss_policy_2: 0.00095
	accuracy_policy_2: 0.98574
	loss_value_2: 0.01334
	loss_reward_2: 0.00357
	loss_policy_3: 0.00088
	accuracy_policy_3: 0.98539
	loss_value_3: 0.01314
	loss_reward_3: 0.00269
	loss_policy_4: 0.00113
	accuracy_policy_4: 0.98555
	loss_value_4: 0.0128
	loss_reward_4: 0.00323
	loss_policy_5: 0.00112
	accuracy_policy_5: 0.98402
	loss_value_5: 0.01316
	loss_reward_5: 0.00493
	loss_policy: 0.00653
	loss_value: 0.13336
	loss_reward: 0.01629
Optimization_Done 55600
[2025-05-08 13:00:45] [command] train weight_iter_55600.pkl 260 279
[2025-05-08 13:00:55] nn step 55650, lr: 0.1.
	loss_policy_0: 0.00496
	accuracy_policy_0: 0.97125
	loss_value_0: 0.08083
	loss_policy_1: 0.00138
	accuracy_policy_1: 0.97551
	loss_value_1: 0.01648
	loss_reward_1: 0.00282
	loss_policy_2: 0.00142
	accuracy_policy_2: 0.97695
	loss_value_2: 0.01641
	loss_reward_2: 0.00403
	loss_policy_3: 0.00166
	accuracy_policy_3: 0.97375
	loss_value_3: 0.01671
	loss_reward_3: 0.00343
	loss_policy_4: 0.00172
	accuracy_policy_4: 0.97012
	loss_value_4: 0.01652
	loss_reward_4: 0.00373
	loss_policy_5: 0.00176
	accuracy_policy_5: 0.96629
	loss_value_5: 0.01675
	loss_reward_5: 0.00514
	loss_policy: 0.01289
	loss_value: 0.16369
	loss_reward: 0.01915
[2025-05-08 13:01:03] nn step 55700, lr: 0.1.
	loss_policy_0: 0.00276
	accuracy_policy_0: 0.98488
	loss_value_0: 0.07774
	loss_policy_1: 0.00097
	accuracy_policy_1: 0.98383
	loss_value_1: 0.01571
	loss_reward_1: 0.00219
	loss_policy_2: 0.00114
	accuracy_policy_2: 0.98297
	loss_value_2: 0.01569
	loss_reward_2: 0.0035
	loss_policy_3: 0.00129
	accuracy_policy_3: 0.9798
	loss_value_3: 0.01572
	loss_reward_3: 0.00287
	loss_policy_4: 0.00134
	accuracy_policy_4: 0.97801
	loss_value_4: 0.01552
	loss_reward_4: 0.00309
	loss_policy_5: 0.00156
	accuracy_policy_5: 0.97426
	loss_value_5: 0.01577
	loss_reward_5: 0.00432
	loss_policy: 0.00907
	loss_value: 0.15615
	loss_reward: 0.01597
[2025-05-08 13:01:10] nn step 55750, lr: 0.1.
	loss_policy_0: 0.00249
	accuracy_policy_0: 0.98551
	loss_value_0: 0.07611
	loss_policy_1: 0.00097
	accuracy_policy_1: 0.98191
	loss_value_1: 0.01549
	loss_reward_1: 0.00187
	loss_policy_2: 0.00115
	accuracy_policy_2: 0.98066
	loss_value_2: 0.01559
	loss_reward_2: 0.00341
	loss_policy_3: 0.00116
	accuracy_policy_3: 0.97961
	loss_value_3: 0.01559
	loss_reward_3: 0.0029
	loss_policy_4: 0.00146
	accuracy_policy_4: 0.97547
	loss_value_4: 0.0154
	loss_reward_4: 0.00314
	loss_policy_5: 0.00152
	accuracy_policy_5: 0.97242
	loss_value_5: 0.01557
	loss_reward_5: 0.00445
	loss_policy: 0.00875
	loss_value: 0.15376
	loss_reward: 0.01577
[2025-05-08 13:01:19] nn step 55800, lr: 0.1.
	loss_policy_0: 0.00223
	accuracy_policy_0: 0.98508
	loss_value_0: 0.07914
	loss_policy_1: 0.00085
	accuracy_policy_1: 0.98164
	loss_value_1: 0.01597
	loss_reward_1: 0.00179
	loss_policy_2: 0.00117
	accuracy_policy_2: 0.98242
	loss_value_2: 0.01588
	loss_reward_2: 0.00354
	loss_policy_3: 0.0013
	accuracy_policy_3: 0.98062
	loss_value_3: 0.01591
	loss_reward_3: 0.00261
	loss_policy_4: 0.00138
	accuracy_policy_4: 0.97863
	loss_value_4: 0.0157
	loss_reward_4: 0.00297
	loss_policy_5: 0.00155
	accuracy_policy_5: 0.9743
	loss_value_5: 0.01599
	loss_reward_5: 0.00449
	loss_policy: 0.00849
	loss_value: 0.1586
	loss_reward: 0.0154
Optimization_Done 55800
[2025-05-08 13:02:35] [command] train weight_iter_55800.pkl 261 280
[2025-05-08 13:02:45] nn step 55850, lr: 0.1.
	loss_policy_0: 0.00236
	accuracy_policy_0: 0.98562
	loss_value_0: 0.08606
	loss_policy_1: 0.00098
	accuracy_policy_1: 0.98078
	loss_value_1: 0.01752
	loss_reward_1: 0.00165
	loss_policy_2: 0.00121
	accuracy_policy_2: 0.97996
	loss_value_2: 0.01748
	loss_reward_2: 0.00314
	loss_policy_3: 0.00144
	accuracy_policy_3: 0.97602
	loss_value_3: 0.01751
	loss_reward_3: 0.0025
	loss_policy_4: 0.00155
	accuracy_policy_4: 0.97203
	loss_value_4: 0.01746
	loss_reward_4: 0.00277
	loss_policy_5: 0.00182
	accuracy_policy_5: 0.96656
	loss_value_5: 0.01775
	loss_reward_5: 0.00403
	loss_policy: 0.00936
	loss_value: 0.17378
	loss_reward: 0.01408
[2025-05-08 13:02:54] nn step 55900, lr: 0.1.
	loss_policy_0: 0.00277
	accuracy_policy_0: 0.98352
	loss_value_0: 0.08883
	loss_policy_1: 0.00105
	accuracy_policy_1: 0.98195
	loss_value_1: 0.01805
	loss_reward_1: 0.00177
	loss_policy_2: 0.0012
	accuracy_policy_2: 0.97938
	loss_value_2: 0.01812
	loss_reward_2: 0.00362
	loss_policy_3: 0.00124
	accuracy_policy_3: 0.97879
	loss_value_3: 0.01826
	loss_reward_3: 0.00319
	loss_policy_4: 0.00133
	accuracy_policy_4: 0.97711
	loss_value_4: 0.01806
	loss_reward_4: 0.00331
	loss_policy_5: 0.00172
	accuracy_policy_5: 0.97203
	loss_value_5: 0.01856
	loss_reward_5: 0.00463
	loss_policy: 0.0093
	loss_value: 0.17987
	loss_reward: 0.01652
[2025-05-08 13:03:01] nn step 55950, lr: 0.1.
	loss_policy_0: 0.00215
	accuracy_policy_0: 0.98531
	loss_value_0: 0.08159
	loss_policy_1: 0.00092
	accuracy_policy_1: 0.98297
	loss_value_1: 0.01657
	loss_reward_1: 0.00166
	loss_policy_2: 0.00105
	accuracy_policy_2: 0.98176
	loss_value_2: 0.01659
	loss_reward_2: 0.0032
	loss_policy_3: 0.00115
	accuracy_policy_3: 0.98254
	loss_value_3: 0.01667
	loss_reward_3: 0.00269
	loss_policy_4: 0.00119
	accuracy_policy_4: 0.9823
	loss_value_4: 0.01659
	loss_reward_4: 0.00302
	loss_policy_5: 0.00153
	accuracy_policy_5: 0.97844
	loss_value_5: 0.01697
	loss_reward_5: 0.00434
	loss_policy: 0.00799
	loss_value: 0.16498
	loss_reward: 0.01491
[2025-05-08 13:03:09] nn step 56000, lr: 0.1.
	loss_policy_0: 0.00191
	accuracy_policy_0: 0.98637
	loss_value_0: 0.07768
	loss_policy_1: 0.00076
	accuracy_policy_1: 0.9834
	loss_value_1: 0.0157
	loss_reward_1: 0.00144
	loss_policy_2: 0.00092
	accuracy_policy_2: 0.985
	loss_value_2: 0.01569
	loss_reward_2: 0.003
	loss_policy_3: 0.00103
	accuracy_policy_3: 0.98547
	loss_value_3: 0.01578
	loss_reward_3: 0.00244
	loss_policy_4: 0.00116
	accuracy_policy_4: 0.98559
	loss_value_4: 0.01569
	loss_reward_4: 0.00271
	loss_policy_5: 0.00118
	accuracy_policy_5: 0.98199
	loss_value_5: 0.01611
	loss_reward_5: 0.00411
	loss_policy: 0.00695
	loss_value: 0.15665
	loss_reward: 0.01369
Optimization_Done 56000
[2025-05-08 13:04:25] [command] train weight_iter_56000.pkl 262 281
[2025-05-08 13:04:34] nn step 56050, lr: 0.1.
	loss_policy_0: 0.00198
	accuracy_policy_0: 0.98441
	loss_value_0: 0.07521
	loss_policy_1: 0.00073
	accuracy_policy_1: 0.98398
	loss_value_1: 0.0152
	loss_reward_1: 0.00181
	loss_policy_2: 0.00083
	accuracy_policy_2: 0.98266
	loss_value_2: 0.01522
	loss_reward_2: 0.00306
	loss_policy_3: 0.00095
	accuracy_policy_3: 0.98215
	loss_value_3: 0.01524
	loss_reward_3: 0.00274
	loss_policy_4: 0.00109
	accuracy_policy_4: 0.98414
	loss_value_4: 0.0152
	loss_reward_4: 0.00277
	loss_policy_5: 0.00114
	accuracy_policy_5: 0.98234
	loss_value_5: 0.01548
	loss_reward_5: 0.00382
	loss_policy: 0.00672
	loss_value: 0.15156
	loss_reward: 0.0142
[2025-05-08 13:04:43] nn step 56100, lr: 0.1.
	loss_policy_0: 0.00234
	accuracy_policy_0: 0.98375
	loss_value_0: 0.08159
	loss_policy_1: 0.00075
	accuracy_policy_1: 0.98258
	loss_value_1: 0.01659
	loss_reward_1: 0.0021
	loss_policy_2: 0.00088
	accuracy_policy_2: 0.98453
	loss_value_2: 0.0167
	loss_reward_2: 0.00352
	loss_policy_3: 0.00107
	accuracy_policy_3: 0.98352
	loss_value_3: 0.01668
	loss_reward_3: 0.0029
	loss_policy_4: 0.00104
	accuracy_policy_4: 0.98586
	loss_value_4: 0.01648
	loss_reward_4: 0.00311
	loss_policy_5: 0.0011
	accuracy_policy_5: 0.98609
	loss_value_5: 0.01706
	loss_reward_5: 0.0045
	loss_policy: 0.00718
	loss_value: 0.16509
	loss_reward: 0.01613
[2025-05-08 13:04:51] nn step 56150, lr: 0.1.
	loss_policy_0: 0.00246
	accuracy_policy_0: 0.98223
	loss_value_0: 0.08057
	loss_policy_1: 0.00073
	accuracy_policy_1: 0.98172
	loss_value_1: 0.01632
	loss_reward_1: 0.00195
	loss_policy_2: 0.00075
	accuracy_policy_2: 0.98277
	loss_value_2: 0.01643
	loss_reward_2: 0.00357
	loss_policy_3: 0.00098
	accuracy_policy_3: 0.98258
	loss_value_3: 0.01627
	loss_reward_3: 0.00312
	loss_policy_4: 0.00096
	accuracy_policy_4: 0.98574
	loss_value_4: 0.01624
	loss_reward_4: 0.00325
	loss_policy_5: 0.00111
	accuracy_policy_5: 0.98398
	loss_value_5: 0.01666
	loss_reward_5: 0.0045
	loss_policy: 0.007
	loss_value: 0.1625
	loss_reward: 0.0164
[2025-05-08 13:04:58] nn step 56200, lr: 0.1.
	loss_policy_0: 0.00251
	accuracy_policy_0: 0.98148
	loss_value_0: 0.08348
	loss_policy_1: 0.00083
	accuracy_policy_1: 0.98121
	loss_value_1: 0.01688
	loss_reward_1: 0.00179
	loss_policy_2: 0.00102
	accuracy_policy_2: 0.98359
	loss_value_2: 0.01702
	loss_reward_2: 0.0035
	loss_policy_3: 0.00103
	accuracy_policy_3: 0.98488
	loss_value_3: 0.01689
	loss_reward_3: 0.00304
	loss_policy_4: 0.00107
	accuracy_policy_4: 0.98488
	loss_value_4: 0.01674
	loss_reward_4: 0.0031
	loss_policy_5: 0.00106
	accuracy_policy_5: 0.98668
	loss_value_5: 0.01722
	loss_reward_5: 0.00449
	loss_policy: 0.00751
	loss_value: 0.16822
	loss_reward: 0.01592
Optimization_Done 56200
[2025-05-08 13:06:10] [command] train weight_iter_56200.pkl 263 282
[2025-05-08 13:06:18] nn step 56250, lr: 0.1.
	loss_policy_0: 0.00281
	accuracy_policy_0: 0.97875
	loss_value_0: 0.08376
	loss_policy_1: 0.0008
	accuracy_policy_1: 0.97977
	loss_value_1: 0.01696
	loss_reward_1: 0.002
	loss_policy_2: 0.00089
	accuracy_policy_2: 0.98062
	loss_value_2: 0.01703
	loss_reward_2: 0.00337
	loss_policy_3: 0.00089
	accuracy_policy_3: 0.98293
	loss_value_3: 0.01687
	loss_reward_3: 0.00322
	loss_policy_4: 0.0009
	accuracy_policy_4: 0.98594
	loss_value_4: 0.01678
	loss_reward_4: 0.0031
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.9877
	loss_value_5: 0.01718
	loss_reward_5: 0.00427
	loss_policy: 0.00727
	loss_value: 0.16857
	loss_reward: 0.01596
[2025-05-08 13:06:27] nn step 56300, lr: 0.1.
	loss_policy_0: 0.00249
	accuracy_policy_0: 0.97855
	loss_value_0: 0.08188
	loss_policy_1: 0.0007
	accuracy_policy_1: 0.97996
	loss_value_1: 0.01641
	loss_reward_1: 0.00196
	loss_policy_2: 0.00079
	accuracy_policy_2: 0.9834
	loss_value_2: 0.01643
	loss_reward_2: 0.00335
	loss_policy_3: 0.00091
	accuracy_policy_3: 0.98379
	loss_value_3: 0.01632
	loss_reward_3: 0.003
	loss_policy_4: 0.00091
	accuracy_policy_4: 0.98664
	loss_value_4: 0.01626
	loss_reward_4: 0.00309
	loss_policy_5: 0.00107
	accuracy_policy_5: 0.98621
	loss_value_5: 0.0167
	loss_reward_5: 0.00416
	loss_policy: 0.00687
	loss_value: 0.16399
	loss_reward: 0.01557
[2025-05-08 13:06:36] nn step 56350, lr: 0.1.
	loss_policy_0: 0.00251
	accuracy_policy_0: 0.97754
	loss_value_0: 0.08008
	loss_policy_1: 0.00085
	accuracy_policy_1: 0.97988
	loss_value_1: 0.01609
	loss_reward_1: 0.00179
	loss_policy_2: 0.00101
	accuracy_policy_2: 0.97973
	loss_value_2: 0.01613
	loss_reward_2: 0.00326
	loss_policy_3: 0.00109
	accuracy_policy_3: 0.98207
	loss_value_3: 0.01588
	loss_reward_3: 0.00273
	loss_policy_4: 0.00105
	accuracy_policy_4: 0.98645
	loss_value_4: 0.01581
	loss_reward_4: 0.00286
	loss_policy_5: 0.00121
	accuracy_policy_5: 0.98504
	loss_value_5: 0.0163
	loss_reward_5: 0.00409
	loss_policy: 0.00772
	loss_value: 0.1603
	loss_reward: 0.01473
[2025-05-08 13:06:43] nn step 56400, lr: 0.1.
	loss_policy_0: 0.00251
	accuracy_policy_0: 0.97871
	loss_value_0: 0.07791
	loss_policy_1: 0.00072
	accuracy_policy_1: 0.9791
	loss_value_1: 0.01559
	loss_reward_1: 0.00168
	loss_policy_2: 0.00076
	accuracy_policy_2: 0.9807
	loss_value_2: 0.01548
	loss_reward_2: 0.00296
	loss_policy_3: 0.00086
	accuracy_policy_3: 0.98039
	loss_value_3: 0.01541
	loss_reward_3: 0.00257
	loss_policy_4: 0.00092
	accuracy_policy_4: 0.9859
	loss_value_4: 0.01526
	loss_reward_4: 0.00265
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98773
	loss_value_5: 0.0157
	loss_reward_5: 0.00384
	loss_policy: 0.00677
	loss_value: 0.15535
	loss_reward: 0.0137
Optimization_Done 56400
[2025-05-08 13:08:02] [command] train weight_iter_56400.pkl 264 283
[2025-05-08 13:08:10] nn step 56450, lr: 0.1.
	loss_policy_0: 0.00321
	accuracy_policy_0: 0.97297
	loss_value_0: 0.0814
	loss_policy_1: 0.00096
	accuracy_policy_1: 0.97613
	loss_value_1: 0.01638
	loss_reward_1: 0.00189
	loss_policy_2: 0.00103
	accuracy_policy_2: 0.97789
	loss_value_2: 0.01642
	loss_reward_2: 0.00318
	loss_policy_3: 0.00106
	accuracy_policy_3: 0.97957
	loss_value_3: 0.01628
	loss_reward_3: 0.00284
	loss_policy_4: 0.00099
	accuracy_policy_4: 0.98395
	loss_value_4: 0.01629
	loss_reward_4: 0.00297
	loss_policy_5: 0.001
	accuracy_policy_5: 0.98512
	loss_value_5: 0.0166
	loss_reward_5: 0.00402
	loss_policy: 0.00824
	loss_value: 0.16337
	loss_reward: 0.01489
[2025-05-08 13:08:19] nn step 56500, lr: 0.1.
	loss_policy_0: 0.00316
	accuracy_policy_0: 0.96891
	loss_value_0: 0.0804
	loss_policy_1: 0.00086
	accuracy_policy_1: 0.97578
	loss_value_1: 0.01624
	loss_reward_1: 0.00179
	loss_policy_2: 0.00098
	accuracy_policy_2: 0.97988
	loss_value_2: 0.01624
	loss_reward_2: 0.00303
	loss_policy_3: 0.00111
	accuracy_policy_3: 0.97723
	loss_value_3: 0.0161
	loss_reward_3: 0.00266
	loss_policy_4: 0.00101
	accuracy_policy_4: 0.98453
	loss_value_4: 0.01593
	loss_reward_4: 0.00279
	loss_policy_5: 0.00104
	accuracy_policy_5: 0.98609
	loss_value_5: 0.01619
	loss_reward_5: 0.00382
	loss_policy: 0.00817
	loss_value: 0.16112
	loss_reward: 0.01409
[2025-05-08 13:08:27] nn step 56550, lr: 0.1.
	loss_policy_0: 0.0034
	accuracy_policy_0: 0.96828
	loss_value_0: 0.08382
	loss_policy_1: 0.00092
	accuracy_policy_1: 0.97285
	loss_value_1: 0.01677
	loss_reward_1: 0.0017
	loss_policy_2: 0.00089
	accuracy_policy_2: 0.97406
	loss_value_2: 0.01667
	loss_reward_2: 0.00303
	loss_policy_3: 0.00105
	accuracy_policy_3: 0.97688
	loss_value_3: 0.01649
	loss_reward_3: 0.00258
	loss_policy_4: 0.0009
	accuracy_policy_4: 0.98402
	loss_value_4: 0.01629
	loss_reward_4: 0.00267
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98789
	loss_value_5: 0.01676
	loss_reward_5: 0.00395
	loss_policy: 0.00817
	loss_value: 0.1668
	loss_reward: 0.01393
[2025-05-08 13:08:36] nn step 56600, lr: 0.1.
	loss_policy_0: 0.00344
	accuracy_policy_0: 0.96844
	loss_value_0: 0.0832
	loss_policy_1: 0.00086
	accuracy_policy_1: 0.96973
	loss_value_1: 0.01667
	loss_reward_1: 0.00173
	loss_policy_2: 0.0009
	accuracy_policy_2: 0.97352
	loss_value_2: 0.01648
	loss_reward_2: 0.00304
	loss_policy_3: 0.00093
	accuracy_policy_3: 0.98004
	loss_value_3: 0.01635
	loss_reward_3: 0.00261
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.98457
	loss_value_4: 0.01619
	loss_reward_4: 0.00269
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.98566
	loss_value_5: 0.01654
	loss_reward_5: 0.0039
	loss_policy: 0.00795
	loss_value: 0.16544
	loss_reward: 0.01399
Optimization_Done 56600
[2025-05-08 13:09:56] [command] train weight_iter_56600.pkl 265 284
[2025-05-08 13:10:04] nn step 56650, lr: 0.1.
	loss_policy_0: 0.0035
	accuracy_policy_0: 0.9632
	loss_value_0: 0.09495
	loss_policy_1: 0.00099
	accuracy_policy_1: 0.96578
	loss_value_1: 0.01902
	loss_reward_1: 0.00197
	loss_policy_2: 0.00103
	accuracy_policy_2: 0.9709
	loss_value_2: 0.019
	loss_reward_2: 0.00317
	loss_policy_3: 0.00094
	accuracy_policy_3: 0.97543
	loss_value_3: 0.01885
	loss_reward_3: 0.00291
	loss_policy_4: 0.00091
	accuracy_policy_4: 0.97957
	loss_value_4: 0.01867
	loss_reward_4: 0.00326
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.98543
	loss_value_5: 0.01897
	loss_reward_5: 0.00403
	loss_policy: 0.00826
	loss_value: 0.18948
	loss_reward: 0.01534
[2025-05-08 13:10:12] nn step 56700, lr: 0.1.
	loss_policy_0: 0.00348
	accuracy_policy_0: 0.96543
	loss_value_0: 0.09149
	loss_policy_1: 0.00099
	accuracy_policy_1: 0.96594
	loss_value_1: 0.0182
	loss_reward_1: 0.00157
	loss_policy_2: 0.00111
	accuracy_policy_2: 0.97203
	loss_value_2: 0.01804
	loss_reward_2: 0.00285
	loss_policy_3: 0.00112
	accuracy_policy_3: 0.97492
	loss_value_3: 0.01777
	loss_reward_3: 0.00244
	loss_policy_4: 0.001
	accuracy_policy_4: 0.98168
	loss_value_4: 0.01746
	loss_reward_4: 0.00261
	loss_policy_5: 0.00116
	accuracy_policy_5: 0.98551
	loss_value_5: 0.01779
	loss_reward_5: 0.00367
	loss_policy: 0.00886
	loss_value: 0.18076
	loss_reward: 0.01314
[2025-05-08 13:10:21] nn step 56750, lr: 0.1.
	loss_policy_0: 0.00287
	accuracy_policy_0: 0.96781
	loss_value_0: 0.08012
	loss_policy_1: 0.00088
	accuracy_policy_1: 0.96262
	loss_value_1: 0.01609
	loss_reward_1: 0.00135
	loss_policy_2: 0.00094
	accuracy_policy_2: 0.96566
	loss_value_2: 0.01584
	loss_reward_2: 0.00248
	loss_policy_3: 0.00094
	accuracy_policy_3: 0.97059
	loss_value_3: 0.01555
	loss_reward_3: 0.00216
	loss_policy_4: 0.00098
	accuracy_policy_4: 0.97801
	loss_value_4: 0.01538
	loss_reward_4: 0.00218
	loss_policy_5: 0.00105
	accuracy_policy_5: 0.9852
	loss_value_5: 0.01552
	loss_reward_5: 0.00316
	loss_policy: 0.00766
	loss_value: 0.1585
	loss_reward: 0.01133
[2025-05-08 13:10:28] nn step 56800, lr: 0.1.
	loss_policy_0: 0.00392
	accuracy_policy_0: 0.96371
	loss_value_0: 0.09437
	loss_policy_1: 0.00108
	accuracy_policy_1: 0.96242
	loss_value_1: 0.0188
	loss_reward_1: 0.00156
	loss_policy_2: 0.00121
	accuracy_policy_2: 0.96363
	loss_value_2: 0.01849
	loss_reward_2: 0.00287
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.96941
	loss_value_3: 0.01827
	loss_reward_3: 0.00251
	loss_policy_4: 0.00112
	accuracy_policy_4: 0.98051
	loss_value_4: 0.01796
	loss_reward_4: 0.00263
	loss_policy_5: 0.00109
	accuracy_policy_5: 0.98629
	loss_value_5: 0.01813
	loss_reward_5: 0.00379
	loss_policy: 0.00961
	loss_value: 0.18602
	loss_reward: 0.01337
Optimization_Done 56800
[2025-05-08 13:11:46] [command] train weight_iter_56800.pkl 266 285
[2025-05-08 13:11:54] nn step 56850, lr: 0.1.
	loss_policy_0: 0.00355
	accuracy_policy_0: 0.96387
	loss_value_0: 0.08463
	loss_policy_1: 0.00097
	accuracy_policy_1: 0.95988
	loss_value_1: 0.01693
	loss_reward_1: 0.00126
	loss_policy_2: 0.00101
	accuracy_policy_2: 0.96578
	loss_value_2: 0.01672
	loss_reward_2: 0.00237
	loss_policy_3: 0.0009
	accuracy_policy_3: 0.97043
	loss_value_3: 0.01642
	loss_reward_3: 0.00206
	loss_policy_4: 0.00087
	accuracy_policy_4: 0.97957
	loss_value_4: 0.01609
	loss_reward_4: 0.00214
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.98777
	loss_value_5: 0.01613
	loss_reward_5: 0.00303
	loss_policy: 0.00828
	loss_value: 0.16691
	loss_reward: 0.01086
[2025-05-08 13:12:03] nn step 56900, lr: 0.1.
	loss_policy_0: 0.004
	accuracy_policy_0: 0.96051
	loss_value_0: 0.08262
	loss_policy_1: 0.00106
	accuracy_policy_1: 0.9618
	loss_value_1: 0.01649
	loss_reward_1: 0.0013
	loss_policy_2: 0.00103
	accuracy_policy_2: 0.96562
	loss_value_2: 0.01621
	loss_reward_2: 0.00231
	loss_policy_3: 0.00097
	accuracy_policy_3: 0.97223
	loss_value_3: 0.01594
	loss_reward_3: 0.00207
	loss_policy_4: 0.00113
	accuracy_policy_4: 0.97828
	loss_value_4: 0.01572
	loss_reward_4: 0.00222
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98508
	loss_value_5: 0.01586
	loss_reward_5: 0.00316
	loss_policy: 0.00921
	loss_value: 0.16283
	loss_reward: 0.01106
[2025-05-08 13:12:11] nn step 56950, lr: 0.1.
	loss_policy_0: 0.0036
	accuracy_policy_0: 0.96086
	loss_value_0: 0.07883
	loss_policy_1: 0.00092
	accuracy_policy_1: 0.9582
	loss_value_1: 0.01578
	loss_reward_1: 0.00125
	loss_policy_2: 0.00087
	accuracy_policy_2: 0.9648
	loss_value_2: 0.01555
	loss_reward_2: 0.00219
	loss_policy_3: 0.00096
	accuracy_policy_3: 0.97105
	loss_value_3: 0.01531
	loss_reward_3: 0.00188
	loss_policy_4: 0.00095
	accuracy_policy_4: 0.97703
	loss_value_4: 0.01508
	loss_reward_4: 0.00203
	loss_policy_5: 0.00084
	accuracy_policy_5: 0.98609
	loss_value_5: 0.01519
	loss_reward_5: 0.00295
	loss_policy: 0.00815
	loss_value: 0.15576
	loss_reward: 0.0103
[2025-05-08 13:12:20] nn step 57000, lr: 0.1.
	loss_policy_0: 0.00339
	accuracy_policy_0: 0.96387
	loss_value_0: 0.0797
	loss_policy_1: 0.00101
	accuracy_policy_1: 0.95684
	loss_value_1: 0.01592
	loss_reward_1: 0.00129
	loss_policy_2: 0.00101
	accuracy_policy_2: 0.95992
	loss_value_2: 0.0157
	loss_reward_2: 0.00226
	loss_policy_3: 0.00097
	accuracy_policy_3: 0.9675
	loss_value_3: 0.01543
	loss_reward_3: 0.00201
	loss_policy_4: 0.00084
	accuracy_policy_4: 0.97922
	loss_value_4: 0.01505
	loss_reward_4: 0.00218
	loss_policy_5: 0.00091
	accuracy_policy_5: 0.98516
	loss_value_5: 0.01514
	loss_reward_5: 0.003
	loss_policy: 0.00813
	loss_value: 0.15693
	loss_reward: 0.01074
Optimization_Done 57000
[2025-05-08 13:13:39] [command] train weight_iter_57000.pkl 267 286
[2025-05-08 13:13:47] nn step 57050, lr: 0.1.
	loss_policy_0: 0.00387
	accuracy_policy_0: 0.96797
	loss_value_0: 0.08858
	loss_policy_1: 0.00107
	accuracy_policy_1: 0.96051
	loss_value_1: 0.01776
	loss_reward_1: 0.00151
	loss_policy_2: 0.00104
	accuracy_policy_2: 0.9657
	loss_value_2: 0.01761
	loss_reward_2: 0.00246
	loss_policy_3: 0.00111
	accuracy_policy_3: 0.96961
	loss_value_3: 0.01718
	loss_reward_3: 0.00212
	loss_policy_4: 0.00106
	accuracy_policy_4: 0.97348
	loss_value_4: 0.01703
	loss_reward_4: 0.00225
	loss_policy_5: 0.00112
	accuracy_policy_5: 0.98324
	loss_value_5: 0.01704
	loss_reward_5: 0.00323
	loss_policy: 0.00927
	loss_value: 0.1752
	loss_reward: 0.01157
[2025-05-08 13:13:56] nn step 57100, lr: 0.1.
	loss_policy_0: 0.00364
	accuracy_policy_0: 0.96656
	loss_value_0: 0.08155
	loss_policy_1: 0.00097
	accuracy_policy_1: 0.95625
	loss_value_1: 0.01645
	loss_reward_1: 0.00136
	loss_policy_2: 0.00096
	accuracy_policy_2: 0.96535
	loss_value_2: 0.01623
	loss_reward_2: 0.0023
	loss_policy_3: 0.00095
	accuracy_policy_3: 0.9682
	loss_value_3: 0.01592
	loss_reward_3: 0.002
	loss_policy_4: 0.00088
	accuracy_policy_4: 0.97348
	loss_value_4: 0.01564
	loss_reward_4: 0.00208
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.98523
	loss_value_5: 0.01568
	loss_reward_5: 0.00299
	loss_policy: 0.00826
	loss_value: 0.16147
	loss_reward: 0.01073
[2025-05-08 13:14:04] nn step 57150, lr: 0.1.
	loss_policy_0: 0.0039
	accuracy_policy_0: 0.96574
	loss_value_0: 0.09061
	loss_policy_1: 0.00105
	accuracy_policy_1: 0.95871
	loss_value_1: 0.01816
	loss_reward_1: 0.00147
	loss_policy_2: 0.00121
	accuracy_policy_2: 0.9643
	loss_value_2: 0.01797
	loss_reward_2: 0.0024
	loss_policy_3: 0.00115
	accuracy_policy_3: 0.96637
	loss_value_3: 0.01775
	loss_reward_3: 0.00203
	loss_policy_4: 0.00103
	accuracy_policy_4: 0.97312
	loss_value_4: 0.0174
	loss_reward_4: 0.00228
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.985
	loss_value_5: 0.01736
	loss_reward_5: 0.0033
	loss_policy: 0.00932
	loss_value: 0.17926
	loss_reward: 0.01147
[2025-05-08 13:14:13] nn step 57200, lr: 0.1.
	loss_policy_0: 0.00406
	accuracy_policy_0: 0.95977
	loss_value_0: 0.08862
	loss_policy_1: 0.001
	accuracy_policy_1: 0.96031
	loss_value_1: 0.01786
	loss_reward_1: 0.00139
	loss_policy_2: 0.00118
	accuracy_policy_2: 0.96363
	loss_value_2: 0.01745
	loss_reward_2: 0.00247
	loss_policy_3: 0.00106
	accuracy_policy_3: 0.96645
	loss_value_3: 0.01711
	loss_reward_3: 0.00212
	loss_policy_4: 0.00102
	accuracy_policy_4: 0.97113
	loss_value_4: 0.01685
	loss_reward_4: 0.00226
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.98492
	loss_value_5: 0.01684
	loss_reward_5: 0.00327
	loss_policy: 0.00924
	loss_value: 0.17473
	loss_reward: 0.01151
Optimization_Done 57200
[2025-05-08 13:15:26] [command] train weight_iter_57200.pkl 268 287
[2025-05-08 13:15:36] nn step 57250, lr: 0.1.
	loss_policy_0: 0.0045
	accuracy_policy_0: 0.96254
	loss_value_0: 0.0928
	loss_policy_1: 0.00115
	accuracy_policy_1: 0.96156
	loss_value_1: 0.01866
	loss_reward_1: 0.00163
	loss_policy_2: 0.00108
	accuracy_policy_2: 0.96707
	loss_value_2: 0.01852
	loss_reward_2: 0.00267
	loss_policy_3: 0.00124
	accuracy_policy_3: 0.96867
	loss_value_3: 0.01818
	loss_reward_3: 0.0023
	loss_policy_4: 0.00113
	accuracy_policy_4: 0.97332
	loss_value_4: 0.01786
	loss_reward_4: 0.00238
	loss_policy_5: 0.0011
	accuracy_policy_5: 0.98383
	loss_value_5: 0.01789
	loss_reward_5: 0.00341
	loss_policy: 0.0102
	loss_value: 0.18391
	loss_reward: 0.01239
[2025-05-08 13:15:43] nn step 57300, lr: 0.1.
	loss_policy_0: 0.0041
	accuracy_policy_0: 0.95906
	loss_value_0: 0.08809
	loss_policy_1: 0.0011
	accuracy_policy_1: 0.95926
	loss_value_1: 0.01765
	loss_reward_1: 0.00138
	loss_policy_2: 0.00116
	accuracy_policy_2: 0.96348
	loss_value_2: 0.01737
	loss_reward_2: 0.00216
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.96512
	loss_value_3: 0.01715
	loss_reward_3: 0.0019
	loss_policy_4: 0.00116
	accuracy_policy_4: 0.97141
	loss_value_4: 0.01696
	loss_reward_4: 0.00214
	loss_policy_5: 0.00102
	accuracy_policy_5: 0.98543
	loss_value_5: 0.01697
	loss_reward_5: 0.00292
	loss_policy: 0.00975
	loss_value: 0.1742
	loss_reward: 0.0105
[2025-05-08 13:15:51] nn step 57350, lr: 0.1.
	loss_policy_0: 0.00418
	accuracy_policy_0: 0.95977
	loss_value_0: 0.08862
	loss_policy_1: 0.00115
	accuracy_policy_1: 0.95914
	loss_value_1: 0.0178
	loss_reward_1: 0.00133
	loss_policy_2: 0.0011
	accuracy_policy_2: 0.96062
	loss_value_2: 0.01752
	loss_reward_2: 0.00226
	loss_policy_3: 0.00112
	accuracy_policy_3: 0.96219
	loss_value_3: 0.01734
	loss_reward_3: 0.00194
	loss_policy_4: 0.00107
	accuracy_policy_4: 0.97285
	loss_value_4: 0.01701
	loss_reward_4: 0.0021
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.98574
	loss_value_5: 0.01709
	loss_reward_5: 0.00305
	loss_policy: 0.00951
	loss_value: 0.17538
	loss_reward: 0.01068
[2025-05-08 13:16:00] nn step 57400, lr: 0.1.
	loss_policy_0: 0.00465
	accuracy_policy_0: 0.95832
	loss_value_0: 0.09234
	loss_policy_1: 0.00108
	accuracy_policy_1: 0.95863
	loss_value_1: 0.01842
	loss_reward_1: 0.0015
	loss_policy_2: 0.00124
	accuracy_policy_2: 0.96121
	loss_value_2: 0.01822
	loss_reward_2: 0.00247
	loss_policy_3: 0.00128
	accuracy_policy_3: 0.96039
	loss_value_3: 0.01787
	loss_reward_3: 0.00191
	loss_policy_4: 0.00111
	accuracy_policy_4: 0.97176
	loss_value_4: 0.01763
	loss_reward_4: 0.00215
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98641
	loss_value_5: 0.01759
	loss_reward_5: 0.00327
	loss_policy: 0.01037
	loss_value: 0.18207
	loss_reward: 0.0113
Optimization_Done 57400
[2025-05-08 13:17:18] [command] train weight_iter_57400.pkl 269 288
[2025-05-08 13:17:28] nn step 57450, lr: 0.1.
	loss_policy_0: 0.00362
	accuracy_policy_0: 0.97395
	loss_value_0: 0.11932
	loss_policy_1: 0.00099
	accuracy_policy_1: 0.97387
	loss_value_1: 0.02399
	loss_reward_1: 0.00156
	loss_policy_2: 0.001
	accuracy_policy_2: 0.97547
	loss_value_2: 0.02393
	loss_reward_2: 0.00247
	loss_policy_3: 0.00119
	accuracy_policy_3: 0.97309
	loss_value_3: 0.02405
	loss_reward_3: 0.00231
	loss_policy_4: 0.00104
	accuracy_policy_4: 0.97965
	loss_value_4: 0.0241
	loss_reward_4: 0.00257
	loss_policy_5: 0.00111
	accuracy_policy_5: 0.98574
	loss_value_5: 0.02446
	loss_reward_5: 0.00313
	loss_policy: 0.00896
	loss_value: 0.23984
	loss_reward: 0.01206
[2025-05-08 13:17:34] nn step 57500, lr: 0.1.
	loss_policy_0: 0.00349
	accuracy_policy_0: 0.97082
	loss_value_0: 0.10034
	loss_policy_1: 0.00104
	accuracy_policy_1: 0.96875
	loss_value_1: 0.02005
	loss_reward_1: 0.00134
	loss_policy_2: 0.00107
	accuracy_policy_2: 0.97301
	loss_value_2: 0.02
	loss_reward_2: 0.00227
	loss_policy_3: 0.00117
	accuracy_policy_3: 0.96949
	loss_value_3: 0.01991
	loss_reward_3: 0.00202
	loss_policy_4: 0.00103
	accuracy_policy_4: 0.97789
	loss_value_4: 0.01967
	loss_reward_4: 0.00217
	loss_policy_5: 0.00094
	accuracy_policy_5: 0.98594
	loss_value_5: 0.01993
	loss_reward_5: 0.00297
	loss_policy: 0.00874
	loss_value: 0.19991
	loss_reward: 0.01076
[2025-05-08 13:17:43] nn step 57550, lr: 0.1.
	loss_policy_0: 0.00411
	accuracy_policy_0: 0.96645
	loss_value_0: 0.10286
	loss_policy_1: 0.00109
	accuracy_policy_1: 0.96543
	loss_value_1: 0.02054
	loss_reward_1: 0.00144
	loss_policy_2: 0.00113
	accuracy_policy_2: 0.96832
	loss_value_2: 0.02027
	loss_reward_2: 0.00223
	loss_policy_3: 0.00123
	accuracy_policy_3: 0.96848
	loss_value_3: 0.02002
	loss_reward_3: 0.00195
	loss_policy_4: 0.00104
	accuracy_policy_4: 0.97453
	loss_value_4: 0.01979
	loss_reward_4: 0.00222
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.98676
	loss_value_5: 0.01985
	loss_reward_5: 0.00293
	loss_policy: 0.00951
	loss_value: 0.20334
	loss_reward: 0.01077
[2025-05-08 13:17:52] nn step 57600, lr: 0.1.
	loss_policy_0: 0.00389
	accuracy_policy_0: 0.965
	loss_value_0: 0.09155
	loss_policy_1: 0.00105
	accuracy_policy_1: 0.96168
	loss_value_1: 0.01822
	loss_reward_1: 0.00136
	loss_policy_2: 0.00098
	accuracy_policy_2: 0.96812
	loss_value_2: 0.01798
	loss_reward_2: 0.00213
	loss_policy_3: 0.00106
	accuracy_policy_3: 0.96379
	loss_value_3: 0.01772
	loss_reward_3: 0.00184
	loss_policy_4: 0.00098
	accuracy_policy_4: 0.97254
	loss_value_4: 0.01735
	loss_reward_4: 0.00192
	loss_policy_5: 0.00074
	accuracy_policy_5: 0.98711
	loss_value_5: 0.01732
	loss_reward_5: 0.00284
	loss_policy: 0.00871
	loss_value: 0.18013
	loss_reward: 0.01009
Optimization_Done 57600
[2025-05-08 13:19:09] [command] train weight_iter_57600.pkl 270 289
[2025-05-08 13:19:18] nn step 57650, lr: 0.1.
	loss_policy_0: 0.00363
	accuracy_policy_0: 0.96785
	loss_value_0: 0.09636
	loss_policy_1: 0.0011
	accuracy_policy_1: 0.96699
	loss_value_1: 0.01921
	loss_reward_1: 0.00126
	loss_policy_2: 0.00108
	accuracy_policy_2: 0.96922
	loss_value_2: 0.01898
	loss_reward_2: 0.00194
	loss_policy_3: 0.00117
	accuracy_policy_3: 0.96586
	loss_value_3: 0.0187
	loss_reward_3: 0.00178
	loss_policy_4: 0.00111
	accuracy_policy_4: 0.97359
	loss_value_4: 0.0184
	loss_reward_4: 0.00195
	loss_policy_5: 0.00087
	accuracy_policy_5: 0.98762
	loss_value_5: 0.01842
	loss_reward_5: 0.0026
	loss_policy: 0.00895
	loss_value: 0.19008
	loss_reward: 0.00953
[2025-05-08 13:19:25] nn step 57700, lr: 0.1.
	loss_policy_0: 0.004
	accuracy_policy_0: 0.96719
	loss_value_0: 0.09762
	loss_policy_1: 0.0011
	accuracy_policy_1: 0.96422
	loss_value_1: 0.01945
	loss_reward_1: 0.00134
	loss_policy_2: 0.0011
	accuracy_policy_2: 0.96688
	loss_value_2: 0.01922
	loss_reward_2: 0.00216
	loss_policy_3: 0.00118
	accuracy_policy_3: 0.96523
	loss_value_3: 0.01887
	loss_reward_3: 0.00181
	loss_policy_4: 0.00104
	accuracy_policy_4: 0.97164
	loss_value_4: 0.01859
	loss_reward_4: 0.00199
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.9873
	loss_value_5: 0.01851
	loss_reward_5: 0.00289
	loss_policy: 0.00921
	loss_value: 0.19225
	loss_reward: 0.01019
[2025-05-08 13:19:34] nn step 57750, lr: 0.1.
	loss_policy_0: 0.00442
	accuracy_policy_0: 0.96402
	loss_value_0: 0.10188
	loss_policy_1: 0.00113
	accuracy_policy_1: 0.96316
	loss_value_1: 0.02012
	loss_reward_1: 0.00144
	loss_policy_2: 0.00113
	accuracy_policy_2: 0.96586
	loss_value_2: 0.01984
	loss_reward_2: 0.0023
	loss_policy_3: 0.00118
	accuracy_policy_3: 0.96672
	loss_value_3: 0.01945
	loss_reward_3: 0.0021
	loss_policy_4: 0.00107
	accuracy_policy_4: 0.97148
	loss_value_4: 0.01922
	loss_reward_4: 0.00228
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.98762
	loss_value_5: 0.01922
	loss_reward_5: 0.00313
	loss_policy: 0.00989
	loss_value: 0.19973
	loss_reward: 0.01125
[2025-05-08 13:19:42] nn step 57800, lr: 0.1.
	loss_policy_0: 0.00403
	accuracy_policy_0: 0.96434
	loss_value_0: 0.09258
	loss_policy_1: 0.00103
	accuracy_policy_1: 0.95949
	loss_value_1: 0.01845
	loss_reward_1: 0.00132
	loss_policy_2: 0.00095
	accuracy_policy_2: 0.96621
	loss_value_2: 0.01802
	loss_reward_2: 0.00205
	loss_policy_3: 0.00103
	accuracy_policy_3: 0.96449
	loss_value_3: 0.01781
	loss_reward_3: 0.00181
	loss_policy_4: 0.00085
	accuracy_policy_4: 0.97195
	loss_value_4: 0.01743
	loss_reward_4: 0.00202
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.9873
	loss_value_5: 0.01744
	loss_reward_5: 0.0027
	loss_policy: 0.00861
	loss_value: 0.18173
	loss_reward: 0.00989
Optimization_Done 57800
[2025-05-08 13:20:55] [command] train weight_iter_57800.pkl 271 290
[2025-05-08 13:21:05] nn step 57850, lr: 0.1.
	loss_policy_0: 0.00402
	accuracy_policy_0: 0.96875
	loss_value_0: 0.10183
	loss_policy_1: 0.00109
	accuracy_policy_1: 0.9666
	loss_value_1: 0.02015
	loss_reward_1: 0.00134
	loss_policy_2: 0.00114
	accuracy_policy_2: 0.96723
	loss_value_2: 0.01981
	loss_reward_2: 0.00202
	loss_policy_3: 0.00117
	accuracy_policy_3: 0.96582
	loss_value_3: 0.01945
	loss_reward_3: 0.00178
	loss_policy_4: 0.00109
	accuracy_policy_4: 0.97301
	loss_value_4: 0.01911
	loss_reward_4: 0.00194
	loss_policy_5: 0.00094
	accuracy_policy_5: 0.98766
	loss_value_5: 0.01898
	loss_reward_5: 0.00267
	loss_policy: 0.00946
	loss_value: 0.19934
	loss_reward: 0.00975
[2025-05-08 13:21:14] nn step 57900, lr: 0.1.
	loss_policy_0: 0.00445
	accuracy_policy_0: 0.9641
	loss_value_0: 0.09914
	loss_policy_1: 0.00102
	accuracy_policy_1: 0.96383
	loss_value_1: 0.01968
	loss_reward_1: 0.00136
	loss_policy_2: 0.00109
	accuracy_policy_2: 0.96574
	loss_value_2: 0.01934
	loss_reward_2: 0.002
	loss_policy_3: 0.00103
	accuracy_policy_3: 0.96539
	loss_value_3: 0.01897
	loss_reward_3: 0.00165
	loss_policy_4: 0.00105
	accuracy_policy_4: 0.97125
	loss_value_4: 0.01866
	loss_reward_4: 0.002
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.9873
	loss_value_5: 0.01854
	loss_reward_5: 0.00268
	loss_policy: 0.00952
	loss_value: 0.19434
	loss_reward: 0.00969
[2025-05-08 13:21:20] nn step 57950, lr: 0.1.
	loss_policy_0: 0.00442
	accuracy_policy_0: 0.96516
	loss_value_0: 0.10092
	loss_policy_1: 0.00129
	accuracy_policy_1: 0.96035
	loss_value_1: 0.02018
	loss_reward_1: 0.00135
	loss_policy_2: 0.00121
	accuracy_policy_2: 0.96535
	loss_value_2: 0.01986
	loss_reward_2: 0.00222
	loss_policy_3: 0.00126
	accuracy_policy_3: 0.96434
	loss_value_3: 0.01943
	loss_reward_3: 0.00184
	loss_policy_4: 0.00124
	accuracy_policy_4: 0.97027
	loss_value_4: 0.019
	loss_reward_4: 0.00221
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98758
	loss_value_5: 0.01892
	loss_reward_5: 0.0029
	loss_policy: 0.01042
	loss_value: 0.1983
	loss_reward: 0.01052
[2025-05-08 13:21:29] nn step 58000, lr: 0.1.
	loss_policy_0: 0.00477
	accuracy_policy_0: 0.96207
	loss_value_0: 0.10233
	loss_policy_1: 0.00106
	accuracy_policy_1: 0.96465
	loss_value_1: 0.02033
	loss_reward_1: 0.0015
	loss_policy_2: 0.00112
	accuracy_policy_2: 0.96527
	loss_value_2: 0.02
	loss_reward_2: 0.00243
	loss_policy_3: 0.00108
	accuracy_policy_3: 0.9668
	loss_value_3: 0.01962
	loss_reward_3: 0.00195
	loss_policy_4: 0.00098
	accuracy_policy_4: 0.97191
	loss_value_4: 0.01935
	loss_reward_4: 0.00229
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.98723
	loss_value_5: 0.01929
	loss_reward_5: 0.00306
	loss_policy: 0.00986
	loss_value: 0.20092
	loss_reward: 0.01123
Optimization_Done 58000
[2025-05-08 13:22:53] [command] train weight_iter_58000.pkl 272 291
[2025-05-08 13:23:03] nn step 58050, lr: 0.1.
	loss_policy_0: 0.00405
	accuracy_policy_0: 0.9657
	loss_value_0: 0.09242
	loss_policy_1: 0.00109
	accuracy_policy_1: 0.96289
	loss_value_1: 0.01824
	loss_reward_1: 0.00123
	loss_policy_2: 0.00105
	accuracy_policy_2: 0.96898
	loss_value_2: 0.01788
	loss_reward_2: 0.00184
	loss_policy_3: 0.00111
	accuracy_policy_3: 0.9673
	loss_value_3: 0.01762
	loss_reward_3: 0.00158
	loss_policy_4: 0.001
	accuracy_policy_4: 0.97125
	loss_value_4: 0.01733
	loss_reward_4: 0.00181
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.9877
	loss_value_5: 0.01721
	loss_reward_5: 0.00246
	loss_policy: 0.00911
	loss_value: 0.18069
	loss_reward: 0.00891
[2025-05-08 13:23:10] nn step 58100, lr: 0.1.
	loss_policy_0: 0.00407
	accuracy_policy_0: 0.96664
	loss_value_0: 0.09542
	loss_policy_1: 0.00104
	accuracy_policy_1: 0.96441
	loss_value_1: 0.01887
	loss_reward_1: 0.00128
	loss_policy_2: 0.00113
	accuracy_policy_2: 0.96625
	loss_value_2: 0.01856
	loss_reward_2: 0.00199
	loss_policy_3: 0.00107
	accuracy_policy_3: 0.96613
	loss_value_3: 0.01819
	loss_reward_3: 0.00168
	loss_policy_4: 0.00108
	accuracy_policy_4: 0.97188
	loss_value_4: 0.01797
	loss_reward_4: 0.00185
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.98742
	loss_value_5: 0.01794
	loss_reward_5: 0.00261
	loss_policy: 0.00926
	loss_value: 0.18696
	loss_reward: 0.00942
[2025-05-08 13:23:18] nn step 58150, lr: 0.1.
	loss_policy_0: 0.00438
	accuracy_policy_0: 0.9652
	loss_value_0: 0.09784
	loss_policy_1: 0.00109
	accuracy_policy_1: 0.96422
	loss_value_1: 0.01928
	loss_reward_1: 0.0013
	loss_policy_2: 0.00108
	accuracy_policy_2: 0.9659
	loss_value_2: 0.01916
	loss_reward_2: 0.00203
	loss_policy_3: 0.00126
	accuracy_policy_3: 0.96293
	loss_value_3: 0.01876
	loss_reward_3: 0.00163
	loss_policy_4: 0.0011
	accuracy_policy_4: 0.97035
	loss_value_4: 0.01833
	loss_reward_4: 0.00188
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.98812
	loss_value_5: 0.01836
	loss_reward_5: 0.00266
	loss_policy: 0.00979
	loss_value: 0.19174
	loss_reward: 0.0095
[2025-05-08 13:23:27] nn step 58200, lr: 0.1.
	loss_policy_0: 0.00433
	accuracy_policy_0: 0.96555
	loss_value_0: 0.0982
	loss_policy_1: 0.00109
	accuracy_policy_1: 0.96387
	loss_value_1: 0.01951
	loss_reward_1: 0.00138
	loss_policy_2: 0.00115
	accuracy_policy_2: 0.96742
	loss_value_2: 0.0192
	loss_reward_2: 0.00206
	loss_policy_3: 0.00119
	accuracy_policy_3: 0.96316
	loss_value_3: 0.01877
	loss_reward_3: 0.00176
	loss_policy_4: 0.00109
	accuracy_policy_4: 0.97176
	loss_value_4: 0.01824
	loss_reward_4: 0.00205
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.98707
	loss_value_5: 0.01812
	loss_reward_5: 0.0028
	loss_policy: 0.00982
	loss_value: 0.19205
	loss_reward: 0.01004
Optimization_Done 58200
[2025-05-08 13:24:43] [command] train weight_iter_58200.pkl 273 292
[2025-05-08 13:24:53] nn step 58250, lr: 0.1.
	loss_policy_0: 0.00477
	accuracy_policy_0: 0.96828
	loss_value_0: 0.10684
	loss_policy_1: 0.00127
	accuracy_policy_1: 0.96445
	loss_value_1: 0.0212
	loss_reward_1: 0.00144
	loss_policy_2: 0.0014
	accuracy_policy_2: 0.96672
	loss_value_2: 0.02087
	loss_reward_2: 0.00232
	loss_policy_3: 0.0013
	accuracy_policy_3: 0.96742
	loss_value_3: 0.0206
	loss_reward_3: 0.00204
	loss_policy_4: 0.00119
	accuracy_policy_4: 0.97156
	loss_value_4: 0.0201
	loss_reward_4: 0.00218
	loss_policy_5: 0.00092
	accuracy_policy_5: 0.9891
	loss_value_5: 0.02009
	loss_reward_5: 0.00304
	loss_policy: 0.01086
	loss_value: 0.2097
	loss_reward: 0.01102
[2025-05-08 13:25:01] nn step 58300, lr: 0.1.
	loss_policy_0: 0.00444
	accuracy_policy_0: 0.96648
	loss_value_0: 0.09795
	loss_policy_1: 0.00115
	accuracy_policy_1: 0.96441
	loss_value_1: 0.01939
	loss_reward_1: 0.00136
	loss_policy_2: 0.00119
	accuracy_policy_2: 0.96785
	loss_value_2: 0.01892
	loss_reward_2: 0.00202
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.96465
	loss_value_3: 0.01858
	loss_reward_3: 0.0018
	loss_policy_4: 0.00115
	accuracy_policy_4: 0.97102
	loss_value_4: 0.01815
	loss_reward_4: 0.00194
	loss_policy_5: 0.00096
	accuracy_policy_5: 0.98727
	loss_value_5: 0.01801
	loss_reward_5: 0.00273
	loss_policy: 0.01009
	loss_value: 0.19101
	loss_reward: 0.00985
[2025-05-08 13:25:08] nn step 58350, lr: 0.1.
	loss_policy_0: 0.00413
	accuracy_policy_0: 0.96605
	loss_value_0: 0.09183
	loss_policy_1: 0.00108
	accuracy_policy_1: 0.96195
	loss_value_1: 0.01814
	loss_reward_1: 0.00133
	loss_policy_2: 0.00111
	accuracy_policy_2: 0.96695
	loss_value_2: 0.01773
	loss_reward_2: 0.00197
	loss_policy_3: 0.00113
	accuracy_policy_3: 0.96426
	loss_value_3: 0.0173
	loss_reward_3: 0.00157
	loss_policy_4: 0.001
	accuracy_policy_4: 0.97
	loss_value_4: 0.01697
	loss_reward_4: 0.00173
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.98719
	loss_value_5: 0.01702
	loss_reward_5: 0.00252
	loss_policy: 0.00918
	loss_value: 0.17899
	loss_reward: 0.00912
[2025-05-08 13:25:17] nn step 58400, lr: 0.1.
	loss_policy_0: 0.00421
	accuracy_policy_0: 0.96488
	loss_value_0: 0.09313
	loss_policy_1: 0.00102
	accuracy_policy_1: 0.96375
	loss_value_1: 0.01839
	loss_reward_1: 0.00139
	loss_policy_2: 0.00101
	accuracy_policy_2: 0.96699
	loss_value_2: 0.01798
	loss_reward_2: 0.00206
	loss_policy_3: 0.00119
	accuracy_policy_3: 0.9625
	loss_value_3: 0.01765
	loss_reward_3: 0.00176
	loss_policy_4: 0.00095
	accuracy_policy_4: 0.97266
	loss_value_4: 0.01722
	loss_reward_4: 0.00197
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.98805
	loss_value_5: 0.01735
	loss_reward_5: 0.00256
	loss_policy: 0.00914
	loss_value: 0.18172
	loss_reward: 0.00973
Optimization_Done 58400
[2025-05-08 13:26:35] [command] train weight_iter_58400.pkl 274 293
[2025-05-08 13:26:45] nn step 58450, lr: 0.1.
	loss_policy_0: 0.00433
	accuracy_policy_0: 0.96539
	loss_value_0: 0.09818
	loss_policy_1: 0.00123
	accuracy_policy_1: 0.96109
	loss_value_1: 0.01957
	loss_reward_1: 0.00155
	loss_policy_2: 0.0011
	accuracy_policy_2: 0.96512
	loss_value_2: 0.01918
	loss_reward_2: 0.00222
	loss_policy_3: 0.00118
	accuracy_policy_3: 0.96297
	loss_value_3: 0.01872
	loss_reward_3: 0.0019
	loss_policy_4: 0.00107
	accuracy_policy_4: 0.97094
	loss_value_4: 0.01839
	loss_reward_4: 0.00221
	loss_policy_5: 0.00086
	accuracy_policy_5: 0.98586
	loss_value_5: 0.01838
	loss_reward_5: 0.00291
	loss_policy: 0.00977
	loss_value: 0.19242
	loss_reward: 0.01079
[2025-05-08 13:26:54] nn step 58500, lr: 0.1.
	loss_policy_0: 0.00479
	accuracy_policy_0: 0.96656
	loss_value_0: 0.10892
	loss_policy_1: 0.0012
	accuracy_policy_1: 0.96355
	loss_value_1: 0.02177
	loss_reward_1: 0.00195
	loss_policy_2: 0.00122
	accuracy_policy_2: 0.96578
	loss_value_2: 0.02149
	loss_reward_2: 0.00284
	loss_policy_3: 0.00138
	accuracy_policy_3: 0.9634
	loss_value_3: 0.02119
	loss_reward_3: 0.00232
	loss_policy_4: 0.00117
	accuracy_policy_4: 0.96973
	loss_value_4: 0.02074
	loss_reward_4: 0.00251
	loss_policy_5: 0.00093
	accuracy_policy_5: 0.98797
	loss_value_5: 0.02084
	loss_reward_5: 0.00336
	loss_policy: 0.0107
	loss_value: 0.21495
	loss_reward: 0.01299
[2025-05-08 13:27:01] nn step 58550, lr: 0.1.
	loss_policy_0: 0.00458
	accuracy_policy_0: 0.9641
	loss_value_0: 0.10007
	loss_policy_1: 0.00115
	accuracy_policy_1: 0.96258
	loss_value_1: 0.01979
	loss_reward_1: 0.00155
	loss_policy_2: 0.00109
	accuracy_policy_2: 0.96605
	loss_value_2: 0.01931
	loss_reward_2: 0.00227
	loss_policy_3: 0.00118
	accuracy_policy_3: 0.9623
	loss_value_3: 0.01908
	loss_reward_3: 0.00197
	loss_policy_4: 0.0011
	accuracy_policy_4: 0.96988
	loss_value_4: 0.01877
	loss_reward_4: 0.00214
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.9907
	loss_value_5: 0.0187
	loss_reward_5: 0.00294
	loss_policy: 0.00982
	loss_value: 0.19572
	loss_reward: 0.01087
[2025-05-08 13:27:09] nn step 58600, lr: 0.1.
	loss_policy_0: 0.0049
	accuracy_policy_0: 0.96402
	loss_value_0: 0.10635
	loss_policy_1: 0.00112
	accuracy_policy_1: 0.96312
	loss_value_1: 0.02096
	loss_reward_1: 0.00166
	loss_policy_2: 0.0013
	accuracy_policy_2: 0.96449
	loss_value_2: 0.02049
	loss_reward_2: 0.00246
	loss_policy_3: 0.00128
	accuracy_policy_3: 0.96355
	loss_value_3: 0.02011
	loss_reward_3: 0.00202
	loss_policy_4: 0.00122
	accuracy_policy_4: 0.96887
	loss_value_4: 0.01974
	loss_reward_4: 0.00221
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.98969
	loss_value_5: 0.01952
	loss_reward_5: 0.00305
	loss_policy: 0.0106
	loss_value: 0.20716
	loss_reward: 0.0114
Optimization_Done 58600
[2025-05-08 13:28:30] [command] train weight_iter_58600.pkl 275 294
[2025-05-08 13:28:39] nn step 58650, lr: 0.1.
	loss_policy_0: 0.00487
	accuracy_policy_0: 0.96535
	loss_value_0: 0.10533
	loss_policy_1: 0.00121
	accuracy_policy_1: 0.9632
	loss_value_1: 0.02099
	loss_reward_1: 0.00169
	loss_policy_2: 0.00119
	accuracy_policy_2: 0.96555
	loss_value_2: 0.02037
	loss_reward_2: 0.0023
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.96387
	loss_value_3: 0.01991
	loss_reward_3: 0.00194
	loss_policy_4: 0.00115
	accuracy_policy_4: 0.9675
	loss_value_4: 0.01955
	loss_reward_4: 0.00217
	loss_policy_5: 0.00078
	accuracy_policy_5: 0.98855
	loss_value_5: 0.01934
	loss_reward_5: 0.00292
	loss_policy: 0.01039
	loss_value: 0.20549
	loss_reward: 0.01103
[2025-05-08 13:28:46] nn step 58700, lr: 0.1.
	loss_policy_0: 0.00466
	accuracy_policy_0: 0.96215
	loss_value_0: 0.09924
	loss_policy_1: 0.00113
	accuracy_policy_1: 0.96305
	loss_value_1: 0.01968
	loss_reward_1: 0.00161
	loss_policy_2: 0.00107
	accuracy_policy_2: 0.96633
	loss_value_2: 0.01935
	loss_reward_2: 0.00219
	loss_policy_3: 0.00119
	accuracy_policy_3: 0.96363
	loss_value_3: 0.01885
	loss_reward_3: 0.00189
	loss_policy_4: 0.00102
	accuracy_policy_4: 0.9691
	loss_value_4: 0.01842
	loss_reward_4: 0.00218
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.98871
	loss_value_5: 0.01819
	loss_reward_5: 0.00283
	loss_policy: 0.00983
	loss_value: 0.19373
	loss_reward: 0.0107
[2025-05-08 13:28:55] nn step 58750, lr: 0.1.
	loss_policy_0: 0.00489
	accuracy_policy_0: 0.96344
	loss_value_0: 0.1062
	loss_policy_1: 0.00117
	accuracy_policy_1: 0.96297
	loss_value_1: 0.02096
	loss_reward_1: 0.00168
	loss_policy_2: 0.00123
	accuracy_policy_2: 0.96621
	loss_value_2: 0.02058
	loss_reward_2: 0.00232
	loss_policy_3: 0.00125
	accuracy_policy_3: 0.96426
	loss_value_3: 0.02007
	loss_reward_3: 0.00198
	loss_policy_4: 0.00135
	accuracy_policy_4: 0.96734
	loss_value_4: 0.01962
	loss_reward_4: 0.00226
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.98828
	loss_value_5: 0.01945
	loss_reward_5: 0.00289
	loss_policy: 0.0109
	loss_value: 0.20689
	loss_reward: 0.01114
[2025-05-08 13:29:03] nn step 58800, lr: 0.1.
	loss_policy_0: 0.00489
	accuracy_policy_0: 0.96348
	loss_value_0: 0.10381
	loss_policy_1: 0.00116
	accuracy_policy_1: 0.96234
	loss_value_1: 0.02052
	loss_reward_1: 0.00163
	loss_policy_2: 0.00118
	accuracy_policy_2: 0.96301
	loss_value_2: 0.01986
	loss_reward_2: 0.00231
	loss_policy_3: 0.0013
	accuracy_policy_3: 0.96379
	loss_value_3: 0.01945
	loss_reward_3: 0.00187
	loss_policy_4: 0.00112
	accuracy_policy_4: 0.96676
	loss_value_4: 0.01894
	loss_reward_4: 0.00217
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.98949
	loss_value_5: 0.01869
	loss_reward_5: 0.00289
	loss_policy: 0.01055
	loss_value: 0.20126
	loss_reward: 0.01086
Optimization_Done 58800
[2025-05-08 13:30:19] [command] train weight_iter_58800.pkl 276 295
[2025-05-08 13:30:29] nn step 58850, lr: 0.1.
	loss_policy_0: 0.00476
	accuracy_policy_0: 0.96539
	loss_value_0: 0.10302
	loss_policy_1: 0.0012
	accuracy_policy_1: 0.9632
	loss_value_1: 0.0205
	loss_reward_1: 0.00156
	loss_policy_2: 0.00125
	accuracy_policy_2: 0.96426
	loss_value_2: 0.02
	loss_reward_2: 0.00219
	loss_policy_3: 0.0013
	accuracy_policy_3: 0.96387
	loss_value_3: 0.01952
	loss_reward_3: 0.00184
	loss_policy_4: 0.00122
	accuracy_policy_4: 0.96691
	loss_value_4: 0.01904
	loss_reward_4: 0.00208
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.9898
	loss_value_5: 0.01875
	loss_reward_5: 0.00275
	loss_policy: 0.01059
	loss_value: 0.20083
	loss_reward: 0.01042
[2025-05-08 13:30:37] nn step 58900, lr: 0.1.
	loss_policy_0: 0.00488
	accuracy_policy_0: 0.96445
	loss_value_0: 0.10674
	loss_policy_1: 0.00125
	accuracy_policy_1: 0.96305
	loss_value_1: 0.02102
	loss_reward_1: 0.00158
	loss_policy_2: 0.00133
	accuracy_policy_2: 0.96344
	loss_value_2: 0.02053
	loss_reward_2: 0.00233
	loss_policy_3: 0.00128
	accuracy_policy_3: 0.96352
	loss_value_3: 0.01985
	loss_reward_3: 0.00198
	loss_policy_4: 0.00131
	accuracy_policy_4: 0.96758
	loss_value_4: 0.01942
	loss_reward_4: 0.00216
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.98977
	loss_value_5: 0.01918
	loss_reward_5: 0.00287
	loss_policy: 0.01091
	loss_value: 0.20674
	loss_reward: 0.01092
[2025-05-08 13:30:44] nn step 58950, lr: 0.1.
	loss_policy_0: 0.00502
	accuracy_policy_0: 0.96512
	loss_value_0: 0.10972
	loss_policy_1: 0.00131
	accuracy_policy_1: 0.96258
	loss_value_1: 0.02179
	loss_reward_1: 0.00168
	loss_policy_2: 0.00138
	accuracy_policy_2: 0.96184
	loss_value_2: 0.02132
	loss_reward_2: 0.00238
	loss_policy_3: 0.00131
	accuracy_policy_3: 0.9632
	loss_value_3: 0.02061
	loss_reward_3: 0.00194
	loss_policy_4: 0.00129
	accuracy_policy_4: 0.96637
	loss_value_4: 0.02009
	loss_reward_4: 0.00219
	loss_policy_5: 0.00076
	accuracy_policy_5: 0.99055
	loss_value_5: 0.01974
	loss_reward_5: 0.00291
	loss_policy: 0.01108
	loss_value: 0.21326
	loss_reward: 0.01111
[2025-05-08 13:30:53] nn step 59000, lr: 0.1.
	loss_policy_0: 0.00531
	accuracy_policy_0: 0.96395
	loss_value_0: 0.11263
	loss_policy_1: 0.00129
	accuracy_policy_1: 0.9607
	loss_value_1: 0.02229
	loss_reward_1: 0.00185
	loss_policy_2: 0.00149
	accuracy_policy_2: 0.96164
	loss_value_2: 0.0217
	loss_reward_2: 0.00251
	loss_policy_3: 0.00137
	accuracy_policy_3: 0.96281
	loss_value_3: 0.02117
	loss_reward_3: 0.00226
	loss_policy_4: 0.00125
	accuracy_policy_4: 0.96602
	loss_value_4: 0.02072
	loss_reward_4: 0.0024
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.99062
	loss_value_5: 0.0204
	loss_reward_5: 0.00313
	loss_policy: 0.01144
	loss_value: 0.21891
	loss_reward: 0.01216
Optimization_Done 59000
[2025-05-08 13:32:12] [command] train weight_iter_59000.pkl 277 296
[2025-05-08 13:32:22] nn step 59050, lr: 0.1.
	loss_policy_0: 0.00534
	accuracy_policy_0: 0.96773
	loss_value_0: 0.12856
	loss_policy_1: 0.00118
	accuracy_policy_1: 0.96887
	loss_value_1: 0.02548
	loss_reward_1: 0.0018
	loss_policy_2: 0.0014
	accuracy_policy_2: 0.96598
	loss_value_2: 0.02505
	loss_reward_2: 0.0025
	loss_policy_3: 0.00143
	accuracy_policy_3: 0.96773
	loss_value_3: 0.02443
	loss_reward_3: 0.00223
	loss_policy_4: 0.00146
	accuracy_policy_4: 0.96906
	loss_value_4: 0.02387
	loss_reward_4: 0.00239
	loss_policy_5: 0.00112
	accuracy_policy_5: 0.98766
	loss_value_5: 0.02372
	loss_reward_5: 0.00321
	loss_policy: 0.01193
	loss_value: 0.2511
	loss_reward: 0.01213
[2025-05-08 13:32:30] nn step 59100, lr: 0.1.
	loss_policy_0: 0.00523
	accuracy_policy_0: 0.96676
	loss_value_0: 0.11938
	loss_policy_1: 0.00132
	accuracy_policy_1: 0.96645
	loss_value_1: 0.02348
	loss_reward_1: 0.00167
	loss_policy_2: 0.00131
	accuracy_policy_2: 0.96602
	loss_value_2: 0.02296
	loss_reward_2: 0.00243
	loss_policy_3: 0.00126
	accuracy_policy_3: 0.9657
	loss_value_3: 0.0224
	loss_reward_3: 0.00222
	loss_policy_4: 0.00125
	accuracy_policy_4: 0.96824
	loss_value_4: 0.02192
	loss_reward_4: 0.00228
	loss_policy_5: 0.00081
	accuracy_policy_5: 0.98941
	loss_value_5: 0.02165
	loss_reward_5: 0.00289
	loss_policy: 0.01117
	loss_value: 0.23177
	loss_reward: 0.0115
[2025-05-08 13:32:37] nn step 59150, lr: 0.1.
	loss_policy_0: 0.00524
	accuracy_policy_0: 0.9666
	loss_value_0: 0.12036
	loss_policy_1: 0.00131
	accuracy_policy_1: 0.96469
	loss_value_1: 0.02376
	loss_reward_1: 0.00166
	loss_policy_2: 0.00133
	accuracy_policy_2: 0.96422
	loss_value_2: 0.02329
	loss_reward_2: 0.00238
	loss_policy_3: 0.00134
	accuracy_policy_3: 0.96613
	loss_value_3: 0.02264
	loss_reward_3: 0.00202
	loss_policy_4: 0.00122
	accuracy_policy_4: 0.96734
	loss_value_4: 0.0221
	loss_reward_4: 0.00214
	loss_policy_5: 0.00083
	accuracy_policy_5: 0.99086
	loss_value_5: 0.02178
	loss_reward_5: 0.00298
	loss_policy: 0.01128
	loss_value: 0.23393
	loss_reward: 0.01118
[2025-05-08 13:32:46] nn step 59200, lr: 0.1.
	loss_policy_0: 0.00466
	accuracy_policy_0: 0.9652
	loss_value_0: 0.10848
	loss_policy_1: 0.00121
	accuracy_policy_1: 0.96379
	loss_value_1: 0.02143
	loss_reward_1: 0.00165
	loss_policy_2: 0.00124
	accuracy_policy_2: 0.9657
	loss_value_2: 0.0209
	loss_reward_2: 0.00216
	loss_policy_3: 0.00132
	accuracy_policy_3: 0.96293
	loss_value_3: 0.02036
	loss_reward_3: 0.00204
	loss_policy_4: 0.00127
	accuracy_policy_4: 0.96723
	loss_value_4: 0.01984
	loss_reward_4: 0.0021
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.98887
	loss_value_5: 0.01973
	loss_reward_5: 0.00272
	loss_policy: 0.01061
	loss_value: 0.21075
	loss_reward: 0.01068
Optimization_Done 59200
[2025-05-08 13:34:02] [command] train weight_iter_59200.pkl 278 297
[2025-05-08 13:34:10] nn step 59250, lr: 0.1.
	loss_policy_0: 0.00487
	accuracy_policy_0: 0.96758
	loss_value_0: 0.11639
	loss_policy_1: 0.00115
	accuracy_policy_1: 0.96684
	loss_value_1: 0.02295
	loss_reward_1: 0.00167
	loss_policy_2: 0.00126
	accuracy_policy_2: 0.96582
	loss_value_2: 0.02251
	loss_reward_2: 0.00242
	loss_policy_3: 0.00124
	accuracy_policy_3: 0.96551
	loss_value_3: 0.02202
	loss_reward_3: 0.00188
	loss_policy_4: 0.00124
	accuracy_policy_4: 0.96883
	loss_value_4: 0.02148
	loss_reward_4: 0.00212
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.98953
	loss_value_5: 0.02117
	loss_reward_5: 0.00302
	loss_policy: 0.01058
	loss_value: 0.22652
	loss_reward: 0.01112
[2025-05-08 13:34:18] nn step 59300, lr: 0.1.
	loss_policy_0: 0.00502
	accuracy_policy_0: 0.96555
	loss_value_0: 0.1112
	loss_policy_1: 0.00119
	accuracy_policy_1: 0.96809
	loss_value_1: 0.022
	loss_reward_1: 0.00158
	loss_policy_2: 0.00129
	accuracy_policy_2: 0.96375
	loss_value_2: 0.02164
	loss_reward_2: 0.00221
	loss_policy_3: 0.00125
	accuracy_policy_3: 0.96586
	loss_value_3: 0.02107
	loss_reward_3: 0.00193
	loss_policy_4: 0.00117
	accuracy_policy_4: 0.96883
	loss_value_4: 0.0206
	loss_reward_4: 0.00204
	loss_policy_5: 0.0009
	accuracy_policy_5: 0.98844
	loss_value_5: 0.02039
	loss_reward_5: 0.00274
	loss_policy: 0.01082
	loss_value: 0.21689
	loss_reward: 0.0105
[2025-05-08 13:34:26] nn step 59350, lr: 0.1.
	loss_policy_0: 0.00546
	accuracy_policy_0: 0.9652
	loss_value_0: 0.12435
	loss_policy_1: 0.0013
	accuracy_policy_1: 0.96578
	loss_value_1: 0.02447
	loss_reward_1: 0.00181
	loss_policy_2: 0.00131
	accuracy_policy_2: 0.96477
	loss_value_2: 0.02388
	loss_reward_2: 0.0024
	loss_policy_3: 0.00147
	accuracy_policy_3: 0.96418
	loss_value_3: 0.02343
	loss_reward_3: 0.00212
	loss_policy_4: 0.00152
	accuracy_policy_4: 0.96695
	loss_value_4: 0.02293
	loss_reward_4: 0.00226
	loss_policy_5: 0.00097
	accuracy_policy_5: 0.98973
	loss_value_5: 0.02267
	loss_reward_5: 0.00307
	loss_policy: 0.01203
	loss_value: 0.24172
	loss_reward: 0.01165
[2025-05-08 13:34:35] nn step 59400, lr: 0.1.
	loss_policy_0: 0.00466
	accuracy_policy_0: 0.96508
	loss_value_0: 0.10851
	loss_policy_1: 0.00117
	accuracy_policy_1: 0.96473
	loss_value_1: 0.02139
	loss_reward_1: 0.0015
	loss_policy_2: 0.00126
	accuracy_policy_2: 0.9657
	loss_value_2: 0.02098
	loss_reward_2: 0.00224
	loss_policy_3: 0.00131
	accuracy_policy_3: 0.96312
	loss_value_3: 0.02058
	loss_reward_3: 0.00203
	loss_policy_4: 0.0012
	accuracy_policy_4: 0.96605
	loss_value_4: 0.02011
	loss_reward_4: 0.00211
	loss_policy_5: 0.00085
	accuracy_policy_5: 0.98984
	loss_value_5: 0.01981
	loss_reward_5: 0.00264
	loss_policy: 0.01044
	loss_value: 0.21138
	loss_reward: 0.01052
Optimization_Done 59400
[2025-05-08 13:35:54] [command] train weight_iter_59400.pkl 279 298
[2025-05-08 13:36:04] nn step 59450, lr: 0.1.
	loss_policy_0: 0.00404
	accuracy_policy_0: 0.97312
	loss_value_0: 0.11665
	loss_policy_1: 0.0012
	accuracy_policy_1: 0.96875
	loss_value_1: 0.0231
	loss_reward_1: 0.00152
	loss_policy_2: 0.0012
	accuracy_policy_2: 0.96941
	loss_value_2: 0.02258
	loss_reward_2: 0.0022
	loss_policy_3: 0.00116
	accuracy_policy_3: 0.97246
	loss_value_3: 0.02208
	loss_reward_3: 0.00204
	loss_policy_4: 0.00113
	accuracy_policy_4: 0.97422
	loss_value_4: 0.0215
	loss_reward_4: 0.00199
	loss_policy_5: 0.00073
	accuracy_policy_5: 0.9932
	loss_value_5: 0.02139
	loss_reward_5: 0.00277
	loss_policy: 0.00945
	loss_value: 0.22732
	loss_reward: 0.01052
[2025-05-08 13:36:12] nn step 59500, lr: 0.1.
	loss_policy_0: 0.00438
	accuracy_policy_0: 0.96992
	loss_value_0: 0.11394
	loss_policy_1: 0.001
	accuracy_policy_1: 0.96883
	loss_value_1: 0.02233
	loss_reward_1: 0.00141
	loss_policy_2: 0.00109
	accuracy_policy_2: 0.96875
	loss_value_2: 0.02192
	loss_reward_2: 0.00216
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.96992
	loss_value_3: 0.0215
	loss_reward_3: 0.00178
	loss_policy_4: 0.00122
	accuracy_policy_4: 0.97246
	loss_value_4: 0.02104
	loss_reward_4: 0.00192
	loss_policy_5: 0.00077
	accuracy_policy_5: 0.99371
	loss_value_5: 0.02086
	loss_reward_5: 0.00267
	loss_policy: 0.00966
	loss_value: 0.2216
	loss_reward: 0.00994
[2025-05-08 13:36:19] nn step 59550, lr: 0.1.
	loss_policy_0: 0.00412
	accuracy_policy_0: 0.97016
	loss_value_0: 0.11492
	loss_policy_1: 0.00107
	accuracy_policy_1: 0.97
	loss_value_1: 0.02268
	loss_reward_1: 0.00147
	loss_policy_2: 0.00112
	accuracy_policy_2: 0.96977
	loss_value_2: 0.02244
	loss_reward_2: 0.00224
	loss_policy_3: 0.0012
	accuracy_policy_3: 0.97031
	loss_value_3: 0.02201
	loss_reward_3: 0.0018
	loss_policy_4: 0.00122
	accuracy_policy_4: 0.97152
	loss_value_4: 0.02162
	loss_reward_4: 0.00202
	loss_policy_5: 0.00075
	accuracy_policy_5: 0.99438
	loss_value_5: 0.02139
	loss_reward_5: 0.0028
	loss_policy: 0.00949
	loss_value: 0.22506
	loss_reward: 0.01033
[2025-05-08 13:36:28] nn step 59600, lr: 0.1.
	loss_policy_0: 0.00523
	accuracy_policy_0: 0.96867
	loss_value_0: 0.12354
	loss_policy_1: 0.00116
	accuracy_policy_1: 0.9684
	loss_value_1: 0.02439
	loss_reward_1: 0.00161
	loss_policy_2: 0.00112
	accuracy_policy_2: 0.97008
	loss_value_2: 0.02403
	loss_reward_2: 0.00235
	loss_policy_3: 0.0013
	accuracy_policy_3: 0.97074
	loss_value_3: 0.02361
	loss_reward_3: 0.00203
	loss_policy_4: 0.00137
	accuracy_policy_4: 0.96984
	loss_value_4: 0.02322
	loss_reward_4: 0.00212
	loss_policy_5: 0.00098
	accuracy_policy_5: 0.99367
	loss_value_5: 0.02285
	loss_reward_5: 0.0029
	loss_policy: 0.01117
	loss_value: 0.24164
	loss_reward: 0.01101
Optimization_Done 59600
[2025-05-08 13:37:47] [command] train weight_iter_59600.pkl 280 299
[2025-05-08 13:37:56] nn step 59650, lr: 0.1.
	loss_policy_0: 0.005
	accuracy_policy_0: 0.96867
	loss_value_0: 0.13149
	loss_policy_1: 0.00124
	accuracy_policy_1: 0.96719
	loss_value_1: 0.02593
	loss_reward_1: 0.00191
	loss_policy_2: 0.00125
	accuracy_policy_2: 0.96777
	loss_value_2: 0.02571
	loss_reward_2: 0.00274
	loss_policy_3: 0.00125
	accuracy_policy_3: 0.96953
	loss_value_3: 0.02549
	loss_reward_3: 0.00218
	loss_policy_4: 0.00141
	accuracy_policy_4: 0.97059
	loss_value_4: 0.02502
	loss_reward_4: 0.00252
	loss_policy_5: 0.00089
	accuracy_policy_5: 0.99277
	loss_value_5: 0.02474
	loss_reward_5: 0.0034
	loss_policy: 0.01104
	loss_value: 0.25839
	loss_reward: 0.01275
[2025-05-08 13:38:05] nn step 59700, lr: 0.1.
	loss_policy_0: 0.00416
	accuracy_policy_0: 0.96891
	loss_value_0: 0.11484
	loss_policy_1: 0.00106
	accuracy_policy_1: 0.96711
	loss_value_1: 0.02268
	loss_reward_1: 0.00157
	loss_policy_2: 0.00105
	accuracy_policy_2: 0.9668
	loss_value_2: 0.02228
	loss_reward_2: 0.00221
	loss_policy_3: 0.00108
	accuracy_policy_3: 0.9682
	loss_value_3: 0.02201
	loss_reward_3: 0.00199
	loss_policy_4: 0.0011
	accuracy_policy_4: 0.96945
	loss_value_4: 0.02147
	loss_reward_4: 0.00205
	loss_policy_5: 0.00072
	accuracy_policy_5: 0.9932
	loss_value_5: 0.02116
	loss_reward_5: 0.00269
	loss_policy: 0.00918
	loss_value: 0.22444
	loss_reward: 0.01052
[2025-05-08 13:38:12] nn step 59750, lr: 0.1.
	loss_policy_0: 0.00421
	accuracy_policy_0: 0.96711
	loss_value_0: 0.10892
	loss_policy_1: 0.00097
	accuracy_policy_1: 0.96777
	loss_value_1: 0.02139
	loss_reward_1: 0.0015
	loss_policy_2: 0.00097
	accuracy_policy_2: 0.96957
	loss_value_2: 0.02105
	loss_reward_2: 0.00207
	loss_policy_3: 0.00105
	accuracy_policy_3: 0.96773
	loss_value_3: 0.02071
	loss_reward_3: 0.0018
	loss_policy_4: 0.00097
	accuracy_policy_4: 0.97062
	loss_value_4: 0.0204
	loss_reward_4: 0.00196
	loss_policy_5: 0.00064
	accuracy_policy_5: 0.99398
	loss_value_5: 0.02008
	loss_reward_5: 0.00266
	loss_policy: 0.00881
	loss_value: 0.21255
	loss_reward: 0.01
[2025-05-08 13:38:20] nn step 59800, lr: 0.1.
	loss_policy_0: 0.0045
	accuracy_policy_0: 0.96727
	loss_value_0: 0.11862
	loss_policy_1: 0.00105
	accuracy_policy_1: 0.96906
	loss_value_1: 0.02344
	loss_reward_1: 0.00157
	loss_policy_2: 0.00118
	accuracy_policy_2: 0.96891
	loss_value_2: 0.0232
	loss_reward_2: 0.00221
	loss_policy_3: 0.00124
	accuracy_policy_3: 0.96797
	loss_value_3: 0.02273
	loss_reward_3: 0.00184
	loss_policy_4: 0.0013
	accuracy_policy_4: 0.96848
	loss_value_4: 0.02227
	loss_reward_4: 0.00203
	loss_policy_5: 0.00082
	accuracy_policy_5: 0.99395
	loss_value_5: 0.02176
	loss_reward_5: 0.00284
	loss_policy: 0.01009
	loss_value: 0.23202
	loss_reward: 0.01049
Optimization_Done 59800
[2025-05-08 13:39:34] [command] train weight_iter_59800.pkl 281 300
[2025-05-08 13:39:42] nn step 59850, lr: 0.1.
	loss_policy_0: 0.00436
	accuracy_policy_0: 0.96977
	loss_value_0: 0.12976
	loss_policy_1: 0.00129
	accuracy_policy_1: 0.96625
	loss_value_1: 0.02565
	loss_reward_1: 0.00171
	loss_policy_2: 0.00128
	accuracy_policy_2: 0.96645
	loss_value_2: 0.02519
	loss_reward_2: 0.00274
	loss_policy_3: 0.00133
	accuracy_policy_3: 0.96805
	loss_value_3: 0.02483
	loss_reward_3: 0.00222
	loss_policy_4: 0.00132
	accuracy_policy_4: 0.96969
	loss_value_4: 0.02431
	loss_reward_4: 0.00219
	loss_policy_5: 0.00091
	accuracy_policy_5: 0.99031
	loss_value_5: 0.024
	loss_reward_5: 0.00319
	loss_policy: 0.01048
	loss_value: 0.25375
	loss_reward: 0.01205
[2025-05-08 13:39:50] nn step 59900, lr: 0.1.
	loss_policy_0: 0.00476
	accuracy_policy_0: 0.96879
	loss_value_0: 0.12969
	loss_policy_1: 0.0011
	accuracy_policy_1: 0.97078
	loss_value_1: 0.02557
	loss_reward_1: 0.00182
	loss_policy_2: 0.00125
	accuracy_policy_2: 0.96852
	loss_value_2: 0.02531
	loss_reward_2: 0.00256
	loss_policy_3: 0.00133
	accuracy_policy_3: 0.96934
	loss_value_3: 0.02485
	loss_reward_3: 0.00215
	loss_policy_4: 0.00141
	accuracy_policy_4: 0.9675
	loss_value_4: 0.0245
	loss_reward_4: 0.00229
	loss_policy_5: 0.00101
	accuracy_policy_5: 0.99199
	loss_value_5: 0.02426
	loss_reward_5: 0.00316
	loss_policy: 0.01086
	loss_value: 0.25417
	loss_reward: 0.01198
[2025-05-08 13:39:59] nn step 59950, lr: 0.1.
	loss_policy_0: 0.00471
	accuracy_policy_0: 0.96871
	loss_value_0: 0.12428
	loss_policy_1: 0.00126
	accuracy_policy_1: 0.9668
	loss_value_1: 0.02463
	loss_reward_1: 0.00175
	loss_policy_2: 0.00121
	accuracy_policy_2: 0.96793
	loss_value_2: 0.02441
	loss_reward_2: 0.00249
	loss_policy_3: 0.00124
	accuracy_policy_3: 0.96926
	loss_value_3: 0.02401
	loss_reward_3: 0.00199
	loss_policy_4: 0.00128
	accuracy_policy_4: 0.9709
	loss_value_4: 0.02348
	loss_reward_4: 0.00221
	loss_policy_5: 0.00088
	accuracy_policy_5: 0.99305
	loss_value_5: 0.02315
	loss_reward_5: 0.00305
	loss_policy: 0.01057
	loss_value: 0.24396
	loss_reward: 0.01149
[2025-05-08 13:40:06] nn step 60000, lr: 0.1.
	loss_policy_0: 0.00454
	accuracy_policy_0: 0.96855
	loss_value_0: 0.12437
	loss_policy_1: 0.00116
	accuracy_policy_1: 0.96816
	loss_value_1: 0.02468
	loss_reward_1: 0.00163
	loss_policy_2: 0.00116
	accuracy_policy_2: 0.96672
	loss_value_2: 0.0245
	loss_reward_2: 0.0024
	loss_policy_3: 0.00131
	accuracy_policy_3: 0.96922
	loss_value_3: 0.0239
	loss_reward_3: 0.00196
	loss_policy_4: 0.00127
	accuracy_policy_4: 0.97035
	loss_value_4: 0.02331
	loss_reward_4: 0.00209
	loss_policy_5: 0.0008
	accuracy_policy_5: 0.99469
	loss_value_5: 0.02305
	loss_reward_5: 0.00297
	loss_policy: 0.01024
	loss_value: 0.24381
	loss_reward: 0.01105
Optimization_Done 60000
